---
title: "Learning to <br>run tests"
subtitle: "Developer Fundamentals — Module 2 of 4<br><br>_Click [{{< fa chevron-right >}}](#learning-objectives) to start_"
lightbox: true
format:
  revealjs:
    controls: true
    controls-tutorial: true
    help: true
    controls-back-arrows: visible
    transition: slide
    theme: [default, ../assets/slides.scss]
    slide-number: true
    chalkboard: false
    preview-links: auto
    view-distance: 2
    logo: /validmind.png
    footer: "{{< var validmind.training >}} | [Home {{< fa person-walking-dashed-line-arrow-right >}}](/training/training.qmd)"
  html:
  # Change this to the file name prepended by a _ to get around the global HTML output settings required by _metadata.yml
    output-file: _learning-to-run-tests.html
    search: false
title-slide-attributes:
  data-background-color: "#083E44"
  data-background-image: "../assets/home-hero.svg"
skip_preview: true
---

# Learning objectives

_"As a **developer** who has registered a model with {{< var vm.product >}}, I want to identify relevant tests to run from {{< var vm.product >}}'s test repository, run and log tests for my model, and insert the test results into my model's documentation."_

::: {.tc}
<br>
This second module is part of a four-part series:
<br><br>
[Developer Fundamentals](/training/developer-fundamentals/developer-fundamentals-register.qmd){.button target="_blank"}
:::


## Module 2 — Contents

::: {.f2}
1. [{{< var vm.product >}} for model development](#validmind-for-model-development)
2. [Explore ValidMind tests](#explore-validmind-tests)
3. [Run tests with the {{< var validmind.developer >}}](#run-validmind-tests)
4. [Log tests to the {{< var validmind.platform >}}](#log-validmind-tests)
5. [Test an existing model](#test-an-existing-model)

:::

First, let's make sure you can log in to {{< var vm.product >}}.

{{< include /training/assets/_revealjs-navigation.qmd >}}

## Can you log in?

To continue, you need to have been [onboarded](developer-fundamentals-register.qmd#register){target="_blank"} onto {{< var validmind.training >}} with the [**{{< fa code >}} Developer**]{.bubble} role and completed the first module of this course:


::: {.tc}
<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->
[Using {{< var vm.product >}} for model development](using-validmind-for-model-development.html){.button target="_blank"}
:::

<br>Log in to check your access:

:::: {.columns}
::: {.column width="50%"}
::: {.tc}
[Log in to JupyterHub](https://jupyterhub.validmind.ai/){.button target="_blank"}
:::

:::
::: {.column width="50%"}
::: {.tc}
[Log in to {{< var vm.product >}}](https://app.prod.validmind.ai){.button target="_blank"}
:::

:::
::::

::: {.tc}
Be sure to return to this page afterwards.
:::

<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# ValidMind for model development {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Jupyter Notebook series

::: {.f3}
When you run these notebooks, they will generate a draft of model documentation and upload it to {{< var vm.product >}}, complete with test supporting test results.

::: {.f5 .nt2 .pl2 .mb4}
<br>
You will need to have already completed **101 Set up the {{< var validmind.developer >}}** during the first module to proceed.
:::

:::
:::

::: {.column width="70%" .bl .pl4 .f3}
### {{< var vm.product >}} for model development

Our series of four introductory notebooks for model developers include sample code and how-to information to get you started with {{< var vm.product >}}:

1. [101 Set up the {{< var validmind.developer >}}](/notebooks/tutorials/model_development/101-set_up_validmind.ipynb){target="_blank"}
2. [102 Start the model development process](/notebooks/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
3. [103 Integrate custom tests](/notebooks/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
4. [104 Finalize testing and documentation](/notebooks/tutorials/model_development/104-finalize_testing_documentation.ipynb){target="_blank"}

::: {.f4 .pl3 .pr3 .embed}
In this second module, we'll run through **102 Start the model development process** together. 
:::

:::
::::

Let's continue our journey with **102 Start the model development process** on the next page. {{< fa hand-point-right >}}

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html" background-interactive="yes" data-preload="yes"}

:::: {.absolute bottom=15 left=0 right=50 .w-100 .f4 .tc .pl4 .pr4 .overlay}
**102 Start the model development process**

::: {.f5}
During this course, we'll run through these notebooks together, and at the end of your learning journey you'll have a fully documented sample model ready for review.

:::

For now, **scroll through this notebook** to explore. When you are done, click [{{< fa chevron-right >}}]() to continue.

::::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Explore ValidMind tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/developer/model-testing/test-descriptions.html" background-interactive="true" data-preload="yes"}

::: footer
:::: {.absolute bottom=0 left=50 right=50 .w-95 .f3 .tc .pl4 .overlay}
**{{< var vm.product >}} test repository**

::: {.f4}
{{< var vm.product >}} provides a wealth out-of-the-box of tests to help you ensure that your model is being built appropriately.

In this module, you'll become familiar with the individual tests available in {{< var vm.product >}}, as well as how to run them and change parameters as necessary.

:::

For now, **scroll through these test descriptions** to explore. When you're done, click [{{< fa chevron-right >}}]() to continue.

::::
:::

## {background-iframe="https://app.prod.validmind.ai/model-inventory" background-interactive="true" data-preload="yes"}

:::: {.absolute bottom=0 left=50 right=50 .w-95 .f4 .tc .pl4 .overlay}
**Welcome back to the model inventory**

::: {.f5}
First, let's connect back up to your model in the {{< var validmind.platform >}}:

1. Select the name of your model you registered for this course to open up the model details page.
2. On the left sidebar that appears for your model, click **Getting Started**.
3. Locate the code snippet and click **Copy snippet to clipboard**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#initialize-the-validmind-library" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Connect to your model**

::: {.f5}
With your code snippet copied to your clipboard:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the following cells in the Setting up section: <br>**Initialize the {{< var validmind.developer >}}** / **Import sample dataset**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::


## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#identify-qualitative-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Identify qualitative tests**

::: {.f5}
Next, we'll use the [`list_tests()` function](/notebooks/EXECUTED/model_development/101-set_up_validmind.ipynb#explore-available-tests){target="_blank"} to pinpoint tests we want to run:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run all the cells under the Setting up section: **Identify qualitative tests**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.
::::


## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#initialize-the-validmind-datasets" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Initialize {{< var vm.product >}} datasets**

::: {.f5}
Then, we'll use the [`init_dataset()` function](/validmind/validmind.qmd#init_dataset){target="_blank"} to connect the sample data with a {{< var vm.product >}} `Dataset` object in preparation for running tests:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the following cell in the Setting up section: **Initialize the {{< var vm.product >}} datasets**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Run ValidMind tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-tabular-data-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run tabular data tests**

::: {.f5}
You run individual tests by calling the [`run_test` function](/validmind/validmind/tests.qmd#run_test){target="_blank"} provided by the `validmind.tests` module:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run all the cells under the Running tests section: **Run tabular data tests**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#utilize-test-output" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Utilize test output**

::: {.f5}
You can utilize the output from a ValidMind test for further use, for example, if you want to remove highly correlated features:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run all the cells under the Running tests section: **Utilize test output**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Log ValidMind tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Document test results

::: {.tc}
[Learn more ...](/validmind/validmind/vm_models.qmd#log){.button target="_blank"}

:::

<br>Try it **live** on the next page. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
Every test result returned by the `run_test()` function has a `.log()` method that can be used to send the test results to the {{< var validmind.platform >}}:

- When using `run_documentation_tests()`, documentation sections will be automatically populated with the results of all tests registered in the documentation template.
- When logging individual test results to the platform, you'll need to manually add those results to the desired section of the model documentation.

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-and-log-multiple-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run & log multiple tests**

::: {.f5}
The [`run_documentation_tests()` function](/validmind/validmind.qmd#run_documentation_tests){target="_blank"} allows you to run multiple tests at once and automatically log the results to your documentation:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the following cell in the Documenting results section: **Run and log multiple tests**.
:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-and-log-an-individual-test" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run & log an individual test**

::: {.f5}
Next, we'll run an individual test and log the result to the {{< var validmind.platform >}}:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the following cell in the Running tests section: **Run and log an individual test**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Work with test results

::: {.tc}
[Learn more ...](/notebooks/EXECUTED/model_development/102-start_development_process.ipynb#add-individual-test-results-to-model-documentation){.button target="_blank"}

:::

<br>Try it **live** on the next page. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f4}
### Add individual test results to model documentation

With the test results logged, let's head to the model we connected to at the beginning of this notebook and insert our test results into the documentation:

1. From the **Inventory** in the {{< var validmind.platform >}}, go to the model you connected to earlier.

2. In the left sidebar that appears for your model, click **Documentation**.

3. Locate the Data Preparation section and click on **2.3 Correlations and Interactions** to expand that section.

4. Hover under the Pearson Correlation Matrix content block until a horizontal dashed line with a **+** button appears, indicating that you can insert a new block.

5. Click **+** and then select **Test-Driven Block**:

    - In the search bar, type in `HighPearsonCorrelation`.
    - Select `HighPearsonCorrelation:balanced_raw_dataset` as the test.

6. Finally, click **Insert 1 Test Result to Document** to add the test result to the documentation.

    Confirm that the individual results for the high correlation test has been correctly inserted into section **2.3 Correlations and Interactions** of the documentation.

:::
::::


## {background-iframe="https://app.prod.validmind.ai/model-inventory/" background-interactive="true" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f5 .tc .pl4 .overlay}
**Insert a test-driven block**

::: {.f6}
`HighPearsonCorrelation:balanced_raw_dataset`

:::

When you're done, click [{{< fa chevron-right >}}]() to continue.

::::


# Test an existing model {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Model testing with {{< var vm.product >}}

<br>Try it **live** on the next pages. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
So far, we’ve focused on the data assessment and pre-processing that usually occurs prior to any models being built. Now, let’s instead assume we have already built a model and we want to incorporate some model results into our documentation:


::: {.panel-tabset}

### 1. Train your model

Using {{< var vm.product >}} tests, we’ll train a simple logistic regression model on our dataset and evaluate its performance by using the `LogisticRegression` class from the `sklearn.linear_model`.

### 2. Initialize the model object

The last step for evaluating the model’s performance is to initialize the {{< var vm.product >}} `Dataset` and `Model` objects in preparation for assigning model predictions to each dataset.

### 3. Assign predictions

Once the model has been registered you can assign model predictions to the training and test datasets. The `assign_predictions()` method from the `Dataset` object can link existing predictions to any number of models.

### 4. Run the model evaluation tests
In this next example, we’ll focus on running the tests within the Model Development section of the model documentation. Only tests associated with this section will be executed, and the corresponding results will be updated in the model documentation.

:::

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#train-simple-logistic-regression-model" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Train your model**

::: {.f5}
Using {{< var vm.product >}} tests, we'll train a simple logistic regression model on our dataset and evaluate its performance:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run all the cells under the Model testing section: **Train simple logistic regression model**.
:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#initialize-model-evaluation-objects" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Initialize a model object**

::: {.f5}
Use the `init_dataset` and [`init_model` functions](/validmind/validmind.qmd#init_model){target="_blank"} to initialize these objects:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the cell under the following Model testing section: **Initialize model evaluation objects**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#assign-predictions" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Assign predictions**

::: {.f5}
Use the [`assign_predictions()` method](/validmind/validmind/vm_models.qmd#assign_predictions){target="_blank"} from the `Dataset` object to link existing predictions to any number of models:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the cell under the following Model testing section: **Assign predictions**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-the-model-evaluation-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run the model evaluation tests**

::: {.f5}
Finally, we'll run only the tests within the Model Development section of the model documentation:

1. Continue with **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the cell under the following Model testing section: **Run the model evaluation tests**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::



# Next steps {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Learning to run tests

:::

::: {.column width="70%" .bl .pl4 .f3}
In this second module, you learned how to:

- [ ] Identify relevant tests to run from {{< var vm.product >}}'s test repository
- [ ] Initialize {{< var vm.product >}} `Dataset` and `Model` objects
- [ ] Run out-of-the-box tests with the {{< var validmind.developer >}}
- [ ] Log test results to the {{< var validmind.platform >}}
- [ ] Insert logged test results into your model's documentation

:::
::::

::: {.tc}
<br>
Continue your model development journey with:
<br><br>
<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->
[Implementing custom tests](implementing-custom-tests.html){.button target="_blank"}
:::