---
title: "Implementing <br>custom tests"
subtitle: "Developer Fundamentals — Module 3 of 4<br><br>_Click [{{< fa chevron-right >}}](#learning-objectives) to start_"
lightbox: true
# REMOVE THE SEARCH FALSE TOGGLE WHEN THE COURSE IS READY TO BE PUBLISHED
search: false
format:
  revealjs:
    controls: true
    controls-tutorial: true
    help: true
    controls-back-arrows: visible
    transition: slide
    theme: [default, ../assets/slides.scss]
    slide-number: true
    chalkboard: false
    preview-links: auto
    view-distance: 2
    logo: /validmind.png
    footer: "{{< var validmind.training >}} | [Home {{< fa person-walking-dashed-line-arrow-right >}}](/training/training.qmd)"
  html:
  # Change this to the file name prepended by a _ to get around the global HTML output settings required by _metadata.yml
    output-file: _using-validmind-for-model-development.html
    search: false
title-slide-attributes:
  data-background-color: "#083E44"
  data-background-image: "../assets/home-hero.svg"
skip_preview: true
---

# Learning objectives

_"As a **developer** who understands how to run and log out-of-the-box tests with {{< var vm.product >}}, I want to update my documentation template to  include manually inserted test results, implement, run, and log custom tests, and include those custom tests in my documentation template."_

::: {.tc}
<br>
This third module is part of a four-part series:
<br><br>
[Developer Fundamentals](/training/developer-fundamentals/developer-fundamentals-register.qmd){.button target="_blank"}
:::


## Module 3 — Contents

:::: {.columns .f3}
::: {.column width="30%" .mt4 .pr4}
### INTRODUCTION

- [{{< var vm.product >}} for model development](#validmind-for-model-development)
:::

::: {.column width="35%" .mt4 .pr4}
### PART 1

- [Implement custom inline tests](#implement-custom-tests)
- [Use external test providers](#use-external-test-providers)

:::

::: {.column width="35%" .mt4}
### PART 2 

- [Include custom test results](#include-custom-test-results)
- [Configure the model's documentation template](#configure-documentation-template)
:::
::::

First, let's make sure you can log in to {{< var vm.product >}}.

{{< include /training/assets/_revealjs-navigation.qmd >}}

## Can you log in?

To continue, you need to have been [onboarded](developer-fundamentals-register.qmd#register) onto {{< var validmind.training >}} with the [**{{< fa code >}} Developer**]{.bubble} role and completed the first two modules of this course:

:::: {.columns}
::: {.column width="60%"}
::: {.tc}
[Using {{< var vm.product >}} for model development](using-validmind-for-model-development.html){.button target="_blank"}
:::

:::
::: {.column width="40%"}
::: {.tc}
[Learning to run tests](learning-to-run-tests.html){.button target="_blank"}
:::

:::
::::


<br>Log in to check your access:

:::: {.columns}
::: {.column width="50%"}
::: {.tc}
[Log in to JupyterHub](https://jupyterhub.validmind.ai/){.button target="_blank"}
:::

:::
::: {.column width="50%"}
::: {.tc}
[Log in to {{< var vm.product >}}](https://app.prod.validmind.ai){.button target="_blank"}
:::

:::
::::

::: {.tc}
Be sure to return to this page afterwards.
:::

<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# ValidMind for model development {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

<!-- USE `.scrollable` IN AN H2  -->
## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Jupyter Notebook series

::: {.f3}
When you run these notebooks, they will generate a draft of model documentation and upload it to {{< var vm.product >}}, complete with test supporting test results.

::: {.f5 .nt2 .pl2 .mb4}
<br>

You will need have already completed **101** and **102** during the first and second modules to proceed.
:::

:::
:::

::: {.column width="70%" .bl .pl4 .f3}
### {{< var vm.product >}} for model development

Our series of four introductory notebooks for model developers include sample code and how-to information to get you started with {{< var vm.product >}}:

1. [101 Set up the {{< var validmind.developer >}}](/notebooks/tutorials/model_development/101-set_up_validmind.ipynb){target="_blank"}
2. [102 Start the model development process](/notebooks/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
3. [103 Integrate custom tests](/notebooks/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
4. [104 Finalize testing and documentation](/notebooks/tutorials/model_development/104-finalize_testing_documentation.ipynb){target="_blank"}

::: {.f4 .pl3 .pr3 .embed}
In this third module, we'll run through the remaining two notebooks **103** in PART 1 and **104** in PART 2 together.
:::

:::
::::

<br>

Let's continue our journey with **PART 1** on the next page. {{< fa hand-point-right >}}

# PART 1 {background-color="#083E44" background-image="/assets/img/solutions-hero.png"}

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html" background-interactive="yes" data-preload="yes"}

:::: {.absolute bottom=15 left=0 right=50 .w-100 .f4 .tc .pl4 .pr4 .overlay}
**103 Integrate custom tests**

::: {.f5}
This is the third notebook in our introductory series, which will walk you through how to implement different types of custom tests with {{< var vm.product >}}.

:::

**Scroll through this notebook** to explore. When you are done, click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="https://app.prod.validmind.ai/model-inventory" background-interactive="true" data-preload="yes"}

:::: {.absolute bottom=0 left=50 right=50 .w-95 .f4 .tc .pl4 .overlay}
**Welcome back to the model inventory**

::: {.f5}
First, let's connect back up to your model in the {{< var validmind.platform >}}:

1. Select the name of your model you registered for this course to open up the model details page.
2. On the left sidebar that appears for your model, click **Getting Started**.
3. Locate the code snippet and click **Copy snippet to clipboard**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Implement custom tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#initialize-the-validmind-library" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Connect to your model**

::: {.f5}
With your code snippet copied to your clipboard:

1. Open **103 Integrate custom testss**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells under the **Setting up** section.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

<!-- USE `.scrollable` IN AN H2  -->
## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Custom inline tests

<!-- YOU MIGHT NEED TO MOVE THIS TO THE BOTTOM OUTSIDE OF THE 2 COLUMNS -->
<br>Try it **live** on the next pages. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
Let's implement a custom *inline test* that calculates the confusion matrix for a binary classification model.

- An inline test refers to a test written and executed within the same environment as the code being tested — in the following example, right in our Jupyter Notebook —  without requiring a separate test file or framework.
- You'll note that the custom test function is just a regular Python function that can include and require any Python library as you see fit.

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#create-a-confusion-matrix-plot" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Create a custom test**

::: {.f5}
The [`@vm.test` wrapper](https://docs.validmind.ai/validmind/validmind.html#test) allow you to create a reusable test:

1. Open **103 Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells in the following section under Implementing a custom inline test: **Create a confusion matrix plot**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#add-parameters-to-custom-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Adjust your custom test**

::: {.f5}
Custom tests can take parameters just like any other function:

1. Open **103 Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells in the following sections under Implementing a custom inline test: <br>**Add parameters to custom tests** / **Pass parameters to custom tests**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#log-the-confusion-matrix-results" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Log your custom test**

::: {.f5}
Use the [`.log()` method](/validmind/validmind/vm_models.html#TestResult.log) to send the results of your custom test to the {{< var validmind.platform >}}:

1. Open **103 Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run the cell in the following section under Implementing a custom inline test: **Log the confusion matrix results**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Use external test providers {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

<!-- USE `.scrollable` IN AN H2  -->
## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Reusable custom tests

<!-- YOU MIGHT NEED TO MOVE THIS TO THE BOTTOM OUTSIDE OF THE 2 COLUMNS -->
<br>Try it **live** on the next pages. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
### External test providers

Creating inline custom tests with a function is a great way to customize your model documentation.

- However, sometimes you may want to reuse the same set of tests across multiple models and share them with others in your organization.
- In this case, you can create an external custom test provider that will allow you to load custom tests from a local folder or a Git repository.

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#create-custom-tests-folder" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Create custom tests folder**

::: {.f5}
Create a new folder that will contain reusable custom tests from your existing inline tests:

1. Open **103 Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run the cell under the following Using external test providers section: **Create custom tests folder**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#save-an-inline-test" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Save inline test**

::: {.f5}
The `@vm.test` decorator also includes a convenience method that allows you to save the test to a Python file at a specified path:

1. Open **103 Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells under the Using external test providers section: **Save an inline test**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/103-integrate_custom_tests.html#register-a-local-test-provider" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Register local test provider**

::: {.f5}
Next, let's initialize a test provider that will tell the {{< var validmind.developer >}} where to find your saved custom tests:

1. Open **103 Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/103-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells under the Using external test providers section: **Register a local test provider**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::



# PART 2 {background-color="#083E44" background-image="/assets/img/solutions-hero.png"}

<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Include custom test results {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

<!-- USE `.scrollable` IN AN H2  -->
## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Document test results

::: {.tc}
<!-- CHANGE THIS TO THE EXTERNAL USER GUIDE LINK -->

[Learn more ...](/validmind/validmind/vm_models.html#TestResult.log){.button target="_blank"}
:::

<!-- YOU MIGHT NEED TO MOVE THIS TO THE BOTTOM OUTSIDE OF THE 2 COLUMNS -->
<br>Try it **live** on the next page. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
Every test result returned by the `run_test()` function has a `.log()` method that can be used to send the test results to the {{< var validmind.platform >}}:

- When using `run_documentation_tests()`, documentation sections will be automatically populated with the results of all tests registered in the documentation template.
- When logging individual test results to the platform, you'll need to manually add those results to the desired section of the model documentation.

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-and-log-multiple-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run & log multiple tests**

::: {.f5}
[`run_documentation_tests()`](/validmind/validmind.html#run_documentation_tests) allows you to run multiple tests at once and automatically log the results to your documentation:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the following cell in the Documenting results section: **Run and log multiple tests**.
:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-and-log-an-individual-test" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run & log an individual test**

::: {.f5}
Next, we'll run an individual test and log the result to the {{< var validmind.platform >}}:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the following cell in the Running tests section: **Run and log an individual test**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

<!-- USE `.scrollable` IN AN H2  -->
## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Work with test results

::: {.tc}
<!-- CHANGE THIS TO THE EXTERNAL USER GUIDE LINK -->

[Learn more ...](/notebooks/EXECUTED/model_development/102-start_development_process.ipynb#add-individual-test-results-to-model-documentation){.button target="_blank"}
:::

<!-- YOU MIGHT NEED TO MOVE THIS TO THE BOTTOM OUTSIDE OF THE 2 COLUMNS -->
<br>Try it **live** on the next page. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f4}
### Add individual test results to model documentation

With the test results logged, let's head to the model we connected to at the beginning of this notebook and insert our test results into the documentation:

1. From the **Inventory** in the {{< var validmind.platform >}}, go to the model you connected to earlier.

2. In the left sidebar that appears for your model, click **Documentation**.

3. Locate the Data Preparation section and click on **2.3 Correlations and Interactions** to expand that section.

4. Hover under the Pearson Correlation Matrix content block until a horizontal dashed line with a **+** button appears, indicating that you can insert a new block.

5. Click **+** and then select **Test-Driven Block**:

    - In the search bar, type in `HighPearsonCorrelation`.
    - Select `HighPearsonCorrelation:balanced_raw_dataset` as the test.

6. Finally, click **Insert 1 Test Result to Document** to add the test result to the documentation.

    Confirm that the individual results for the high correlation test has been correctly inserted into section **2.3 Correlations and Interactions** of the documentation.

:::
::::


## {background-iframe="https://app.prod.validmind.ai/model-inventory/" background-interactive="true" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f5 .tc .pl4 .overlay}
**Insert a test-driven  block**

::: {.f6}
`HighPearsonCorrelation:balanced_raw_dataset`

:::

When you're done, click [{{< fa chevron-right >}}]() to continue.

::::


# Configure documentation template {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

<!-- USE `.scrollable` IN AN H2  -->
## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Model testing with {{< var vm.product >}}

<!-- YOU MIGHT NEED TO MOVE THIS TO THE BOTTOM OUTSIDE OF THE 2 COLUMNS -->
<br>Try it **live** on the next pages. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
So far, we’ve focused on the data assessment and pre-processing that usually occurs prior to any models being built. Now, let’s instead assume we have already built a model and we want to incorporate some model results into our documentation:


::: {.panel-tabset}

### 1. Train your model

Using {{< var vm.product >}} tests, we’ll train a simple logistic regression model on our dataset and evaluate its performance by using the `LogisticRegression` class from the `sklearn.linear_model`.

### 2. Initialize the model object

The last step for evaluating the model’s performance is to initialize the {{< var vm.product >}} `Dataset` and `Model` objects in preparation for assigning model predictions to each dataset.

### 3. Assign predictions

Once the model has been registered you can assign model predictions to the training and test datasets. The `assign_predictions()` method from the `Dataset` object can link existing predictions to any number of models.

### 4. Run the model evaluation tests
In this next example, we’ll focus on running the tests within the Model Development section of the model documentation. Only tests associated with this section will be executed, and the corresponding results will be updated in the model documentation.

:::

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#train-simple-logistic-regression-model" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Train your model**

::: {.f5}
Using {{< var vm.product >}} tests, we'll train a simple logistic regression model on our dataset and evaluate its performance:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run all the cells under the Model testing section: **Train simple logistic regression model**.
:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#initialize-model-evaluation-objects" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Initialize a model object**

::: {.f5}
Use the `init_dataset` and [`init_model` functions](https://docs.validmind.ai/validmind/validmind.html#init_model) to initialize these objects:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the cell under the following Model testing section: **Initialize model evaluation objects**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#assign-predictions" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Assign predictions**

::: {.f5}
Use the [`assign_predictions()` method](https://docs.validmind.ai/validmind/validmind/vm_models.html#VMDataset.assign_predictions) from the `Dataset` object to link existing predictions to any number of models:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the cell under the following Model testing section: **Assign predictions**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/102-start_development_process.html#run-the-model-evaluation-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run the model evaluation tests**

::: {.f5}
Finally, we'll run only the tests within the Model Development section of the model documentation:

1. Open **102 Start the model development process**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/102-start_development_process.ipynb){target="_blank"}
2. Run the cell under the following Model testing section: **Run the model evaluation tests**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::



# Next steps {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Learning to run tests

:::

::: {.column width="70%" .bl .pl4 .f3}
In this second module, you learned how to:

- [ ] Identify relevant tests to run from {{< var vm.product >}}'s test repository
- [ ] Initialize {{< var vm.product >}} `Dataset` and `Model` objects
- [ ] Run out-of-the-box tests with the {{< var validmind.developer >}}
- [ ] Log test results to the {{< var validmind.platform >}}
- [ ] Insert logged test results into your model's documentation

:::
::::

::: {.tc}
<br>
Continue your model development journey with:
<br><br>
<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->
[Implementing custom tests](implementing-custom-tests.html){.button}
:::