---
title: "Running Data <br>Quality Tests"
subtitle: "Validator Fundamentals — Module 2 of 4<br><br>_Click [{{< fa chevron-right >}}](#learning-objectives) to start_"
lightbox: true
format:
  revealjs:
    controls: true
    controls-tutorial: true
    help: true
    controls-back-arrows: visible
    transition: slide
    theme: [default, ../assets/slides.scss]
    slide-number: true
    chalkboard: false
    preview-links: auto
    view-distance: 2
    logo: /validmind.png
    footer: "{{< var validmind.training >}} | [Home {{< fa person-walking-dashed-line-arrow-right >}}](/training/training.qmd)"
    revealjs-plugins:
      - slideover
  html:
  # Change this to the file name prepended by a _ to get around the global HTML output settings required by _metadata.yml
    output-file: _running-data-quality-tests.html
    search: false
title-slide-attributes:
  data-background-color: "#083E44"
  data-background-image: "../assets/home-hero.svg"
skip_preview: true
---

# Learning objectives {.center}

_"As a **validator** who has connected to a champion model via the {{< var validmind.developer >}}, I want to identify relevant tests to run from {{< var vm.product >}}'s test repository, run and log data quality tests, and insert the test results into my model's validation report."_

::: {.tc}
<br>
This second module is part of a four-part series:
<br><br>
[Validator Fundamentals](/training/validator-fundamentals/validator-fundamentals-register.qmd){.button target="_blank"}
:::


## Module 2 — Contents {.center}

::: {.f2}
1. [{{< var vm.product >}} for model validation](#validmind-for-model-validation)
2. [Explore ValidMind tests](#explore-validmind-tests)
3. [Run tests with the {{< var validmind.developer >}}](#run-validmind-tests)
4. [Log tests to the {{< var validmind.platform >}}](#log-validmind-tests)
5. [Prepare datasets for model evaluation testing](#prepare-datasets-for-model-evaluation)

:::

First, let's make sure you can log in to {{< var vm.product >}}.

{{< include /training/assets/_revealjs-navigation.qmd >}}

## Before you begin {.center}

::: {.panel-tabset}

### Prerequisite course

To continue, you need to have been [onboarded](validator-fundamentals-register.qmd#register){target="_blank"} onto {{< var validmind.training >}} with the [**{{< fa circle-check >}} Validator**]{.bubble} role and completed the first module of this course:

::: {.tc}
<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->
[Using {{< var vm.product >}} for Model Validation](using-validmind-for-model-validation.html){.button target="_blank"}
:::

:::: {.tc .mt5 .f2 .embed}
Already logged in and refreshed this module? Click [{{< fa chevron-right >}}]() to continue.

:::

### Log in

1. Log in to check your access:

:::: {.flex .flex-wrap .justify-around}

::: {.w-50-ns .tc}

[Log in to JupyterHub]({{< var url.jupyterhub >}}/){.button target="_blank"}

:::

::: {.w-50-ns .tc}
[Log in to {{< var vm.product >}}](https://app.prod.validmind.ai){.button target="_blank"}
:::

::::


::: {.tc .f3}
Be sure to return to this page afterwards.
:::

2. After you successfully log in, refresh the page to connect this training module up to the {{< var validmind.platform >}}: 

::: {.tc}
<button class="button" onClick="window.location.reload();">Refresh Page</button>

:::


:::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# ValidMind for model validation {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Jupyter Notebook series

::: {.f3}
These notebooks walk you through how to validate a model using {{< var vm.product >}}, complete with supporting test results attached as evidence to your validation report.

::: {.f5 .nt2 .pl2 .mb4}
<br>
You will need to have already completed **1 — Set up the {{< var validmind.developer >}} for validation** during the first module to proceed.
:::

:::
:::

::: {.column width="70%" .bl .pl4 .f3}
### {{< var vm.product >}} for model validation

Our series of four introductory notebooks for model validators include sample code and how-to information to get you started with {{< var vm.product >}}:

1 — [Set up the {{< var validmind.developer >}} for validation](/notebooks/tutorials/model_validation/1-set_up_validmind_for_validation.ipynb){target="_blank"}<br>
2 — [Start the model validation process](/notebooks/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}<br>
3 — [Developing a potential challenger model](/notebooks/tutorials/model_validation/3-developing_challenger_model.ipynb){target="_blank"}<br>
4 — [Finalize testing and reporting](/notebooks/tutorials/model_validation/4-finalize_validation_reporting.ipynb){target="_blank"}<br>
<br>

::: {.f4 .pl3 .pr3 .embed}
In this second module, we'll run through **2 — Start the model validation process** together.
:::

:::

::::

Let's continue our journey with **2 — Start the model validation process** on the next page. {{< fa hand-point-right >}}

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html" background-interactive="yes" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**2 — Start the model validation process**

During this course, we'll run through these notebooks together, and at the end of your learning journey you'll have a fully supported sample validation report ready for review.

For now, **scroll through this notebook** to explore. When you are done, click [{{< fa chevron-right >}}]() to continue.

::::

<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Explore ValidMind tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/developer/model-testing/test-descriptions.html" background-interactive="true" data-preload="yes"}

:::: {.slideover--l .three-quarters .auto-collapse-5}
**{{< var vm.product >}} test repository**

{{< var vm.product >}} provides a wealth out-of-the-box of tests to help you ensure that your model is being built appropriately.

In this module, you'll become familiar with the individual tests available in {{< var vm.product >}}, as well as how to run them and change parameters as necessary.

For now, **scroll through these test descriptions** to explore. When you're done, click [{{< fa chevron-right >}}]() to continue.

::::

## Get your code snippet

:::: {.columns}

::: {.column width="80%"}

<!-- FOR COPYING THE CODE SNIPPET WE NEED TO USE THIS IFRAME WORKAROUND -->

<div style="zoom: 0.6; overflow: hidden;">
  <iframe 
    src="https://app.prod.validmind.ai/model-inventory" 
    width="100%" 
    height="750" 
    style="border: 1px solid #083E44; border-radius: 8px; border-right-width: 2px; border-bottom-width: 3px;"
    data-preload="yes" 
    sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    allow="clipboard-read; clipboard-write">
  </iframe>
</div>

:::

::: {.column width="20%" .f4}

::: {.f5}
{{< var vm.product >}} generates a unique *code snippet* for each registered model to connect with your validation environment:

1. From the **{{< fa cubes >}} Inventory**, select the name of your model to open up the model details page.
2. On the left sidebar that appears for your model, click **Getting Started**.
3. Locate the code snippet and click **Copy snippet to clipboard**.

:::

When you're done, click [{{< fa chevron-right >}}]() to continue.

:::

::::

:::: {.tc .f6 .embed}
**Can't load the {{< var validmind.platform >}}?**

Make sure you're logged in and have refreshed the page in a Chromium-based web browser.

:::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#initialize-the-validmind-library" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Connect to your model**

With your code snippet copied to your clipboard:

1. Open **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run the cell under the **Setting up** section.

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#load-the-sample-dataset" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Load the sample dataset**

After you've successfully initialized the {{< var validmind.developer >}}, let's import the sample dataset that was used to develop the dummy champion model:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run the cell under the **Load the sample dataset** section.

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#identify-qualitative-tests" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Identify qualitative tests**

Next, we'll use the [`list_tests()` function](/notebooks/EXECUTED/model_validation/1-set_up_validmind_for_validation.ipynb#explore-available-tests){target="_blank"} to pinpoint tests we want to run:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run all the cells under the Verifying data quality adjustments section: **Identify qualitative tests**

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.
::::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#initialize-the-validmind-datasets" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Initialize {{< var vm.product >}} datasets**

Then, we'll use the [`init_dataset()` function](/validmind/validmind.qmd#init_dataset){target="_blank"} to connect the sample data with a {{< var vm.product >}} `Dataset` object in preparation for running tests:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run the following cell in the  Verifying data quality adjustments section: **Initialize the {{< var vm.product >}} datasets**

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Run ValidMind tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#run-data-quality-tests" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Run data quality tests**

You run individual tests by calling the [`run_test()` function](/validmind/validmind/tests.qmd#run_test){target="_blank"} provided by the `validmind.tests` module:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run all the cells under the Verifying data quality adjustments section: **Run data quality tests**

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#remove-highly-correlated-features" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Remove highly correlated features**

You can utilize the output from a ValidMind test for further use, for example, if you want to remove highly correlated features:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run all the cells under the Verifying data quality adjustments section: **Remove highly correlated features**

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::


<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# Log ValidMind tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Document test results

::: {.tc}
[Learn more ...](/validmind/validmind/vm_models.qmd#log){.button target="_blank"}

:::

<br>Try it **live** on the next page. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f3}
Every test result returned by the `run_test()` function has a `.log()` method that can be used to send the test results to the {{< var validmind.platform >}}:

- When logging individual test results to the platform, you'll need to manually add those results to the desired section of the validation report.
- To demonstrate how to add test results to your validation report, we'll log our data quality tests and insert the results via the {{< var validmind.platform >}}.

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#configure-and-run-comparison-tests" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Configure and run comparison tests**

You can leverage the {{< var validmind.developer >}} to easily run comparison tests, between both datasets and models. Here, we compare the original raw dataset and the final preprocessed dataset, then log the results to the {{< var validmind.platform >}}:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run all the cells under the Documenting test results section: **Configure and run comparison tests**

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#log-tests-with-unique-identifiers" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Log tests with unique identifiers**

When running individual tests, you can use a custom `result_id` to tag the individual result with a unique identifier:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run the cell under the following Documenting test results section:  **Log tests with unique identifiers**.

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Add test results to reporting

::: {.tc}
[Learn more ...](/notebooks/EXECUTED/model_validation/2-start_validation_process.ipynb#add-test-results-to-reporting){.button target="_blank"}

:::

<br>Try it **live** on the next page. {{< fa hand-point-right >}}
:::

::: {.column width="70%" .bl .pl4 .f4}
### Link validator evidence

With some test results logged, let's head to the model we connected to at the beginning of this notebook and insert our test results into the validation report as evidence.

While the example below focuses on a specific test result, you can follow the same general procedure for your other results:

::: {.panel-tabset}

### 1. Link data quality test results
1. From the **{{< fa cubes >}} Inventory** in the {{< var validmind.platform >}}, go to the model you connected to earlier.

2. In the left sidebar that appears for your model, click **{{< fa shield >}} Validation Report**.

3. Locate the Data Preparation section and click on **2.2.1. Data Quality** to expand that section.

4. Under the Class Imbalance Assessment section, locate Validator Evidence then click **Link Evidence to Report**.

5. Select the Class Imbalance test results we logged: **ValidMind Data Validation Class Imbalance** 

    ![The ClassImbalance tests selected](/notebooks/tutorials/model_validation/selecting-class-imbalance-results.png){fig-alt="Screenshot showing the ClassImbalance tests selected" .screenshot}

6. Click **Update Linked Evidence** to add the test results to the validation report.

    Confirm that the results for the Class Imbalance test you inserted has been correctly inserted into section **2.2.1. Data Quality** of the report.

### 2. Review Class Imbalance test results

- Once linked as evidence to section **2.2.1. Data Quality** note that the ValidMind Data Validation Class Imbalance test results are flagged as **Requires Attention** — as they include comparative results from our initial raw dataset.
- Click **See evidence details** to review the LLM-generated description that summarizes the test results, that confirm that our final preprocessed dataset actually passes our test:

  ![ClassImbalance test generated description in the text editor](/notebooks/tutorials/model_validation/class-imbalance-results-detail.png){fig-alt="Screenshot showing the ClassImbalance test generated description in the text editor" .screenshot}

:::

:::

::::

## {background-iframe="https://app.prod.validmind.ai/model-inventory/" background-interactive="true" data-preload="yes"}

:::: {.slideover--b .auto-collapse-10}
::: {.tc}
**Link validator evidence**
:::

1. Select the name of your model you registered for this course to open up the model details page.
2. In the left sidebar that appears for your model, click **{{< fa shield >}} Validation Report**.
3. Locate the Data Preparation section and click on **2.2.1. Data Quality** to expand that section.
4. Under the Class Imbalance Assessment section, locate Validator Evidence then click **Link Evidence to Report**.
5. Select the Class Imbalance test results we logged: **ValidMind Data Validation Class Imbalance**
6. Click **Update Linked Evidence** to add the test results to the validation report.

When you're done, click [{{< fa chevron-right >}}]() to continue.

::::

# Prepare datasets for model evaluation {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/notebooks/EXECUTED/model_validation/2-start_validation_process.html#split-the-preprocessed-dataset" data-preload="yes"}

:::: {.slideover--r .three-quarters}
**Split the preprocessed dataset**

So far, we've rebalanced our raw dataset and used the results of {{< var vm.product >}} tests to additionally remove highly correlated features from our dataset. Next, let's now spilt our dataset into train and test in preparation for model evaluation testing:

1. Continue with **2 — Start the model validation process**: [{{< fa square-arrow-up-right >}} JupyterHub]({{< var url.jupyterhub >}}/hub/user-redirect/lab/tree/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}
2. Run all the cells under the **Split the preprocessed dataset** section.

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# In summary {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Running data quality tests
:::

::: {.column width="70%" .bl .pl4 .f3}
In this second module, you learned how to:

- [x] Identify relevant tests to run from {{< var vm.product >}}'s test repository
- [x] Initialize {{< var vm.product >}} `Dataset` objects
- [x] Run out-of-the-box tests with the {{< var validmind.developer >}}
- [x] Log test results to the {{< var validmind.platform >}}
- [x] Attach logged test results onto your model's validation report

:::
::::

::: {.tc}
<br>
Continue your model development journey with:
<br><br>
<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->
[Developing Challenger Models](developing-challenger-models.html){.button target="_blank"}
:::