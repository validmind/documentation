---
title: "Developing <br>Challenger Models"
subtitle: "Validator Fundamentals — Module 3 of 4<br><br>_Click [{{< fa chevron-right >}}](#learning-objectives) to start_"
lightbox: true
format:
  revealjs:
    controls: true
    controls-tutorial: true
    help: true
    controls-back-arrows: visible
    transition: slide
    theme: [default, ../assets/slides.scss]
    slide-number: true
    chalkboard: false
    preview-links: auto
    view-distance: 2
    logo: /validmind.png
    footer: "{{< var validmind.training >}} | [Home {{< fa person-walking-dashed-line-arrow-right >}}](/training/training.qmd)"
  html:
  # Change this to the file name prepended by a _ to get around the global HTML output settings required by _metadata.yml
    output-file: _developing-challenger-models.html
    search: false
title-slide-attributes:
  data-background-color: "#083E44"
  data-background-image: "../assets/home-hero.svg"
skip_preview: true
---

# Learning objectives {.center}

_"As a **validator** who has already run and logged data quality tests with {{< var vm.product >}}, I want to next run both out-of-the-box and custom model evaluation tests for the champion model and a potential challenger model, and use the results of my testing to log model findings."_

::: {.tc}
<br>
This third module is part of a four-part series:
<br><br>
[Validator Fundamentals](/training/validator-fundamentals/validator-fundamentals-register.qmd){.button target="_blank"}
:::


## Module 3 — Contents {.center}

:::: {.columns .f3}
::: {.column width="50%" .mt4 .pr4}
### Introduction
- [{{< var vm.product >}} for model validation](#validmind-for-model-validation)
:::

:::

:::: {.columns .f3}

::: {.column width="50%" .mt4 .pr4}
### Section 1

- [Train potential challenger model](#train-a-challenger-model)
- [Run model evaluation tests](#run-model-evaluation-tests)

:::

::: {.column width="50%" .mt4}
### Section 2

- [Implement a custom test](#implement-custom-tests)
- [Verify development test runs](#verify-test-runs)
:::

::::


<br>
First, let's make sure you can log in to {{< var vm.product >}}.

{{< include /training/assets/_revealjs-navigation.qmd >}}

## Before you begin {.center}

::: {.panel-tabset}

### Prerequisite course

To continue, you need to have been [onboarded](developer-fundamentals-register.qmd#register){target="_blank"} onto {{< var validmind.training >}} with the [**{{< fa circle-check >}} Validator**]{.bubble} role and completed the first two modules of this course:


<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->

:::: {.columns}
::: {.column width="60%"}
::: {.tc}
[Using {{< var vm.product >}} for Model Validation](using-validmind-for-model-validation.html){.button target="_blank"}
:::

:::
::: {.column width="40%"}
::: {.tc}
[Running Data Quality Tests](running-data-quality-tests.html){.button target="_blank"}
:::

:::
::::

:::: {.tc .mt5 .f2 .embed}
Already logged in and refreshed this module? Click [{{< fa chevron-right >}}]() to continue.

:::

### Log in

1. Log in to check your access:

:::: {.flex .flex-wrap .justify-around}

::: {.w-50-ns .tc}

[Log in to JupyterHub](https://jupyterhub.validmind.ai/){.button target="_blank"}

:::

::: {.w-50-ns .tc}
[Log in to {{< var vm.product >}}](https://app.prod.validmind.ai){.button target="_blank"}
:::

::::


::: {.tc .f3}
Be sure to return to this page afterwards.
:::

2. After you successfully log in, refresh the page to connect this training module up to the {{< var validmind.platform >}}: 

::: {.tc}
<button class="button" onClick="window.location.reload();">Refresh Page</button>

:::

:::

<!-- USING THE VARIABLE IN THE HEADING MESSES UP THE PAGE ANCHOR -->

# ValidMind for model validation {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Jupyter Notebook series

::: {.f3}
These notebooks walk you through how to validate a model using {{< var vm.product >}}, complete with supporting test results attached as evidence to your validation report.

::: {.f5 .nt2 .pl2 .mb4}
<br>

You will need to have already completed notebooks **1** and **2** during the first and second modules to proceed.

:::

:::
:::

::: {.column width="70%" .bl .pl4 .f3}
### {{< var vm.product >}} for model development

Our series of four introductory notebooks for model validators include sample code and how-to information to get you started with {{< var vm.product >}}:

1 — [Set up the {{< var validmind.developer >}} for validation](/notebooks/tutorials/model_validation/1-set_up_validmind_for_validation.ipynb){target="_blank"}<br>
2 — [Start the model validation process](/notebooks/tutorials/model_validation/2-start_validation_process.ipynb){target="_blank"}<br>
3 — [Developing a potential challenger model](/notebooks/tutorials/model_validation/3-developing_challenger_model.ipynb){target="_blank"}<br>
4 — [Finalize testing and reporting](/notebooks/tutorials/model_validation/4-finalize_validation_reporting.ipynb){target="_blank"}<br>
<br>

::: {.f4 .pl3 .pr3 .embed}
In this third module, we'll run through the remaining two notebooks **3** in Section 1 and **4** in Section 2 together.
:::

:::
::::

Let's continue our journey with **Section 1** on the next page. {{< fa hand-point-right >}}

# Section 1 {background-color="#083E44" background-image="/assets/img/about-us-esphere.svg"}

<!-- CHANGE THIS TO EXECUTED LATER -->

## {background-iframe="/notebooks/tutorials/model_validation/3-developing_challenger_model.html" background-interactive="yes" data-preload="yes"}

:::: {.absolute bottom=15 left=0 right=50 .w-100 .f4 .tc .pl4 .pr4 .overlay}
**3 — Developing a potential challenger model**

::: {.f5}
This is the third notebook in our introductory series, which will walk you through how to evaluate your champion model against a potential challenger with {{< var vm.product >}}.

:::

**Scroll through this notebook** to explore. When you are done, click [{{< fa chevron-right >}}]() to continue.

::::

## Get your code snippet

:::: {.columns}

::: {.column width="80%"}

<!-- FOR COPYING THE CODE SNIPPET WE NEED TO USE THIS IFRAME WORKAROUND -->

<div style="zoom: 0.6; overflow: hidden;">
  <iframe 
    src="https://app.prod.validmind.ai/model-inventory" 
    width="100%" 
    height="750" 
    style="border: 1px solid #083E44; border-radius: 8px; border-right-width: 2px; border-bottom-width: 3px;"
    data-preload="yes" 
    sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    allow="clipboard-read; clipboard-write">
  </iframe>
</div>

:::

::: {.column width="20%" .f4}

::: {.f5}
{{< var vm.product >}} generates a unique *code snippet* for each registered model to connect with your validation environment:

1. From the **{{< fa cubes >}} Inventory**, select the name of your model to open up the model details page.
2. On the left sidebar that appears for your model, click **Getting Started**.
3. Locate the code snippet and click **Copy snippet to clipboard**.

:::

When you're done, click [{{< fa chevron-right >}}]() to continue.

:::

::::

:::: {.tc .f6 .embed}
**Can't load the {{< var validmind.platform >}}?**

Make sure you're logged in and have refreshed the page in a Chromium-based web browser.

:::

<!-- CHANGE THIS TO EXECUTED LATER -->

## {background-iframe="/notebooks/tutorials/model_validation/3-developing_challenger_model.html#initialize-the-validmind-library" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Connect to your model**

::: {.f5}
With your code snippet copied to your clipboard:

1. Open **3 — Developing a potential challenger model**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_validation/3-developing_challenger_model.ipynb){target="_blank"}
2. Run all the cells under the **Setting up** section.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

<!-- CHANGE THIS TO EXECUTED LATER -->

## {background-iframe="/notebooks/tutorials/model_validation/3-developing_challenger_model.html#import-the-champion-model" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Import the champion model**

::: {.f5}
Next, let's import the champion model submitted by the model development team in the format of a `.pkl` file for evaluation:

1. Continue with **3 — Developing a potential challenger model**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_validation/3-developing_challenger_model.ipynb){target="_blank"}
2. Run the cell under the **Import the champion model** section.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# Train a challenger model {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="60%" .pr4 .f2}
Champion vs. challenger models

:::

::: {.column width="40%" .bl .pl4 .f3}
Try it **live** on the next pages. {{< fa hand-point-right >}}

:::

::::

::: {.f4}

We're curious how an alternate model compares to our champion model, so let's train a challenger model as a basis for our testing:

::: {.panel-tabset}

### Champion logistic regression model

- Our champion *logistic regression model* is a simpler, parametric model that assumes a linear relationship between the independent variables and the log-odds of the outcome.
- While logistic regression may not capture complex patterns as effectively, it offers a high degree of interpretability and is easier to explain to stakeholders.
- However, model risk is not calculated in isolation from a single factor, but rather in consideration with trade-offs in predictive performance, ease of interpretability, and overall alignment with business objectives.

### Challenger random forest classification model

- A *random forest classification model* is an ensemble machine learning algorithm that uses multiple decision trees to classify data. In ensemble learning, multiple models are combined to improve prediction accuracy and robustness.
- Random forest classification models generally have higher accuracy because they capture complex, non-linear relationships, but as a result they lack transparency in their predictions.

:::

:::

<!-- CHANGE THIS TO EXECUTED LATER -->

## {background-iframe="/notebooks/tutorials/model_validation/3-developing_challenger_model.html#training-a-potential-challenger-model" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Random forest classification model**

Let's train our potential challenger model:

::: {.f5}
1. Continue with **3 — Developing a potential challenger model**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_validation/3-developing_challenger_model.ipynb){target="_blank"}
2. Run the cell under the following Training a potential challenger model section: **Random forest classification model**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

<!-- CHANGE THIS TO EXECUTED LATER -->

## {background-iframe="/notebooks/tutorials/model_validation/3-developing_challenger_model.html#initializing-the-model-objects" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Initialize the model objects**

::: {.f5}
In addition to the initialized datasets, you'll also need to initialize a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data for each of our two models using [the `vm.init_model()` method](/validmind/validmind.qmd#init_model):

1. Continue with **3 — Developing a potential challenger model**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_validation/3-developing_challenger_model.ipynb){target="_blank"}
2. Run all the cells under the section **Initializing the model objects**.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# Run model evaluation tests* {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#create-custom-tests-folder" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run model performance tests**

::: {.f5}
Create a new folder that will contain reusable custom tests from your existing inline tests:

1. Continue with **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run the cell under the following Using external test providers section: **Create custom tests folder**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {.scrollable}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Log a model finding

::: {.f5 .nt2 .pl2 .mb4}
(Scroll down for the full instructions.)
:::

::: {.tc}
[Learn more ...](/notebooks/EXECUTED/model_development/3-integrate_custom_tests.ipynb#add-test-results-to-documentation){.button target="_blank"}

:::

<br>Try it **live** on the next page. {{< fa hand-point-right >}}

:::

::: {.column width="70%" .bl .pl4 .f4}
### Add test results to model documentation

With the custom test results logged, let's head to the model we connected to at the beginning of this notebook and insert our test results into the documentation:

1. From the **{{< fa cubes >}} Inventory** in the {{< var validmind.platform >}}, go to the model you connected to earlier.

2. In the left sidebar that appears for your model, click **{{< fa book-open >}} Documentation**.

3. Locate the Data Preparation section and click on **3.2 Model Evaluation** to expand that section.

4. Hover under the Pearson Correlation Matrix content block until a horizontal dashed line with a **+** button appears, indicating that you can insert a new block.

5. Click **+** and then select **Test-Driven Block**:

    - In the search bar, type in `ConfusionMatrix`.
    - Select the custom `ConfusionMatrix` tests you logged above:

    ![The ConfusionMatrix tests selected](/notebooks/EXECUTED/model_development/selecting-confusion-matrix-test.png){fig-alt="Screenshot showing the ConfusionMatrix tests selected" .screenshot}

6. Finally, click **Insert 2 Test Results to Document** to add the test results to the documentation.

    Confirm that the two individual results for the confusion matrix tests have been correctly inserted into section **3.2 Model Evaluation** of the documentation.

:::
::::


## {background-iframe="https://app.prod.validmind.ai/model-inventory/" background-interactive="true" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f5 .tc .pl4 .overlay}
**Create a model finding**

::: {.f6}
3.2 Model Evaluation — `my_custom_tests.ConfusionMatrix:test_dataset_normalized` / `my_test_provider.ConfusionMatrix`

:::

When you're done, click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#save-an-inline-test" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run diagnostic tests**

::: {.f5}
The `@vm.test` decorator also includes a convenience method that allows you to save the test to a Python file at a specified path:

1. Continue with **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells under the Using external test providers section: **Save an inline test**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#register-a-local-test-provider" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Run feature importance tests**

::: {.f5}
Next, let's initialize a test provider that will tell the {{< var validmind.developer >}} where to find your saved custom tests:

1. Continue with **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells under the Using external test providers section: **Register a local test provider**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# Section 2 {background-color="#083E44" background-image="/assets/img/about-us-esphere.svg"}

## {background-iframe="/notebooks/EXECUTED/model_development/4-finalize_testing_documentation.html" background-interactive="yes" data-preload="yes"}

:::: {.absolute bottom=15 left=0 right=50 .w-100 .f4 .tc .pl4 .pr4 .overlay}
**4 — Finalize testing and reporting**

::: {.f5}
This is the final notebook in our introductory series, which will walk you through wrapping custom test results into your documentation, as well as how to update the configuration for the entire model documentation template to suit your needs.
:::

**Scroll through this notebook** to explore. When you are done, click [{{< fa chevron-right >}}]() to continue.

::::

## Retrieve your code snippet

:::: {.columns}

::: {.column width="80%"}

<!-- FOR COPYING THE CODE SNIPPET WE NEED TO USE THIS IFRAME WORKAROUND -->

<div style="zoom: 0.6; overflow: hidden;">
  <iframe 
    src="https://app.prod.validmind.ai/model-inventory" 
    width="100%" 
    height="750" 
    style="border: 1px solid #083E44; border-radius: 8px; border-right-width: 2px; border-bottom-width: 3px;"
    data-preload="yes" 
    sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
    allow="clipboard-read; clipboard-write">
  </iframe>
</div>

:::

::: {.column width="20%"}

::: {.f4}
As usual, let's connect back up to your model in the {{< var validmind.platform >}}:

1. From the **{{< fa cubes >}} Inventory**, select the name of your model to open up the model details page.
2. On the left sidebar that appears for your model, click **Getting Started**.
3. Locate the code snippet and click **Copy snippet to clipboard**.

:::

:::

::::

When you're done, click [{{< fa chevron-right >}}]() to continue.

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#initialize-the-validmind-library" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Connect to your model**

::: {.f5}
With your code snippet copied to your clipboard:

1. Open **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells under the **Setting up** section.

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# Implement custom tests {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Custom tests

<br>Try it **live** on the next pages. {{< fa hand-point-right >}}

:::

::: {.column width="70%" .bl .pl4 .f3}
Let's implement a *custom test* that calculates the confusion matrix:

- You'll note that the custom test function is just a regular Python function that can include and require any Python library as you see fit.
- In a usual model validation situation, you would load a saved custom test provided by the model development team. In the following section, we'll have you implement the same custom test and make it available for reuse, to familiarize you with the processes.

:::
::::

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#create-a-confusion-matrix-plot" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Implement a custom inline test**

::: {.f5}
An *inline test* refers to a test written and executed within the same environment as the code being tested — in the following example, right in our Jupyter Notebook —  without requiring a separate test file or framework:

1. Continue with **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells in the following section under Implementing a custom inline test: **Create a confusion matrix plot**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#add-parameters-to-custom-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Use external test providers**

::: {.f5}
Sometimes you may want to reuse the same set of custom tests across multiple models and share them with others in your organization, like the model development team would have done with you in this example workflow featured in this series of notebooks. In this case, you can create an external custom test provider that will allow you to load custom tests from a local folder or a Git repository:

1. Continue with **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells in the following sections under Implementing a custom inline test: <br>**Add parameters to custom tests** / **Pass parameters to custom tests**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# Verify test runs {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {background-iframe="/notebooks/EXECUTED/model_development/3-integrate_custom_tests.html#add-parameters-to-custom-tests" data-preload="yes"}

:::: {.absolute bottom=15 .w-100 .f4 .tc .pl4 .overlay}
**Verify model development testing**

::: {.f5}
Our final task is to verify that all the tests provided by the model development team were run and reported accurately. Note the appended `result_ids` to delineate which dataset we ran the test with for the relevant tests:

1. Continue with **3 — Integrate custom tests**: [{{< fa square-arrow-up-right >}} JupyterHub](https://jupyterhub.validmind.ai/hub/user-redirect/lab/tree/tutorials/model_development/3-integrate_custom_tests.ipynb){target="_blank"}
2. Run all the cells in the following sections under Implementing a custom inline test: <br>**Add parameters to custom tests** / **Pass parameters to custom tests**

:::

When you're done, return to this page and click [{{< fa chevron-right >}}]() to continue.

::::

# In summary {background-color="#083E44" background-image="/training/assets/home-hero.svg"}

## {.scrollable .center}

:::: {.columns}
::: {.column width="30%" .pr4 .f2}
Developing challenger models

:::

::: {.column width="70%" .bl .pl4 .f3}
In this third module, you learned how to:

- [x] Implement, run, and log custom inline tests
- [x] Register external test providers for reusable custom tests
- [x] Include custom test results in your documentation template
- [x] Configure your model's documentation template

:::
::::

::: {.tc}
<br>
Continue your model development journey with:
<br><br>
<!-- IMPORTANT: USE THE .HTML PATH AND NOT THE .QMD PATH FOR THE REVEALJS OUTPUT -->
[Finalizing Validation Reports](finalizing-validation-reports.html){.button target="_blank"}
:::