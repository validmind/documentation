---
title: "Enhancements -- September 26, 2023"
keywords: "release notes, model risk management, ValidMind"
---

<!---[SC-2108] Multi-class tests should report metrics for individual classes and many other changes by @cachafla in [#228](https://github.com/validmind/validmind-python/pull/228)--->

- **Multi-class test improvements**. We made a number of changes to metrics to improve the developer experience:
   - A new `fail_fast` argument can be passed to `run_test_plan`, `run_test_suite` and `run_documentation_tests`, used to fail and raise an exception on the first error encountered. This change is useful for debugging.
   - `ClassifierPerformance` test now determines if you are testing a binary or a multi-class model. When testing a multi-class model, we now report additional per-class, macro and weighted average metrics.
   - Fixed F1 score metric so it works correctly  for binary and multi-class models.

<!---SC-2042: Add multi-class classification support by @AnilSorathiya in [#221](https://github.com/validmind/validmind-python/pull/221)--->
- **Added multi-class classification support**. The Developer Framework now supports a multi-class version of some the existing metrics, such as confusion matrix, accuracy, precision, recall, and more. Also, the dataset and model interfaces now support dealing with multiple targets.

<!---SC-2100: Implement classification model comparison metrics by @AnilSorathiya in [#231](https://github.com/validmind/validmind-python/pull/231)--->
- **Implemented classification model comparison metrics**. Added a model performance comparison test for classification tasks. The test includes metrics such as accuracy, F1, precision, recall, and roc_auc score.