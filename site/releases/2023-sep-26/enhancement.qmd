---
title: "Enhancements -- September 26, 2023"
keywords: "release notes, model risk management, ValidMind"
---

<!---[SC-2108] Multi-class tests should report metrics for individual classes and many other changes by @cachafla in [#228](https://github.com/validmind/validmind-python/pull/228)--->

- **Multi-class test improvements**. We made a number of changes to metrics to improve the developer experience:
   - A new `fail_fast` argument can be passed to `run_test_plan`, `run_test_suite` and `run_documentation_tests`, used to fail and raise an exception on the first error encountered. This change is useful for debugging.
   - `ClassifierPerformance` test now determines if you are testing a binary or a multi-class model. When testing a multi-class model, we now report additional per-class, macro and weighted average metrics.
   - Fixed F1 score metric so it works correctly  for binary and multi-class models.

<!---SC-2042: Add multi-class classification support by @AnilSorathiya in [#221](https://github.com/validmind/validmind-python/pull/221)--->
- **Added multi-class classification support**. The Developer Framework now supports a multi-class version of some the existing metrics, such as confusion matrix, accuracy, precision, recall, and more. Also, the dataset and model interfaces now support dealing with multiple targets.

<!---SC-2100: Implement classification model comparison metrics by @AnilSorathiya in [#231](https://github.com/validmind/validmind-python/pull/231)--->
- **Implemented classification model comparison metrics**. Added a model performance comparison test for classification tasks. The test includes metrics such as accuracy, F1, precision, recall, and roc_auc score.


<!---feat: [sc-2196] Users should be able to filter tests by task type and tags by @johnwalz97 in [#241](https://github.com/validmind/validmind-python/pull/241)--->
- **feat: [sc-2196] Users should be able to filter tests by task type and tags**. Adding a new search feature to the `validmind.tests.list_tests` function to allow for better test discoverability now that test metadata has been implemented. The `list_tests` function in the `tests` module now supports the following arguments:
* `filter` - defaults to `None` but, if set, will match tests by ID, task_types or tags using a combination of substring and fuzzy string matching
* `task` - defaults to `None` but, if set, will further narrow matching tests (assuming `filter` has been passed) by exact matching the `task` to the test's `task_type` metadata
* `tags` - defaults to `None` but, if a list is passed, will again narrow the matched tests by exact matching on tags


<!---Add new `run_test` helper function by @cachafla in [#243](https://github.com/validmind/validmind-python/pull/243)--->
- **Add new `run_test` helper function**. - Updated `Quickstart` notebook to have a consistent experience:
    - We are now running `vm.preview_template()` after initializing ValidMind
    - We are now running `vm.run_documentation_tests()` instead of running a test suite that is not connected to the template
- Added support for new `run_test` helper function. This function allows executing any individual test independent of a test suite or a documentation template.

### Example usage of `run_test`

1a) Assume a user could discover existing tests by calling `list_tests()` or `describe_test()`:

*list_tests()*:

<img width="1020" alt="image" src="https://github.com/validmind/validmind-python/assets/21595/d85821f7-f43c-40fb-a68c-96d9a0e6cfe2">

*describe_test()*:

<img width="1001" alt="image" src="https://github.com/validmind/validmind-python/assets/21595/839c0bf1-898a-452d-8025-85534cc5ad7d">

1b) Or users could directly view the tests associated with a documentation template by running `preview_template()`:

<img width="846" alt="image" src="https://github.com/validmind/validmind-python/assets/21595/67488af6-0f19-4d81-bf50-3dc95d0fb305">


2) Using the test ID users will be able to run a given test and pass additional configuration and inputs to it:

```python
# No params
test_results = vm.tests.run_test(
    "class_imbalance",
    dataset=vm_dataset
)

# Custom params
test_results = vm.tests.run_test(
    "class_imbalance",
    params={"min_percent_threshold": 30},
    dataset=vm_dataset
)
```

<img width="1032" alt="image" src="https://github.com/validmind/validmind-python/assets/21595/e202c0fb-ca90-4ab2-ba88-e2e524b32ff0">

3) Users will be able to send the results of the test to ValidMind by calling `.log()`:

```python
test_results.log()
```


<!---John6797/sc 1790/framework should track additional test metadata by @johnwalz97 in [#239](https://github.com/validmind/validmind-python/pull/239)--->
- **John6797/sc 1790/framework should track additional test metadata**. Added a `metadata` property to every ValidMind test class. The `metadata` property includes a `task_types` field and a `tags` field which both serve to categorize the tests based on what data and model types they work with, what category of test they fall into, etc.


<!---[SC-2144] Support hugging face summarization models by @AnilSorathiya in [#238](https://github.com/validmind/validmind-python/pull/238)--->
- **[SC-2144] Support hugging face summarization models**. Added support for Hugging Face summarization models in the developer framework to demonstrates capabilities to test and document summarization models.

### This PR includes 

- The financial news dataset
- initialize hugging face summarization model using the `init_model` interface.
- Implement an initial set of metrics that are relevant for testing summarization models
- Implement a demo notebook that tests and documents a Hugging Face model for summarization
-


<!---[SC-2216] Implement demo notebook for summarization model using LLMs by @cachafla in [#242](https://github.com/validmind/validmind-python/pull/242)--->
- **[SC-2216] Implement demo notebook for summarization model using LLMs**. - Added a new notebook that demonstrates how to run a "text summarization metrics" test suite for a LLM instructed to act as a financial news summarizer.

## Extra

@AnilSorathiya @juanmleng @johnwalz97 here's the prompt template I used for this notebook:

```python
prompt_template = """
You are an AI with expertise in summarizing financial news. 
Your task is to provide a concise summary of the specific news article provided below.
Before proceeding, take a moment to understand the context and nuances of the financial terminology used in the article.

Article to Summarize:

'''
{article}
'''

Please respond with a concise summary of the article's main points.
Ensure that your summary is based on the content of the article and not on external information or assumptions.
""".strip()
```


