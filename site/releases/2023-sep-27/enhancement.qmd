---
title: "Enhancements -- September 26, 2023"
keywords: "release notes, model risk management, ValidMind"
---

<!---[SC-2108] Multi-class tests should report metrics for individual classes and many other changes by @cachafla in [#228](https://github.com/validmind/validmind-python/pull/228)--->

- **Multi-class test improvements**. We made a number of changes to metrics to improve the developer experience:
   - A new `fail_fast` argument can be passed to `run_test_plan`, `run_test_suite` and `run_documentation_tests`, used to fail and raise an exception on the first error encountered. This change is useful for debugging.
   - `ClassifierPerformance` test now determines if you are testing a binary or a multi-class model. When testing a multi-class model, we now report additional per-class, macro and weighted average metrics.
   - Fixed F1 score metric so it works correctly  for binary and multi-class models.

<!---SC-2042: Add multi-class classification support by @AnilSorathiya in [#221](https://github.com/validmind/validmind-python/pull/221)--->
- **Added multi-class classification support**. The Developer Framework now supports a multi-class version of some the existing metrics, such as confusion matrix, accuracy, precision, recall, and more. Also, the dataset and model interfaces now support dealing with multiple targets.

<!---SC-2100: Implement classification model comparison metrics by @AnilSorathiya in [#231](https://github.com/validmind/validmind-python/pull/231)--->
- **Implemented classification model comparison metrics**. Added a model performance comparison test for classification tasks. The test includes metrics such as accuracy, F1, precision, recall, and roc_auc score.

<!---John6797/sc 1790/framework should track additional test metadata by @johnwalz97 in [#239](https://github.com/validmind/validmind-python/pull/239)--->
- **Track additional test metadata**. Added a `metadata` property to every ValidMind test class. The `metadata` property includes a `task_types` field and a `tags` field which both serve to categorize the tests based on what data and model types they work with, what category of test they fall into, and more.

<!---feat: [sc-2196] Users should be able to filter tests by task type and tags by @johnwalz97 in [#241](https://github.com/validmind/validmind-python/pull/241)--->
- **Filter tests by task type and tags**. We added a new search feature to the `validmind.tests.list_tests` function to allow for better test discoverability, now that test metadata has been implemented. The `list_tests` function in the `tests` module now supports the following arguments:
   
   * `filter`: If set, will match tests by ID, task_types or tags using a combination of substring and fuzzy string matching. Defaults to `None`. 
   * `task`: If set, will further narrow matching tests (assuming `filter` has been passed) by exact matching the `task` to the test's `task_type` metadata. Defaults to `None`. 
   * `tags`: If a list is passed, will again narrow the matched tests by exact matching on tags. Defaults to `None`. 