---
title: "April 8, 2025"
date: 2025-04-08
subtitle: "Unified version `23.03.05`"
listing:
  - id: development-series
    type: grid
    grid-columns: 1
    max-description-length: 250
    contents:
    - path: ../../../developer/validmind-library.qmd#for-model-development
      title: "{{< var vm.product >}} for model development {{< fa chevron-right >}}"
      description: "Learn how to use {{< var vm.product >}} for your end-to-end model documentation process based on common model development scenarios with our series of four introductory notebooks."
  - id: raw-data
    type: grid
    grid-columns: 1
    max-description-length: 250
    contents:
    - path: ../../../notebooks/how_to/understand_utilize_rawdata.ipynb
      title: "Understand and utilize `RawData` in {{< var vm.product >}} tests {{< fa chevron-right >}}"
      description: "Learn how to access, inspect, and utilize `RawData` from {{< var vm.product >}} tests."
  - id: credit-risk
    type: grid
    grid-columns: 1
    max-description-length: 250
    contents:
    - path: ../../../notebooks/code_samples/credit_risk/application_scorecard_with_ml.ipynb
      title: "Document an application scorecard model {{< fa chevron-right >}}"
      categories: ["Machine Learning"]
      description: "tagline UPDATE"
---

DON'T FORGET THE SUMMARY!

::: {.highlights}

## Release highlights — `23.03.05`

::: {.callout}
Our documentation now follows the new **unified versioning scheme** for our software as of our [`25.01` release on January 31, 2025](/releases/2025/2025-jan-31/release-notes.qmd).
:::

### {{< var validmind.developer >}} (v2.8.13)

<!---
PR #326: Restructured old "Introduction for model developers" notebook
URL: https://github.com/validmind/validmind-library/pull/326
Labels: documentation, highlight
--->
#### New introduction for model development notebook series

We've revamped our old *Introduction for model developers* notebook into a series of four introductory notebooks — **ValidMind for model development:**

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
1. Set up the ValidMind Library
2. Start the model development process
3. Integrate custom tests
4. Finalize testing and documentation

:::

::: {.w-40-ns .tc}
[{{< fa brands github >}} Access Notebooks on GitHub](https://github.com/validmind/validmind-library/tree/main/notebooks/tutorials/model_development){.button .button-green target="_blank"}

:::

::::

::: {.column-margin}
:::{#development-series}
:::

:::

These new notebooks break down using ValidMind for your end-to-end model documentation process based on common model development scenarios:

- Learn the basics of the ValidMind Library with these interactive notebooks designed to introduce you to basic ValidMind concepts and get you familiar with tasks such as how to work with documentation templates, running and logging tests with ValidMind, and more.
- After you've completed your learning journey with these notebooks, you'll have a fully documented sample model ready for review.

<!---
PR #285: John6797/sc 7792/add raw data to validmind library tests
URL: https://github.com/validmind/validmind-library/pull/285
Labels: enhancement
--->
#### Surfaced `RawData` in ValidMind tests

Test functions in the {{< var validmind.developer >}} can now return a special object called *`RawData`*, which holds intermediate or unprocessed data produced somewhere in the test logic but not returned as part of the test's visible output, such as in tables or figures:

- The `RawData` feature allows you to customize the output of tests, making it a powerful tool for creating custom tests and post-processing functions.
- `RawData` is useful when running post-processing functions with tests to recompute tabular outputs, redraw figures, or even create new outputs entirely.

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
To complement this new feature, we've also released an accompanying notebook showing you how to access, inspect, and utilize `RawData` from {{< var vm.product >}} tests:

:::

::: {.w-40-ns .tc}
[{{< fa brands github >}} Access Notebook on GitHub](https://github.com/validmind/validmind-library/tree/main/notebooks/how_to/understand_utilize_rawdata.ipynb){.button .button-green target="_blank"}

:::

::::

::: {.column-margin}
:::{#raw-data}
:::

:::

<!---
PR #290: [SC-8008] Ongoing monitoring notebook for application scorecard model
URL: https://github.com/validmind/validmind-library/pull/290
Labels: enhancement
--->
#### New ongoing monitoring notebook for application scorecard models

Add two new notebooks for the scorecard model:

- `application_scorecard_ongoing_monitoring.ipynb`: Includes new metrics for data and model drift. It also populates the ongoing monitoring document for the scorecard model.


<!---
PR #280: [SC-7864] Create credit risk scorecard notebook using XGBoost
URL: https://github.com/validmind/validmind-library/pull/280
Labels: enhancement
--->
#### New document credit risk scorecard model notebook

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
We've added a new variations to our **Document an application scorecard** notebooks family:

:::

::: {.w-40-ns .tc}
[{{< fa brands github >}} Access Notebook on GitHub](/notebooks/code_samples/credit_risk/application_scorecard_with_ml.ipynb){.button .button-green target="_blank"}

:::

::::

::: {.column-margin}
:::{#credit-risk}
:::

:::

- Learn how to document application scorecard models using the {{< var vm.developer >}}. These notebooks provide a step-by-step guide for loading a demo dataset, preprocessing data, training models, and documenting the model.
- EDIT ME

<!---
PR #295: feat: add print_env function
URL: https://github.com/validmind/validmind-library/pull/295
Labels: enhancement
--->
#### Ability to capture {{< var validmind.developer >}} environment

You can now retrieve information about your local environment running {{< var vm.product >}} using `print_env()`, allowing you to use this information to debug potential issues, as well as share this information with our team to assist you in troubleshooting.

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
The output of this call includes:

- Your installed {{< var validmind.developer >}} version
- Your Python version and related details, such as installed dependencies and their versions
- Your operating system
- Timestamp of the return

:::

::: {.w-20-ns .tc}
[print_env()](/validmind/validmind.qmd#print_env){.button .button-green target="_blank"}

:::

::::

::: {.column-margin}
**Example `print_env()` output:**

```bash
{'validmind': {'version': '2.8.13'},
 'python': {'version': '3.10.13',
            'implementation': 'CPython',
            'compiler': 'Clang 15.0.0 (clang-1500.3.9.4)'},
 'platform': 'macOS-15.4-arm64-arm-64bit',
 'pip': {'aequitas': '1.0.0',
         ...
         },
 'timestamp': '2025-04-14T14:30:54.633089'}
```
:::

<!---
PR #263: Add Initial Support for Post-Processing Functions
URL: https://github.com/validmind/validmind-library/pull/263
Labels: enhancement, highlight
--->
#### Add initial support for post-processing functions*

Adding support for post-processing functions

You can modify test outputs without changing the test code.
 
Generated PR summary: 
 
This update to the ValidMind project focuses on enhancing post-processing capabilities and raw data handling. You can now utilize a `post_process_fn` parameter in the `run_test` function, allowing the customization of test outputs through user-defined post-processing functions. This enables modifications to the `TestResult` object after running a test, such as altering tables and creating figures.

The update introduces a `RawData` class that encapsulates raw data generated during test execution. This class allows for easier inspection and serialization of additional data not directly shown in test results. A new Jupyter notebook, `post_processing_functions.ipynb`, is available to demonstrate how to use these post-processing functions with practical examples like modifying tables and creating figures.

The Python version has been updated in the GitHub Actions workflow from 3.8 to 3.11, ensuring compatibility with newer Python features. The enhancement also extends to the `ROCCurve` test, which now returns both a `RawData` object and a Plotly figure for flexible data visualization.

Lastly, various code refinements include improvements for JSON serialization of numpy arrays using a `HumanReadableEncoder`, along with refactored logic for handling multiple figure types. These changes are designed to improve customization and detail in handling test results within the ValidMind library.

### {{< var validmind.platform >}} (v1.31.10)

<!---
PR #1269: [SC-8611] As an admin I am able to manage model stakeholders
URL: https://github.com/validmind/frontend/pull/1269
Labels: enhancement
--->
#### Ability to manage model stakeholder types

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
We've introduced the ability to define custom model stakeholder types, allowing you to control granular permissions on each model in your inventory:

:::

::: {.w-40-ns .tc}
[Manage model stakeholder types](/guide/configuration/manage-model-stakeholder-types.qmd){.button .button-green target="_blank"}

:::

::::

- Model stakeholders determine specific responsibilities and access levels for model review and approval processes for each model in your model inventory, such as read or edit access to model inventory fields.
- Each model stakeholder also belongs to user groups* which determine which models they can see, and have user roles* with attached role permissions* which define the level of access they have to overarching {{< var validmind.platform >}} features.

::: {.callout}
Model stakeholders are now also available as a field defined in conditional arguments within model workflow steps*.

:::

### Documentation

<!---
PR #640: Switch Python API reference to Quarto
URL: https://github.com/validmind/documentation/pull/640
Labels: documentation, highlight
--->
#### Improved {{< var vm.api >}} reference

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
We've completely redesigned our {{< var validmind.api >}} reference using the same tools we use to produce the rest of our product documentation, allowing us to more easily keep this information up to date and ensure its accuracy.

:::

::: {.w-40-ns .tc}
[{{< var validmind.api >}} reference](/validmind/validmind.qmd){.button .button-green target="_blank"}

:::

::::

- Now featuring more intuitive navigation wrapped in a familiar sidebar, enhanced code signature styling, and integration with our main docs site search, these improvements aim to empower users to maximize the potential of the {{< var validmind.developer >}}.
- The updated reference structure mirrors the Python package layout, ensuring backward compatibility with our old Python API reference while providing a more reader-friendly experience.

::: {.column-margin}
:::{#python-api}
:::

:::


:::



## Enhancements

### {{< var validmind.developer >}} (v2.8.13)

<!---
PR #274: [SC-7759] Exposing static descriptions in test results
URL: https://github.com/validmind/validmind-library/pull/274
Labels: enhancement
--->
#### Exposing static descriptions in test results

Added `doc` property to `TestResult` class

The `doc` property clearly separates the static test documentation from the LLM-generated descriptions. This change helps distinguish between the original test documentation (accessed via `result.doc`) and the dynamic descriptions generated by the LLM. You can access these via `result.description`.

The `doc` property contains the test's docstring. It provides direct access to users which clarifies the test's purpose and behavior.
 
Generated PR summary: 
 
This update to the ValidMind project includes various enhancements and bug fixes. You can now document test results more comprehensively with the addition of a `doc` attribute to the `TestResult` class, which stores related documentation. Functions such as `build_test_result`, `_run_composite_test`, `_run_comparison_test`, and `run_test` are updated to incorporate this documentation into the test result pipeline. The ROC curve tests in `test_ROCCurve.py`, including `test_roc_curve_structure` and `test_perfect_separation`, are enhanced to accommodate changes in the `ROCCurve` function's return values, which now include a tuple of `RawData` and `Figure`. The improvements add assertions to verify that the `RawData` object has expected fields like `fpr`, `tpr`, and `auc`. Additionally, modifications to the test execution process ensure that if any unit tests fail in `tests/test_unit_tests.py`, it exits with a failure status, enhancing overall testing robustness.

<!---
PR #256: [SC-7588] Explore dynamic llm context injection in test descriptions
URL: https://github.com/validmind/validmind-library/pull/256
Labels: enhancement
--->
#### Explore dynamic llm context injection in test descriptions

Added the capability to include contextual information in LLM-based descriptions  
The changes mainly focus on improving the generation of descriptions for test results. You can do this by incorporating additional context that you specify through environment variables.
 
Generated PR summary: 
 
This update enhances the ValidMind library by enabling you to include contextual information in LLM-based descriptions. The improvements focus on generating more informative test result descriptions through additional context specified via environment variables.

Notable changes include the addition of a Jupyter notebook, `llm_descriptions_context.ipynb`, which demonstrates how to incorporate context into LLM-based descriptions. You can see examples of setting up environments, initializing datasets and models, and comparing test results with and without added context.

A new function, `_get_llm_global_context()`, is introduced in `test_descriptions.py` for retrieving context from environment variables, ensuring that context is utilized only if enabled and non-empty. The `generate_description` function now incorporates this context when creating description input data.

Additionally, the `user.jinja` template has been updated to conditionally add the context in generated descriptions if available. This allows you to align LLM-generated insights more closely with specific business requirements or policies by providing extra information about tests or use cases.


<!---
PR #293: [SC-8072] Support threshold lines in unit metric plots
URL: https://github.com/validmind/validmind-library/pull/293
Labels: enhancement
--->
#### Support threshold lines in unit metric plots

**Description**

Enhances the `log_metric()` function  

Enhances the `log_metric()` function to accept multiple named thresholds as a dictionary input. This enhancement allows you to define and track multiple threshold levels for each unit metric. For instance, you can specify thresholds like "high_risk": 0.6, "medium_risk": 0.7, and "low_risk": 0.8.

**Changes**

- Added threshold support in `log_metric()`.
- Added notebook `how_to\log_metrics_over_time.ipynb`.

**Testing**

- Tested with various threshold configurations.
- Verified threshold line rendering in the UI.
- Validated that threshold updates reflect immediately in visualization.

**Example**

```
log_metric(
    key="AUC Score",
    value=auc,
    recorded_at=datetime(2024, 1, 1),
    thresholds={
        "high_risk": 0.6,
        "medium_risk": 0.7,
        "low_risk": 0.8,
    }
)
```

<img width="949" alt="log_metric_auc_4" src="https://github.com/user-attachments/assets/9df62ede-b51f-46e3-b5bc-e9caa5ca685f"/>
 
Generated PR summary: 
 
This update enhances the ValidMind library by adding support for logging thresholds alongside metrics. The `log_metric` and `alog_metric` functions in the `api_client.py` file now include an optional `thresholds` parameter, allowing you to specify threshold values for metrics. This feature helps you visualize metrics over time and identify potential issues by setting reference lines in metric visualizations. Additionally, the notebook `log_metrics_over_time.ipynb` has been updated with examples of logging multiple metrics with custom thresholds, demonstrating how to use this new functionality.

<!---
PR #337: [SC 8805] support config option for logging test results
URL: https://github.com/validmind/validmind-library/pull/337
Labels: documentation, enhancement
--->
#### Support config option for logging test results

We now have configuration options 

You can pass configuration options through `result.log()` to display and customize the test results block in UI documents.

Available config options:

- hideTitle
- hideText
- hideParams
- hideTables
- hideFigures

```python
test = vm.tests.run_test(
    "validmind.data_validation.TabularDescriptionTables:raw_dataset",
    input_grid={
        "dataset": [vm_raw_dataset],
    },
)
test.log(
    config={
        "hideFigures": False,
        "hideTables": True
    }
)
```
 
Generated PR summary: 
 
This release includes several enhancements and bug fixes to the `validmind` API client and result logging functionalities. You can now specify configuration options when displaying test results through a `config` parameter added to the `alog_test_result` and `log_async` methods. These options allow you to hide titles, text, parameters, tables, and figures in the document view as desired. A new method, `validate_log_config`, verifies that provided configuration options are valid by checking for invalid keys and ensuring all values are boolean. If any issues are found, an `InvalidParameterError` is raised.

The error handling is improved with the introduction of the new `InvalidParameterError` class to manage cases involving invalid parameters. Code refactoring efforts include updating the `log_metric` function to return results consistently from `run_async`, aiding in potential error handling. Additionally, docstrings across the codebase received updates for improved clarity.

Test updates accompany these changes as well, specifically altering the `test_log_test_result` case to incorporate the new configuration parameter and ensure thorough testing of logging functionality with these improvements. These enhancements offer greater flexibility in how test results are logged and displayed while strengthening error management and overall code clarity.


<!-- COVERED BY MODEL STAKEHOLDER RELEASE ABOVE  -->

<!---
PR #1278: Update stakeholder role mapping in workflows builder and filtering logic
URL: https://github.com/validmind/frontend/pull/1278
Labels: 
--->
<!-- #### Update stakeholder role mapping in workflows builder and filtering logic

Replace this comment with your description
 
Generated PR summary: 
 
This update centralizes and standardizes field identifier handling by introducing a shared constant (FIELD_IDENTIFIERS) throughout the application. In the JsonLogicHumanizer component, role mappings are refined to filter roles based on their scope; roles not of 'Model' scope are mapped to the USER_ROLES identifier, while 'Model' scoped roles are treated as model stakeholder roles using USER_MODEL_STAKEHOLDERS. Merged field mappers now combine custom, general role, and stakeholder mappers for consistent processing of JSON logic queries.

The ApprovalPanel component's query builder is enhanced to use FIELD_IDENTIFIERS instead of hardcoded strings for improved consistency and maintainability. It also introduces new configurations for model stakeholder roles, allowing these roles to be properly selectable and mapped in the UI.

The ApprovalNode component updates conditions to check against FIELD_IDENTIFIERS, which clarifies references to user roles, stakeholder roles, and custom fields. The humanizeJsonLogic utility's outputs now consistently align with updated constants based on role assignments.

Finally, in the useQueryBuilder hook, FIELD_IDENTIFIERS are exported for consistent usage across components, with query fields adjusted to reflect new role identifier mappings. These changes enhance code maintainability by reducing hardcoded values and ensuring consistency in role-based operations across the system. -->


## Bug fixes

### {{< var validmind.platform >}} (v1.31.10)

<!---
PR #1245: Fix scrolling in documentation table of contents
URL: https://github.com/validmind/frontend/pull/1245
Labels: bug
--->
#### Improved table of contents navigation for model documentation

Previously, users experienced non-optimal behavior with the table of contents sidebar for their model's **{{< fa book-open >}} Documentation** pages, such as being unable to scroll to reveal additional headings when the sections of the Document Outline* exceeded the length of the visible page, and navigation that auto-collapsed counterintuitively.

Now, users can scroll to reveal previously hidden table of contents headings via a fixed navigation sidebar.

<!---
PR #1250: Not able to see long output on the UI
URL: https://github.com/validmind/frontend/pull/1250
Labels: bug
--->
#### Fixed data table navigation in large test-driven blocks

Previously, when test results logged to the {{< var validmind.platform >}} contained more than 20 pages of data in a table were inserted as test-driven blocks into documentation, the previous and next buttons did not render correctly for pagination.

Now, test results logged and inserted as test-driven blocks* into the documentation display pagination navigation buttons correctly regardless of the size of the data table.


<!-- Customer-managed ValidMind -->

<!---
PR #674: Add URL configuration to Docker images
URL: https://github.com/validmind/documentation/pull/674
Labels: infrastructure

NOTE: CUSTOMER-MANAGED VALIDMIND RELEASE ONLY, HIDDEN FOR GENERAL RELEASE
--->

<!-- ### Add URL configuration to Docker images

You can now configure the Docker image for our static docs site. You can do so via a Kubernetes manifest or a config file.

Configuration parameters

- `VALIDMIND_URL` — Where you access the ValidMind Platform.
- `JUPYTERHUB_URL` — Where you access JupyterHub.

Configuration feature

This feature lets you configure the site's Docker image to match your specific requirements. This process simplifies deployment in your own infrastructure.

Supported methods

You can set these URLs using one of the following supported methods: [URL configuration for Docker](https://github.com/validmind/documentation/blob/main/README.md#url-configuration-for-docker).
 
Generated PR summary: 
 
This update enhances the Docker configuration for the ValidMind documentation site by introducing several key improvements. The Dockerfile now includes the installation of `jq` for handling JSON data, and a new entrypoint script `docker_entrypoint.sh` is added to replace URL placeholders with actual values during runtime. Additionally, the script is set to be executable. A new `config.json` file stores default URL values, allowing dynamic URL configurations. Various documentation files have been updated to use placeholders for URLs, which are replaced at runtime by the entrypoint script.

The Makefile sees enhancements with new targets `docker-site` and `docker-site-lite`, designed for building the site with Docker-specific settings. The `docker-build` target has been updated to utilize the new `docker-site-lite` process.

A sample Kubernetes manifest, `validmind-docs.yaml`, is included to facilitate deploying the documentation site with URLs configurable through environment variables or a ConfigMap. Additionally, two scripts manage URL handling: `modify_urls.sh` replaces actual URLs with placeholders during build, while `docker_entrypoint.sh` substitutes these placeholders with real URLs at runtime, using either environment variables or values from `config.json`.

These changes offer enhanced flexibility in deploying the documentation site across different environments by enabling dynamic configuration of URLs based on deployment settings. -->


{{< include /releases/_how-to-upgrade.qmd >}}