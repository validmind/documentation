---
title: "April 8, 2025"
---

## Release highlights

<!---
PR #640: Switch Python API reference to Quarto
URL: https://github.com/validmind/documentation/pull/640
Labels: documentation, highlight
--->
### Switch Python API reference to Quarto

<!---
PR #263: Add Initial Support for Post-Processing Functions
URL: https://github.com/validmind/validmind-library/pull/263
Labels: enhancement, highlight
--->
### Add initial support for post-processing functions

Adding support for post-processing functions

You can modify test outputs without changing the test code.
 
Generated PR summary: 
 
This update to the ValidMind project focuses on enhancing post-processing capabilities and raw data handling. You can now utilize a `post_process_fn` parameter in the `run_test` function, allowing the customization of test outputs through user-defined post-processing functions. This enables modifications to the `TestResult` object after running a test, such as altering tables and creating figures.

The update introduces a `RawData` class that encapsulates raw data generated during test execution. This class allows for easier inspection and serialization of additional data not directly shown in test results. A new Jupyter notebook, `post_processing_functions.ipynb`, is available to demonstrate how to use these post-processing functions with practical examples like modifying tables and creating figures.

The Python version has been updated in the GitHub Actions workflow from 3.8 to 3.11, ensuring compatibility with newer Python features. The enhancement also extends to the `ROCCurve` test, which now returns both a `RawData` object and a Plotly figure for flexible data visualization.

Lastly, various code refinements include improvements for JSON serialization of numpy arrays using a `HumanReadableEncoder`, along with refactored logic for handling multiple figure types. These changes are designed to improve customization and detail in handling test results within the ValidMind library.

<!---
PR #326: Restructured old "Introduction for model developers" notebook
URL: https://github.com/validmind/validmind-library/pull/326
Labels: documentation, highlight
--->
### Restructured old introduction for model developers notebook

## Enhancements

<!---
PR #274: [SC-7759] Exposing static descriptions in test results
URL: https://github.com/validmind/validmind-library/pull/274
Labels: enhancement
--->
### Exposing static descriptions in test results

Added `doc` property to `TestResult` class

The `doc` property clearly separates the static test documentation from the LLM-generated descriptions. This change helps distinguish between the original test documentation (accessed via `result.doc`) and the dynamic descriptions generated by the LLM. You can access these via `result.description`.

The `doc` property contains the test's docstring. It provides direct access to users which clarifies the test's purpose and behavior.
 
Generated PR summary: 
 
This update to the ValidMind project includes various enhancements and bug fixes. You can now document test results more comprehensively with the addition of a `doc` attribute to the `TestResult` class, which stores related documentation. Functions such as `build_test_result`, `_run_composite_test`, `_run_comparison_test`, and `run_test` are updated to incorporate this documentation into the test result pipeline. The ROC curve tests in `test_ROCCurve.py`, including `test_roc_curve_structure` and `test_perfect_separation`, are enhanced to accommodate changes in the `ROCCurve` function's return values, which now include a tuple of `RawData` and `Figure`. The improvements add assertions to verify that the `RawData` object has expected fields like `fpr`, `tpr`, and `auc`. Additionally, modifications to the test execution process ensure that if any unit tests fail in `tests/test_unit_tests.py`, it exits with a failure status, enhancing overall testing robustness.

<!---
PR #256: [SC-7588] Explore dynamic llm context injection in test descriptions
URL: https://github.com/validmind/validmind-library/pull/256
Labels: enhancement
--->
### Explore dynamic llm context injection in test descriptions

Added the capability to include contextual information in LLM-based descriptions  
The changes mainly focus on improving the generation of descriptions for test results. You can do this by incorporating additional context that you specify through environment variables.
 
Generated PR summary: 
 
This update enhances the ValidMind library by enabling you to include contextual information in LLM-based descriptions. The improvements focus on generating more informative test result descriptions through additional context specified via environment variables.

Notable changes include the addition of a Jupyter notebook, `llm_descriptions_context.ipynb`, which demonstrates how to incorporate context into LLM-based descriptions. You can see examples of setting up environments, initializing datasets and models, and comparing test results with and without added context.

A new function, `_get_llm_global_context()`, is introduced in `test_descriptions.py` for retrieving context from environment variables, ensuring that context is utilized only if enabled and non-empty. The `generate_description` function now incorporates this context when creating description input data.

Additionally, the `user.jinja` template has been updated to conditionally add the context in generated descriptions if available. This allows you to align LLM-generated insights more closely with specific business requirements or policies by providing extra information about tests or use cases.

<!---
PR #280: [SC-7864] Create credit risk scorecard notebook using XGBoost
URL: https://github.com/validmind/validmind-library/pull/280
Labels: enhancement
--->
### Create credit risk scorecard notebook using `XGBoost`

Add new application scorecard notebooks using ML with additional testing

- `application_scorecard_with_ml.ipynb`: running individual tests
- `application_scorecard_full_suite`: using `run_documentation_tests()`
 
Generated PR summary: 
 
This release of the ValidMind Library, version 2.7.4, introduces enhancements and bug fixes with a focus on credit risk scorecard modeling. You can now explore two new Jupyter notebooks that demonstrate using the application scorecard model with ValidMind. These notebooks guide you through loading demo datasets, preprocessing data, training models, and documenting them.

New tests have been added to evaluate different aspects such as feature relevance through mutual information scores, default rates across credit score bands, probability estimates calibration via calibration curves, and threshold optimization for binary classification models. You'll also find tests that illustrate model parameters for transparency and assess the alignment between credit scores and predicted probabilities.

Enhancements to existing tests improve their functionality and accuracy. Notably, the `TooManyZeroValues` test now considers a row count and percentage threshold for zero values.

The dataset splitting functionality in `lending_club.py` is more flexible with support for an optional validation set. A utility function `get_demo_test_config` has been introduced to generate default test configurations for demonstration purposes.

The update also addresses several bugs by correcting test logic and enhancing test coverage.

<!---
PR #290: [SC-8008] Ongoing monitoring notebook for application scorecard model
URL: https://github.com/validmind/validmind-library/pull/290
Labels: enhancement
--->
### Ongoing monitoring notebook for application scorecard model

Add two new notebooks for the scorecard model:

- `application_scorecard_ongoing_monitoring.ipynb`: Includes new metrics for data and model drift. It also populates the ongoing monitoring document for the scorecard model.

- `application_scorecard_executive.ipynb`: This high-level notebook documents the scorecard model with just one command `lending_club.document_model()`.
 
Generated PR summary: 
 
This update to the ValidMind Library introduces enhancements and bug fixes related to credit risk scorecard models and ongoing monitoring capabilities. You can now access new Jupyter notebooks that guide you through using the ValidMind Library with credit risk datasets. These include demonstrations on building and documenting application scorecard models, integrating machine learning models into scorecards, and focusing on ongoing monitoring.

Custom tests have been introduced for scorecard models, allowing you to define and run your own tests using the ValidMind Library. This includes evaluating discrimination metrics across different score bands.

Ongoing monitoring has been enhanced with new tests designed to evaluate various aspects such as probability calibration changes, classification discrimination metrics comparison, class distribution drift, classification accuracy metrics comparison, confusion matrix metrics comparison, cumulative prediction probability distributions comparison, feature distribution changes, prediction distributions across features assessment, correlation changes between predictions and features evaluation, prediction probability distributions comparison using histograms and quantiles, ROC curves comparison, score bands drift analysis, score distributions comparisons through histograms, and differences in prediction distributions assessment.

The library also sees improvements in dataset loading, preprocessing, and feature engineering functions with verbosity control for cleaner outputs. Additionally, the library version is updated from 2.7.5 to 2.7.6. Integration tests have been modified to skip certain ongoing monitoring tests that require specific conditions.

These updates enhance your ability to manage and monitor credit risk models effectively while providing increased flexibility and valuable insights into model performance.

<!---
PR #285: John6797/sc 7792/add raw data to validmind library tests
URL: https://github.com/validmind/validmind-library/pull/285
Labels: enhancement
--->
### Add raw data to `validmind` library tests

Adding raw data storage

Raw data storage is added across all of our test library. Every ValidMind test now returns a `RawData` object. You can use the `RawData` object in post-processing functions to recreate any test output. The feature continues the trend towards more flexibility and customizability.
 
Generated PR summary: 
 
This release introduces significant updates to the AI test scripts and unit tests by integrating a new `RawData` mechanism. This allows you to store and retrieve intermediate data used in test functions, enhancing post-processing capabilities and providing more detailed insights into test results.

The script `add_test_description.py` has been removed, suggesting a change in how test descriptions are managed or generated. In its place, a new script called `bulk_ai_test_updates.py` is added to facilitate bulk updates on test files using AI, supporting actions like adding descriptions and raw data.

Many test scripts now utilize the `RawData` class to capture intermediate data useful for regenerating test results during post-processing. Test scripts have been updated to return this raw data alongside traditional outputs like figures and tables.

Unit tests are also updated to accommodate these changes, particularly by checking for the presence of `RawData` in test results and verifying its contents. New unit tests ensure the correct functionality of the `RawData` integration.

Codebase refactoring improves readability and maintainability through reorganization of imports, removal of redundant code, and adherence to coding standards. Security and dependency management are enhanced by ensuring all necessary dependencies are correctly imported and utilized, reducing runtime errors and improving overall security.

<!---
PR #293: [SC-8072] Support threshold lines in unit metric plots
URL: https://github.com/validmind/validmind-library/pull/293
Labels: enhancement
--->
### Support threshold lines in unit metric plots

**Description**

Enhances the `log_metric()` function  

Enhances the `log_metric()` function to accept multiple named thresholds as a dictionary input. This enhancement allows you to define and track multiple threshold levels for each unit metric. For instance, you can specify thresholds like "high_risk": 0.6, "medium_risk": 0.7, and "low_risk": 0.8.

**Changes**

- Added threshold support in `log_metric()`.
- Added notebook `how_to\log_metrics_over_time.ipynb`.

**Testing**

- Tested with various threshold configurations.
- Verified threshold line rendering in the UI.
- Validated that threshold updates reflect immediately in visualization.

**Example**

```
log_metric(
    key="AUC Score",
    value=auc,
    recorded_at=datetime(2024, 1, 1),
    thresholds={
        "high_risk": 0.6,
        "medium_risk": 0.7,
        "low_risk": 0.8,
    }
)
```

<img width="949" alt="log_metric_auc_4" src="https://github.com/user-attachments/assets/9df62ede-b51f-46e3-b5bc-e9caa5ca685f"/>
 
Generated PR summary: 
 
This update enhances the ValidMind library by adding support for logging thresholds alongside metrics. The `log_metric` and `alog_metric` functions in the `api_client.py` file now include an optional `thresholds` parameter, allowing you to specify threshold values for metrics. This feature helps you visualize metrics over time and identify potential issues by setting reference lines in metric visualizations. Additionally, the notebook `log_metrics_over_time.ipynb` has been updated with examples of logging multiple metrics with custom thresholds, demonstrating how to use this new functionality.

<!---
PR #295: feat: add print_env function
URL: https://github.com/validmind/validmind-library/pull/295
Labels: enhancement
--->
### Add `print_env` function

Add `validmind.print_env()` function to dump running environment

Add a new top-level function for printing the running environment. 

This is helpful when debugging or asking others for help to debug.

```python
import validmind

validmind.print_env()
```
 
Generated PR summary: 
 
This release introduces a new `print_env` function in the `validmind/tests/run.py` module. The function uses the `pprint` module to pretty-print the run metadata obtained from the `_get_run_metadata` function. Additionally, the `print_env` function is now included in the `__all__` list in the `validmind/__init__.py` file, making it accessible as part of the module's public API. You can now utilize this function to easily format and view metadata information.

<!---
PR #337: [SC 8805] support config option for logging test results
URL: https://github.com/validmind/validmind-library/pull/337
Labels: documentation, enhancement
--->
### Support config option for logging test results

We now have configuration options 

You can pass configuration options through `result.log()` to display and customize the test results block in UI documents.

Available config options:

- hideTitle
- hideText
- hideParams
- hideTables
- hideFigures

```python
test = vm.tests.run_test(
    "validmind.data_validation.TabularDescriptionTables:raw_dataset",
    input_grid={
        "dataset": [vm_raw_dataset],
    },
)
test.log(
    config={
        "hideFigures": False,
        "hideTables": True
    }
)
```
 
Generated PR summary: 
 
This release includes several enhancements and bug fixes to the `validmind` API client and result logging functionalities. You can now specify configuration options when displaying test results through a `config` parameter added to the `alog_test_result` and `log_async` methods. These options allow you to hide titles, text, parameters, tables, and figures in the document view as desired. A new method, `validate_log_config`, verifies that provided configuration options are valid by checking for invalid keys and ensuring all values are boolean. If any issues are found, an `InvalidParameterError` is raised.

The error handling is improved with the introduction of the new `InvalidParameterError` class to manage cases involving invalid parameters. Code refactoring efforts include updating the `log_metric` function to return results consistently from `run_async`, aiding in potential error handling. Additionally, docstrings across the codebase received updates for improved clarity.

Test updates accompany these changes as well, specifically altering the `test_log_test_result` case to incorporate the new configuration parameter and ensure thorough testing of logging functionality with these improvements. These enhancements offer greater flexibility in how test results are logged and displayed while strengthening error management and overall code clarity.

<!---
PR #1269: [SC-8611] As an admin I am able to manage model stakeholders
URL: https://github.com/validmind/frontend/pull/1269
Labels: enhancement
--->
### As an admin you are able to manage model stakeholders

Customer admins can now create new stakeholder roles and edit permissions under the new settings option.

The new stakeholder roles are available in the inventory model overview page for user assignment.
 
Generated PR summary: 
 
This release introduces several functional enhancements and bug fixes focused on managing stakeholder roles within the application. You now have access to a new Stakeholders page with dedicated routes, such as `/settings/stakeholders` and `/settings/stakeholders/:roleCUID`, allowing you to manage model stakeholder types separately from standard roles. The RoleDetails component adapts based on whether you are viewing stakeholders, updating breadcrumb labels, and limiting editing permissions for locked stakeholder roles. 

Additionally, the Model Inventory's UI logic has been improved to support toggling visibility for additional roles and managing stakeholder roles dynamically. A new popover component lets you control which roles are visible. Consistent field identifiers have been introduced and are utilized across various components to ensure uniform handling of user and stakeholder roles.

Bug fixes include console warnings for missing event fields in the ActivityFeedWidget, improved formatting logic in PermissionsList to handle custom field prefixes correctly, and minor tweaks in toast notifications and UI elements like buttons and grids. These updates enhance your experience by refining the management of stakeholder types, incorporating UI improvements, and ensuring consistent data handling.


## Bug fixes

<!---
PR #1245: Fix scrolling in documentation table of contents
URL: https://github.com/validmind/frontend/pull/1245
Labels: bug
--->
### Fix scrolling in documentation table of contents

https://github.com/user-attachments/assets/5edee12d-0905-4abf-b88a-671e7a7d345c
 
Generated PR summary: 
 
The layout of the DocumentationPage component has been improved to enhance functionality and user experience. The sidebar is now fixed with a set width, maintaining consistency and visibility across the application. This change involves making the sidebar span from top to bottom of the viewport and adding a right border with a color that adjusts based on the active color mode for better visual separation. The sidebar remains scrollable while staying in place, ensuring ease of navigation. Additionally, the main content area’s left margin has been adjusted to prevent overlay or layout issues caused by the fixed sidebar. These updates provide a more stable and user-friendly layout, particularly benefiting navigation on larger screens.

<!---
PR #1250: Not able to see long output on the UI
URL: https://github.com/validmind/frontend/pull/1250
Labels: bug
--->
### Not able to see long output on the UI

Fixed an issue

Tables with more than 20 pages were not rendering `Previous`/`Next` buttons correctly.

https://github.com/user-attachments/assets/abc23cbd-21ae-4f81-b42a-610bf88fc5e8
 
Generated PR summary: 
 
The DataTable component's pagination functionality now includes a dynamic system that enhances usability and scalability, particularly when managing a large number of pages. The pagination always displays the first and last pages, with dynamically calculated visible page buttons based on your current page and a set maximum visible limit. To maintain a clean interface when there are many pages, the component uses ellipsis menus to represent skipped page numbers, utilizing Chakra UI's Menu components. This improved navigation ensures you have a clear pathway to access any page directly through visible buttons or via dropdown menus listing hidden pages.

## Other

<!---
PR #674: Add URL configuration to Docker images
URL: https://github.com/validmind/documentation/pull/674
Labels: infrastructure
--->
### Add URL configuration to Docker images

You can now configure the Docker image for our static docs site. You can do so via a Kubernetes manifest or a config file.

Configuration parameters

- `VALIDMIND_URL` — Where you access the ValidMind Platform.
- `JUPYTERHUB_URL` — Where you access JupyterHub.

Configuration feature

This feature lets you configure the site's Docker image to match your specific requirements. This process simplifies deployment in your own infrastructure.

Supported methods

You can set these URLs using one of the following supported methods: [URL configuration for Docker](https://github.com/validmind/documentation/blob/main/README.md#url-configuration-for-docker).
 
Generated PR summary: 
 
This update enhances the Docker configuration for the ValidMind documentation site by introducing several key improvements. The Dockerfile now includes the installation of `jq` for handling JSON data, and a new entrypoint script `docker_entrypoint.sh` is added to replace URL placeholders with actual values during runtime. Additionally, the script is set to be executable. A new `config.json` file stores default URL values, allowing dynamic URL configurations. Various documentation files have been updated to use placeholders for URLs, which are replaced at runtime by the entrypoint script.

The Makefile sees enhancements with new targets `docker-site` and `docker-site-lite`, designed for building the site with Docker-specific settings. The `docker-build` target has been updated to utilize the new `docker-site-lite` process.

A sample Kubernetes manifest, `validmind-docs.yaml`, is included to facilitate deploying the documentation site with URLs configurable through environment variables or a ConfigMap. Additionally, two scripts manage URL handling: `modify_urls.sh` replaces actual URLs with placeholders during build, while `docker_entrypoint.sh` substitutes these placeholders with real URLs at runtime, using either environment variables or values from `config.json`.

These changes offer enhanced flexibility in deploying the documentation site across different environments by enabling dynamic configuration of URLs based on deployment settings.

<!---
PR #1278: Update stakeholder role mapping in workflows builder and filtering logic
URL: https://github.com/validmind/frontend/pull/1278
Labels: 
--->
### Update stakeholder role mapping in workflows builder and filtering logic

Replace this comment with your description
 
Generated PR summary: 
 
This update centralizes and standardizes field identifier handling by introducing a shared constant (FIELD_IDENTIFIERS) throughout the application. In the JsonLogicHumanizer component, role mappings are refined to filter roles based on their scope; roles not of 'Model' scope are mapped to the USER_ROLES identifier, while 'Model' scoped roles are treated as model stakeholder roles using USER_MODEL_STAKEHOLDERS. Merged field mappers now combine custom, general role, and stakeholder mappers for consistent processing of JSON logic queries.

The ApprovalPanel component's query builder is enhanced to use FIELD_IDENTIFIERS instead of hardcoded strings for improved consistency and maintainability. It also introduces new configurations for model stakeholder roles, allowing these roles to be properly selectable and mapped in the UI.

The ApprovalNode component updates conditions to check against FIELD_IDENTIFIERS, which clarifies references to user roles, stakeholder roles, and custom fields. The humanizeJsonLogic utility's outputs now consistently align with updated constants based on role assignments.

Finally, in the useQueryBuilder hook, FIELD_IDENTIFIERS are exported for consistent usage across components, with query fields adjusted to reflect new role identifier mappings. These changes enhance code maintainability by reducing hardcoded values and ensuring consistency in role-based operations across the system.



{{< include /releases/_how-to-upgrade.qmd >}}
