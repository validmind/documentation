---
title: "October 25, 2023"
---

## Release highlights

We've introduced a new guide for modifying configurations and parameters within the {{< var vm.developer >}}, and new features to the {{< var vm.platform >}} that enable you to remove blocks of content from documentation and work with your settings more effectively.

### {{< var validmind.developer >}} (v1.22.0)

<!--- NR the next three notebooks have been commented out as they are not ready to be released.--->
<!---[SC-2284] Support regression models by @AnilSorathiya in [#259](https://github.com/validmind/developer-framework/pull/259)
#### Support for regression models

The {{< var validmind.developer >}} has added support for regression models. The updates include:

- A new demo notebook featuring a simple regression model
- Utilization of the standard California housing tabular dataset for the demo
- Addition of new tests, `Errors` and `R-squared`, to support regression model evaluation
- Use of existing tabular dataset tests for data validation

[Try it ...](/notebooks/code_samples/regression/quickstart_regression_full_suite.ipynb)--->

<!---[SC-2411] Clustering models support by @AnilSorathiya in [#271](https://github.com/validmind/developer-framework/pull/271)
#### Support for clustering models

The {{< var validmind.developer >}} has added support for clustering models. The updates include:

- A new demo notebook for a simple clustering model 
- Utilization of a standard digits dataset for the demo
- Addition of new tests to support clustering model evaluation

[Try it ...](/notebooks/code_samples/clustering/quickstart_custer_demo.ipynb)--->

<!---John6797/sc 2416/embeddings models support by @johnwalz97 in [#272](https://github.com/validmind/developer-framework/pull/272)
#### Support for embeddings models

We added initial support for text embeddings models in the {{< var validmind.developer >}} which enables you to create, use and test a BERT embeddings model utilizing the Hugging Face library. The updates include:

- A new demo notebook
- A new folder in `model_validation` tests for embeddings, along with initial versions of tests for text embedding models
- Support for `feature_extraction` tasks in the Hugging Face model wrapper of the {{< var validmind.developer >}}
- Updated software dependencies

[Try it ...](/notebooks/POC/bert-embeddings-model-ow-poc.ipynb)--->

<!---[SC-2236] Demo notebook for changing config/parameters by @AnilSorathiya in [#251](https://github.com/validmind/developer-framework/pull/251)--->
<!-- #### New notebook to demonstrate how to change configuration parameters

This notebook serves as a guide for modifying configuration and parameters within the {{< var validmind.developer >}}. It includes the following features:

- A preview template allowing users to select a sample test for configuration
- Instructions on how to pass custom configurations to `run_documentation_tests()`
- An option to run documentation tests focused on a specific section, avoiding the need to run the entire template

[Try it ...](/notebooks/how_to/configure_test_parameters.ipynb) -->

### {{< var validmind.platform >}} (v1.8.0)

<!---Feature: Remove block from documentation by @gtagle in [#467](https://github.com/validmind/frontend/pull/467)--->
#### Remove blocks from documentation

You can now remove blocks of text or test-related content from model documentation in the editor of the {{< var vm.platform >}}. This new feature gives you more control over your documentation and enables you to remove content that is no longer needed.

![](/guide/model-documentation/remove-test-driven-block.gif){width=50% fig-alt="A gif showcasing the process of removing a test-driven block"}

To remove text blocks and test-driven blocks from model documentation, you first select the block you want to remove and click {{< fa trash-can >}}, either in the text-block's toolbar or in the test-driven's block single-button toolbar:

[Try it ...](/guide/model-documentation/work-with-content-blocks.qmd#remove-content-blocks)

<!--- NR I don't think we have any user-facing docs for this feature ... 
  [Try it ...]()--->

<!---feat: Add settings landing page by @wkm97 in [#466](https://github.com/validmind/frontend/pull/466)--->
#### New Settings landing page

A new **{{< fa gear >}} Settings** landing page now organizes more of your settings for the ValidMind platform in one convenient place.

![New Settings page](settings-page.png){fig-alt="A screenshot showing the new platform UI settings page"}

From this page you can manage:

- Your organization, including the name and the API and secret key you use to connect to the ValidMind platform.
- The documentation templates that standardize the documentation table of contents for your projects and configure the required validation tests for specific model use cases.
- Workflows that determine the statuses of your model and how it transitions through your model risk management process according to your requirements.

[Try it ...](https://app.dev.vm.validmind.ai/settings/)

## Enhancements

<!---[SC-2346] Rouge and Bert score tests should show average scores by @juanmleng in [#263](https://github.com/validmind/developer-framework/pull/263)--->
- **Rouge and Bert Score tests now show average scores**: Introduced `RougeMetricsAggregate` and `BertScoreAggregate` to offer a high-level overview of model performance across a large number of text rows. These tests complement the detailed row-by-row analysis provided by `RougeMetrics` and `BertScore`.
<!--- NR this notebook is not currently included in our docs site:
Tested these metrics running `foundational_models_summarization_high_code.ipynb` --->

<!---[SC-2143] Tests for safety toxicity and bias in text summarization by @juanmleng in [#258](https://github.com/validmind/developer-framework/pull/258)--->
- **Added tests for safety toxicity and bias in text summarization**. We introduced several new tests to evaluate safety and bias risks in text summarization:
  
  - `ToxicityScore`: Measures safety risk
  - `ToxicityHistogram`: Provides a distribution of safety risk scores
  - `RegardScore`: Evaluates bias risk
  - `RegardHistogram`: Shows distribution of bias risk scores
<!--- NR this notebook is not currently included in our docs site:
To test these metrics, see notebook `foundation_models_summarization_bias.ipynb`---> 

## Bug fixes

<!---[SC-2303] Shap test issue resolved by @juanmleng in [#262](https://github.com/validmind/developer-framework/pull/262)--->
- **Shap test issue resolved**. - We set `matplotlib` to version `3.7.x` in `pyproject.toml` to fix an incompatibility with the latest `matplotlib` version (`3.8.0`). This incompatibility was causing SHAP plot errors. We will keep track of `matplotlib` releases for future updates. Once fixed, we will consider updating the version. 

## Documentation updates

<!---Platform overview rewrite by @nrichers in [#138](https://github.com/validmind/documentation/pull/138)--->
- **Platform overview rewrite**. We expanded our platform overview to provide more background information about what ValidMind offers and how we enable you to comply with policies and regulations such as SR 11-7 and SS1/23. [Try it ...](/about/overview.qmd)

<!---QuickStart updates for the closed beta by @nrichers in [#141](https://github.com/validmind/documentation/pull/141)--->
- **QuickStart updates for the closed beta**. We updated the QuickStart section of our documentation to reflect recent UI and sign-up flow changes. [Try it ...](/get-started/quickstart.qmd)

<!---John6797/sc 2211/update test descriptions by @johnwalz97 in [#244](https://github.com/validmind/developer-framework/pull/244)
- **John6797/sc 2211/update test descriptions**. Add full markdown descriptions to all tests
--->

<!--- Rework how we include notebooks in our docs site [#139](https://github.com/validmind/documentation/pull/139)--->
- **Improved handling of Jupyter Notebooks for code samples and testing how-to content**. We now programmatically embed these notebooks in our documentation site and generate a downloadable `notebooks.zip` file with all notebooks and supporting datasets. Try it:
  
  - [Code samples](/developer/samples-jupyter-notebooks.qmd)
  - [Tests and test suites](/developer/model-testing/testing-overview.qmd)

## How to upgrade

To access the latest version of the [{{< var validmind.platform >}}](http://app.prod.validmind.ai/), reload your browser tab.

To upgrade the {{< var validmind.developer >}}:

- [Using JupyterHub](/get-started/developer/try-with-jupyterhub.qmd): Reload your browser tab and re-run the `%pip install --upgrade validmind` cell.

- [In your own developer environment](/developer/model-documentation/install-and-initialize-client-library.qmd): Restart your notebook and re-run:
  
  ```python
  %pip install validmind
  ```