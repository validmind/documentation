---
title: "September 27, 2023"
date: 2023-09-27
aliases: 
  - /releases/2023-sep-27/highlights.html
listing:
  id: beta-announcement
  type: grid
  grid-columns: 1
  # image-height: 100%
  contents:
    - path: https://validmind.com/announcing-validminds-closed-beta-coming-soon
      title: "Read the announcement"
      description: "Interested in our LLM support? Large language model support and more will be available in our closed beta."
  fields: [title, description, reading-time]
---

In this release, we've added support for large language models (LLMs) to enhance the capabilities of the {{< var validmind.developer >}} in preparation for the closed beta,[^1] along with a number of new demo notebooks that you can try out. 

Other enhancements provide improvements for the developer experience and with our documentation site.

::: {.highlights}

## {{< fa bullhorn >}} Release highlights

### {{< var validmind.developer >}} (v1.19.0)

<!---John6797/sc 2062/sentiment analysis demo notebook should support by @johnwalz97 in [#224](https://github.com/validmind/validmind-python/pull/224)--->
#### Large language model (LLM) support

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
We added initial support for large language models (LLMs) in {{< var vm.product >}} via the new `FoundationModel` class. 

- You can now create an instance of a `FoundationModel` and specify `predict_fn` and a `prompt`, and pass that into any test suite, for example.
- The `predict_fn` must be defined by the user and implements the logic for calling the Foundation LLM, usually via the {{< var validmind.api >}}.

:::

::: {.w-30-ns}
:::{#beta-announcement}
:::

:::

::::

**To demonstrate the capabilities of LLM support, this release also includes new demo notebooks:**

<!---John6797/sc 2088/implement prompt validation tests poc by @johnwalz97 in [#232](https://github.com/validmind/validmind-python/pull/232)--->
**Prompt validation demo notebook for LLMs**

As a proof of concept, we added initial native prompt validation tests to the {{< var vm.developer >}}, including a notebook and simple template to test out these metrics on a sentiment analysis LLM model we built.

**Text summarization model demo notebook for LLMs**

We added a new notebook in the {{< var vm.developer >}} that includes the financial news dataset, initializes a Hugging Face summarization model using the `init_model` interface, implements relevant metrics for testing, and demonstrates how to run a _text summarization metrics_ test suite for an LLM instructed as a financial news summarizer.

:::: {.flex .flex-wrap .justify-around}

::: {.w-50-ns}
[Prompt validation for large language models (LLMs)](/notebooks/use_cases/nlp_and_llm/prompt_validation_demo.ipynb){.button .button-green}

:::

::: {.w-50-ns}
[Automate news summarization using LLMs](/notebooks/use_cases/nlp_and_llm/llm_summarization_demo.ipynb){.button .button-green}

:::

::::

<!---SC-2061: Support hugging face models by @AnilSorathiya in [#222](https://github.com/validmind/validmind-python/pull/222)--->
#### Support for Hugging Face models

{{< var vm.product >}} can now validate pre-trained models from the HuggingFace Hub, including any language model compatible with the HF transformers API. 

To illustrate this new feature, we have included a **financial news sentiment analysis demo** that runs documentation tests for a Hugging Face model with text classification using the `financial_phrasebank`:[^2]

::: {.tc}
[Sentiment analysis of financial data using Hugging Face NLP models](/notebooks/use_cases/nlp_and_llm/hugging_face_integration_demo.ipynb){.button .button-green}
:::

<!---[SC-1956] Credit risk scorecard demo notebooks by @juanmleng in [#223](https://github.com/validmind/validmind-python/pull/223)--->
<!--- NR As per Andres, these notebooks need to be updated before they can be shared again
### New credit risk scorecard demo notebooks 

We added new demo notebooks and supporting files for very popular financial services model use cases:

  - Developing credit risk scorecards. [Try it ...](/notebooks/code_samples/probability_of_default/credit_risk_scorecard_development_demo.ipynb)
  - Validating credit risk scorecards. [Try it ...](/notebooks/code_samples/probability_of_default/credit_risk_scorecard_validation_demo.ipynb)
 --->

#### A better developer experience with `run_test()`

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
We added a new `run_test()` helper function that streamlines running tests for you. This function allows executing any individual test independent of a test suite or a documentation template. A one-line command can execute a test, making it easier to run tests with various parameters and options. 

:::

::: {.w-20-ns}
[run_test()](/validmind/validmind/tests.qmd#run_test){.button target="_blank" .button-green}

:::

::::

::: {.column-margin}
**For example:**

```python
run_test("ClassImbalance", dataset=dataset, params=params, send=True)
```
:::

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
<!---Add new `run_test` helper function by @cachafla in [#243](https://github.com/validmind/validmind-python/pull/243)--->
We also updated the quickstart notebook to have a consistent experience.

:::

::: {.w-40-ns}
[Quickstart for model documentation](/notebooks/quickstart/quickstart_model_documentation.ipynb){.button .button-green}

:::

::::

This notebook:

- Now runs [`vm.preview_template()`](/validmind/validmind.qmd#preview_template) after initializing {{< var vm.product >}}
- Now runs [`vm.run_documentation_tests()`](/validmind/validmind.qmd#run_documentation_tests) instead of running a test suite that is not connected to the template

##### Example usage for `run_test`

Discover existing tests by calling `list_tests()` or `describe_test()`:

**`list_tests()`:**

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
![Examples for `list_tests()`](list-tests.png){fig-alt="A screenshot showing examples for `list_tests()`" .screenshot}

:::

::: {.w-20-ns .tc}
[list_tests()](/validmind/validmind/tests.qmd#list_tests){.button target="_blank" .button-green}

:::

::::

**`describe_test()`:**

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
![Examples for `describe_test()`](describe-test.png){fig-alt="A screenshot showing examples for `describe_test()`" .screenshot}

:::

::: {.w-20-ns .tc}
[describe_test()](/validmind/validmind/tests.qmd#describe_test){.button target="_blank" .button-green}

:::

::::


View the tests associated with a documentation template by running **`preview_template()`:**

:::: {.flex .flex-wrap .justify-around}

::: {.w-50-ns}
![Examples for `preview_template()`](preview-template.png){fig-alt="A screenshot showing examples for `preview_template()`" .screenshot}

::: {.tc}
[preview_template()](/validmind/validmind.qmd#preview_template){.button target="_blank" .button-green}
:::

:::

::: {.w-40-ns}

 Using the test ID, run a given test and pass in additional configuration parameters and inputs:

```python
# No params
test_results = vm.tests.run_test(
    "class_imbalance",
    dataset=vm_dataset
)

# Custom params
test_results = vm.tests.run_test(
    "class_imbalance",
    params={"min_percent_threshold": 30},
    dataset=vm_dataset
)
```

:::

::::

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
**Output:**

![Example output `test_results`](test-results.png){fig-alt="A screenshot showing the example output `test_results`" .screenshot}

:::

::: {.w-30-ns}

Send the results of the test to {{< var vm.product >}} by calling **`.log()`:**

```python
test_results.log()
```

::: {.tc}
[log()](/validmind/validmind/vm_models.qmd#log){.button target="_blank" .button-green} 
:::

:::

::::

<!---
### {{< var validmind.platform >}} (v1.6.0)
--->

:::


## Enhancements

### {{< var validmind.developer >}} (v1.19.0)

<!---[SC-2108] Multi-class tests should report metrics for individual classes and many other changes by @cachafla in [#228](https://github.com/validmind/validmind-python/pull/228)--->

#### Multi-class test improvements

We made a number of changes to tests to improve the developer experience:

:::: {.flex .flex-wrap .justify-around}

::: {.w-70-ns}
- A new `fail_fast` argument can be passed to `run_test_plan()`, `run_test_suite()` and `run_documentation_tests()`, used to fail and raise an exception on the first error encountered. This change is useful for debugging.
- `ClassifierPerformance` test now determines if you are testing a binary or a multi-class model. When testing a multi-class model, we now report additional per-class, macro and weighted average tests.
- Fixed F1 score test so it works correctly for binary and multi-class models.

:::

::: {.w-30-ns}
[run_test_suite()](/validmind/validmind.qmd#run_test_suite){.button target="_blank"}

[run_documentation_tests()](/validmind/validmind.qmd#run_documentation_tests){.button target="_blank"}

[ClassifierPerformance](/tests/model_validation/sklearn/ClassifierPerformance.md){.button}

[MinimumF1Score](/tests/model_validation/sklearn/MinimumF1Score.md){.button}

:::

::::

<!---SC-2042: Add multi-class classification support by @AnilSorathiya in [#221](https://github.com/validmind/validmind-python/pull/221)--->
#### Added multi-class classification support

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
- The {{< var vm.developer >}} now supports a multi-class version of some the existing tests, such as confusion matrix, accuracy, precision, recall, and more. 
- Also, the dataset and model interfaces now support dealing with multiple targets.

:::

::: {.w-20-ns}
[Test descriptions](/developer/model-testing/test-descriptions.qmd){.button}

:::

::::

<!---SC-2100: Implement classification model comparison tests by @AnilSorathiya in [#231](https://github.com/validmind/validmind-python/pull/231)--->
#### Implemented classification model comparison tests

- Added a model performance comparison test for classification tasks. 
- The test includes metrics such as `accuracy`, `F1`, `precision`, `recall`, and `roc_auc` score.

::: {.column-margin}
[Developer reference](/validmind/validmind.qmd){.button target="_blank"}

:::

<!---John6797/sc 1790/framework should track additional test metadata by @johnwalz97 in [#239](https://github.com/validmind/validmind-python/pull/239)--->
#### Track additional test metadata

- Added a `metadata` property to every {{< var vm.product >}} test class. 
- The `metadata` property includes a `task_types` field and a `tags` field which both serve to categorize the tests based on what data and model types they work with, what category of test they fall into, and more.

<!---feat: [sc-2196] Users should be able to filter tests by task type and tags by @johnwalz97 in [#241](https://github.com/validmind/validmind-python/pull/241)--->
#### Filter tests by task type and tags
:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
We added a new search feature to the `validmind.tests.list_tests` function to allow for better test discoverability. 
:::

::: {.w-20-ns}
[list_tests()](/validmind/validmind/tests.qmd#list_tests){.button target="_blank"}

:::

::::

**The `list_tests` function in the `tests` module now supports the following arguments:**
   
   - `filter`: If set, will match tests by ID, task_types or tags using a combination of substring and fuzzy string matching. Defaults to `None`. 
   - `task`: If set, will further narrow matching tests (assuming `filter` has been passed) by exact matching the `task` to the test's `task_type` metadata. Defaults to `None`. 
   - `tags`: If a list is passed, will again narrow the matched tests by exact matching on tags. Defaults to `None`. 

## Documentation

### User guide updates

<!---User journey improvements by @nrichers in [#128](https://github.com/validmind/documentation/pull/128)--->
#### User journey improvements

We enhanced the architecture and content of our external docs site to make the user journey more efficient for model developers and model validators who are new to our products:

   - **Reworked the "Get Started" section** to include more conceptual information and an overview of the high-level workflows.
   - **Revised the "What is the {{< var validmind.developer >}}?"** section to provide an end-to-end overview of the workflow that model developers should follow as they adopt the {{< var vm.developer >}}.  

:::: {.flex .flex-wrap .justify-around}

::: {.w-30-ns .tc}
[Get started](/get-started/get-started.qmd){.button}

:::

::: {.w-50-ns .tc}
[{{< var validmind.developer >}}](/developer/validmind-library.qmd){.button}

:::

::::

### Site enhancements

<!---Docs site improvements by @nrichers in [#132](https://github.com/validmind/documentation/pull/132)--->
#### Docs site improvements

We made a number of incremental improvements to our user guide:

   - **New dropdown for the {{< var validmind.developer >}}** that gives faster access to the most important bits, such as our code samples and the reference documentation — Click on **{{< fa cube >}} Developers** in the top navigation bar to see it in action!
   - **Publication date** for each page that reflects the last time the source file was touched.
   - **Previous and next topic footers** for related topics that make it easier to keep reading.
   - **Expanded overview** for key {{< var vm.product >}} concepts with some additional information.
   - **Lighter background for diagrams** that improves legibility.

{{< include /releases/_how-to-upgrade.qmd >}}



<!-- FOOTNOTES -->

[^1]: [Announcing Our Closed Beta for Large Language Model Testing — Coming Soon!](https://validmind.com/announcing-validminds-closed-beta-coming-soon/?)

[^2]: [**Hugging Face:** takala/financial_phrasebank](https://huggingface.co/datasets/takala/financial_phrasebank)

