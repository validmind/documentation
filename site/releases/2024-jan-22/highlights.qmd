---
title: "January 22, 2024"
keywords: "release notes, ai risk, model risk management, ValidMind"
---

## Release highlights


### ValidMind Developer Framework (v1.25.3)

## Enhancements

<!---John6797/sc 2524/support for azure open ai api keys dev framework by @johnwalz97 in [#300](https://github.com/validmind/developer-framework/pull/300)--->
- **John6797/sc 2524/support for azure open ai api keys dev framework**. - Added support for running LLM-powered tests with the Azure OpenAI Service. In order to use Azure Open AI endpoints users only need to set the following environment variables before calling `vm.init()`:
  - AZURE_OPENAI_KEY
  - AZURE_OPENAI_ENDPOINT
  - AZURE_OPENAI_MODEL
  - AZURE_OPENAI_VERSION (optional)

More information can be found here: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints#authentication


<!---[SC-2438] Support for adding feature columns parameter to init dataset by @juanmleng in [#279](https://github.com/validmind/developer-framework/pull/279)--->
- **[SC-2438] Support for adding feature columns parameter to init dataset**. 
At present, `init_dataset` loads the entire dataset, using all available features for prediction tasks. While this works well, this imposes a constraint when implementing metrics that report by segment, and it becomes inefficient with large datasets containing numerous features, where only a small subset is actually pertinent for prediction.

This PR implements the addition of a `feature_columns` argument, accepting a list of feature names for prediction. The function would then internally filter the dataset to keep only these specified features for prediction-related tasks. The rest of the dataset,  would not be used for predictions and can be used for other tasks such segmentation. 

Example of use: 

```
feature_columns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary']

vm_train_ds = vm.init_dataset(
    dataset=train_df,
    target_column=demo_dataset.target_column,
    feature_columns=feature_columns
)
```

**Changes**

- Support for `init_dataset` to take a `feature_columns` parameter required changes in these files: 
   - `client.py `
   - `dataset.py`
   - `huggingface.py`
   - `pytorch.py`
   - `sklearn.py`
   
- Two new metrics have been implemented to report by segment and show the use of `feature_columns`: 
   - `MissingValuesBySegment`
   - `DescriptiveStatisticsBySegment`


**Testing**

Notebook `configure_dataset_featutes.ipynb` has been added to show:
- how to use the `feature_columns` parameter when initizalizing validmind datasets and model objects 
- how `feature_columns` can be used to report by segment 




<!---R package plus corresponding RMarkdown updates for tighter integration by @erichare in [#265](https://github.com/validmind/developer-framework/pull/265)--->
- **R package plus corresponding RMarkdown updates for tighter integration**. Coming soon


<!---[SC-2707] Support for multiple sections in run_documentation_tests() by @juanmleng in [#307](https://github.com/validmind/developer-framework/pull/307)--->
- **[SC-2707] Support for multiple sections in run_documentation_tests()**. - Modify `client.py` to allow users to run multiple sections using `run_documentation_tests()`
- The changes are compatible with the current use. 
- Running one section: 
```
results = vm.run_documentation_tests(
    section="data_preparation",
    dataset=vm_dataset,
)
```

- Running multiple sections: 
```
results = vm.run_documentation_tests(
    section=["data_preparation", "model_development"],
    dataset=vm_dataset,
)
```

How to notebook: 
-` how_to/run_documentation_sections.ipynb`


<!---[SC 2527] `init_dataset` should not attempt to preprocess the input dataset by @AnilSorathiya in [#297](https://github.com/validmind/developer-framework/pull/297)--->
- **[SC 2527] `init_dataset` should not attempt to preprocess the input dataset**. - Improved performance of `init_dataset`
- Improved the output table format of `DatasetDescription` and `Duplicates`


<!---feat: [sc-2468] Allow arbitrary test context (V2) by @johnwalz97 in [#312](https://github.com/validmind/developer-framework/pull/312)--->
- **feat: [sc-2468] Allow arbitrary test context (V2)**. More work on formalizing and standardizing test inputs. The Developer Framework now supports passing arbitrary/custom inputs along with standard inputs (`dataset`, `model` and `models`) as an `inputs` dictionary when running individual tests or test suites.

In addition, standard inputs are now considered deprecated. The legacy behavior will continue to be supported before it's fully removed.

### Legacy syntax for passing inputs

```python
full_suite = vm.run_documentation_tests(
    dataset=vm_dataset,
    model=vm_model
)
```

### New syntax for passing inputs

```python
full_suite = vm.run_documentation_tests(
    inputs={
        "dataset": vm_dataset,
        "model": vm_model,
    },
)
```

<!---[SC-2628] `init_model` improvements  by @AnilSorathiya in [#304](https://github.com/validmind/developer-framework/pull/304)--->
- **[SC-2628] `init_model` improvements **. Users can now pass a dataset with pre-computed model predictions. By default our developer framework will compute model predictions when calling `init_model`. In the following example we will try to call the `predict()` method of the model for the training and test datasets:

```python
vm_model = vm.init_model(
    model,
    train_ds=vm_train_ds,
    test_ds=vm_test_ds,
)
```

This can pose an issue when datasets are too big: `init_model` might take too long to compute. In general, users should be able to pass datasets that have a column with pre-computed predictions:

|x1|x2|...|target_column|prediction_column|
|-|-|-|-|-|
|0.1|0.2|...|0|0|
|0.2|0.4|1...|1|1|

It is now possible to pass a `prediction_column` when initializing a new dataset. When `init_model` sees this, it won't generate model predictions at run time:

```python
vm.init_dataset(
     dataset=df,
     feature_columns=[...],
     target_column= ...,
     extra_columns={
         prediction_column: 'name-of-prediction-column',
    },
)
```

## Bug fixes

<!---Fix support for openai >=1.0 by @cachafla in [#311](https://github.com/validmind/developer-framework/pull/311)--->
- **Fix support for openai >=1.0**. - Updated LLM related demo notebooks to properly support `openai >=1.0.0`

## Deprecations

<!---[SC-2770] Update code samples and how to notebooks to use test inputs by @juanmleng in [#313](https://github.com/validmind/developer-framework/pull/313)--->
- **[SC-2770] Update code samples and how to notebooks to use test inputs**. Updated notebooks in `code_samples` and `how_to` to use `inputs={...}`. 

- New interface:
```
test_suite = vm.run_documentation_tests(
    inputs = {
        "dataset": vm_dataset,
        "model": vm_model,
    }
)
```
- Deprecated interface: 
```
test_suite = vm.run_documentation_tests(
        dataset = vm_dataset,
        model = vm_model,
)
```

<!---[SC-2771] Remove deprecated high level api methods by @juanmleng in [#310](https://github.com/validmind/developer-framework/pull/310)--->
- **[SC-2771] Remove deprecated high level api methods**. Remove `run_template` and `run_test_plan` from the developer framework.


<!---feat: [sc-2468] Allow arbitrary test context (V2) by @johnwalz97 in [#312](https://github.com/validmind/developer-framework/pull/312)--->
- **feat: [sc-2468] Allow arbitrary test context (V2)**. More work on formalizing and standardizing test inputs. The Developer Framework now supports passing arbitrary/custom inputs along with standard inputs (`dataset`, `model` and `models`) as an `inputs` dictionary when running individual tests or test suites.

In addition, standard inputs are now considered deprecated. The legacy behavior will continue to be supported before it's fully removed.

### Legacy syntax for passing inputs

```python
full_suite = vm.run_documentation_tests(
    dataset=vm_dataset,
    model=vm_model
)
```

### New syntax for passing inputs

```python
full_suite = vm.run_documentation_tests(
    inputs={
        "dataset": vm_dataset,
        "model": vm_model,
    },
)
```

## Documentation updates

<!---Improvements to run_documentation_sections notebook by @juanmleng in [#309](https://github.com/validmind/developer-framework/pull/309)--->
- **Improvements to run_documentation_sections notebook**. Add a bit more description to some sections in `run_documentation_sections.ipynb`


<!---[SC-2628] `init_model` improvements  by @AnilSorathiya in [#304](https://github.com/validmind/developer-framework/pull/304)--->
- **[SC-2628] `init_model` improvements **. Users can now pass a dataset with pre-computed model predictions. By default our developer framework will compute model predictions when calling `init_model`. In the following example we will try to call the `predict()` method of the model for the training and test datasets:

```python
vm_model = vm.init_model(
    model,
    train_ds=vm_train_ds,
    test_ds=vm_test_ds,
)
```

This can pose an issue when datasets are too big: `init_model` might take too long to compute. In general, users should be able to pass datasets that have a column with pre-computed predictions:

|x1|x2|...|target_column|prediction_column|
|-|-|-|-|-|
|0.1|0.2|...|0|0|
|0.2|0.4|1...|1|1|

It is now possible to pass a `prediction_column` when initializing a new dataset. When `init_model` sees this, it won't generate model predictions at run time:

```python
vm.init_dataset(
     dataset=df,
     feature_columns=[...],
     target_column= ...,
     extra_columns={
         prediction_column: 'name-of-prediction-column',
    },
)
```

## How to upgrade

To access the latest version of the [ValidMind Platform UI](http://app.prod.validmind.ai/), reload your browser tab.

To upgrade the ValidMind Developer Framework:

- [Using Jupyter Hub](../../guide/try-developer-framework-with-jupyterhub.qmd): reload your browser tab and re-run the `!pip install --upgrade validmind` cell.

- [Using Docker](../../guide/try-developer-framework-with-docker.qmd): pull the latest Docker image:
    
    ```jsx
    docker pull validmind/validmind-jupyter-demo:latest
    
    ```
    
- [In your own developer environment](../../guide/install-and-initialize-developer-framework.qmd): restart your notebook and re-run:
    
    ```python
    !pip install validmind
    ```