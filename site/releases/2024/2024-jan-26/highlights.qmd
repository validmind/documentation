---
title: "January 26, 2024"
date: 2024-01-26
aliases: 
  - /releases/2024-jan-26/highlights.html
---

This release includes numerous improvements to the {{< var vm.developer >}}, including new features for model and dataset initialization, easier testing, support for additional inputs and the Azure OpenAI API, updated notebooks, bug fixes, and much more.

::: {.highlights}

## {{< fa bullhorn >}} Release highlights

### {{< var validmind.developer >}} (v1.25.3)

<!---[SC-2628] `init_model` improvements  by @AnilSorathiya in [#304](https://github.com/validmind/validmind-library/pull/304)--->

#### Improvements to `init_model()`

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}

- When initializing a model, you can now pass a dataset with pre-computed model predictions if they are available. 
- By default, if no prediction column is specified when calling `init_model()`, the {{< var validmind.developer >}} will compute the model predictions on the entire dataset.

:::

::: {.w-20-ns}

[init_model()](/validmind/validmind.qmd#init_model){.button target="_blank" .button-green}

:::

::::

To illustrate how passing a dataset that includes a prediction column can help, consider the following example without a prediction column: 

:::: {.flex .flex-wrap .justify-around}

::: {.w-40-ns}

```python
vm_model = vm.init_model(
    model,
    train_ds=vm_train_ds,
    test_ds=vm_test_ds,
)
```

Internally, this example invokes the `predict()` method of the model for the training and test datasets when the model is initialized. 

This approach can be problematic with large datasets: `init_model` can simply take too long to compute.

:::

::: {.w-40-ns}

You can now avoid this issue by providing a dataset with a column containing pre-computed predictions, similar to the following example. 

If `init_model` detects this column, it will not generate model predictions at runtime. 

|x1|x2|...|target_column|prediction_column|
|-|-|-|-|-|
|0.1|0.2|...|0|0|
|0.2|0.4|1...|1|1|


:::

::::

::: {.column-margin}
Usage example with a prediction column: 

```python
vm.init_dataset(
     dataset=df,
     feature_columns=[...],
     target_column= ...,
     extra_columns={
         prediction_column: 'NAME-OF-PREDICTION-COLUMN',
    },
)
```
:::


<!---[SC-2438] Support for adding feature columns parameter to init dataset by @juanmleng in [#279](https://github.com/validmind/validmind-library/pull/279)--->
#### Improvements to `init_dataset()`

When initializing a dataset, the new `feature_columns` argument lets you specify a list of feature names for prediction to improve efficiency. Internally, the function filters the dataset to retain only these specified features for prediction-related tasks, leaving the remaining dataset available for other purposes, such as segmentation. 

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}

- This improvement replaces the existing behavior of `init_dataset()`, which loaded the entire dataset, incorporating all available features for prediction tasks. 
- While this approach worked well, it could impose limitations when generating segmented tests and proved somewhat inefficient with large datasets containing numerous features, of which only a small subset were relevant for prediction.

:::

::: {.w-20-ns .tc}
[init_dataset()](/validmind/validmind.qmd#init_dataset){.button target="_blank" .button-green}

:::

::::

:::: {.flex .flex-wrap .justify-around}

::: {.w-40-ns}

**Usage example:**

```python
feature_columns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary']

vm_train_ds = vm.init_dataset(
    dataset=train_df,
    target_column=demo_dataset.target_column,
    feature_columns=feature_columns
)
```

:::

::: {.w-50-ns}

A new notebook illustrates how you can configure these dataset features: 

- How to utilize the `feature_columns` parameter when initizalizing `validmind` datasets and model objects
- How `feature_columns` can be used to report by segment

::: {.tc}
[Configure dataset features](/notebooks/how_to/configure_dataset_features.ipynb){.button.button-green}
:::

:::

::::


<!---[SC-2707] Support for multiple sections in run_documentation_tests() by @juanmleng in [#307](https://github.com/validmind/validmind-library/pull/307)--->
#### Improvements to `run_documentation_tests()`

The `run_documentation_tests()` function, used to collect and run all the tests associated with a template, now supports running multiple sections at a time. 

:::: {.flex .flex-wrap .justify-around}

::: {.w-60-ns}
- This means that you no longer need to call the same function twice for two different sections, reducing the potential for errors and enabling you to use a single `config` object. 
- The previous behavior was to allow running only one section at a time. 
- This change maintains backward compatibility with the existing syntax, requiring no updates to your code.

:::

::: {.w-30-ns}

[run_documentation_tests()](/validmind/validmind.qmd#run_documentation_tests){.button target="_blank" .button-green}

:::

::::

:::: {.flex .flex-wrap .justify-around}

::: {.w-50-ns}
**Existing example usage:** Multiple function calls are needed to run multiple sections.

```python
full_suite = vm.run_documentation_tests(
    inputs = {
        ...
    },
    section="section_1",
    config={
        "validmind.tests.data_validation.ClassImbalance": ...
    } 
)

full_suite = vm.run_documentation_tests(
    inputs = {
        ...
    },
    section="section_2",
    config={
        "validmind.tests.data_validation.Duplicates": ...
    } 
)
```

:::

::: {.w-40-ns}
**New example usage:** A single function call runs multiple sections.

```python
full_suite = vm.run_documentation_tests(
    inputs = {
        ...
    },
    section=["section_1", "section_2"],
    config={
        "validmind.tests.data_validation.ClassImbalance": ...,
        "validmind.tests.data_validation.Duplicates": ...
    } 
)
```

::: {.tc}
[Run individual documentation sections](/notebooks/how_to/run_documentation_sections.ipynb){.button .button-green}
:::

:::

::::

<!---feat: [sc-2468] Allow arbitrary test context (V2) by @johnwalz97 in [#312](https://github.com/validmind/validmind-library/pull/312)--->
#### Support for custom inputs

The {{< var validmind.developer >}} now supports passing custom inputs as an `inputs` dictionary when running individual tests or test suites. This support replaces the standard inputs `dataset`, `model`, and `models`, which are now deprecated.[^1]

:::: {.flex .flex-wrap .justify-around}

::: {.w-50-ns}
**New recommended syntax for passing inputs:**

```python
test_suite = vm.run_documentation_tests(
    inputs={
        "dataset": vm_dataset,
        "model": vm_model,
    },
)
```

:::

::: {.w-40-ns}
<!---[SC-2770] Update code samples and how to notebooks to use test inputs by @juanmleng in [#313](https://github.com/validmind/validmind-library/pull/313)--->
To make it easier for you to adopt custom inputs, we have updated our how-to notebooks and code samples to use the new recommended syntax:

[Code samples](/developer/samples-jupyter-notebooks.qmd){.button .button-green}

:::

::::

:::


## Enhancements

### {{< var validmind.developer >}} (v1.25.3)

<!---John6797/sc 2524/support for azure open ai api keys dev framework by @johnwalz97 in [#300](https://github.com/validmind/validmind-library/pull/300)--->
#### Support for Azure OpenAI Service 

The {{< var validmind.developer >}} now supports running LLM-powered tests with the Azure OpenAI Service via API,[^2] in addition to the previously supported OpenAI API. 

To work with Azure OpenAI API endpoints, you need to set the following environment variables before calling `init()`:

:::: {.flex .flex-wrap .justify-around}

::: {.w-80-ns}
  - `AZURE_OPENAI_KEY`: API key for authentication
  - `AZURE_OPENAI_ENDPOINT`: API endpoint URL
  - `AZURE_OPENAI_MODEL`: Specifies the language model or service to use
  - `AZURE_OPENAI_VERSION` (optional): Allows specifying a specific version of the service if available

:::

::: {.w-20-ns}

[init()](/validmind/validmind.qmd#init){.button target="_blank"}

:::

::::

<!---R package plus corresponding RMarkdown updates for tighter integration by @erichare in [#265](https://github.com/validmind/validmind-library/pull/265)--->
<!---- **R package plus corresponding RMarkdown updates for tighter integration**. Coming soon--->

<!---[SC 2527] `init_dataset` should not attempt to preprocess the input dataset by @AnilSorathiya in [#297](https://github.com/validmind/validmind-library/pull/297)--->
<!---- **`init_dataset` should not attempt to preprocess the input dataset**.
  - Improved performance of `init_dataset`
  - Improved the output table format of `DatasetDescription` and `Duplicates`--->

## Bug fixes

### {{< var validmind.developer >}} (v1.25.3)

<!---Fix support for openai >=1.0 by @cachafla in [#311](https://github.com/validmind/validmind-library/pull/311)--->
####  Fixed support for OpenAI library >=1.0
- We have updated our demonstration notebooks for large language models (LLMs) to provide the correct support for `openai >= 1.0.0`. 
- Previously, some notebooks were using an older version of the OpenAI client API.

## Breaking changes

### {{< var validmind.developer >}} (v1.25.3)

<!---[SC-2771] Remove deprecated high level api methods by @juanmleng in [#310](https://github.com/validmind/validmind-library/pull/310)--->
#### Removed deprecated {{< var validmind.api >}} methods

The {{< var vm.api >}} methods `run_template` and `run_test_plan` had been deprecated previously. They have now been removed from the {{< var validmind.developer >}}.

You'll need to update your code to use the recommended high-level API methods:

- **`run_template` (removed):** Use [`run_documentation_tests()`](/validmind/validmind.qmd#run_documentation_tests){target="_blank"}
- **`run_test_plan` (removed):** Use [`run_test_suite()`](/validmind/validmind.qmd#run_test_suite){target="_blank"}


## Deprecations

### {{< var validmind.developer >}} (v1.25.3)

<!---feat: [sc-2468] Allow arbitrary test context (V2) by @johnwalz97 in [#312](https://github.com/validmind/validmind-library/pull/312)--->
#### <span id="custom-input-legacy">Standard inputs are deprecated</span> 

The {{< var validmind.developer >}} now supports passing custom inputs[^3] as an `inputs` dictionary when running individual tests or test suites. 

- As a result, the standard inputs `dataset`, `model`, and `models` are deprecated and might be removed in a future release.
- You should update your code to use the new, recommended syntax.

:::: {.flex .flex-wrap .justify-around}

::: {.w-40-ns}
**Deprecated legacy usage for passing inputs:**

```python
  test_suite = vm.run_documentation_tests(
      dataset=vm_dataset,
      model=vm_model
  )
```

:::

::: {.w-50-ns}
**New recommended usage for passing inputs:**

```python
  test_suite = vm.run_documentation_tests(
      inputs={
          "dataset": vm_dataset,
          "model": vm_model,
      },
  )
```

:::

::::

## Documentation

### User guide updates

#### Updated Python requirements

- We have updated our user guide to clarify the Python versions supported by the {{< var validmind.developer >}}. 
- We now support **Python {{< var version.python >}}**. 

{{< include /releases/_how-to-upgrade.qmd >}}

<!-- FOOTNOTES -->

[^1]: [Standard inputs are deprecated](#custom-input-legacy)

[^2]: To learn more about configuring Azure OpenAI Service, see [Authentication](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints#authentication) in the official Microsoft documentation.

[^3]: [Support for custom inputs](#support-for-custom-inputs)