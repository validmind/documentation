
---
title: Validmind Library Reference
toc-depth: 4
---

The ValidMind Library is a suite of developer tools and methods designed to automate the documentation and validation of your models.

Designed to be model agnostic, the ValidMind Library provides all the standard functionality without requiring you to rewrite any functions as long as your model is built in Python.

With a rich array of documentation tools and test suites, from documenting descriptions of your datasets to testing your models for weak spots and overfit areas, the ValidMind Library helps you automate model documentation by feeding the ValidMind Platform with documentation artifacts and test results.

To install the client library:

```bash
pip install validmind
```

To initialize the client library, paste the code snippet with the client integration details directly into your
development source code, replacing this example with your own:

```python
import validmind as vm

vm.init(
  api_host = "https://api.dev.vm.validmind.ai/api/v1/tracking/tracking",
  api_key = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  api_secret = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  project = "<project-identifier>"
)
```

After you have pasted the code snippet into your development source code and executed the code, the Python client
library will register with ValidMind. You can now use the ValidMind Library to document and test your models,
and to upload to the ValidMind Platform.


## api_client

**Kind**: module



ValidMind API client

Note that this takes advantage of the fact that python modules are singletons to store and share
the configuration and session across the entire project regardless of where the client is imported.








### aget_metadata

**Kind**: function
**Labels**: async


Gets a metadata object from ValidMind API.

Args:
    content_id (str): Unique content identifier for the metadata

Raises:
    Exception: If the API call fails

Returns:
    dict: Metadata object



async def aget_metadata(content_id: str) -> Dict[str, Any]





#### Parameters:

- **content_id** (`str`)



### alog_figure

**Kind**: function
**Labels**: async


Logs a figure

Args:
    figure (Figure): The Figure object wrapper

Raises:
    Exception: If the API call fails

Returns:
    dict: The response from the API



async def alog_figure(figure: Figure) -> Dict[str, Any]





#### Parameters:

- **figure** (`Figure`)



### alog_input

**Kind**: function
**Labels**: async


Logs input information - internal use for now (don't expose via public API)

Args:
    input_id (str): The input_id of the input
    type (str): The type of the input
    metadata (dict): The metadata of the input

Raises:
    Exception: If the API call fails

Returns:
    dict: The response from the API



async def alog_input(input_id: str, type: str, metadata: Dict) -> Dict[str, Any]





#### Parameters:

- **input_id** (`str`)

- **type** (`str`)

- **metadata** (`Dict`)



### alog_metadata

**Kind**: function
**Labels**: async


Logs free-form metadata to ValidMind API.

Args:
    content_id (str): Unique content identifier for the metadata
    text (str, optional): Free-form text to assign to the metadata. Defaults to None.
    _json (dict, optional): Free-form key-value pairs to assign to the metadata. Defaults to None.

Raises:
    Exception: If the API call fails

Returns:
    dict: The response from the API



async def alog_metadata(content_id: str, text: Optional = None, _json: Optional = None) -> Dict[str, Any]





#### Parameters:

- **content_id** (`str`)

- **text** (`Optional`) = `None`

- **_json** (`Optional`) = `None`



### alog_metric

**Kind**: function
**Labels**: async


See log_metric for details



async def alog_metric(key: str, value: Union, inputs: Optional = None, params: Optional = None, recorded_at: Optional = None)





#### Parameters:

- **key** (`str`)

- **value** (`Union`)

- **inputs** (`Optional`) = `None`

- **params** (`Optional`) = `None`

- **recorded_at** (`Optional`) = `None`



### alog_test_result

**Kind**: function
**Labels**: async


Logs test results information

This method will be called automatically from any function running tests but
can also be called directly if the user wants to run tests on their own.

Args:
    result (dict): A dictionary representing the test result
    section_id (str, optional): The section ID add a test driven block to the documentation
    position (int): The position in the section to add the test driven block

Raises:
    Exception: If the API call fails

Returns:
    dict: The response from the API



async def alog_test_result(result: Dict, section_id: str = None, position: int = None) -> Dict[str, Any]





#### Parameters:

- **result** (`Dict`)

- **section_id** (`str`) = `None`

- **position** (`int`) = `None`



### get_ai_key

**Kind**: function


Calls the api to get an api key for our LLM proxy



def get_ai_key() -> Dict[str, Any]






### get_api_host

**Kind**: function




def get_api_host() -> Optional[str]






### get_api_model

**Kind**: function




def get_api_model() -> Optional[str]






### init

**Kind**: function


Initializes the API client instances and calls the /ping endpoint to ensure
the provided credentials are valid and we can connect to the ValidMind API.

If the API key and secret are not provided, the client will attempt to
retrieve them from the environment variables `VM_API_KEY` and `VM_API_SECRET`.

Args:
    project (str, optional): The project CUID. Alias for model. Defaults to None. [DEPRECATED]
    model (str, optional): The model CUID. Defaults to None.
    api_key (str, optional): The API key. Defaults to None.
    api_secret (str, optional): The API secret. Defaults to None.
    api_host (str, optional): The API host. Defaults to None.
    monitoring (bool): The ongoing monitoring flag. Defaults to False.

Raises:
    ValueError: If the API key and secret are not provided



def init(project: Optional = None, api_key: Optional = None, api_secret: Optional = None, api_host: Optional = None, model: Optional = None, monitoring: bool = False)





#### Parameters:

- **project** (`Optional`) = `None`

- **api_key** (`Optional`) = `None`

- **api_secret** (`Optional`) = `None`

- **api_host** (`Optional`) = `None`

- **model** (`Optional`) = `None`

- **monitoring** (`bool`) = `False`



### log_input

**Kind**: function




def log_input(input_id: str, type: str, metadata: Dict) -> Dict[str, Any]





#### Parameters:

- **input_id** (`str`)

- **type** (`str`)

- **metadata** (`Dict`)



### log_metric

**Kind**: function


Logs a unit metric

Unit metrics are key-value pairs where the key is the metric name and the value is
a scalar (int or float). These key-value pairs are associated with the currently
selected model (inventory model in the ValidMind Platform) and keys can be logged
to over time to create a history of the metric. On the ValidMind Platform, these metrics
will be used to create plots/visualizations for documentation and dashboards etc.

Args:
    key (str): The metric key
    value (float): The metric value
    inputs (list, optional): A list of input IDs that were used to compute the metric.
    params (dict, optional): Dictionary of parameters used to compute the metric.
    recorded_at (str, optional): The timestamp of the metric. Server will use
        current time if not provided.



def log_metric(key: str, value: float, inputs: Optional = None, params: Optional = None, recorded_at: Optional = None)





#### Parameters:

- **key** (`str`)

- **value** (`float`)

- **inputs** (`Optional`) = `None`

- **params** (`Optional`) = `None`

- **recorded_at** (`Optional`) = `None`



### logger

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### reload

**Kind**: function


Reconnect to the ValidMind API and reload the project configuration



def reload()









## client

**Kind**: module



Client interface for all data and model validation functions








### get_test_suite

**Kind**: function


Gets a TestSuite object for the current project or a specific test suite

This function provides an interface to retrieve the TestSuite instance for the
current project or a specific TestSuite instance identified by test_suite_id.
The project Test Suite will contain sections for every section in the project's
documentation template and these Test Suite Sections will contain all the tests
associated with that template section.

Args:
    test_suite_id (str, optional): The test suite name. If not passed, then the
        project's test suite will be returned. Defaults to None.
    section (str, optional): The section of the documentation template from which
        to retrieve the test suite. This only applies if test_suite_id is None.
        Defaults to None.
    args: Additional arguments to pass to the TestSuite
    kwargs: Additional keyword arguments to pass to the TestSuite



def get_test_suite(test_suite_id: str = None, section: str = None, args = (), kwargs = {}) -> TestSuite





#### Parameters:

- **test_suite_id** (`str`) = `None`

- **section** (`str`) = `None`

- **args** (*positional) = `()`

- **kwargs** (*keyword) = `{}`



### init_dataset

**Kind**: function


Initializes a VM Dataset, which can then be passed to other functions
that can perform additional analysis and tests on the data. This function
also ensures we are reading a valid dataset type.

The following dataset types are supported:
- Pandas DataFrame
- Polars DataFrame
- Numpy ndarray
- Torch TensorDataset

Args:
    dataset : dataset from various python libraries
    model (VMModel): ValidMind model object
    targets (vm.vm.DatasetTargets): A list of target variables
    target_column (str): The name of the target column in the dataset
    feature_columns (list): A list of names of feature columns in the dataset
    extra_columns (dictionary):  A dictionary containing the names of the
    prediction_column and group_by_columns in the dataset
    class_labels (dict): A list of class labels for classification problems
    type (str): The type of dataset (one of DATASET_TYPES)
    input_id (str): The input ID for the dataset (e.g. "my_dataset"). By default,
        this will be set to `dataset` but if you are passing this dataset as a
        test input using some other key than `dataset`, then you should set
        this to the same key.

Raises:
    ValueError: If the dataset type is not supported

Returns:
    vm.vm.Dataset: A VM Dataset instance



def init_dataset(dataset, model = None, index = None, index_name: str = None, date_time_index: bool = False, columns: list = None, text_column: str = None, target_column: str = None, feature_columns: list = None, extra_columns: dict = None, class_labels: dict = None, type: str = None, input_id: str = None, __log = True) -> VMDataset





#### Parameters:

- **dataset**

- **model** = `None`

- **index** = `None`

- **index_name** (`str`) = `None`

- **date_time_index** (`bool`) = `False`

- **columns** (`list`) = `None`

- **text_column** (`str`) = `None`

- **target_column** (`str`) = `None`

- **feature_columns** (`list`) = `None`

- **extra_columns** (`dict`) = `None`

- **class_labels** (`dict`) = `None`

- **type** (`str`) = `None`

- **input_id** (`str`) = `None`

- **__log** = `True`



### init_model

**Kind**: function


Initializes a VM Model, which can then be passed to other functions
that can perform additional analysis and tests on the data. This function
also ensures we are creating a model supported libraries.

Args:
    model: A trained model or VMModel instance
    input_id (str): The input ID for the model (e.g. "my_model"). By default,
        this will be set to `model` but if you are passing this model as a
        test input using some other key than `model`, then you should set
        this to the same key.
    attributes (dict): A dictionary of model attributes
    predict_fn (callable): A function that takes an input and returns a prediction
    **kwargs: Additional arguments to pass to the model

Raises:
    ValueError: If the model type is not supported

Returns:
    vm.VMModel: A VM Model instance



def init_model(model: object = None, input_id: str = 'model', attributes: dict = None, predict_fn: callable = None, __log = True, kwargs = {}) -> VMModel





#### Parameters:

- **model** (`object`) = `None`

- **input_id** (`str`) = `'model'`

- **attributes** (`dict`) = `None`

- **predict_fn** (`callable`) = `None`

- **__log** = `True`

- **kwargs** (*keyword) = `{}`



### init_r_model

**Kind**: function


Initializes a VM Model for an R model

R models must be saved to disk and the filetype depends on the model type...
Currently we support the following model types:

- LogisticRegression `glm` model in R: saved as an RDS file with `saveRDS`
- LinearRegression `lm` model in R: saved as an RDS file with `saveRDS`
- XGBClassifier: saved as a .json or .bin file with `xgb.save`
- XGBRegressor: saved as a .json or .bin file with `xgb.save`

LogisticRegression and LinearRegression models are converted to sklearn models by extracting
the coefficients and intercept from the R model. XGB models are loaded using the xgboost
since xgb models saved in .json or .bin format can be loaded directly with either Python or R

Args:
    model_path (str): The path to the R model saved as an RDS or XGB file
    model_type (str): The type of the model (one of R_MODEL_TYPES)

Returns:
    vm.vm.Model: A VM Model instance



def init_r_model(model_path: str, input_id: str = 'model') -> VMModel





#### Parameters:

- **model_path** (`str`)

- **input_id** (`str`) = `'model'`



### logger

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### preview_template

**Kind**: function


Preview the documentation template for the current project

This function will display the documentation template for the current project. If
the project has not been initialized, then an error will be raised.

Raises:
    ValueError: If the project has not been initialized



def preview_template()






### run_documentation_tests

**Kind**: function


Collect and run all the tests associated with a template

This function will analyze the current project's documentation template and collect
all the tests associated with it into a test suite. It will then run the test
suite, log the results to the ValidMind API, and display them to the user.

Args:
    section (str or list, optional): The section(s) to preview. Defaults to None.
    send (bool, optional): Whether to send the results to the ValidMind API. Defaults to True.
    fail_fast (bool, optional): Whether to stop running tests after the first failure. Defaults to False.
    inputs (dict, optional): A dictionary of test inputs to pass to the TestSuite
    config: A dictionary of test parameters to override the defaults
    **kwargs: backwards compatibility for passing in test inputs using keyword arguments

Returns:
    TestSuite or dict: The completed TestSuite instance or a dictionary of TestSuites if section is a list.

Raises:
    ValueError: If the project has not been initialized



def run_documentation_tests(section = None, send = True, fail_fast = False, inputs = None, config = None, kwargs = {})





#### Parameters:

- **section** = `None`

- **send** = `True`

- **fail_fast** = `False`

- **inputs** = `None`

- **config** = `None`

- **kwargs** (*keyword) = `{}`



### run_test_suite

**Kind**: function


High Level function for running a test suite

This function provides a high level interface for running a test suite. A test suite is
a collection of tests. This function will automatically find the correct test suite
class based on the test_suite_id, initialize each of the tests, and run them.

Args:
    test_suite_id (str): The test suite name (e.g. 'classifier_full_suite')
    config (dict, optional): A dictionary of parameters to pass to the tests in the
        test suite. Defaults to None.
    send (bool, optional): Whether to post the test results to the API. send=False
        is useful for testing. Defaults to True.
    fail_fast (bool, optional): Whether to stop running tests after the first failure. Defaults to False.
    inputs (dict, optional): A dictionary of test inputs to pass to the TestSuite e.g. `model`, `dataset`
        `models` etc. These inputs will be accessible by any test in the test suite. See the test
        documentation or `vm.describe_test()` for more details on the inputs required for each.
    **kwargs: backwards compatibility for passing in test inputs using keyword arguments

Raises:
    ValueError: If the test suite name is not found or if there is an error initializing the test suite

Returns:
    TestSuite: the TestSuite instance



def run_test_suite(test_suite_id, send = True, fail_fast = False, config = None, inputs = None, kwargs = {})





#### Parameters:

- **test_suite_id**

- **send** = `True`

- **fail_fast** = `False`

- **config** = `None`

- **inputs** = `None`

- **kwargs** (*keyword) = `{}`






## client_config

**Kind**: module



Central class to track configuration of the ValidMind Library
client against the ValidMind API








### ClientConfig

**Kind**: class
**Labels**: dataclass


Configuration class for the ValidMind API client. This is instantiated
when initializing the API client.








### client_config

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``







## datasets

**Kind**: module



Example datasets that can be used with the ValidMind Library.








### classification

**Kind**: module


Entrypoint for classification datasets.








### credit_risk

**Kind**: module


Entrypoint for credit risk datasets.








### nlp

**Kind**: module


Example datasets that can be used with the ValidMind Library.








### regression

**Kind**: module


Entrypoint for regression datasets











## errors

**Kind**: module



This module contains all the custom errors that are used in the ValidMind Library.

The following base errors are defined for others:
- BaseError
- APIRequestError








### APIRequestError

**Kind**: class


Generic error for API request errors that are not known.








### BaseError

**Kind**: class









### GetTestSuiteError

**Kind**: class


When the test suite could not be found.








### InitializeTestSuiteError

**Kind**: class


When the test suite was found but could not be initialized.








### InvalidAPICredentialsError

**Kind**: class









### InvalidContentIdPrefixError

**Kind**: class


When an invalid text content_id is sent to the API.








### InvalidInputError

**Kind**: class


When an invalid input object.








### InvalidMetricResultsError

**Kind**: class


When an invalid metric results object is sent to the API.








### InvalidProjectError

**Kind**: class









### InvalidRequestBodyError

**Kind**: class


When a POST/PUT request is made with an invalid request body.








### InvalidTestParametersError

**Kind**: class


When an invalid parameters for the test.








### InvalidTestResultsError

**Kind**: class


When an invalid test results object is sent to the API.








### InvalidTextObjectError

**Kind**: class


When an invalid Metadat (Text) object is sent to the API.








### InvalidValueFormatterError

**Kind**: class


When an invalid value formatter is provided when serializing results.








### InvalidXGBoostTrainedModelError

**Kind**: class


When an invalid XGBoost trained model is used when calling init_r_model.








### LoadTestError

**Kind**: class


Exception raised when an error occurs while loading a test








### MismatchingClassLabelsError

**Kind**: class


When the class labels found in the dataset don't match the provided target labels.








### MissingAPICredentialsError

**Kind**: class









### MissingCacheResultsArgumentsError

**Kind**: class


When the cache_results function is missing arguments.








### MissingClassLabelError

**Kind**: class


When the one or more class labels are missing from provided dataset targets.








### MissingDependencyError

**Kind**: class


When a required dependency is missing.








### MissingDocumentationTemplate

**Kind**: class


When the client config is missing the documentation template.








### MissingModelIdError

**Kind**: class









### MissingOrInvalidModelPredictFnError

**Kind**: class


When the pytorch model is missing a predict function or its predict
method does not have the expected arguments.








### MissingRExtrasError

**Kind**: class


When the R extras have not been installed.








### MissingRequiredTestInputError

**Kind**: class


When a required test context variable is missing.








### MissingTextContentIdError

**Kind**: class


When a Text object is sent to the API without a content_id.








### MissingTextContentsError

**Kind**: class


When a Text object is sent to the API without a "text" attribute.








### SkipTestError

**Kind**: class


Useful error to throw when a test cannot be executed.








### TestInputInvalidDatasetError

**Kind**: class


When an invalid dataset is used in a test context.








### UnsupportedColumnTypeError

**Kind**: class


When an unsupported column type is found on a dataset.








### UnsupportedDatasetError

**Kind**: class


When an unsupported dataset is used.








### UnsupportedFigureError

**Kind**: class


When an unsupported figure object is constructed.








### UnsupportedModelError

**Kind**: class


When an unsupported model is used.








### UnsupportedModelForSHAPError

**Kind**: class


When an unsupported model is used for SHAP importance.








### UnsupportedRModelError

**Kind**: class


When an unsupported R model is used.








### raise_api_error

**Kind**: function


Safely try to parse JSON from the response message in case the API
returns a non-JSON string or if the API returns a non-standard error



def raise_api_error(error_string)





#### Parameters:

- **error_string**



### should_raise_on_fail_fast

**Kind**: function


Determine whether an error should be raised when fail_fast is True.



def should_raise_on_fail_fast(error) -> bool





#### Parameters:

- **error**






## html_templates

**Kind**: module










### content_blocks

**Kind**: module












## input_registry

**Kind**: module



Central class to register inputs








### InputRegistry

**Kind**: class









### input_registry

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``







## logging

**Kind**: module



ValidMind logging module.








### get_logger

**Kind**: function


Get a logger for the given module name



def get_logger(name = 'validmind', log_level = None)





#### Parameters:

- **name** = `'validmind'`

- **log_level** = `None`



### init_sentry

**Kind**: function


Initialize Sentry SDK for sending logs back to ValidMind

This will usually only be called by the api_client module to initialize the
sentry connection after the user calls `validmind.init()`. This is because the DSN
and other config options will be returned by the API.

Args:
    config (dict): The config dictionary returned by the API
        - send_logs (bool): Whether to send logs to Sentry (gets removed)
        - dsn (str): The Sentry DSN
        ...: Other config options for Sentry



def init_sentry(server_config)





#### Parameters:

- **server_config**



### log_performance

**Kind**: function


Decorator to log the time it takes to run a function

Args:
    name (str, optional): The name of the function. Defaults to None.
    logger (logging.Logger, optional): The logger to use. Defaults to None.
    force (bool, optional): Whether to force logging even if env var is off

Returns:
    function: The decorated function



def log_performance(name = None, logger = None, force = False)





#### Parameters:

- **name** = `None`

- **logger** = `None`

- **force** = `False`



### log_performance_async

**Kind**: function
**Labels**: async


Decorator to log the time it takes to run an async function

Args:
    func (function): The function to decorate
    name (str, optional): The name of the function. Defaults to None.
    logger (logging.Logger, optional): The logger to use. Defaults to None.
    force (bool, optional): Whether to force logging even if env var is off

Returns:
    function: The decorated function



async def log_performance_async(func, name = None, logger = None, force = False)





#### Parameters:

- **func**

- **name** = `None`

- **logger** = `None`

- **force** = `False`



### send_single_error

**Kind**: function


Send a single error to Sentry

Args:
    error (Exception): The exception to send



def send_single_error(error: Exception)





#### Parameters:

- **error** (`Exception`)






## models

**Kind**: module










### foundation

**Kind**: module









### function

**Kind**: module









### huggingface

**Kind**: module









### metadata

**Kind**: module









### pipeline

**Kind**: module









### pytorch

**Kind**: module









### r_model

**Kind**: module









### sklearn

**Kind**: module












## template

**Kind**: module










### CONTENT_TYPE_MAP

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### get_template_test_suite

**Kind**: function


Get a TestSuite instance containing all tests in a template

This function will collect all tests used in a template into a dynamically-created
TestSuite object

Args:
    template: A valid flat template
    section: The section of the template to run (if not provided, run all sections)

Returns:
    The TestSuite instance



def get_template_test_suite(template, section = None)





#### Parameters:

- **template**

- **section** = `None`



### logger

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### preview_template

**Kind**: function


Preview a template in Jupyter Notebook

Args:
    template (dict): The template to preview



def preview_template(template)





#### Parameters:

- **template**






## test_suites

**Kind**: module



Entrypoint for test suites.








### classifier

**Kind**: module


Test suites for sklearn-compatible classifier models

Ideal setup is to have the API client to read a
custom test suite from the project's configuration








### cluster

**Kind**: module


Test suites for sklearn-compatible clustering models

Ideal setup is to have the API client to read a
custom test suite from the project's configuration








### core_test_suites

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### custom_test_suites

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### describe_suite

**Kind**: function


Describes a Test Suite by ID

Args:
    test_suite_id: Test Suite ID
    verbose: If True, describe all plans and tests in the Test Suite

Returns:
    pandas.DataFrame: A formatted table with the Test Suite description



def describe_suite(test_suite_id: str, verbose = False)





#### Parameters:

- **test_suite_id** (`str`)

- **verbose** = `False`



### describe_test_suite

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### embeddings

**Kind**: module


Test suites for embeddings models

Ideal setup is to have the API client to read a
custom test suite from the project's configuration








### get_by_id

**Kind**: function


Returns the test suite by ID



def get_by_id(test_suite_id: str)





#### Parameters:

- **test_suite_id** (`str`)



### list_suites

**Kind**: function


Returns a list of all available test suites



def list_suites(pretty: bool = True)





#### Parameters:

- **pretty** (`bool`) = `True`



### llm

**Kind**: module


Test suites for LLMs








### logger

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### nlp

**Kind**: module


Test suites for NLP models








### parameters_optimization

**Kind**: module


Test suites for sklearn-compatible hyper parameters tunning

Ideal setup is to have the API client to read a
custom test suite from the project's configuration








### register_test_suite

**Kind**: function


Registers a custom test suite



def register_test_suite(suite_id: str, suite: TestSuite)





#### Parameters:

- **suite_id** (`str`)

- **suite** (`TestSuite`)



### regression

**Kind**: module









### statsmodels_timeseries

**Kind**: module


Time Series Test Suites from statsmodels








### summarization

**Kind**: module


Test suites for llm summarization models








### tabular_datasets

**Kind**: module


Test suites for tabular datasets








### text_data

**Kind**: module


Test suites for text datasets








### time_series

**Kind**: module


Time Series Test Suites











## tests

**Kind**: module



ValidMind Tests Module








### comparison

**Kind**: module









### data_validation

**Kind**: module









### decorator

**Kind**: module


Decorators for creating and registering tests with the ValidMind Library.








### load

**Kind**: module


Module for listing and loading tests.








### logger

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### model_validation

**Kind**: module









### output

**Kind**: module









### prompt_validation

**Kind**: module









### register_test_provider

**Kind**: function


Register an external test provider

Args:
    namespace (str): The namespace of the test provider
    test_provider (TestProvider): The test provider



def register_test_provider(namespace: str, test_provider: TestProvider) -> None





#### Parameters:

- **namespace** (`str`)

- **test_provider** (`TestProvider`)



### run

**Kind**: module









### test_providers

**Kind**: module












## unit_metrics

**Kind**: module










### describe_metric

**Kind**: function


Describe a metric



def describe_metric(metric_id: str, kwargs = {})





#### Parameters:

- **metric_id** (`str`)

- **kwargs** (*keyword) = `{}`



### list_metrics

**Kind**: function


List all metrics



def list_metrics(kwargs = {})





#### Parameters:

- **kwargs** (*keyword) = `{}`



### run_metric

**Kind**: function


Run a metric



def run_metric(metric_id: str, kwargs = {})





#### Parameters:

- **metric_id** (`str`)

- **kwargs** (*keyword) = `{}`






## utils

**Kind**: module










### DEFAULT_BIG_NUMBER_DECIMALS

**Kind**: attribute
**Labels**: module-attribute






**Value**: `2`




### DEFAULT_SMALL_NUMBER_DECIMALS

**Kind**: attribute
**Labels**: module-attribute






**Value**: `4`




### NumpyEncoder

**Kind**: class









### display

**Kind**: function


Display widgets with extra goodies (syntax highlighting, MathJax, etc.)



def display(widget_or_html, syntax_highlighting = True, mathjax = True)





#### Parameters:

- **widget_or_html**

- **syntax_highlighting** = `True`

- **mathjax** = `True`



### format_dataframe

**Kind**: function


Format a pandas DataFrame for display purposes



def format_dataframe(df: ) -> {'cls': 'ExprAttribute', 'values': [{'cls': 'ExprName', 'name': 'pd'}, {'cls': 'ExprName', 'name': 'DataFrame'}]}





#### Parameters:

- **df** (``)



### format_key_values

**Kind**: function


Round the values on each dict's value to a given number of decimal places.

We do this for display purposes before sending data to ValidMind. Rules:

- Assume the dict is in this form: {key1: value1, key2: value2, ...}
- Check if we are rendering "big" numbers greater than 10 or just numbers between 0 and 1
- If the column's smallest number has more decimals 6, use that number's precision
  so we can avoid rendering a 0 instead
- If the column's smallest number has less decimals than 6, use 6 decimal places



def format_key_values(key_values: Dict) -> Dict[str, Any]





#### Parameters:

- **key_values** (`Dict`)



### format_number

**Kind**: function


Format a number for display purposes. If the number is a float, round it
to 4 decimal places.



def format_number(number)





#### Parameters:

- **number**



### format_records

**Kind**: function


Round the values on each dataframe's column to a given number of decimal places.
The returned value is converted to a dict in "records" with Pandas's to_dict() function.

We do this for display purposes before sending data to ValidMind. Rules:

- Check if we are rendering "big" numbers greater than 10 or just numbers between 0 and 1
- If the column's smallest number has more decimals 6, use that number's precision
  so we can avoid rendering a 0 instead
- If the column's smallest number has less decimals than 6, use 6 decimal places



def format_records(df: ) -> List[]





#### Parameters:

- **df** (``)



### fuzzy_match

**Kind**: function


Check if a string matches another string using fuzzy matching

Args:
    string (str): The string to check
    search_string (str): The string to search for
    threshold (float): The similarity threshold to use (Default: 0.7)

Returns:
    True if the string matches the search string, False otherwise



def fuzzy_match(string: str, search_string: str, threshold = 0.7)





#### Parameters:

- **string** (`str`)

- **search_string** (`str`)

- **threshold** = `0.7`



### get_dataset_info

**Kind**: function


Attempts to extract all dataset info from a dataset object instance



def get_dataset_info(dataset)





#### Parameters:

- **dataset**



### get_full_typename

**Kind**: function


We determine types based on type names so we don't have to import
(and therefore depend on) PyTorch, TensorFlow, etc.



def get_full_typename(o: Any) -> Any





#### Parameters:

- **o** (`Any`)



### get_model_info

**Kind**: function


Attempts to extract all model info from a model object instance



def get_model_info(model)





#### Parameters:

- **model**



### inspect_obj

**Kind**: function




def inspect_obj(obj)





#### Parameters:

- **obj**



### is_matplotlib_typename

**Kind**: function




def is_matplotlib_typename(typename: str) -> bool





#### Parameters:

- **typename** (`str`)



### is_notebook

**Kind**: function


Checks if the code is running in a Jupyter notebook or IPython shell

https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook



def is_notebook() -> bool






### is_plotly_typename

**Kind**: function




def is_plotly_typename(typename: str) -> bool





#### Parameters:

- **typename** (`str`)



### logger

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### md_to_html

**Kind**: function


Converts Markdown to HTML using mistune with plugins



def md_to_html(md: str, mathml = False) -> str





#### Parameters:

- **md** (`str`)

- **mathml** = `False`



### nan_to_none

**Kind**: function




def nan_to_none(obj)





#### Parameters:

- **obj**



### params

**Kind**: attribute
**Labels**: module-attribute






**Value**: ``




### precision_and_scale

**Kind**: function


https://stackoverflow.com/questions/3018758/determine-precision-and-scale-of-particular-number-in-python

Returns a (precision, scale) tuple for a given number.



def precision_and_scale(x)





#### Parameters:

- **x**



### preview_test_config

**Kind**: function




def preview_test_config(config)





#### Parameters:

- **config**



### run_async

**Kind**: function


Helper function to run functions asynchronously

This takes care of the complexity of running the logging functions asynchronously. It will
detect the type of environment we are running in (ipython notebook or not) and run the
function accordingly.

Args:
    func (function): The function to run asynchronously
    *args: The arguments to pass to the function
    **kwargs: The keyword arguments to pass to the function

Returns:
    The result of the function



def run_async(func, args = (), name = None, kwargs = {})





#### Parameters:

- **func**

- **args** (*positional) = `()`

- **name** = `None`

- **kwargs** (*keyword) = `{}`



### run_async_check

**Kind**: function


Helper function to run functions asynchronously if the task doesn't already exist



def run_async_check(func, args = (), kwargs = {})





#### Parameters:

- **func**

- **args** (*positional) = `()`

- **kwargs** (*keyword) = `{}`



### summarize_data_quality_results

**Kind**: function


TODO: generalize this to work with metrics and test results

Summarize the results of the data quality test suite



def summarize_data_quality_results(results)





#### Parameters:

- **results**



### test_id_to_name

**Kind**: function


Convert a test ID to a human-readable name.

Args:
    test_id (str): The test identifier, typically in CamelCase or snake_case.

Returns:
    str: A human-readable name derived from the test ID.



def test_id_to_name(test_id: str) -> str





#### Parameters:

- **test_id** (`str`)






## vm_models

**Kind**: module



Models entrypoint








### dataset

**Kind**: module









### figure

**Kind**: module


Figure objects track the figure schema supported by the ValidMind API








### input

**Kind**: module


Base class for ValidMind Input types








### model

**Kind**: module


Model class wrapper module








### result

**Kind**: module











