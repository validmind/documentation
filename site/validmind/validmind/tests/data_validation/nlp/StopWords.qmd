---
title: "[validmind](/validmind/validmind.qmd).StopWords"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- docstring.jinja2 -->

Threshold based tests

<!-- function.qmd.jinja2 -->

## StopWords[()]{.muted}

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'nlp'</span>, <span class="s">'text_data'</span>, <span class="s">'frequency_analysis'</span>, <span class="s">'visualization'</span>)</span></span><span class="decorator">@<span class="n">tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span></span>

<span class="kw">def</span><span class="name">StopWords</span>(<span class="params"><span class="n">dataset</span><span class="p">:</span><span class="n">VMDataset</span><span class="muted">,</span></span><span class="params"><span class="n">min_percent_threshold</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="kc">0.5</span><span class="muted">,</span></span><span class="params"><span class="n">num_words</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="kc">25</span></span>):

:::

<!-- docstring.jinja2 -->

Evaluates and visualizes the frequency of English stop words in a text dataset against a defined threshold.

### Purpose

The StopWords threshold test is a tool designed for assessing the quality of text data in an ML model. It focuses on the identification and analysis of "stop words" in a given dataset. Stop words are frequent, common, yet semantically insignificant words (for example: "the", "and", "is") in a language. This test evaluates the proportion of stop words to the total word count in the dataset, in essence, scrutinizing the frequency of stop word usage. The core objective is to highlight the prevalent stop words based on their usage frequency, which can be instrumental in cleaning the data from noise and improving ML model performance.

### Test Mechanism

The StopWords test initiates on receiving an input of a 'VMDataset' object. Absence of such an object will trigger an error. The methodology involves inspection of the text column of the VMDataset to create a 'corpus' (a collection of written texts). Leveraging the Natural Language Toolkit's (NLTK) stop word repository, the test screens the corpus for any stop words and documents their frequency. It further calculates the percentage usage of each stop word compared to the total word count in the corpus. This percentage is evaluated against a predefined 'min_percent_threshold'. If this threshold is breached, the test returns a failed output. Top prevailing stop words along with their usage percentages are returned, facilitated by a bar chart visualization of these stop words and their frequency.

### Signs of High Risk

- A percentage of any stop words exceeding the predefined 'min_percent_threshold'.
- High frequency of stop words in the dataset which may adversely affect the application's analytical performance due to noise creation.

### Strengths

- The ability to scrutinize and quantify the usage of stop words.
- Provides insights into potential noise in the text data due to stop words.
- Directly aids in enhancing model training efficiency.
- Includes a bar chart visualization feature to easily interpret and action upon the stop words frequency information.

### Limitations

- The test only supports English stop words, making it less effective with datasets of other languages.
- The 'min_percent_threshold' parameter may require fine-tuning for different datasets, impacting the overall effectiveness of the test.
- Contextual use of the stop words within the dataset is not considered, potentially overlooking their significance in certain contexts.
- The test focuses specifically on the frequency of stop words, not providing direct measures of model performance or predictive accuracy.
