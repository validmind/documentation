---
title: "[validmind](/validmind/validmind.qmd).Toxicity"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## Toxicity<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'nlp'</span>, <span class="s">'text_data'</span>, <span class="s">'data_validation'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'nlp'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">Toxicity</span>(<span class="param"><span class="n">dataset</span></span>)<span class="p"> → </span><span class="return-annotation"><span class="n">Tuple</span><span class="p">\[</span><span class="n">plt</span>.<span class="n">Figure</span><span class="p">, </span><a href="/validmind/validmind/vm_models.qmd#rawdata">validmind.vm_models.RawData</a><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Assesses the toxicity of text data within a dataset to visualize the distribution of toxicity scores.

### Purpose

The Toxicity test aims to evaluate the level of toxic content present in a text dataset by leveraging a pre-trained toxicity model. It helps in identifying potentially harmful or offensive language that may negatively impact users or stakeholders.

### Test Mechanism

This test uses a pre-trained toxicity evaluation model and applies it to each text entry in the specified column of a dataset’s dataframe. The procedure involves:

- Loading a pre-trained toxicity model.
- Extracting the text from the specified column in the dataset.
- Computing toxicity scores for each text entry.
- Generating a KDE (Kernel Density Estimate) plot to visualize the distribution of these toxicity scores.

### Signs of High Risk

- High concentration of high toxicity scores in the KDE plot.
- A significant proportion of text entries with toxicity scores above a predefined threshold.
- Wide distribution of toxicity scores, indicating inconsistency in content quality.

### Strengths

- Provides a visual representation of toxicity distribution, making it easier to identify outliers.
- Uses a robust pre-trained model for toxicity evaluation.
- Can process large text datasets efficiently.

### Limitations

- Depends on the accuracy and bias of the pre-trained toxicity model.
- Does not provide context-specific insights, which may be necessary for nuanced understanding.
- May not capture all forms of subtle or indirect toxic language.
