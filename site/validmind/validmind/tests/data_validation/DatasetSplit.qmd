---
title: "[validmind](/validmind/validmind.qmd).DatasetSplit"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## DatasetSplit<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'tabular_data'</span>, <span class="s">'time_series_data'</span>, <span class="s">'text_data'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'classification'</span>, <span class="s">'regression'</span>, <span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">DatasetSplit</span>(<span class="param"><span class="n">datasets</span><span class="p">:</span><span class="n">List</span><span class="p">\[</span><a href="/validmind/validmind/vm_models.qmd#vmdataset">validmind.vm_models.VMDataset</a><span class="p">\]</span></span>)<span class="p"> â†’ </span><span class="return-annotation"><span class="n">Tuple</span><span class="p">\[</span><span class="n">List</span><span class="p">\[</span><span class="n">Dict</span><span class="p">\[</span><span class="nb">str</span><span class="p">, </span><span class="n">Any</span><span class="p">\]</span><span class="p">\]</span><span class="p">, </span><a href="/validmind/validmind/vm_models.qmd#rawdata">validmind.vm_models.RawData</a><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Evaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML model.

### Purpose

The DatasetSplit test is designed to evaluate and visualize the distribution of data among training, testing, and validation datasets, if available, within a given machine learning model. The main purpose is to assess whether the model's datasets are split appropriately, as an imbalanced split might affect the model's ability to learn from the data and generalize to unseen data.

### Test Mechanism

The DatasetSplit test first calculates the total size of all available datasets in the model. Then, for each individual dataset, the methodology involves determining the size of the dataset and its proportion relative to the total size. The results are then conveniently summarized in a table that shows dataset names, sizes, and proportions. Absolute size and proportion of the total dataset size are displayed for each individual dataset.

### Signs of High Risk

- A very small training dataset, which may result in the model not learning enough from the data.
- A very large training dataset and a small test dataset, which may lead to model overfitting and poor generalization to unseen data.
- A small or non-existent validation dataset, which might complicate the model's performance assessment.

### Strengths

- The DatasetSplit test provides a clear, understandable visualization of dataset split proportions, which can highlight any potential imbalance in dataset splits quickly.
- It covers a wide range of task types including classification, regression, and text-related tasks.
- The metric is not tied to any specific data type and is applicable to tabular data, time series data, or text data.

### Limitations

- The DatasetSplit test does not provide any insight into the quality or diversity of the data within each split, just the size and proportion.
- The test does not give any recommendations or adjustments for imbalanced datasets.
- Potential lack of compatibility with more complex modes of data splitting (for example, stratified or time-based splits) could limit the applicability of this test.
