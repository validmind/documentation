---
title: "[validmind](/validmind/validmind.html).Conciseness"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

## call_model<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">call_model</span>(<span class="params"><span class="n">system_prompt</span><span class="p">:</span><span class="nb">str</span><span class="muted">,</span></span><span class="params"><span class="n">user_prompt</span><span class="p">:</span><span class="nb">str</span><span class="muted">,</span></span><span class="params"><span class="n">temperature</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="kc">0.0</span><span class="muted">,</span></span><span class="params"><span class="n">seed</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="kc">42</span></span>):

:::

<!-- docstring.jinja2 -->

Call LLM with the given prompts and return the response

## get_explanation<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_explanation</span>(<span class="param"><span class="n">response</span><span class="p">:</span><span class="nb">str</span></span>):

:::

<!-- docstring.jinja2 -->

Get just the explanation from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> "<some-explanation>"

## get_score<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_score</span>(<span class="param"><span class="n">response</span><span class="p">:</span><span class="nb">str</span></span>):

:::

<!-- docstring.jinja2 -->

Get just the score from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> 8

<!-- function.qmd.jinja2 -->

## Conciseness[()]{.muted}

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'llm'</span>, <span class="s">'zero_shot'</span>, <span class="s">'few_shot'</span>)</span></span><span class="decorator">@<span class="n">tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span></span>

<span class="kw">def</span><span class="name">Conciseness</span>(<span class="params"><span class="n">model</span><span class="muted">,</span></span><span class="params"><span class="n">min_threshold</span><span class="o">=</span><span class="kc">7</span></span>):

:::

<!-- docstring.jinja2 -->

Analyzes and grades the conciseness of prompts provided to a Large Language Model.

### Purpose

The Conciseness Assessment is designed to evaluate the brevity and succinctness of prompts provided to a Language Learning Model (LLM). A concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed.

### Test Mechanism

Using an LLM, this test conducts a conciseness analysis on input prompts. The analysis grades the prompt on a scale from 1 to 10, where the grade reflects how well the prompt delivers clear instructions without being verbose. Prompts that score equal to or above a predefined threshold (default set to 7) are deemed successfully concise. This threshold can be adjusted to meet specific requirements.

### Signs of High Risk

- Prompts that consistently score below the predefined threshold.
- Prompts that are overly wordy or contain unnecessary information.
- Prompts that create confusion or ambiguity due to excess or unnecessary information.

### Strengths

- Ensures clarity and effectiveness of the prompts.
- Promotes brevity and preciseness in prompts without sacrificing essential information.
- Useful for models like LLMs, where input prompt length and clarity greatly influence model performance.
- Provides a quantifiable measure of prompt conciseness.

### Limitations

- The conciseness score is based on an AI's assessment, which might not fully capture human interpretation of conciseness.
- The predefined threshold for conciseness could be subjective and might need adjustment based on application.
- The test is dependent on the LLMâ€™s understanding of conciseness, which might vary from model to model.

<!-- class.qmd.jinja2 -->

## [class]{.muted} MissingRequiredTestInputError

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">class</span><span class="name">MissingRequiredTestInputError</span>:

:::

<!-- docstring.jinja2 -->

When a required test context variable is missing.

**Inherited members**

- **From BaseError**: [class BaseError[()]{.muted}](#class-baseerror), [__init__[()]{.muted}](#__init__), [__str__[()]{.muted}](#__str__), [description[()]{.muted}](#description)
- **From builtins.BaseException**: with_traceback, add_note
