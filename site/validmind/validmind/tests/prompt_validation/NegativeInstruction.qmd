---
title: "[validmind](/validmind/validmind.html).NegativeInstruction"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

## call_model<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">call_model</span>(<span class="params"><span class="n">system_prompt</span><span class="p">:</span><span class="nb">str</span><span class="muted">,</span></span><span class="params"><span class="n">user_prompt</span><span class="p">:</span><span class="nb">str</span><span class="muted">,</span></span><span class="params"><span class="n">temperature</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="kc">0.0</span><span class="muted">,</span></span><span class="params"><span class="n">seed</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="kc">42</span></span>):

:::

<!-- docstring.jinja2 -->

Call LLM with the given prompts and return the response

## get_explanation<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_explanation</span>(<span class="param"><span class="n">response</span><span class="p">:</span><span class="nb">str</span></span>):

:::

<!-- docstring.jinja2 -->

Get just the explanation from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> "<some-explanation>"

## get_score<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_score</span>(<span class="param"><span class="n">response</span><span class="p">:</span><span class="nb">str</span></span>):

:::

<!-- docstring.jinja2 -->

Get just the score from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> 8

<!-- function.qmd.jinja2 -->

## NegativeInstruction[()]{.muted}

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'llm'</span>, <span class="s">'zero_shot'</span>, <span class="s">'few_shot'</span>)</span></span><span class="decorator">@<span class="n">tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span></span>

<span class="kw">def</span><span class="name">NegativeInstruction</span>(<span class="params"><span class="n">model</span><span class="muted">,</span></span><span class="params"><span class="n">min_threshold</span><span class="o">=</span><span class="kc">7</span></span>):

:::

<!-- docstring.jinja2 -->

Evaluates and grades the use of affirmative, proactive language over negative instructions in LLM prompts.

### Purpose

The Negative Instruction test is utilized to scrutinize the prompts given to a Large Language Model (LLM). The objective is to ensure these prompts are expressed using proactive, affirmative language. The focus is on instructions indicating what needs to be done rather than what needs to be avoided, thereby guiding the LLM more efficiently towards the desired output.

### Test Mechanism

An LLM is employed to evaluate each prompt. The prompt is graded based on its use of positive instructions with scores ranging between 1-10. This grade reflects how effectively the prompt leverages affirmative language while shying away from negative or restrictive instructions. A prompt that attains a grade equal to or above a predetermined threshold (7 by default) is regarded as adhering effectively to the best practices of positive instruction. This threshold can be custom-tailored through the test parameters.

### Signs of High Risk

- Low score obtained from the LLM analysis, indicating heavy reliance on negative instructions in the prompts.
- Failure to surpass the preset minimum threshold.
- The LLM generates ambiguous or undesirable outputs as a consequence of the negative instructions used in the prompt.

### Strengths

- Encourages the usage of affirmative, proactive language in prompts, aiding in more accurate and advantageous model responses.
- The test result provides a comprehensible score, helping to understand how well a prompt follows the positive instruction best practices.

### Limitations

- Despite an adequate score, a prompt could still be misleading or could lead to undesired responses due to factors not covered by this test.
- The test necessitates an LLM for evaluation, which might not be available or feasible in certain scenarios.
- A numeric scoring system, while straightforward, may oversimplify complex issues related to prompt designing and instruction clarity.
- The effectiveness of the test hinges significantly on the predetermined threshold level, which can be subjective and may need to be adjusted according to specific use-cases.

<!-- class.qmd.jinja2 -->

## [class]{.muted} MissingRequiredTestInputError

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">class</span><span class="name">MissingRequiredTestInputError</span>:

:::

<!-- docstring.jinja2 -->

When a required test context variable is missing.

**Inherited members**

- **From BaseError**: [class BaseError[()]{.muted}](#class-baseerror), [__init__[()]{.muted}](#__init__), [__str__[()]{.muted}](#__str__), [description[()]{.muted}](#description)
- **From builtins.BaseException**: with_traceback, add_note
