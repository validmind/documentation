---
title: "[validmind](/validmind/validmind.qmd).ClassifierPerformance"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## ClassifierPerformance<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'sklearn'</span>, <span class="s">'binary_classification'</span>, <span class="s">'multiclass_classification'</span>, <span class="s">'model_performance'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'classification'</span>, <span class="s">'text_classification'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">ClassifierPerformance</span>(<span class="params"><span class="n">dataset</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmdataset">validmind.vm_models.VMDataset</a><span class="muted">,</span></span><span class="params"><span class="n">model</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmmodel">validmind.vm_models.VMModel</a><span class="muted">,</span></span><span class="params"><span class="n">average</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s1">'macro'</span></span>)<span class="p"> â†’ </span><span class="return-annotation"><span class="n">Dict</span><span class="p">\[</span><span class="nb">str</span><span class="p">, </span><span class="n">List</span><span class="p">\[</span><span class="n">Dict</span><span class="p">\[</span><span class="nb">str</span><span class="p">, </span><span class="nb">float</span><span class="p">\]</span><span class="p">\]</span><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Evaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy, and ROC AUC scores.

### Purpose

The Classifier Performance test is designed to evaluate the performance of Machine Learning classification models. It accomplishes this by computing precision, recall, F1-Score, and accuracy, as well as the ROC AUC (Receiver operating characteristic - Area under the curve) scores, thereby providing a comprehensive analytic view of the models' performance. The test is adaptable, handling binary and multiclass models equally effectively.

### Test Mechanism

The test produces a report that includes precision, recall, F1-Score, and accuracy, by leveraging the `classification_report` from scikit-learn's metrics module. For multiclass models, macro and weighted averages for these scores are also calculated. Additionally, the ROC AUC scores are calculated and included in the report using the `multiclass_roc_auc_score` function. The outcome of the test (report format) differs based on whether the model is binary or multiclass.

### Signs of High Risk

- Low values for precision, recall, F1-Score, accuracy, and ROC AUC, indicating poor performance.
- Imbalance in precision and recall scores.
- A low ROC AUC score, especially scores close to 0.5 or lower, suggesting a failing model.

### Strengths

- Versatile, capable of assessing both binary and multiclass models.
- Utilizes a variety of commonly employed performance metrics, offering a comprehensive view of model performance.
- The use of ROC-AUC as a metric is beneficial for evaluating unbalanced datasets.

### Limitations

- Assumes correctly identified labels for binary classification models.
- Specifically designed for classification models and not suitable for regression models.
- May provide limited insights if the test dataset does not represent real-world scenarios adequately.

<!-- function.qmd.jinja2 -->

## multiclass_roc_auc_score<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">multiclass_roc_auc_score</span>(<span class="params"><span class="n">y_test</span><span class="muted">,</span></span><span class="params"><span class="n">y_pred</span><span class="muted">,</span></span><span class="params"><span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span></span>):

:::
