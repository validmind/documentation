---
title: "[validmind](/validmind/validmind.qmd).HyperParametersTuning"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## custom_recall<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">custom_recall</span>(<span class="params"><span class="n">y_true</span><span class="muted">,</span></span><span class="params"><span class="n">y_pred_proba</span><span class="muted">,</span></span><span class="params"><span class="n">threshold</span><span class="o">=</span><span class="kc">0.5</span></span>):

:::

<!-- function.qmd.jinja2 -->

## HyperParametersTuning<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'sklearn'</span>, <span class="s">'model_performance'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'clustering'</span>, <span class="s">'classification'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">HyperParametersTuning</span>(<span class="params"><span class="n">model</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmmodel">validmind.vm_models.VMModel</a><span class="muted">,</span></span><span class="params"><span class="n">dataset</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmdataset">validmind.vm_models.VMDataset</a><span class="muted">,</span></span><span class="params"><span class="n">param_grid</span><span class="p">:</span><span class="nb">dict</span><span class="muted">,</span></span><span class="params"><span class="n">scoring</span><span class="p">:</span><span class="n">Union</span><span class="p">\[</span><span class="nb">str</span><span class="p">, </span><span class="n">List</span><span class="p">, </span><span class="n">Dict</span><span class="p">\]</span><span class="o">=</span><span class="kc">None</span><span class="muted">,</span></span><span class="params"><span class="n">thresholds</span><span class="p">:</span><span class="n">Union</span><span class="p">\[</span><span class="nb">float</span><span class="p">, </span><span class="n">List</span><span class="p">\[</span><span class="nb">float</span><span class="p">\]</span><span class="p">\]</span><span class="o">=</span><span class="kc">None</span><span class="muted">,</span></span><span class="params"><span class="n">fit_params</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="kc">None</span></span>)<span class="p"> â†’ </span><span class="return-annotation"><span class="n">Tuple</span><span class="p">\[</span><span class="n">List</span><span class="p">\[</span><span class="n">Dict</span><span class="p">\[</span><span class="nb">str</span><span class="p">, </span><span class="nb">float</span><span class="p">\]</span><span class="p">\]</span><span class="p">, </span><a href="/validmind/validmind/vm_models.qmd#rawdata">validmind.vm_models.RawData</a><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Performs exhaustive grid search over specified parameter ranges to find optimal model configurations across different metrics and decision thresholds.

### Purpose

The Hyperparameter Tuning test systematically explores the model's parameter space to identify optimal configurations. It supports multiple optimization metrics and decision thresholds, providing a comprehensive view of how different parameter combinations affect various aspects of model performance.

### Test Mechanism

The test uses scikit-learn's GridSearchCV to perform cross-validation for each parameter combination. For each specified threshold and optimization metric, it creates a scoring dictionary with threshold-adjusted metrics, performs grid search with cross-validation, records best parameters and corresponding scores, and combines results into a comparative table. This process is repeated for each optimization metric to provide a comprehensive view of model performance under different configurations.

### Signs of High Risk

- Large performance variations across different parameter combinations
- Significant discrepancies between different optimization metrics
- Best parameters at the edges of the parameter grid
- Unstable performance across different thresholds
- Overly complex model configurations (risk of overfitting)
- Very different optimal parameters for different metrics
- Cross-validation scores showing high variance
- Extreme parameter values in best configurations

### Strengths

- Comprehensive exploration of parameter space
- Supports multiple optimization metrics
- Allows threshold optimization
- Provides comparative view across different configurations
- Uses cross-validation for robust evaluation
- Helps understand trade-offs between different metrics
- Enables systematic parameter selection
- Supports both classification and clustering tasks

### Limitations

- Computationally expensive for large parameter grids
- May not find global optimum (limited to grid points)
- Cannot handle dependencies between parameters
- Memory intensive for large datasets
- Limited to scikit-learn compatible models
- Cross-validation splits may not preserve time series structure
- Grid search may miss optimal values between grid points
- Resource intensive for high-dimensional parameter spaces
