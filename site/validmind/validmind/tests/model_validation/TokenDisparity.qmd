---
title: "[validmind](/validmind/validmind.qmd).TokenDisparity"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## TokenDisparity<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'nlp'</span>, <span class="s">'text_data'</span>, <span class="s">'visualization'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">TokenDisparity</span>(<span class="params"><span class="n">dataset</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmdataset">validmind.vm_models.VMDataset</a><span class="muted">,</span></span><span class="params"><span class="n">model</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmmodel">validmind.vm_models.VMModel</a></span>)<span class="p"> â†’ </span><span class="return-annotation"><span class="n">Tuple</span><span class="p">\[</span><span class="n">pd</span>.<span class="n">DataFrame</span><span class="p">, </span><span class="n">go</span>.<span class="n">Figure</span><span class="p">, </span><a href="/validmind/validmind/vm_models.qmd#rawdata">validmind.vm_models.RawData</a><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Evaluates the token disparity between reference and generated texts, visualizing the results through histograms and bar charts, alongside compiling a comprehensive table of descriptive statistics for token counts.

### Purpose

The Token Disparity test aims to assess the difference in the number of tokens between reference texts and texts generated by the model. Understanding token disparity is essential for evaluating how well the generated content matches the expected length and richness of the reference texts.

### Test Mechanism

The test extracts true and predicted values from the dataset and model. It computes the number of tokens in each reference and generated text. The results are visualized using histograms and bar charts to display the distribution of token counts. Additionally, a table of descriptive statistics, including the mean, median, standard deviation, minimum, and maximum token counts, is compiled to provide a detailed summary of token usage.

### Signs of High Risk

- Significant disparity in token counts between reference and generated texts could indicate issues with text generation quality, such as verbosity or lack of detail.
- Consistently low token counts in generated texts compared to references might suggest that the model is producing incomplete or overly concise outputs.

### Strengths

- Provides a simple yet effective evaluation of text length and token usage.
- Visual representations (histograms and bar charts) make it easier to interpret the distribution and trends of token counts.
- Descriptive statistics offer a concise summary of the model's performance in generating texts of appropriate length.

### Limitations

- Token counts alone do not provide a complete assessment of text quality and should be supplemented with other metrics and qualitative analysis.
