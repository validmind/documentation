---
title: "[validmind](/validmind/validmind.qmd).ToxicityScore"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## ToxicityScore<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'nlp'</span>, <span class="s">'text_data'</span>, <span class="s">'visualization'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">ToxicityScore</span>(<span class="params"><span class="n">dataset</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmdataset">validmind.vm_models.VMDataset</a><span class="muted">,</span></span><span class="params"><span class="n">model</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmmodel">validmind.vm_models.VMModel</a></span>)<span class="p"> â†’ </span><span class="return-annotation"><span class="n">Tuple</span><span class="p">\[</span><span class="n">pd</span>.<span class="n">DataFrame</span><span class="p">, </span><span class="n">go</span>.<span class="n">Figure</span><span class="p">, </span><a href="/validmind/validmind/vm_models.qmd#rawdata">validmind.vm_models.RawData</a><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Assesses the toxicity levels of texts generated by NLP models to identify and mitigate harmful or offensive content.

### Purpose

The ToxicityScore metric is designed to evaluate the toxicity levels of texts generated by models. This is crucial for identifying and mitigating harmful or offensive content in machine-generated texts.

### Test Mechanism

The function starts by extracting the input, true, and predicted values from the provided dataset and model. The toxicity score is computed for each text using a preloaded `toxicity` evaluation tool. The scores are compiled into dataframes, and histograms and bar charts are generated to visualize the distribution of toxicity scores. Additionally, a table of descriptive statistics (mean, median, standard deviation, minimum, and maximum) is compiled for the toxicity scores, providing a comprehensive summary of the model's performance.

### Signs of High Risk

- Drastic spikes in toxicity scores indicate potentially toxic content within the associated text segment.
- Persistent high toxicity scores across multiple texts may suggest systemic issues in the model's text generation process.

### Strengths

- Provides a clear evaluation of toxicity levels in generated texts, helping to ensure content safety and appropriateness.
- Visual representations (histograms and bar charts) make it easier to interpret the distribution and trends of toxicity scores.
- Descriptive statistics offer a concise summary of the model's performance in generating non-toxic texts.

### Limitations

- The accuracy of the toxicity scores is contingent upon the underlying `toxicity` tool.
- The scores provide a broad overview but do not specify which portions or tokens of the text are responsible for high toxicity.
- Supplementary, in-depth analysis might be needed for granular insights.
