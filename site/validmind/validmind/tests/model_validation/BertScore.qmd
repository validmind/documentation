---
title: "[validmind](/validmind/validmind.qmd).BertScore"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## BertScore<span class="suffix"></span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator">@<span class="n">tags(<span class="s">'nlp'</span>, <span class="s">'text_data'</span>, <span class="s">'visualization'</span>)</span></span>

<span class="decorator">@<span class="n">tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span> </span>

<span class="kw">def</span><span class="name">BertScore</span>(<span class="params"><span class="n">dataset</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmdataset">validmind.vm_models.VMDataset</a><span class="muted">,</span></span><span class="params"><span class="n">model</span><span class="p">:</span><a href="/validmind/validmind/vm_models.qmd#vmmodel">validmind.vm_models.VMModel</a><span class="muted">,</span></span><span class="params"><span class="n">evaluation_model</span><span class="o">=</span><span class="s1">'distilbert-base-uncased'</span></span>)<span class="p"> â†’ </span><span class="return-annotation"><span class="n">Tuple</span><span class="p">\[</span><span class="n">pd</span>.<span class="n">DataFrame</span><span class="p">, </span><span class="n">go</span>.<span class="n">Figure</span><span class="p">, </span><a href="/validmind/validmind/vm_models.qmd#rawdata">validmind.vm_models.RawData</a><span class="p">\]</span></span>:

:::

<!-- docstring.jinja2 -->

Assesses the quality of machine-generated text using BERTScore metrics and visualizes results through histograms and bar charts, alongside compiling a comprehensive table of descriptive statistics.

### Purpose

This function is designed to assess the quality of text generated by machine learning models using BERTScore metrics. BERTScore evaluates text generation models' performance by calculating precision, recall, and F1 score based on BERT contextual embeddings.

### Test Mechanism

The function starts by extracting the true and predicted values from the provided dataset and model. It then initializes the BERTScore evaluator. For each pair of true and predicted texts, the function calculates the BERTScore metrics and compiles them into a dataframe. Histograms and bar charts are generated for each BERTScore metric (Precision, Recall, and F1 Score) to visualize their distribution. Additionally, a table of descriptive statistics (mean, median, standard deviation, minimum, and maximum) is compiled for each metric, providing a comprehensive summary of the model's performance. The test uses the `evaluation_model` param to specify the huggingface model to use for evaluation. `microsoft/deberta-xlarge-mnli` is the best-performing model but is very large and may be slow without a GPU. `microsoft/deberta-large-mnli` is a smaller model that is faster to run and `distilbert-base-uncased` is much lighter and can run on a CPU but is less accurate.

### Signs of High Risk

- Consistently low scores across BERTScore metrics could indicate poor quality in the generated text, suggesting that the model fails to capture the essential content of the reference texts.
- Low precision scores might suggest that the generated text contains a lot of redundant or irrelevant information.
- Low recall scores may indicate that important information from the reference text is being omitted.
- An imbalanced performance between precision and recall, reflected by a low F1 Score, could signal issues in the model's ability to balance informativeness and conciseness.

### Strengths

- Provides a multifaceted evaluation of text quality through different BERTScore metrics, offering a detailed view of model performance.
- Visual representations (histograms and bar charts) make it easier to interpret the distribution and trends of the scores.
- Descriptive statistics offer a concise summary of the model's strengths and weaknesses in generating text.

### Limitations

- BERTScore relies on the contextual embeddings from BERT models, which may not fully capture all nuances of text similarity.
- The evaluation relies on the availability of high-quality reference texts, which may not always be obtainable.
- While useful for comparison, BERTScore metrics alone do not provide a complete assessment of a model's performance and should be supplemented with other metrics and qualitative analysis.
