---
title: Specificity
sidebar: validmind-reference
---

## call_model[()]{.muted}

```python
def call_model(
    system_prompt: {'cls': 'ExprName', 'name': 'str'},    user_prompt: {'cls': 'ExprName', 'name': 'str'},    temperature: {'cls': 'ExprName', 'name': 'float'} = 0.0,    seed: {'cls': 'ExprName', 'name': 'int'} = 42):
```

Call LLM with the given prompts and return the response

## get_explanation[()]{.muted}

```python
def get_explanation(
    response: {'cls': 'ExprName', 'name': 'str'}):
```

Get just the explanation from the response string TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> "<some-explanation>"

## get_score[()]{.muted}

```python
def get_score(
    response: {'cls': 'ExprName', 'name': 'str'}):
```

Get just the score from the response string TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> 8

## Specificity[()]{.muted}

```python
def Specificity(
    model,
    min_threshold = 7):
```

Evaluates and scores the specificity of prompts provided to a Large Language Model (LLM), based on clarity, detail, and relevance.

### Purpose

The Specificity Test evaluates the clarity, precision, and effectiveness of the prompts provided to a Language Model (LLM). It aims to ensure that the instructions embedded in a prompt are indisputably clear and relevant, thereby helping to remove ambiguity and steer the LLM towards desired outputs. This level of specificity significantly affects the accuracy and relevance of LLM outputs.

### Test Mechanism

The Specificity Test employs an LLM to grade each prompt based on clarity, detail, and relevance parameters within a specificity scale that extends from 1 to 10. On this scale, prompts scoring equal to or more than a predefined threshold (set to 7 by default) pass the evaluation, while those scoring below this threshold fail it. Users can adjust this threshold as per their requirements.

### Signs of High Risk

- Prompts scoring consistently below the established threshold
- Vague or ambiguous prompts that do not provide clear direction to the LLM
- Overly verbose prompts that may confuse the LLM instead of providing clear guidance

### Strengths

- Enables precise and clear communication with the LLM to achieve desired outputs
- Serves as a crucial means to measure the effectiveness of prompts
- Highly customizable, allowing users to set their threshold based on specific use cases

### Limitations

- This test doesn't consider the content comprehension capability of the LLM
- High specificity score doesn't guarantee a high-quality response from the LLM, as the model's performance is also dependent on various other factors
- Striking a balance between specificity and verbosity can be challenging, as overly detailed prompts might confuse or mislead the model

## [class]{.muted} MissingRequiredTestInputError

```python
class MissingRequiredTestInputError(BaseError):
```

When a required test context variable is missing.

**Inherited members**

- **From BaseError**: [class BaseError[()]{.muted}](#class-baseerror), [__init__[()]{.muted}](#__init__), [__str__[()]{.muted}](#__str__), [description[()]{.muted}](#description)
- **From builtins.BaseException**: with_traceback, add_note
