---
title: "[validmind](/reference/validmind.html).Bias"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

## call_model<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">call_model</span>(<span class="params"><span class="n">system_prompt</span><span class="p">:</span><span class="nb">str</span><span class="muted">,</span></span><span class="params"><span class="n">user_prompt</span><span class="p">:</span><span class="nb">str</span><span class="muted">,</span></span><span class="params"><span class="n">temperature</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="kc">0.0</span><span class="muted">,</span></span><span class="params"><span class="n">seed</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="kc">42</span></span>):

:::

<!-- docstring.jinja2 -->

Call LLM with the given prompts and return the response

## get_explanation<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_explanation</span>(<span class="param"><span class="n">response</span><span class="p">:</span><span class="nb">str</span></span>):

:::

<!-- docstring.jinja2 -->

Get just the explanation from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> "<some-explanation>"

## get_score<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_score</span>(<span class="param"><span class="n">response</span><span class="p">:</span><span class="nb">str</span></span>):

:::

<!-- docstring.jinja2 -->

Get just the score from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> 8

<!-- function.qmd.jinja2 -->

## Bias[()]{.muted}

<!-- signatures.jinja2 -->

::: {.signature}

<span class="decorators"><span class="decorator"><span class="n">@tags(<span class="s">'llm'</span>, <span class="s">'few_shot'</span>)</span></span><span class="decorator"><span class="n">@tasks(<span class="s">'text_classification'</span>, <span class="s">'text_summarization'</span>)</span></span></span>

<span class="kw">def</span><span class="name">Bias</span>(<span class="params"><span class="n">model</span><span class="muted">,</span></span><span class="params"><span class="n">min_threshold</span><span class="o">=</span><span class="kc">7</span></span>):

:::

<!-- docstring.jinja2 -->

Assesses potential bias in a Large Language Model by analyzing the distribution and order of exemplars in the

prompt.

### Purpose

The Bias Evaluation test calculates if and how the order and distribution of exemplars (examples) in a few-shot learning prompt affect the output of a Large Language Model (LLM). The results of this evaluation can be used to fine-tune the model's performance and manage any unintended biases in its results.

### Test Mechanism

This test uses two checks:

1. **Distribution of Exemplars:** The number of positive vs. negative examples in a prompt is varied. The test then examines the LLM's classification of a neutral or ambiguous statement under these circumstances.
1. **Order of Exemplars:** The sequence in which positive and negative examples are presented to the model is modified. Their resultant effect on the LLM's response is studied.

For each test case, the LLM grades the input prompt on a scale of 1 to 10. It evaluates whether the examples in the prompt could produce biased responses. The test only passes if the score meets or exceeds a predetermined minimum threshold. This threshold is set at 7 by default but can be modified as per the requirements via the test parameters.

### Signs of High Risk

- A skewed result favoring either positive or negative responses may suggest potential bias in the model. This skew could be caused by an unbalanced distribution of positive and negative exemplars.
- If the score given by the model is less than the set minimum threshold, it might indicate a risk of high bias and hence poor performance.

### Strengths

- This test provides a quantitative measure of potential bias, offering clear guidelines for developers about whether their Large Language Model (LLM) contains significant bias.
- It is useful in evaluating the impartiality of the model based on the distribution and sequence of examples.
- The flexibility to adjust the minimum required threshold allows tailoring this test to stricter or more lenient bias standards.

### Limitations

- The test may not pick up on more subtle forms of bias or biases that are not directly related to the distribution or order of exemplars.
- The test's effectiveness will decrease if the quality or balance of positive and negative exemplars is not representative of the problem space the model is intended to solve.
- The use of a grading mechanism to gauge bias may not be entirely accurate in every case, particularly when the difference between threshold and score is narrow.

<!-- class.qmd.jinja2 -->

## [class]{.muted} MissingRequiredTestInputError

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">class</span><span class="name">MissingRequiredTestInputError</span>:

:::

<!-- docstring.jinja2 -->

When a required test context variable is missing.

**Inherited members**

- **From BaseError**: [class BaseError[()]{.muted}](#class-baseerror), [__init__[()]{.muted}](#__init__), [__str__[()]{.muted}](#__str__), [description[()]{.muted}](#description)
- **From builtins.BaseException**: with_traceback, add_note
