---
title: "[validmind](/reference/validmind.html).Clarity"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

## call_model<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">call_model</span>(<span class="params"><span class="n">system_prompt</span><span class="p">:</span><span class="nb"><!-- types.jinja2 - format_type -->str</span><span class="muted">,</span></span><span class="params"><span class="n">user_prompt</span><span class="p">:</span><span class="nb"><!-- types.jinja2 - format_type -->str</span><span class="muted">,</span></span><span class="params"><span class="n">temperature</span><span class="p">:</span><span class="nb"><!-- types.jinja2 - format_type -->float</span><span class="o"> = </span><span class="kc">0.0</span><span class="muted">,</span></span><span class="params"><span class="n">seed</span><span class="p">:</span><span class="nb"><!-- types.jinja2 - format_type -->int</span><span class="o"> = </span><span class="kc">42</span></span>)

:::

<!-- docstring.jinja2 -->

Call LLM with the given prompts and return the response

## get_explanation<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_explanation</span>(<span class="params"><span class="n">response</span><span class="p">:</span><span class="nb"><!-- types.jinja2 - format_type -->str</span></span>)

:::

<!-- docstring.jinja2 -->

Get just the explanation from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> "<some-explanation>"

## get_score<span class='muted'>()</span>

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">get_score</span>(<span class="params"><span class="n">response</span><span class="p">:</span><span class="nb"><!-- types.jinja2 - format_type -->str</span></span>)

:::

<!-- docstring.jinja2 -->

Get just the score from the response string

TODO: use json response mode instead of this

```
e.g. "Score: 8
```

Explanation: <some-explanation>" -> 8

<!-- function.qmd.jinja2 -->

## Clarity[()]{.muted}

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">def</span><span class="name">Clarity</span>(<span class="params"><span class="n">model</span><span class="muted">,</span></span><span class="params"><span class="n">min_threshold</span><span class="o"> = </span><span class="kc">7</span></span>)

:::

<!-- docstring.jinja2 -->

Evaluates and scores the clarity of prompts in a Large Language Model based on specified guidelines.

### Purpose

The Clarity evaluation metric is used to assess how clear the prompts of a Large Language Model (LLM) are. This assessment is particularly important because clear prompts assist the LLM in more accurately interpreting and responding to instructions.

### Test Mechanism

The evaluation uses an LLM to scrutinize the clarity of prompts, factoring in considerations such as the inclusion of relevant details, persona adoption, step-by-step instructions, usage of examples, and specification of desired output length. Each prompt is rated on a clarity scale of 1 to 10, and any prompt scoring at or above the preset threshold (default of 7) will be marked as clear. It is important to note that this threshold can be adjusted via test parameters, providing flexibility in the evaluation process.

### Signs of High Risk

- Prompts that consistently score below the clarity threshold
- Repeated failure of prompts to adhere to guidelines for clarity, including detail inclusion, persona adoption, explicit step-by-step instructions, use of examples, and specification of output length

### Strengths

- Encourages the development of more effective prompts that aid the LLM in interpreting instructions accurately
- Applies a quantifiable measure (a score from 1 to 10) to evaluate the clarity of prompts
- Threshold for clarity is adjustable, allowing for flexible evaluation depending on the context

### Limitations

- Scoring system is subjective and relies on the AIâ€™s interpretation of 'clarity'
- The test assumes that all required factors (detail inclusion, persona adoption, step-by-step instructions, use of examples, and specification of output length) contribute equally to clarity, which might not always be the case
- The evaluation may not be as effective if used on non-textual models

<!-- class.qmd.jinja2 -->

## [class]{.muted} MissingRequiredTestInputError

<!-- signatures.jinja2 -->

::: {.signature}

<span class="kw">class</span><span class="name">MissingRequiredTestInputError</span>()

:::

<!-- docstring.jinja2 -->

When a required test context variable is missing.

**Inherited members**

- **From BaseError**: [class BaseError[()]{.muted}](#class-baseerror), [__init__[()]{.muted}](#__init__), [__str__[()]{.muted}](#__str__), [description[()]{.muted}](#description)
- **From builtins.BaseException**: with_traceback, add_note
