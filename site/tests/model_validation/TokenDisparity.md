# TokenDisparity

Evaluates the token disparity between reference and generated texts, visualizing the results through histograms and
bar charts, alongside compiling a comprehensive table of descriptive statistics for token counts.

### Purpose

The Token Disparity test aims to assess the difference in the number of tokens between reference texts and texts
generated by the model. Understanding token disparity is essential for evaluating how well the generated content
matches the expected length and richness of the reference texts.

### Test Mechanism

The test extracts true and predicted values from the dataset and model. It computes the number of tokens in each
reference and generated text. The results are visualized using histograms and bar charts to display the
distribution of token counts. Additionally, a table of descriptive statistics, including the mean, median, standard
deviation, minimum, and maximum token counts, is compiled to provide a detailed summary of token usage.

### Signs of High Risk

- Significant disparity in token counts between reference and generated texts could indicate issues with text
generation quality, such as verbosity or lack of detail.
- Consistently low token counts in generated texts compared to references might suggest that the model is producing
incomplete or overly concise outputs.

### Strengths

- Provides a simple yet effective evaluation of text length and token usage.
- Visual representations (histograms and bar charts) make it easier to interpret the distribution and trends of
token counts.
- Descriptive statistics offer a concise summary of the model's performance in generating texts of appropriate
length.

### Limitations

- Token counts alone do not provide a complete assessment of text quality and should be supplemented with other
metrics and qualitative analysis.