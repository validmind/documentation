# ToxicityScore

**Purpose:**
The ToxicityScore metric is designed to present a sequential representation of toxicity scores for various texts.
Leveraging line plots, it gives an overview of how toxicity scores evolve across the sequence of texts, highlighting
trends and patterns.

**Test Mechanism:**
The mechanism involves fetching texts from specific columns, computing their toxicity scores using a preloaded
`toxicity` evaluation tool, and then plotting these scores. A multi-panel visualization is created where each
panel is dedicated to a specific text data column. Line plots serve as the primary visual tool, showing the progression of toxicity scores across text sequences. Each
line plot corresponds to a specific text data column, illustrating the variation in toxicity scores as one moves
from one text segment to the next.

**Signs of High Risk:**
Drastic spikes in the line plots, especially those that reach high toxicity values, indicate potentially toxic
content within the associated text segment. If predicted summaries diverge significantly from input or target
texts, it could be indicative of issues in the model's generated content.

**Strengths:**
The ToxicityScore offers a dynamic view of toxicity trends, enabling users to detect patterns or irregularities
across the dataset. This is particularly valuable when comparing predicted content with actual data, helping
highlight any inconsistencies or abnormalities in model output.

**Limitations:**
This metricâ€™s accuracy is contingent upon the underlying `toxicity` tool. The line plots provide a broad overview
of toxicity trends but do not specify which portions or tokens of the text are responsible for high toxicity scores.
Consequently, for granular insights, supplementary, in-depth analysis might be needed.