{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Default Model using ValidMind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Connect Notebook to ValidMind Project\n",
    "- Step 2: Import Raw Data\n",
    "- Step 3: Data Description\n",
    "- Step 4: Data Preparation\n",
    "- Step 5: Data Description on Preprocessed Data \n",
    "- Step 6: Univariate Analysis\n",
    "- Step 7: Multivariate Analysis\n",
    "- Step 8: Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect Notebook to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lending Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Specify the path to the zip file\n",
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Raw Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"xticks_fontsize\": 8}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Target Variable\n",
    "\n",
    "**Definition of Default**\n",
    "\n",
    "We categorizing `Fully Paid` loans as \"default = 0\" and `Charged Off` loans as \"default = 1\". This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk). \n",
    "\n",
    "Loans with `Current` status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add `default` Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "target_column = 'default'\n",
    "df = add_target_column(df, target_column)\n",
    "\n",
    "# Drop 'loan_status' variable \n",
    "df.drop(columns='loan_status', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unused Variables\n",
    "\n",
    "Remove all the **Demographic** and **Customer Behavioural** features which is of no use for default analysis for credit approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "                    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "                    'earliest_cr_line', 'issue_d']\n",
    "\n",
    "df = df.drop(columns=unused_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n",
    "\n",
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df, min_missing_count)\n",
    "df.drop(columns=variables_to_drop, axis=1, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def clean_term_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_rate_columns(df, column):\n",
    "    \"\"\"\n",
    "    Clean interest rate column. Remove the '%' sign and convert to numeric.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    column (str): Name of the interest rate column to be cleaned.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].str.replace('%', '')\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "def clean_emp_length_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_inq_last_6mths(df, column):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "clean_emp_length_column(df, 'emp_length')\n",
    "clean_term_column(df, 'term')\n",
    "clean_inq_last_6mths(df, 'inq_last_6mths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\", \"uint\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns\n",
    "\n",
    "def compute_outliers(series, threshold=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "def remove_iqr_outliers(df, target_column, threshold=1.5):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n",
    "    for col in num_cols:\n",
    "        outliers = compute_outliers(df[col], threshold)\n",
    "        df = df[~df[col].isin(outliers)]\n",
    "    return df\n",
    "\n",
    "df = remove_iqr_outliers(df, target_column, threshold=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.IQROutliersPlots import IQROutliersPlots\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "num_features = get_numerical_columns(df)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5}\n",
    "\n",
    "metric = IQROutliersPlots(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Data  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Split data into train and test \n",
    "X = df.drop(target_column, axis = 1)\n",
    "y = df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "Class imbalance is a common issue in credit risk scorecards and datasets like the Lending Club's. This imbalance arises when the number of defaulting loans (negative class) is significantly smaller than the number of loans that are paid off (positive class). Such imbalance can lead to biased models that favor the majority class, thus affecting predictive performance. \n",
    "\n",
    "Special techniques like oversampling, undersampling, or cost-sensitive learning are often needed to ensure that the minority class is appropriately represented during model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = ClassImbalance(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared Test on Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "cat_features = get_categorical_columns(df_train)\n",
    "params = {\"cat_features\": cat_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ChiSquaredFeaturesTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA Test on Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "num_features = get_numerical_columns(df_train)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Correlation of Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.HeatmapFeatureCorrelations import HeatmapFeatureCorrelations\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = HeatmapFeatureCorrelations(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations of Numerical Features with Target Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "drop_categorical_features = ['addr_state']\n",
    "drop_numerical_features = ['total_rec_int', 'loan_amnt',\n",
    "                           'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n",
    "                           'total_pymnt_inv', 'last_pymnt_amnt',]\n",
    "\n",
    "df_train.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)\n",
    "\n",
    "# Update df_test \n",
    "df_test.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def encode_numerical_features(df):\n",
    "    \n",
    "    # term\n",
    "    df['term'] = df['term'].replace({' 36': '36M', ' 60': '60M'})\n",
    "\n",
    "    # emp_length_int\n",
    "    df['emp_length'] = df['emp_length'].replace('10+', '10')  # Replace '10+' with '10'\n",
    "    df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')  # Convert to numeric\n",
    "    df['emp_length'].fillna(-1, inplace=True)\n",
    "    bins = [0,1,2,3,5,8,10,999]\n",
    "    df['emp_length_bucket'] = pd.cut(df['emp_length'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='emp_length', inplace=True)\n",
    "\n",
    "    # inq_last_6mths\n",
    "    df['inq_last_6mths'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n",
    "    df['inq_last_6mths_bucket'] = pd.cut(df['inq_last_6mths'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='inq_last_6mths', inplace=True)\n",
    "    \n",
    "    # total_acc\n",
    "    df['total_acc'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 999]\n",
    "    df['total_acc_bucket'] = pd.cut(df['total_acc'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='total_acc', inplace=True)\n",
    "\n",
    "    # annual_inc\n",
    "    df['annual_inc'].fillna(-1, inplace=True)\n",
    "    df['annual_inc_1000'] = df['annual_inc']/1000\n",
    "    bins = [-1, 0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 1000, 10000]\n",
    "    df['annual_inc_bucket'] = pd.cut(df['annual_inc_1000'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='annual_inc', inplace=True)\n",
    "    df.drop(columns='annual_inc_1000', inplace=True)\n",
    "    \n",
    "    # int_rate\n",
    "    df['int_rate'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n",
    "    df['int_rate_bucket'] = pd.cut(df['int_rate'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='int_rate', inplace=True)\n",
    "\n",
    "    # installment\n",
    "    df['installment'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 100, 200, 300, 400, 500, 750, 1000, 1500]\n",
    "    df['installment_bucket'] = pd.cut(df['installment'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='installment', inplace=True)\n",
    "\n",
    "    # open_acc\n",
    "    df['open_acc'].replace(\"N/A\", 1, inplace=True)\n",
    "    df['open_acc'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 8, 10, 100]\n",
    "    df['open_acc_bucket'] = pd.cut(df['open_acc'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='open_acc', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = encode_numerical_features(df_train)\n",
    "\n",
    "# Update df_test\n",
    "df_test = encode_numerical_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def find_categorical_features(df):\n",
    "    # Get the column names of features with the data type \"category\"\n",
    "    categorical_features = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    return categorical_features\n",
    "\n",
    "\n",
    "def convert_categorical_to_object(df):\n",
    "    # Find the categorical features\n",
    "    categorical_features = find_categorical_features(df)\n",
    "\n",
    "    # Convert the categorical features to object type\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = convert_categorical_to_object(df_train)\n",
    "\n",
    "# Update df_test\n",
    "df_test = convert_categorical_to_object(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WoE and IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEIVTable import WOEIVTable\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"order_by\": [\"Feature\", \"WoE\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_iv_df = metric.result.metric.value['woe_iv']\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def coarse_classing(df, mappings):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # Loop through each feature and merge set\n",
    "    for feature, merge_sets in mappings.items():\n",
    "        for merge_set in merge_sets:\n",
    "            # Merge the specified categories into a new category\n",
    "            df_new[feature] = df_new[feature].apply(lambda x: f\"[{','.join(merge_set)}]\" if x in merge_set else x)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "# Create a dictionary of features and the sets to merge\n",
    "mappings = {\n",
    "    'sub_grade': [['B2','B3','B4','B5','C3','D1'], ['C1','C2','C4','C5'], ['D3','D4','D5','E3','G4'], ['E1','E2','E4','E5','F1','F2','F3','F4','G1','G2','G3','G5','F5']],\n",
    "    'grade': [['F','G']],\n",
    "    'purpose': [['wedding','major_purchase'], ['credit_card','car'], ['debt_consolidation','other','vacation'], ['medical','moving','house','educational'], ['renewable_energy','small_business']],\n",
    "    'home_ownership': [['MORTGAGE','OWN','RENT']],\n",
    "    'annual_inc_bucket': [['[250, 1000)','[100, 150)','[150, 250)','[1000, 10000)'], ['[50, 75)','[40, 50)'], ['[10, 20)','[0, 10)']],\n",
    "    'emp_length_bucket': [['[2, 3)','[40, 50)','[3, 5)','[1, 2)','[0, 1)','[5, 8)','[8, 10)']],\n",
    "    'inq_last_6mths_bucket': [['[4, 5)','[1, 2)'], ['[5, 10)','[3, 4)']],\n",
    "    'installment_bucket': [['[300, 400)','[200, 300)','[0, 100)'], ['[400, 500)', '[500, 750)']],\n",
    "    'total_acc_bucket': [['[20, 25)','[30, 35)','[15, 20)','[45, 50)','[40, 45)','[35, 40)','[10, 15)','[5, 10)']],\n",
    "    'open_acc_bucket': [['[5, 8)','[8, 10)','[10, 100)','[4, 5)'], ['[1, 2)','[2, 3)']]\n",
    "}\n",
    "\n",
    "df_train = coarse_classing(df_train, mappings)\n",
    "df_train = df_train[~df_train['home_ownership'].isin(['OTHER', 'NONE'])]\n",
    "df_train.drop(columns=\"home_ownership\", inplace=True)\n",
    "\n",
    "# Update df_test\n",
    "df_test = coarse_classing(df_test, mappings)\n",
    "df_test = df_test[~df_test['home_ownership'].isin(['OTHER', 'NONE'])]\n",
    "df_test.drop(columns=\"home_ownership\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def shorten_category_names(df, max_length=20, suffix=\"...\"):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for feature in df_new.columns:\n",
    "        # Check if the column has the \"object\" data type\n",
    "        if df_new[feature].dtype.name == 'object':\n",
    "            # Shorten long category names\n",
    "            df_new[feature] = df_new[feature].apply(lambda x: x[:max_length] + suffix if len(x) > max_length else x)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "df_train = shorten_category_names(df_train, max_length=15, suffix=\"...\")\n",
    "\n",
    "# Update df_test\n",
    "df_test = shorten_category_names(df_test, max_length=15, suffix=\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"order_by\": [\"Feature\", \"WoE\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_iv_df = metric.result.metric.value['woe_iv']\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEIVPlots import WOEIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add WoE as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def woe_encoder(woe_df, original_df, target):\n",
    "    # Initiate an empty DataFrame\n",
    "    woe_encoded_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each feature-category and get the corresponding WoE value\n",
    "    for feature in woe_df['Feature'].unique():\n",
    "        for category in woe_df[woe_df['Feature'] == feature]['Category'].unique():\n",
    "            woe_value = woe_df[(woe_df['Feature'] == feature) & (woe_df['Category'] == category)]['WoE'].values[0]\n",
    "            original_df.loc[original_df[feature] == category, feature] = woe_value\n",
    "\n",
    "        # Convert the feature to float type\n",
    "        original_df[feature] = original_df[feature].astype(float)\n",
    "\n",
    "    # Creating a new dataframe with WoE values\n",
    "    for feature in woe_df['Feature'].unique():\n",
    "        woe_encoded_df = pd.concat([woe_encoded_df, original_df[feature]], axis=1)\n",
    "\n",
    "    # Add the target column to the new DataFrame\n",
    "    woe_encoded_df[target] = original_df[target]\n",
    "\n",
    "    return woe_encoded_df\n",
    "\n",
    "\n",
    "df_train = woe_encoder(woe_iv_df, df_train, target='default')\n",
    "\n",
    "# Update df_test\n",
    "df_test = woe_encoder(woe_iv_df, df_test, target='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = HeatmapFeatureCorrelations(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "y_train = df_train[target_column]\n",
    "X_train = df_train.drop(target_column, axis=1)\n",
    "\n",
    "# Add constant to X_train for intercept term\n",
    "X_train = sm.add_constant(X_train)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Update df_test\n",
    "y_test = df_test[target_column]\n",
    "X_test = df_test.drop(target_column, axis=1)\n",
    "X_test = sm.add_constant(X_test)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "model_fit_glm = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model_fit_glm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "def compute_auc(y_true, y_scores):\n",
    "    \"\"\"Computes the Area Under the Curve (AUC).\"\"\"\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    return auc\n",
    "\n",
    "def compute_gini(y_true, y_scores):\n",
    "    \"\"\"Computes the Gini coefficient.\"\"\"\n",
    "    auc = compute_auc(y_true, y_scores)\n",
    "    gini = 2*auc - 1\n",
    "    return gini\n",
    "\n",
    "def compute_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Computes and prints AUC and GINI for train and test sets.\"\"\"\n",
    "\n",
    "    metrics_dict = {\"Dataset\": [\"Train\", \"Test\"],\n",
    "                    \"AUC\": [],\n",
    "                    \"GINI\": []}\n",
    "\n",
    "    for dataset, X, y in zip([\"Train\", \"Test\"], [X_train, X_test], [y_train, y_test]):\n",
    "        # Get predicted probabilities\n",
    "        y_scores = model.predict(X)\n",
    "\n",
    "        # Compute AUC and GINI\n",
    "        auc = compute_auc(y, y_scores)\n",
    "        gini = compute_gini(y, y_scores)\n",
    "\n",
    "        # Add the metrics to the dictionary\n",
    "        metrics_dict[\"AUC\"].append(auc)\n",
    "        metrics_dict[\"GINI\"].append(gini)\n",
    "\n",
    "    # Convert dictionary to DataFrame for nicer display\n",
    "    metrics_df = pd.DataFrame(metrics_dict)\n",
    "    return metrics_df\n",
    "\n",
    "metrics_df = compute_metrics(model_fit_glm, X_train, y_train, X_test, y_test)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_true, y_scores):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function using y_test and y_scores\n",
    "plot_roc_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cerate VM dataset\n",
    "vm_train_ds = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "vm_test_ds = vm.init_dataset(dataset=df_test,\n",
    "                        target_column=target_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_glm = vm.init_model(\n",
    "    model = model_fit_glm, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = ConfusionMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.ROCCurve import ROCCurve\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "\n",
    "metric = ROCCurve(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More plots on performance in-sample out of sample. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorecard Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Probability of Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_pd(model_fit, X_train):\n",
    "\n",
    "    # Predict probabilities\n",
    "    probabilities = model_fit.predict(X_train)\n",
    "\n",
    "    # The probabilities are a 2D array with probabilities for the two classes.\n",
    "    # We are interested in the probability of default, which is the second column.\n",
    "    pd = probabilities\n",
    "\n",
    "    # Add PD as a new column in X_train\n",
    "    X_train['PD'] = pd\n",
    "\n",
    "    return X_train\n",
    "\n",
    "X_train_pd = compute_pd(model_fit_glm, X_train)\n",
    "df_train_pd = pd.concat([X_train_pd, y_train], axis=1)\n",
    "\n",
    "# Update df_test\n",
    "X_test_pd = compute_pd(model_fit_glm, X_test)\n",
    "df_test_pd = pd.concat([X_test_pd, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_pd_histogram(df_train, df_test, pd_col, target_col):\n",
    "    # Separate PD based on target column for training data\n",
    "    pd_train_0 = df_train[df_train[target_col] == 0][pd_col]\n",
    "    pd_train_1 = df_train[df_train[target_col] == 1][pd_col]\n",
    "\n",
    "    # Separate PD based on target column for testing data\n",
    "    pd_test_0 = df_test[df_test[target_col] == 0][pd_col]\n",
    "    pd_test_1 = df_test[df_test[target_col] == 1][pd_col]\n",
    "\n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n",
    "\n",
    "    # Create histograms for training data\n",
    "    trace_train_0 = go.Histogram(x=pd_train_0, opacity=0.75, name=f'Train {target_col} = 0')\n",
    "    trace_train_1 = go.Histogram(x=pd_train_1, opacity=0.75, name=f'Train {target_col} = 1')\n",
    "\n",
    "    # Create histograms for testing data\n",
    "    trace_test_0 = go.Histogram(x=pd_test_0, opacity=0.75, name=f'Test {target_col} = 0')\n",
    "    trace_test_1 = go.Histogram(x=pd_test_1, opacity=0.75, name=f'Test {target_col} = 1')\n",
    "\n",
    "    # Add traces to the subplots\n",
    "    fig.add_trace(trace_train_0, row=1, col=1)\n",
    "    fig.add_trace(trace_train_1, row=1, col=1)\n",
    "    fig.add_trace(trace_test_0, row=1, col=2)\n",
    "    fig.add_trace(trace_test_1, row=1, col=2)\n",
    "\n",
    "    # Update layout to overlay the histograms in each subplot\n",
    "    fig.update_layout(barmode='overlay', title_text='Histogram of Probability of Default')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_pd_histogram(df_train_pd,\n",
    "                  df_test_pd, \n",
    "                  pd_col='PD', \n",
    "                  target_col=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_cumulative_pd(df_train, df_test, pd_col, target_col):\n",
    "    # Separate PD based on target column for training data\n",
    "    pd_train_0 = np.sort(df_train[df_train[target_col] == 0][pd_col])\n",
    "    pd_train_1 = np.sort(df_train[df_train[target_col] == 1][pd_col])\n",
    "\n",
    "    # Separate PD based on target column for testing data\n",
    "    pd_test_0 = np.sort(df_test[df_test[target_col] == 0][pd_col])\n",
    "    pd_test_1 = np.sort(df_test[df_test[target_col] == 1][pd_col])\n",
    "\n",
    "    # Calculate cumulative distributions\n",
    "    cumulative_pd_train_0 = np.cumsum(pd_train_0) / np.sum(pd_train_0)\n",
    "    cumulative_pd_train_1 = np.cumsum(pd_train_1) / np.sum(pd_train_1)\n",
    "    cumulative_pd_test_0 = np.cumsum(pd_test_0) / np.sum(pd_test_0)\n",
    "    cumulative_pd_test_1 = np.cumsum(pd_test_1) / np.sum(pd_test_1)\n",
    "\n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n",
    "\n",
    "    # Create line plots for training data\n",
    "    trace_train_0 = go.Scatter(x=pd_train_0, y=cumulative_pd_train_0, mode='lines', name=f'Train {target_col} = 0')\n",
    "    trace_train_1 = go.Scatter(x=pd_train_1, y=cumulative_pd_train_1, mode='lines', name=f'Train {target_col} = 1')\n",
    "\n",
    "    # Create line plots for testing data\n",
    "    trace_test_0 = go.Scatter(x=pd_test_0, y=cumulative_pd_test_0, mode='lines', name=f'Test {target_col} = 0')\n",
    "    trace_test_1 = go.Scatter(x=pd_test_1, y=cumulative_pd_test_1, mode='lines', name=f'Test {target_col} = 1')\n",
    "\n",
    "    # Add traces to the subplots\n",
    "    fig.add_trace(trace_train_0, row=1, col=1)\n",
    "    fig.add_trace(trace_train_1, row=1, col=1)\n",
    "    fig.add_trace(trace_test_0, row=1, col=2)\n",
    "    fig.add_trace(trace_test_1, row=1, col=2)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text='Cumulative Probability of Default')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_cumulative_pd(df_train_pd,\n",
    "                  df_test_pd, \n",
    "                  pd_col='PD', \n",
    "                  target_col=target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Credit Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_credit_score(model_fit, X_train, target_score, target_odds, pdo):\n",
    "    # Get logistic regression coefficients\n",
    "    beta = model_fit.params.values\n",
    "\n",
    "    # Get intercept (alpha)\n",
    "    alpha = model_fit.params[0]  # Intercept is the first parameter in statsmodels\n",
    "\n",
    "    # Calculate factor\n",
    "    factor = pdo / np.log(2)\n",
    "\n",
    "    # Calculate offset\n",
    "    offset = target_score - (factor * np.log(target_odds))\n",
    "\n",
    "    # Initialize an empty list to store scores\n",
    "    scores = []\n",
    "\n",
    "    # Loop over each row in the training data\n",
    "    for _, row in X_train.iterrows():\n",
    "        # Initialize score for current row\n",
    "        score_i = 0\n",
    "\n",
    "        # Add contribution of each feature to the score\n",
    "        for i in range(len(beta)):\n",
    "            WoE_i = row[i + 1]  # WoE for feature i, assuming intercept is in the first column\n",
    "            score_i += (beta[i] * WoE_i + alpha / len(beta)) * factor + offset / len(beta)\n",
    "\n",
    "        # Add score to the list of scores\n",
    "        scores.append(score_i)\n",
    "\n",
    "    # Add scores as a new column in X_train\n",
    "    X_train['score'] = scores\n",
    "\n",
    "    return X_train\n",
    "\n",
    "\n",
    "# Set target_score, target_odds, and pdo\n",
    "target_score = 600\n",
    "target_odds = 50\n",
    "pdo = 20\n",
    "\n",
    "# Compute credit scores and add to df_train\n",
    "X_train_scores = compute_credit_score(model_fit_glm, X_train_pd, target_score, target_odds, pdo)\n",
    "df_train_scores = pd.concat([X_train_scores, y_train], axis=1)\n",
    "\n",
    "# Update df_test \n",
    "X_test_scores = compute_credit_score(model_fit_glm, X_test_pd, target_score, target_odds, pdo)\n",
    "df_test_scores = pd.concat([X_test_scores, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_score_histogram(df_train, df_test, score_col, target_col):\n",
    "    # Separate scores based on target column for training data\n",
    "    scores_train_0 = df_train[df_train[target_col] == 0][score_col]\n",
    "    scores_train_1 = df_train[df_train[target_col] == 1][score_col]\n",
    "\n",
    "    # Separate scores based on target column for testing data\n",
    "    scores_test_0 = df_test[df_test[target_col] == 0][score_col]\n",
    "    scores_test_1 = df_test[df_test[target_col] == 1][score_col]\n",
    "\n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n",
    "\n",
    "    # Create histograms for training data\n",
    "    trace_train_0 = go.Histogram(x=scores_train_0, opacity=0.75, name=f'Train {target_col} = 0')\n",
    "    trace_train_1 = go.Histogram(x=scores_train_1, opacity=0.75, name=f'Train {target_col} = 1')\n",
    "\n",
    "    # Create histograms for testing data\n",
    "    trace_test_0 = go.Histogram(x=scores_test_0, opacity=0.75, name=f'Test {target_col} = 0')\n",
    "    trace_test_1 = go.Histogram(x=scores_test_1, opacity=0.75, name=f'Test {target_col} = 1')\n",
    "\n",
    "    # Add traces to the subplots\n",
    "    fig.add_trace(trace_train_0, row=1, col=1)\n",
    "    fig.add_trace(trace_train_1, row=1, col=1)\n",
    "    fig.add_trace(trace_test_0, row=1, col=2)\n",
    "    fig.add_trace(trace_test_1, row=1, col=2)\n",
    "\n",
    "    # Update layout to overlay the histograms in each subplot\n",
    "    fig.update_layout(barmode='overlay', title_text='Histogram of Scores')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_score_histogram(df_train_scores, \n",
    "                     df_test_scores, \n",
    "                     score_col='score', \n",
    "                     target_col=target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_credit_scores(model, scaling_factor=None, base_points=None):\n",
    "    # Set default values if not provided\n",
    "    if scaling_factor is None:\n",
    "        scaling_factor = 20 / np.log(2)\n",
    "    if base_points is None:\n",
    "        base_points = 500\n",
    "\n",
    "    # Get the coefficients from the model\n",
    "    coefficients = model.params.values\n",
    "    \n",
    "    # Get the feature names from the model\n",
    "    selected_features = model.params.index\n",
    "\n",
    "    # Calculate odds ratios\n",
    "    odds_ratios = np.exp(coefficients).reshape(-1)\n",
    "    \n",
    "    # Calculate the scores for each coefficient\n",
    "    scores = scaling_factor * np.log(odds_ratios)\n",
    "    scores = base_points - scores\n",
    "\n",
    "    # Create a DataFrame to store feature names and their corresponding scores\n",
    "    feature_scores = pd.DataFrame({'Feature': selected_features, 'Score': scores})\n",
    "\n",
    "    # Sort the DataFrame in descending order of scores\n",
    "    feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    return feature_scores\n",
    "\n",
    "\n",
    "scores = calculate_credit_scores(model_fit_glm)\n",
    "display(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating Features and Target Variables for Training and Test Sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop(target_column, axis=1)  \n",
    "y_train = df_train[target_column]  \n",
    "\n",
    "X_test = df_test.drop(target_column, axis=1)  \n",
    "y_test = df_test[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"features\": None, \n",
    "          \"declutter\": False,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = HeatmapFeatureCorrelations(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "features = get_numerical_columns(df_train)\n",
    "params = {\"declutter\": True,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vm_df = vm.init_dataset(dataset=X_train)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Statistical Significance "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a GLM Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant to X_train for intercept term\n",
    "#X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run VM Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "# Create VM test and train datasets\n",
    "vm_train_ds = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "vm_test_ds = vm.init_dataset(dataset=df_test,\n",
    "                        target_column=target_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_reg = vm.init_model(\n",
    "    model = model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)\n",
    "list_of_models = [vm_model_reg]\n",
    "test_context = TestContext(models=list_of_models)\n",
    "\n",
    "# Run test\n",
    "metric = RegressionModelsCoeffs(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Significance of Features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run VM Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionFeatureSignificance import RegressionFeatureSignificance\n",
    "\n",
    "params = {\"p_threshold\": 0.1,\n",
    "          \"fontsize\": 12}\n",
    "\n",
    "metric = RegressionFeatureSignificance(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Decision Tree model to calculate feature importance on all preliminary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the model\n",
    "tree_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "tree_model_fit = tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.PermutationFeatureImportance import PermutationFeatureImportance\n",
    "\n",
    "# Create VM model\n",
    "vm_model_pfi = vm.init_model(\n",
    "    model = tree_model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)\n",
    "\n",
    "test_context = TestContext(model=vm_model_pfi)\n",
    "\n",
    "params = {\"fontsize\": None,\n",
    "          \"figure_height\": 1000}\n",
    "\n",
    "metric = PermutationFeatureImportance(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance vs Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.FeatureImportanceAndSignificance import FeatureImportanceAndSignificance\n",
    "\n",
    "test_context = TestContext(models=[vm_model_reg, vm_model_pfi])\n",
    "\n",
    "params = {\"fontsize\": 12,\n",
    "          \"p_threshold\": 0.05,\n",
    "          \"significant_only\": False,\n",
    "          \"figure_height\": 1000,\n",
    "          \"bar_width\": 0.4}\n",
    "\n",
    "metric = FeatureImportanceAndSignificance(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "drop_features = ['total_acc', 'purpose__house', 'purpose__medical', 'home_ownership__OTHER', \n",
    "                 'purpose__vacation', 'purpose__renewable_energy', 'grade__F', \n",
    "                 'purpose__major_purchase', 'purpose__wedding', 'purpose__home_improvement', 'grade__G',\n",
    "                 'purpose__moving', 'purpose__other', 'verification_status__Source Verified']\n",
    "\n",
    "X_train.drop(drop_features, axis=1, inplace=True)\n",
    "X_test.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "# If y_train and y_test are Series objects, convert them to DataFrame\n",
    "if isinstance(y_train, pd.Series):\n",
    "    y_train = y_train.to_frame()\n",
    "if isinstance(y_test, pd.Series):\n",
    "    y_test = y_test.to_frame()\n",
    "\n",
    "# Concatenate X_train with y_train and X_test with y_test\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit GLM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Update VM dataset\n",
    "vm_train_ds = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "vm_test_ds = vm.init_dataset(dataset=df_test,\n",
    "                        target_column=target_column)\n",
    "\n",
    "# Fit model\n",
    "# X_train = sm.add_constant(X_train) #BUG: need to fix model.py to support models with intercept\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "model_fit_glm = model.fit()\n",
    "print(model_fit_glm.summary())\n",
    "\n",
    "\n",
    "# Add constant to the input data if necessary\n",
    "# X_test = sm.add_constant(X_test) #BUG: fix model.py to support intercepts in regression models \n",
    "y_pred = model_fit_glm.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Metric Risk Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Metric Risk Thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Metric Risk Scoring** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create VM model\n",
    "vm_model_glm = vm.init_model(\n",
    "    model = model_fit_glm, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Fit Decision Tree model\n",
    "model_fit_tree = tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_tree = vm.init_model(\n",
    "    model = model_fit_tree, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model Risk Scoring Thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def risk_scoring_thresholds(metric_ranges):\n",
    "    # Create an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each metric in the dictionary\n",
    "    for metric, (min_value, max_value) in metric_ranges.items():\n",
    "        # Calculate the color ranges and round the values\n",
    "        red_range = [round(min_value, 2), round(min_value + (max_value - min_value) / 3, 2)]\n",
    "        amber_range = [round(min_value + (max_value - min_value) / 3, 2), round(min_value + 2 * (max_value - min_value) / 3, 2)]\n",
    "        green_range = [round(min_value + 2 * (max_value - min_value) / 3, 2), round(max_value, 2)]\n",
    "\n",
    "        # Append metric and its corresponding ranges to the rows list\n",
    "        rows.append([metric, red_range, amber_range, green_range])\n",
    "\n",
    "    # Create a dataframe from the rows list\n",
    "    table = pd.DataFrame(rows, columns=[\"Metric\", \"RED\", \"AMBER\", \"GREEN\"])\n",
    "\n",
    "    return table\n",
    "\n",
    "metric_ranges = {\n",
    "    \"Accuracy\": [0.1, 0.9],\n",
    "    \"ROC-AUC\": [0.1, 0.9],\n",
    "    \"Precision\": [0.1, 0.9],\n",
    "    \"Recall\": [0.1, 0.9],\n",
    "    \"F1\": [0.1, 0.9]\n",
    "}\n",
    "\n",
    "risk_thresholds = risk_scoring_thresholds(metric_ranges)\n",
    "display(risk_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "\n",
    "metric = ConfusionMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.ROCCurve import ROCCurve\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "\n",
    "metric = ROCCurve(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.MinimumROCAUCScore import MinimumROCAUCScore\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "\n",
    "metric = MinimumROCAUCScore(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GINI Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def gini(true, pred):\n",
    "    \"\"\"Calculate Gini coefficient given true and predicted labels\"\"\"\n",
    "    gini_score = 2 * roc_auc_score(true, pred) - 1\n",
    "    return gini_score\n",
    "\n",
    "gini_coefficient = gini(y_test, y_pred)\n",
    "(f\"Gini Coefficient: {gini_coefficient}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorecard Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Model Fit Coefficients to Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_scores(model, scaling_factor=None, base_points=None):\n",
    "    # Set default values if not provided\n",
    "    if scaling_factor is None:\n",
    "        scaling_factor = 20 / np.log(2)\n",
    "    if base_points is None:\n",
    "        base_points = 500\n",
    "\n",
    "    # Get the coefficients from the model\n",
    "    coefficients = model.params.values\n",
    "    \n",
    "    # Get the feature names from the model\n",
    "    selected_features = model.params.index\n",
    "\n",
    "    # Calculate odds ratios\n",
    "    odds_ratios = np.exp(coefficients).reshape(-1)\n",
    "    \n",
    "    # Calculate the scores for each coefficient\n",
    "    scores = scaling_factor * np.log(odds_ratios)\n",
    "    scores = base_points - scores\n",
    "\n",
    "    # Create a DataFrame to store feature names and their corresponding scores\n",
    "    feature_scores = pd.DataFrame({'Feature': selected_features, 'Score': scores})\n",
    "\n",
    "    # Sort the DataFrame in descending order of scores\n",
    "    feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    return feature_scores\n",
    "\n",
    "\n",
    "scores = calculate_scores(model_fit)\n",
    "display(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Risk Assessment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Fit Metric** Risk Scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Metric Risk Thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_fit_risk_thresholds = {\n",
    "    \"D-Squared\": {\n",
    "        \"red\": [0, 0.4],\n",
    "        \"amber\": [0.4, 0.7],\n",
    "        \"green\": [0.7, 1.0]\n",
    "    },\n",
    "    \"Ratio of Significant Features\": {\n",
    "        \"red\": [0, 40],\n",
    "        \"amber\": [40, 70],\n",
    "        \"green\": [70, 100]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Metric Risk Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def regression_model_fit_risk_scores(model_fit_glm):\n",
    "    # Risk Measure 1: D Squared\n",
    "    d_squared = 1 - (model_fit_glm.deviance / model_fit_glm.null_deviance)\n",
    "\n",
    "    # Risk Measure 2: Percentage of features with p-value less than 0.05\n",
    "    pvalues = model_fit_glm.pvalues\n",
    "    significant_features = np.sum(pvalues < 0.05)\n",
    "    total_features = pvalues.shape[0]\n",
    "    percent_significant_features = (significant_features / total_features)\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        \"Metric Risk Measure\": [\"D-Squared\", \"Ratio of Significant Features\"],\n",
    "        \"Description\": [\n",
    "            \"D-Squared: Proportion of the variability in the response variable explained by the model.\",\n",
    "            \"Ratio of Significant Features: Percentage of features with a p-value less than 0.05.\"\n",
    "        ],\n",
    "        \"Metric Risk Score\": [d_squared, percent_significant_features],\n",
    "    }\n",
    "\n",
    "    risk_scores = pd.DataFrame(data)\n",
    "\n",
    "    # Round to 1 decimal place\n",
    "    risk_scores[\"Metric Risk Score\"] = risk_scores[\"Metric Risk Score\"].round(1)\n",
    "\n",
    "    return risk_scores\n",
    "\n",
    "model_fit_risk_scores = regression_model_fit_risk_scores(model_fit_glm)\n",
    "display(model_fit_risk_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Metric** Risk Scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Metric Risk Thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_performance_risk_thresholds = {\n",
    "    \"Accuracy\": {\n",
    "        \"red\": [0, 0.5],\n",
    "        \"amber\": [0.5, 0.75],\n",
    "        \"green\": [0.75, 1.0]\n",
    "    },\n",
    "    \"ROC-AUC\": {\n",
    "        \"red\": [0, 0.6],\n",
    "        \"amber\": [0.6, 0.85],\n",
    "        \"green\": [0.85, 1.0]\n",
    "    },\n",
    "    \"Precision\": {\n",
    "        \"red\": [0, 0.4],\n",
    "        \"amber\": [0.4, 0.6],\n",
    "        \"green\": [0.6, 1.0]\n",
    "    },\n",
    "    \"Recall\": {\n",
    "        \"red\": [0, 0.4],\n",
    "        \"amber\": [0.4, 0.6],\n",
    "        \"green\": [0.6, 1.0]\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"red\": [0, 0.4],\n",
    "        \"amber\": [0.4, 0.6],\n",
    "        \"green\": [0.6, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Metric Risk Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def regression_performance_risk_scores(y_true, y_pred_probs, threshold=0.5):\n",
    "    # Threshold the probabilities to get the binary predictions\n",
    "    y_pred = (y_pred_probs > threshold).astype(int)\n",
    "\n",
    "    # Compute the metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Metric Risk Measure\": [\"Accuracy\", \"ROC-AUC\", \"Precision\", \"Recall\", \"F1\"],\n",
    "         \"Description\": [\n",
    "            \"Proportion of the total number of predictions that were correct.\",\n",
    "            \"Aggregate measure of performance across all possible classification thresholds.\",\n",
    "            \"Proportion of positive identifications that were actually correct.\",\n",
    "            \"Proportion of actual positives that were identified correctly.\",\n",
    "            \"Harmonic mean of precision and recall, it tries to find the balance between precision and recall.\"\n",
    "        ],\n",
    "        \"Metric Risk Score\": [accuracy, roc_auc, precision, recall, f1],\n",
    "    })\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "y_pred = model_fit_glm.predict(X_test)\n",
    "model_performance_risk_scores = regression_performance_risk_scores(y_test, y_pred)\n",
    "display(model_performance_risk_scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Risk Assessment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def metric_risk_assessment(risk_thresholds, test_results):\n",
    "    # Prepare thresholds and test_results data\n",
    "    thresholds_df = pd.DataFrame(risk_thresholds).T.reset_index()\n",
    "    thresholds_df.columns = ['Metric Risk Measure', 'RED', 'AMBER', 'GREEN']\n",
    "    \n",
    "    test_results_df = test_results.rename(columns={\"Risk Measure\": \"Metric Risk Measure\", \"Risk Score\": \"Metric Risk Score\"})\n",
    "\n",
    "    # Add a \"GREY\" column to the thresholds DataFrame and initialize it with \"Fail\"\n",
    "    thresholds_df[\"GREY\"] = \"Fail\"\n",
    "\n",
    "    # Replace the range values in the thresholds DataFrame with \"Pass\" or \"Fail\"\n",
    "    for row in thresholds_df.index:\n",
    "        metric = thresholds_df.loc[row, \"Metric Risk Measure\"]\n",
    "        test_result = test_results_df[test_results_df[\"Metric Risk Measure\"] == metric][\"Metric Risk Score\"].values[0]\n",
    "        grey_pass = True\n",
    "\n",
    "        for col in ['RED', 'AMBER', 'GREEN']:\n",
    "            range_values = thresholds_df.loc[row, col]\n",
    "            range_start, range_end = extract_range(range_values)\n",
    "            if range_start is not None and range_end is not None:\n",
    "                if range_start <= test_result <= range_end:\n",
    "                    thresholds_df.loc[row, col] = \"Pass\"\n",
    "                    grey_pass = False\n",
    "                else:\n",
    "                    thresholds_df.loc[row, col] = \"Fail\"\n",
    "\n",
    "        if grey_pass:\n",
    "            thresholds_df.loc[row, \"GREY\"] = \"Pass\"\n",
    "\n",
    "    # Consolidate the risk levels into a single column\n",
    "    risk_levels = ['GREY', 'RED', 'AMBER', 'GREEN']\n",
    "    risk_scoring_table = pd.concat([test_results_df, thresholds_df[risk_levels]], axis=1)\n",
    "    risk_scoring_table['Metric Risk Assessment'] = risk_scoring_table[risk_levels].apply(\n",
    "        lambda x: next((level for level in risk_levels if x[level] == \"Pass\"), None),\n",
    "        axis=1\n",
    "    )\n",
    "    risk_scoring_table.drop(columns=risk_levels, inplace=True)\n",
    "\n",
    "    # Reorder the columns to desired order\n",
    "    risk_scoring_table = risk_scoring_table[['Metric Risk Measure', 'Description', 'Metric Risk Score', 'Metric Risk Assessment']]\n",
    "\n",
    "    return risk_scoring_table\n",
    "\n",
    "def extract_range(value):\n",
    "    if isinstance(value, (list, tuple)) and len(value) == 2:\n",
    "        return value[0], value[1]\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def color_cells(val):\n",
    "    colors = {\"GREEN\": \"green\", \"AMBER\": \"yellow\", \"RED\": \"red\", \"GREY\": \"grey\"}\n",
    "    return 'background-color: %s' % colors[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Compute risk assessments for all metrics\n",
    "model_fit_risk_assessment = metric_risk_assessment(model_fit_risk_thresholds, model_fit_risk_scores)\n",
    "model_performance_risk_assessment = metric_risk_assessment(model_performance_risk_thresholds, model_performance_risk_scores)\n",
    "\n",
    "model_risk_assessment = pd.concat([model_performance_risk_assessment, model_fit_risk_assessment]).reset_index(drop=True)\n",
    "model_risk_assessment.style.applymap(color_cells, subset=['Metric Risk Assessment'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# If 'df' is your DataFrame and 'column_name' is the name of the column\n",
    "unique_values = df['inq_last_6mths'].unique()\n",
    "print(unique_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cardinality of Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots of Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Ratios by Categorical Feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.DefaultRateBarPlots import DefaultRatioBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": target_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = DefaultRatioBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Bar Plots of Default Ratios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.BivariateFeaturesBarPlots import BivariateFeaturesBarPlots\n",
    "\n",
    "# Pass target column to validmind dataset\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure the metric\n",
    "features_pairs = {'home_ownership': 'grade', \n",
    "                  'purpose': 'grade',\n",
    "                  'grade': 'verification_status'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "}\n",
    "\n",
    "metric = BivariateFeaturesBarPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots by Default Status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.BivariateScatterPlots import BivariateScatterPlots\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateScatterPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Histograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.BivariateHistograms import BivariateHistograms\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateHistograms(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Engineering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dummy Catergorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def add_dummy_variables(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generate dummy variables for specified columns in the DataFrame,\n",
    "    concatenate them with the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column + \":\", drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# df_train = add_dummy_variables(df_train, ['grade', 'home_ownership', 'verification_status', 'purpose'])\n",
    "# df_test = add_dummy_variables(df_test, ['grade', 'home_ownership', 'verification_status', 'purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Adjust the X_test DataFrame to match the column structure of the X_train DataFrame\n",
    "# df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of Evidence (WoE) Binning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a modelling perspective, the **WoE** allows us to transform raw variables into a format which provides a more robust base for statistical analysis. Specifically, the WoE measures the predictive power of an individual class of a categorical variable, distinguishing between 'good' (non-defaulters) and 'bad' (defaulters) risks. This is accomplished by comparing the distribution of 'good' and 'bad' risks within a specific category to the overall 'good'/'bad' distribution. If the 'good'/'bad' ratio of a particular category is significantly divergent from the overall ratio, it suggests that category is a strong predictor of credit risk.\n",
    "\n",
    "**Information Value (IV)**, on the other hand, is a fundamental metric we use to quantify the predictive power of each input variable in our scorecards. The IV is calculated by taking the sum of the differences between the WoE of each category and the overall WoE, multiplied by the WoE of that category. In other words, IV measures the total amount of 'information' or predictive power a variable brings to the model. For example, variables with an IV between 0.1 and 0.3 provide a weak predictive power, those between 0.3 and 0.5 a medium predictive power, and those with an IV greater than 0.5 have strong predictive power. Therefore, we utilize the IV to prioritize variables for inclusion in the model and to ensure the model's stability and accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WoE and IV for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#categorical_woe_iv_df = calculate_woe_iv(df_train, target_column, categorical_features)\n",
    "#display(categorical_woe_iv_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEIVPlots import WoEandIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": categorical_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# We can now evaluate on the test set\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range', 'installment']  # Added 'installment'\n",
    "\n",
    "# Handle categorical features\n",
    "df_encoded = pd.get_dummies(df_multivariate, columns=categorical_features)\n",
    "\n",
    "# Split the data\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "glm_model_fit = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = glm_model_fit.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = sm.add_constant(X_test)  # Adding a constant to the test data\n",
    "y_pred = results.predict(X_test)\n",
    "\n",
    "# You can then further analyze y_pred to measure model performance on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale your variables\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ValidMind Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize training and testing datasets for model A\n",
    "vm_train_ds = vm.init_dataset(dataset=X_train, type=\"generic\", target_column='loan_status')\n",
    "vm_test_ds = vm.init_dataset(dataset=X_test, type=\"generic\", target_column='loan_status')\n",
    "\n",
    "# Initialize model A\n",
    "vm_model_A = vm.init_model(\n",
    "    model = glm_model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
