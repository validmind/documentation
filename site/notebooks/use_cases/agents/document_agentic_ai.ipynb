{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7277c38",
   "metadata": {},
   "source": [
    "# Document an agentic AI system\n",
    "\n",
    "Build and document an agentic AI system with the ValidMind Library. Construct a LangGraph-based banking agent, assign AI evaluation metric scores to your agent, and run accuracy, RAGAS, and safety tests, then log those test results to the ValidMind Platform.\n",
    "\n",
    "An _AI agent_ is an autonomous system that interprets inputs, selects from available tools or actions, and executes multi-step behaviors to achieve defined goals. In this notebook, the agent acts as a banking assistant that analyzes user requests and automatically selects and invokes the appropriate specialized banking tool to deliver accurate, compliant, and actionable responses.\n",
    "\n",
    "- This agent enables financial institutions to automate complex banking workflows where different customer requests require different specialized tools and knowledge bases.\n",
    "- Effective validation of agentic AI systems reduces the risks of agents misinterpreting inputs, failing to extract required parameters, or producing incorrect assessments or actions — such as selecting the wrong tool.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For the LLM components in this notebook to function properly, you'll need access to OpenAI.</b></span>\n",
    "<br></br>\n",
    "Before you continue, ensure that a valid <code>OPENAI_API_KEY</code> is set in your <code>.env</code> file.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dd942",
   "metadata": {},
   "source": [
    "::: {.content-hidden when-format=\"html\"}\n",
    "## Contents    \n",
    "- [About ValidMind](#toc1__)    \n",
    "  - [Before you begin](#toc1_1__)    \n",
    "  - [New to ValidMind?](#toc1_2__)    \n",
    "  - [Key concepts](#toc1_3__)    \n",
    "- [Setting up](#toc2__)    \n",
    "  - [Install the ValidMind Library](#toc2_1__)    \n",
    "  - [Initialize the ValidMind Library](#toc2_2__)    \n",
    "    - [Register sample model](#toc2_2_1__)    \n",
    "    - [Apply documentation template](#toc2_2_2__)    \n",
    "    - [Get your code snippet](#toc2_2_3__)    \n",
    "    - [Preview the documentation template](#toc2_2_4__)    \n",
    "  - [Verify OpenAI API access](#toc2_3__)    \n",
    "  - [Initialize the Python environment](#toc2_4__)    \n",
    "- [Building the LangGraph agent](#toc3__)    \n",
    "  - [Test available banking tools](#toc3_1__)    \n",
    "  - [Create LangGraph banking agent](#toc3_2__)    \n",
    "    - [Define system prompt](#toc3_2_1__)    \n",
    "    - [Initialize the LLM](#toc3_2_2__)    \n",
    "    - [Define agent state structure](#toc3_2_3__)    \n",
    "    - [Create agent workflow function](#toc3_2_4__)    \n",
    "    - [Instantiate the banking agent](#toc3_2_5__)    \n",
    "  - [Integrate agent with ValidMind](#toc3_3__)    \n",
    "    - [Import ValidMind components](#toc3_3_1__)    \n",
    "    - [Create agent wrapper function](#toc3_3_2__)    \n",
    "    - [Initialize the ValidMind model object](#toc3_3_3__)    \n",
    "    - [Store the agent reference](#toc3_3_4__)    \n",
    "    - [Verify integration](#toc3_3_5__)    \n",
    "  - [Validate the system prompt](#toc3_4__)    \n",
    "- [Initialize the ValidMind datasets](#toc4__)    \n",
    "  - [Assign predictions](#toc4_1__)    \n",
    "- [Running accuracy tests](#toc5__)    \n",
    "  - [Response accuracy test](#toc5_1__)    \n",
    "  - [Tool selection accuracy test](#toc5_2__)    \n",
    "- [Assigning AI evaluation metric scores](#toc6__)    \n",
    "  - [Identify relevant DeepEval scorers](#toc6_1__)    \n",
    "  - [Assign reasoning scores](#toc6_2__)    \n",
    "    - [Plan quality score](#toc6_2_1__)    \n",
    "    - [Plan adherence score](#toc6_2_2__)    \n",
    "  - [Assign action scores](#toc6_3__)    \n",
    "    - [Tool correctness score](#toc6_3_1__)    \n",
    "    - [Argument correctness score](#toc6_3_2__)    \n",
    "  - [Assign execution scores](#toc6_4__)    \n",
    "    - [Task completion score](#toc6_4_1__)    \n",
    "- [Running RAGAS tests](#toc7__)    \n",
    "  - [Identify relevant RAGAS tests](#toc7_1__)    \n",
    "    - [Faithfulness](#toc7_1_1__)    \n",
    "    - [Response Relevancy](#toc7_1_2__)    \n",
    "    - [Context Recall](#toc7_1_3__)    \n",
    "- [Running safety tests](#toc8__)    \n",
    "    - [AspectCritic](#toc8_1_1__)    \n",
    "    - [Bias](#toc8_1_2__)    \n",
    "- [Next steps](#toc9__)    \n",
    "  - [Work with your model documentation](#toc9_1__)    \n",
    "  - [Customize the banking agent for your use case](#toc9_2__)    \n",
    "  - [Discover more learning resources](#toc9_3__)    \n",
    "- [Upgrade ValidMind](#toc10__)    \n",
    "\n",
    ":::\n",
    "<!-- jn-toc-notebook-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=4\n",
    "\t/jn-toc-notebook-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaad35f",
   "metadata": {},
   "source": [
    "<a id='toc1__'></a>\n",
    "\n",
    "## About ValidMind\n",
    "\n",
    "ValidMind is a suite of tools for managing model risk, including risk associated with AI and statistical models. \n",
    "\n",
    "You use the ValidMind Library to automate documentation and validation tests, and then use the ValidMind Platform to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1f9ef",
   "metadata": {},
   "source": [
    "<a id='toc1_1__'></a>\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad8d8c",
   "metadata": {},
   "source": [
    "<a id='toc1_2__'></a>\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "If you haven't already seen our documentation on the [ValidMind Library](https://docs.validmind.ai/developer/validmind-library.html), we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models and running tests, as well as find code samples and our Python Library API reference.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
    "<br></br>\n",
    "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323caa59",
   "metadata": {},
   "source": [
    "<a id='toc1_3__'></a>\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n",
    "\n",
    "**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n",
    "\n",
    "**Tests**: A function contained in the ValidMind Library, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n",
    "\n",
    "**Metrics**: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\n",
    "\n",
    "**Custom metrics**: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with the ValidMind Library to be used in the ValidMind Platform.\n",
    "\n",
    "**Inputs**: Objects to be evaluated and documented in the ValidMind Library. They can be any of the following:\n",
    "\n",
    "  - **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\n",
    "  - **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\n",
    "  - **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\n",
    "  - **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. (Learn more: [Run tests with multiple datasets](https://docs.validmind.ai/notebooks/how_to/tests/run_tests/configure_tests/run_tests_that_require_multiple_datasets.html))\n",
    "\n",
    "**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\n",
    "\n",
    "**Outputs**: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n",
    "\n",
    "**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\n",
    "\n",
    "Example: the [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba5169",
   "metadata": {},
   "source": [
    "<a id='toc2__'></a>\n",
    "\n",
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53da99c",
   "metadata": {},
   "source": [
    "<a id='toc2_1__'></a>\n",
    "\n",
    "### Install the ValidMind Library\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Recommended Python versions</b></span>\n",
    "<br></br>\n",
    "Python 3.8 <= x <= 3.11</div>\n",
    "\n",
    "Let's begin by installing the ValidMind Library with large language model (LLM) support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"validmind[llm]\" \"langgraph==0.3.21\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9dea3a",
   "metadata": {},
   "source": [
    "<a id='toc2_2__'></a>\n",
    "\n",
    "### Initialize the ValidMind Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848461e",
   "metadata": {},
   "source": [
    "<a id='toc2_2_1__'></a>\n",
    "\n",
    "#### Register sample model\n",
    "\n",
    "Let's first register a sample model for use with this notebook.\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Inventory** and click **+ Register Model**.\n",
    "\n",
    "3. Enter the model details and click **Next >** to continue to assignment of model stakeholders. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
    "\n",
    "4. Select your own name under the **MODEL OWNER** drop-down.\n",
    "\n",
    "5. Click **Register Model** to add the model to your inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0b04b",
   "metadata": {},
   "source": [
    "<a id='toc2_2_2__'></a>\n",
    "\n",
    "#### Apply documentation template\n",
    "\n",
    "Once you've registered your model, let's select a documentation template. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "1. In the left sidebar that appears for your model, click **Documents** and select **Documentation**.\n",
    "\n",
    "2. Under **TEMPLATE**, select `Agentic AI`.\n",
    "\n",
    "3. Click **Use Template** to apply the template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279d5fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Can't select this template?</b></span>\n",
    "<br></br>\n",
    "Your organization administrators may need to add it to your template library:\n",
    "<ul>\n",
    "<li><a href=\"agentic_ai_template.yaml\" style=\"color: #DE257E;\"><b>Download Template YAML</b></a></li>\n",
    "<li><a href=\"https://docs.validmind.ai/guide/templates/customize-document-templates.html\" style=\"color: #DE257E;\"><b>Customize Document Templates</b></a></li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606cb8c",
   "metadata": {},
   "source": [
    "<a id='toc2_2_3__'></a>\n",
    "\n",
    "#### Get your code snippet\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "1. On the left sidebar that appears for your model, select **Getting Started** and click **Copy snippet to clipboard**.\n",
    "2. Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccbefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model identifier credentials from an `.env` file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    # api_host=\"...\",\n",
    "    # api_key=\"...\",\n",
    "    # api_secret=\"...\",\n",
    "    # model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed79cf0",
   "metadata": {},
   "source": [
    "<a id='toc2_2_4__'></a>\n",
    "\n",
    "#### Preview the documentation template\n",
    "\n",
    "Let's verify that you have connected the ValidMind Library to the ValidMind Platform and that the appropriate *template* is selected for your model.\n",
    "\n",
    "You will upload documentation and test results unique to your model based on this template later on. For now, **take a look at the default structure that the template provides with [the `vm.preview_template()` function](https://docs.validmind.ai/validmind/validmind.html#preview_template)** from the ValidMind library and note the empty sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdaa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5ba68",
   "metadata": {},
   "source": [
    "<a id='toc2_3__'></a>\n",
    "\n",
    "### Verify OpenAI API access\n",
    "\n",
    "Verify that a valid `OPENAI_API_KEY` is set in your `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables if using .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed. Make sure OPENAI_API_KEY is set in your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9d3a9",
   "metadata": {},
   "source": [
    "<a id='toc2_4__'></a>\n",
    "\n",
    "### Initialize the Python environment\n",
    "\n",
    "Let's import all the necessary libraries to prepare for building our banking LangGraph agentic system:\n",
    "\n",
    "- **Standard libraries** for data handling and environment management.\n",
    "- **pandas**, a Python library for data manipulation and analytics, as an alias. We'll also configure pandas to show all columns and all rows at full width for easier debugging and inspection.\n",
    "- **LangChain** components for LLM integration and tool management.\n",
    "- **LangGraph** for building stateful, multi-step agent workflows.\n",
    "- **Banking tools** for specialized financial services as defined in [banking_tools.py](banking_tools.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRARY IMPORTS\n",
    "\n",
    "# TypedDict: Defines type-safe dictionaries for the agent's state structure\n",
    "# Annotated: Adds metadata to type hints\n",
    "# Sequence: Type hint for sequences used in the agent\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "# THIRD PARTY IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "# Configure pandas to show all columns and all rows at full width\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# BaseMessage: Represents a base message in the LangChain message system\n",
    "# HumanMessage: Represents a human message in the LangChain message system\n",
    "# SystemMessage: Represents a system message in the LangChain message system\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# ChatOpenAI: Represents an OpenAI chat model in the LangChain library\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# MemorySaver: Represents a checkpoint for saving and restoring agent state\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# StateGraph: Represents a stateful graph in the LangGraph library\n",
    "# END: Represents the end of a graph\n",
    "# START: Represents the start of a graph\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "# add_messages: Adds messages to the state\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# ToolNode: Represents a tool node in the LangGraph library\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# LOCAL IMPORTS FROM banking_tools.py\n",
    "\n",
    "from banking_tools import AVAILABLE_TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109d075",
   "metadata": {},
   "source": [
    "<a id='toc3__'></a>\n",
    "\n",
    "## Building the LangGraph agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15040411",
   "metadata": {},
   "source": [
    "<a id='toc3_1__'></a>\n",
    "\n",
    "### Test available banking tools\n",
    "\n",
    "We'll use the demo banking tools defined in `banking_tools.py` that provide use cases of financial services:\n",
    "\n",
    "- **Credit Risk Analyzer** - Loan applications and credit decisions\n",
    "- **Customer Account Manager** - Account services and customer support\n",
    "- **Fraud Detection System** - Security and fraud prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available tools: {len(AVAILABLE_TOOLS)}\")\n",
    "print(\"\\nTool Details:\")\n",
    "for i, tool in enumerate(AVAILABLE_TOOLS, 1):\n",
    "    print(f\"   - {tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6785a",
   "metadata": {},
   "source": [
    "Let's test each banking tool individually to ensure they're working correctly before integrating them into our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0caff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Credit Risk Analyzer\n",
    "print(\"TEST 1: Credit Risk Analyzer\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    # Access the underlying function using .func\n",
    "    credit_result = AVAILABLE_TOOLS[0].func(\n",
    "        customer_income=75000,\n",
    "        customer_debt=1200,\n",
    "        credit_score=720,\n",
    "        loan_amount=50000,\n",
    "        loan_type=\"personal\"\n",
    "    )\n",
    "    print(credit_result)\n",
    "    print(\"Credit Risk Analyzer test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Credit Risk Analyzer test FAILED: {e}\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b227db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test 2: Customer Account Manager\n",
    "print(\"TEST 2: Customer Account Manager\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    # Test checking balance\n",
    "    account_result = AVAILABLE_TOOLS[1].func(\n",
    "        account_type=\"checking\",\n",
    "        customer_id=\"12345\",\n",
    "        action=\"check_balance\"\n",
    "    )\n",
    "    print(account_result)\n",
    "\n",
    "    # Test getting account info\n",
    "    info_result = AVAILABLE_TOOLS[1].func(\n",
    "        account_type=\"all\",\n",
    "        customer_id=\"12345\", \n",
    "        action=\"get_info\"\n",
    "    )\n",
    "    print(info_result)\n",
    "    print(\"Customer Account Manager test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Customer Account Manager test FAILED: {e}\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test 3: Fraud Detection System\n",
    "print(\"TEST 3: Fraud Detection System\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    fraud_result = AVAILABLE_TOOLS[2].func(\n",
    "        transaction_id=\"TX123\",\n",
    "        customer_id=\"12345\",\n",
    "        transaction_amount=500.00,\n",
    "        transaction_type=\"withdrawal\",\n",
    "        location=\"Miami, FL\",\n",
    "        device_id=\"DEVICE_001\"\n",
    "    )\n",
    "    print(fraud_result)\n",
    "    print(\"Fraud Detection System test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Fraud Detection System test FAILED: {e}\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf04845",
   "metadata": {},
   "source": [
    "<a id='toc3_2__'></a>\n",
    "\n",
    "### Create LangGraph banking agent\n",
    "\n",
    "With our tools ready to go, we'll create our intelligent banking agent with LangGraph that automatically selects and uses the appropriate banking tool based on a user request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df57f0",
   "metadata": {},
   "source": [
    "<a id='toc3_2_1__'></a>\n",
    "\n",
    "#### Define system prompt\n",
    "\n",
    "We'll begin by defining our system prompt, which provides the LLM with context about its role as a banking assistant and guidance on when to use each available tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced banking system prompt with tool selection guidance\n",
    "system_context = \"\"\"You are a professional banking AI assistant with access to specialized banking tools.\n",
    "            Analyze the user's banking request and directly use the most appropriate tools to help them.\n",
    "            \n",
    "            AVAILABLE BANKING TOOLS:\n",
    "            \n",
    "            credit_risk_analyzer - Analyze credit risk for loan applications and credit decisions\n",
    "            - Use for: loan applications, credit assessments, risk analysis, mortgage eligibility\n",
    "            - Examples: \"Analyze credit risk for $50k personal loan\", \"Assess mortgage eligibility for $300k home purchase\"\n",
    "            - Parameters: customer_income, customer_debt, credit_score, loan_amount, loan_type\n",
    "\n",
    "            customer_account_manager - Manage customer accounts and provide banking services\n",
    "            - Use for: account information, transaction processing, product recommendations, customer service\n",
    "            - Examples: \"Check balance for checking account 12345\", \"Recommend products for customer with high balance\"\n",
    "            - Parameters: account_type, customer_id, action, amount, account_details\n",
    "\n",
    "            fraud_detection_system - Analyze transactions for potential fraud and security risks\n",
    "            - Use for: transaction monitoring, fraud prevention, risk assessment, security alerts\n",
    "            - Examples: \"Analyze fraud risk for $500 ATM withdrawal in Miami\", \"Check security for $2000 online purchase\"\n",
    "            - Parameters: transaction_id, customer_id, transaction_amount, transaction_type, location, device_id\n",
    "\n",
    "            BANKING INSTRUCTIONS:\n",
    "            - Analyze the user's banking request carefully and identify the primary need\n",
    "            - If they need credit analysis → use credit_risk_analyzer\n",
    "            - If they need financial calculations → use financial_calculator\n",
    "            - If they need account services → use customer_account_manager\n",
    "            - If they need security analysis → use fraud_detection_system\n",
    "            - Extract relevant parameters from the user's request\n",
    "            - Provide helpful, accurate banking responses based on tool outputs\n",
    "            - Always consider banking regulations, risk management, and best practices\n",
    "            - Be professional and thorough in your analysis\n",
    "\n",
    "            Choose and use tools wisely to provide the most helpful banking assistance.\n",
    "            Describe the response in user friendly manner with details describing the tool output. \n",
    "            Provide the response in at least 500 words.\n",
    "            Generate a concise execution plan for the banking request.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406835c8",
   "metadata": {},
   "source": [
    "<a id='toc3_2_2__'></a>\n",
    "\n",
    "#### Initialize the LLM\n",
    "\n",
    "Let's initialize the LLM that will power our banking agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866066e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the main LLM for banking responses\n",
    "main_llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",\n",
    "    reasoning={\n",
    "        \"effort\": \"low\",\n",
    "        \"summary\": \"auto\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce9685c",
   "metadata": {},
   "source": [
    "Then bind the available banking tools to the LLM, enabling the model to automatically recognize and invoke each tool when appropriate based on request input and the system prompt we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind all banking tools to the main LLM\n",
    "llm_with_tools = main_llm.bind_tools(AVAILABLE_TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad8799",
   "metadata": {},
   "source": [
    "<a id='toc3_2_3__'></a>\n",
    "\n",
    "#### Define agent state structure\n",
    "\n",
    "The agent state defines the data structure that flows through the LangGraph workflow. It includes:\n",
    "\n",
    "- **messages** — The conversation history between the user and agent\n",
    "- **user_input** — The current user request\n",
    "- **session_id** — A unique identifier for the conversation session\n",
    "- **context** — Additional context that can be passed between nodes\n",
    "\n",
    "Defining this state structure maintains the structure throughout the agent's execution and allows for multi-turn conversations with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b926ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Agent State Definition\n",
    "class BankingAgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    user_input: str\n",
    "    session_id: str\n",
    "    context: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce81b7",
   "metadata": {},
   "source": [
    "<a id='toc3_2_4__'></a>\n",
    "\n",
    "#### Create agent workflow function\n",
    "\n",
    "We'll build the LangGraph agent workflow with two main components:\n",
    "\n",
    "1. **LLM node** — Processes user requests, applies the system prompt, and decides whether to use tools.\n",
    "2. **Tools node** — Executes the selected banking tools when the LLM determines they're needed.\n",
    "\n",
    "The workflow begins with the LLM analyzing the request, then uses tools if needed — or ends if the response is complete, and finally returns to the LLM to generate the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_banking_langgraph_agent():\n",
    "    \"\"\"Create a comprehensive LangGraph banking agent with intelligent tool selection.\"\"\"\n",
    "    def llm_node(state: BankingAgentState) -> BankingAgentState:\n",
    "        \"\"\"Main LLM node that processes banking requests and selects appropriate tools.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        # Add system context to messages\n",
    "        enhanced_messages = [SystemMessage(content=system_context)] + list(messages)\n",
    "        # Get LLM response with tool selection\n",
    "        response = llm_with_tools.invoke(enhanced_messages)\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": messages + [response]\n",
    "        }\n",
    "    \n",
    "    def should_continue(state: BankingAgentState) -> str:\n",
    "        \"\"\"Decide whether to use tools or end the conversation.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        # Check if the LLM wants to use tools\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "        \n",
    "    # Create the banking state graph\n",
    "    workflow = StateGraph(BankingAgentState)\n",
    "    # Add nodes\n",
    "    workflow.add_node(\"llm\", llm_node)\n",
    "    workflow.add_node(\"tools\", ToolNode(AVAILABLE_TOOLS))\n",
    "    # Simplified entry point - go directly to LLM\n",
    "    workflow.add_edge(START, \"llm\")\n",
    "    # From LLM, decide whether to use tools or end\n",
    "    workflow.add_conditional_edges(\n",
    "        \"llm\",\n",
    "        should_continue,\n",
    "        {\"tools\": \"tools\", END: END}\n",
    "    )\n",
    "    # Tool execution flows back to LLM for final response\n",
    "    workflow.add_edge(\"tools\", \"llm\")\n",
    "    # Set up memory\n",
    "    memory = MemorySaver()\n",
    "    # Compile the graph\n",
    "    agent = workflow.compile(checkpointer=memory)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb40287",
   "metadata": {},
   "source": [
    "<a id='toc3_2_5__'></a>\n",
    "\n",
    "#### Instantiate the banking agent\n",
    "\n",
    "Now, we'll create an instance of the banking agent by calling the workflow creation function.\n",
    "\n",
    "This compiled agent is ready to process banking requests and will automatically select and use the appropriate tools based on user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the banking intelligent agent\n",
    "banking_agent = create_banking_langgraph_agent()\n",
    "\n",
    "print(\"Banking LangGraph Agent Created Successfully!\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"   - Intelligent banking tool selection\")\n",
    "print(\"   - Comprehensive banking system prompt\")\n",
    "print(\"   - Streamlined workflow: LLM → Tools → Response\")\n",
    "print(\"   - Automatic tool parameter extraction\")\n",
    "print(\"   - Professional banking assistance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12691528",
   "metadata": {},
   "source": [
    "<a id='toc3_3__'></a>\n",
    "\n",
    "### Integrate agent with ValidMind\n",
    "\n",
    "To integrate our LangGraph banking agent with ValidMind, we need to create a wrapper function that ValidMind can use to invoke the agent and extract the necessary information for testing and documentation, allowing ValidMind to run validation tests on the agent's behavior, tool usage, and responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78509b",
   "metadata": {},
   "source": [
    "<a id='toc3_3_1__'></a>\n",
    "\n",
    "#### Import ValidMind components\n",
    "\n",
    "We'll start with importing the necessary ValidMind components for integrating our agent:\n",
    "\n",
    "- `Prompt` from `validmind.models` for handling prompt-based model inputs\n",
    "- `extract_tool_calls_from_agent_output` and `_convert_to_tool_call_list` from `validmind.scorers.llm.deepeval` for extracting and converting tool calls from agent outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import Prompt\n",
    "from validmind.scorers.llm.deepeval import extract_tool_calls_from_agent_output, _convert_to_tool_call_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f2955",
   "metadata": {},
   "source": [
    "<a id='toc3_3_2__'></a>\n",
    "\n",
    "#### Create agent wrapper function\n",
    "\n",
    "We'll then create a wrapper function that:\n",
    "\n",
    "- Accepts input in ValidMind's expected format (with `input` and `session_id` fields)\n",
    "- Invokes the banking agent with the proper state initialization\n",
    "- Captures tool outputs and tool calls for evaluation\n",
    "- Returns a standardized response format that includes the prediction, full output, tool messages, and tool call information\n",
    "- Handles errors gracefully with fallback responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def banking_agent_fn(input):\n",
    "    \"\"\"\n",
    "    Invoke the banking agent with the given input.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initial state for banking agent\n",
    "        initial_state = {\n",
    "            \"user_input\": input[\"input\"],\n",
    "            \"messages\": [HumanMessage(content=input[\"input\"])],\n",
    "            \"session_id\": input[\"session_id\"],\n",
    "            \"context\": {}\n",
    "        }\n",
    "        session_config = {\"configurable\": {\"thread_id\": input[\"session_id\"]}}\n",
    "        result = banking_agent.invoke(initial_state, config=session_config)\n",
    "\n",
    "        from utils import capture_tool_output_messages\n",
    "\n",
    "        # Capture all tool outputs and metadata\n",
    "        captured_data = capture_tool_output_messages(result)\n",
    "    \n",
    "        # Access specific tool outputs, this will be used for RAGAS tests\n",
    "        tool_message = \"\"\n",
    "        for output in captured_data[\"tool_outputs\"]:\n",
    "            tool_message += output['content']\n",
    "        \n",
    "        tool_calls_found = []\n",
    "        messages = result['messages']\n",
    "        for message in messages:\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    # Handle both dictionary and object formats\n",
    "                    if isinstance(tool_call, dict):\n",
    "                        tool_calls_found.append(tool_call['name'])\n",
    "                    else:\n",
    "                        # ToolCall object - use attribute access\n",
    "                        tool_calls_found.append(tool_call.name)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"prediction\": result['messages'][-1].content[0]['text'],\n",
    "            \"output\": result,\n",
    "            \"tool_messages\": [tool_message],\n",
    "            # \"tool_calls\": tool_calls_found,\n",
    "            \"tool_called\": _convert_to_tool_call_list(extract_tool_calls_from_agent_output(result))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Return a fallback response if the agent fails\n",
    "        error_message = f\"\"\"I apologize, but I encountered an error while processing your banking request: {str(e)}.\n",
    "        Please try rephrasing your question or contact support if the issue persists.\"\"\"\n",
    "        return {\n",
    "            \"prediction\": error_message, \n",
    "            \"output\": {\n",
    "                \"messages\": [HumanMessage(content=input[\"input\"]), SystemMessage(content=error_message)],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc90d6",
   "metadata": {},
   "source": [
    "<a id='toc3_3_3__'></a>\n",
    "\n",
    "#### Initialize the ValidMind model object\n",
    "\n",
    "We'll also need to register the banking agent as a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data.\n",
    "\n",
    "You simply initialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model) that:\n",
    "\n",
    "- Associates the wrapper function with the model for prediction\n",
    "- Stores the system prompt template for documentation\n",
    "- Provides a unique `input_id` for tracking and identification\n",
    "- Enables the agent to be used with ValidMind's testing and documentation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent as a model\n",
    "vm_banking_model = vm.init_model(\n",
    "    input_id=\"banking_agent_model\",\n",
    "    predict_fn=banking_agent_fn,\n",
    "    prompt=Prompt(template=system_context)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed446a",
   "metadata": {},
   "source": [
    "<a id='toc3_3_4__'></a>\n",
    "\n",
    "#### Store the agent reference\n",
    "\n",
    "We'll also store a reference to the original banking agent object in the ValidMind model. This allows us to access the full agent functionality directly if needed, while still maintaining the wrapper function interface for ValidMind's testing framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c653471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the banking agent to the vm model\n",
    "vm_banking_model.model = banking_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44ea16",
   "metadata": {},
   "source": [
    "<a id='toc3_3_5__'></a>\n",
    "\n",
    "#### Verify integration\n",
    "\n",
    "Let's confirm that the banking agent has been successfully integrated with ValidMind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e101b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Banking Agent Successfully Integrated with ValidMind!\")\n",
    "print(f\"Model ID: {vm_banking_model.input_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80518d",
   "metadata": {},
   "source": [
    "<a id='toc3_4__'></a>\n",
    "\n",
    "### Validate the system prompt\n",
    "\n",
    "Let's get an initial sense of how well our defined system prompt meets a few best practices for prompt engineering by running a few tests — we'll run evaluation tests later on our agent's performance.\n",
    "\n",
    "You run individual tests by calling [the `run_test` function](https://docs.validmind.ai/validmind/validmind/tests.html#run_test) provided by the `validmind.tests` module. Passing in our agentic model as an input, the tests below rate the prompt on a scale of 1-10 against the following criteria:\n",
    "\n",
    "- **[Clarity](https://docs.validmind.ai/tests/prompt_validation/Clarity.html)** — How clearly the prompt states the task.\n",
    "- **[Conciseness](https://docs.validmind.ai/tests/prompt_validation/Conciseness.html)** — How succinctly the prompt states the task.\n",
    "- **[Delimitation](https://docs.validmind.ai/tests/prompt_validation/Delimitation.html)** — When using complex prompts containing examples, contextual information, or other elements, is the prompt formatted in such a way that each element is clearly separated?\n",
    "- **[NegativeInstruction](https://docs.validmind.ai/tests/prompt_validation/NegativeInstruction.html)** — Whether the prompt contains negative instructions.\n",
    "- **[Specificity](https://docs.validmind.ai/tests/prompt_validation/NegativeInstruction.html)** — How specific the prompt defines the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Clarity\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d52333",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Conciseness\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa89976",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Delimitation\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.NegativeInstruction\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba99915",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Specificity\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d6d77",
   "metadata": {},
   "source": [
    "<a id='toc4__'></a>\n",
    "\n",
    "## Initialize the ValidMind datasets\n",
    "\n",
    "After validation our system prompt, let's import our sample dataset ([banking_test_dataset.py](banking_test_dataset.py)), which we'll use in the next section to evaluate our agent's performance across different banking scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from banking_test_dataset import banking_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268ce6e",
   "metadata": {},
   "source": [
    "The next step is to connect your data with a ValidMind `Dataset` object. **This step is always necessary every time you want to connect a dataset to documentation and produce test results through ValidMind,** but you only need to do it once per dataset.\n",
    "\n",
    "Initialize a ValidMind dataset object using the [`init_dataset` function](https://docs.validmind.ai/validmind/validmind.html#init_dataset) from the ValidMind (`vm`) module. For this example, we'll pass in the following arguments:\n",
    "\n",
    "- **`input_id`** — A unique identifier that allows tracking what inputs are used when running each individual test.\n",
    "- **`dataset`** — The raw dataset that you want to provide as input to tests.\n",
    "- **`text_column`** — The name of the column containing the text input data.\n",
    "- **`target_column`** — A required argument if tests require access to true values. This is the name of the target column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset = vm.init_dataset(\n",
    "    input_id=\"banking_test_dataset\",\n",
    "    dataset=banking_test_dataset,\n",
    "    text_column=\"input\",\n",
    "    target_column=\"possible_outputs\",\n",
    ")\n",
    "\n",
    "print(\"Banking Test Dataset Initialized in ValidMind!\")\n",
    "print(f\"Dataset ID: {vm_test_dataset.input_id}\")\n",
    "print(f\"Dataset columns: {vm_test_dataset._df.columns}\")\n",
    "vm_test_dataset._df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9143fb6",
   "metadata": {},
   "source": [
    "<a id='toc4_1__'></a>\n",
    "\n",
    "### Assign predictions\n",
    "\n",
    "Now that both the model object and the datasets have been registered, we'll assign predictions to capture the banking agent's responses for evaluation:\n",
    "\n",
    "- The [`assign_predictions()` method](https://docs.validmind.ai/validmind/validmind/vm_models.html#assign_predictions) from the `Dataset` object can link existing predictions to any number of models.\n",
    "- This method links the model's class prediction values and probabilities to our `vm_train_ds` and `vm_test_ds` datasets.\n",
    "\n",
    "If no prediction values are passed, the method will compute predictions automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d462663",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_predictions(vm_banking_model)\n",
    "\n",
    "print(\"Banking Agent Predictions Generated Successfully!\")\n",
    "print(f\"Predictions assigned to {len(vm_test_dataset._df)} test cases\")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50467e",
   "metadata": {},
   "source": [
    "<a id='toc5__'></a>\n",
    "\n",
    "## Running accuracy tests\n",
    "\n",
    "Using [`@vm.test`](https://docs.validmind.ai/validmind/validmind.html#test), let's implement some reusable custom *inline tests* to assess the accuracy of our banking agent:\n",
    "\n",
    "- An inline test refers to a test written and executed within the same environment as the code being tested — in this case, right in this Jupyter Notebook —  without requiring a separate test file or framework.\n",
    "- You'll note that the custom test functions are just regular Python functions that can include and require any Python library as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a9b90",
   "metadata": {},
   "source": [
    "<a id='toc5_1__'></a>\n",
    "\n",
    "### Response accuracy test\n",
    "\n",
    "We'll create a custom test that evaluates the banking agent's ability to provide accurate responses by:\n",
    "\n",
    "- Testing against a dataset of predefined banking questions and expected answers.\n",
    "- Checking if responses contain expected keywords and banking terminology.\n",
    "- Providing detailed test results including pass/fail status.\n",
    "- Helping identify any gaps in the agent's banking knowledge or response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90232066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@vm.test(\"my_custom_tests.banking_accuracy_test\")\n",
    "def banking_accuracy_test(model, dataset, list_of_columns):\n",
    "    \"\"\"\n",
    "    The Banking Accuracy Test evaluates whether the agent’s responses include \n",
    "    critical domain-specific keywords and phrases that indicate accurate, compliant,\n",
    "    and contextually appropriate banking information. This test ensures that the agent\n",
    "    provides responses containing the expected banking terminology, risk classifications,\n",
    "    account details, or other domain-relevant information required for regulatory compliance,\n",
    "    customer safety, and operational accuracy.\n",
    "    \"\"\"\n",
    "    df = dataset._df\n",
    "    \n",
    "    # Pre-compute responses for all tests\n",
    "    y_true = dataset.y.tolist()\n",
    "    y_pred = dataset.y_pred(model).tolist()\n",
    "\n",
    "    # Vectorized test results\n",
    "    test_results = []\n",
    "    for response, keywords in zip(y_pred, y_true):\n",
    "        # Convert keywords to list if not already a list\n",
    "        if not isinstance(keywords, list):\n",
    "            keywords = [keywords]\n",
    "        test_results.append(any(str(keyword).lower() in str(response).lower() for keyword in keywords))\n",
    "        \n",
    "    results = pd.DataFrame()\n",
    "    column_names = [col + \"_details\" for col in list_of_columns]\n",
    "    results[column_names] = df[list_of_columns]\n",
    "    results[\"actual\"] = y_pred\n",
    "    results[\"expected\"] = y_true\n",
    "    results[\"passed\"] = test_results\n",
    "    results[\"error\"] = None if test_results else f'Response did not contain any expected keywords: {y_true}'\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed5265",
   "metadata": {},
   "source": [
    "Now that we've defined our custom response accuracy test, we can run the test using the same `run_test()` function we used earlier to validate the system prompt using our sample dataset and agentic model as input, and log the test results to the ValidMind Platform with the [`log()` method](https://docs.validmind.ai/validmind/validmind/vm_models.html#log):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68884d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"my_custom_tests.banking_accuracy_test\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_dataset,\n",
    "        \"model\": vm_banking_model\n",
    "    },\n",
    "    params={\n",
    "        \"list_of_columns\": [\"input\"]\n",
    "    }\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d758ddf",
   "metadata": {},
   "source": [
    "Let's review the first five rows of the test dataset to inspect the results to see how well the banking agent performed. Each column in the output serves a specific purpose in evaluating agent performance:\n",
    "\n",
    "| Column header | Description | Importance |\n",
    "|--------------|-------------|------------|\n",
    "| **`input`** | Original user query or request | Essential for understanding the context of each test case and tracing which inputs led to specific agent behaviors. |\n",
    "| **`expected_tools`** | Banking tools that should be invoked for this request | Enables validation of correct tool selection, which is critical for agentic AI systems where choosing the right tool is a key success metric. |\n",
    "| **`expected_output`** | Expected output or keywords that should appear in the response | Defines the success criteria for each test case, enabling objective evaluation of whether the agent produced the correct result. |\n",
    "| **`session_id`** | Unique identifier for each test session | Allows tracking and correlation of related test runs, debugging specific sessions, and maintaining audit trails. |\n",
    "| **`category`** | Classification of the request type | Helps organize test results by domain and identify performance patterns across different banking use cases. |\n",
    "| **`banking_agent_model_output`** | Complete agent response including all messages and reasoning | Allows you to examine the full output to assess response quality, completeness, and correctness beyond just keyword matching. |\n",
    "| **`banking_agent_model_tool_messages`** | Messages exchanged with the banking tools | Critical for understanding how the agent interacted with tools, what parameters were passed, and what tool outputs were received. |\n",
    "| **`banking_agent_model_tool_called`** | Specific tool that was invoked | Enables validation that the agent selected the correct tool for each request, which is fundamental to agentic AI validation. |\n",
    "| **`possible_outputs`** | Alternative valid outputs or keywords that could appear in the response | Provides flexibility in evaluation by accounting for multiple acceptable response formats or variations. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f233bef",
   "metadata": {},
   "source": [
    "<a id='toc5_2__'></a>\n",
    "\n",
    "### Tool selection accuracy test\n",
    "\n",
    "We'll also create a custom test that evaluates the banking agent's ability to select the correct tools for different requests by:\n",
    "\n",
    "- Testing against a dataset of predefined banking queries with expected tool selections.\n",
    "- Comparing the tools actually invoked by the agent against the expected tools for each request.\n",
    "- Providing quantitative accuracy scores that measure the proportion of expected tools correctly selected.\n",
    "- Helping identify gaps in the agent's understanding of user needs and tool selection logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b46111",
   "metadata": {},
   "source": [
    "First, we'll define a helper function that extracts tool calls from the agent's messages and compares them against the expected tools. This function handles different message formats (dictionary or object) and calculates accuracy scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68798be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tool_calls_simple(messages, expected_tools):\n",
    "    \"\"\"Simple validation of tool calls without RAGAS dependency issues.\"\"\"\n",
    "    \n",
    "    tool_calls_found = []\n",
    "    \n",
    "    for message in messages:\n",
    "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            for tool_call in message.tool_calls:\n",
    "                # Handle both dictionary and object formats\n",
    "                if isinstance(tool_call, dict):\n",
    "                    tool_calls_found.append(tool_call['name'])\n",
    "                else:\n",
    "                    # ToolCall object - use attribute access\n",
    "                    tool_calls_found.append(tool_call.name)\n",
    "    \n",
    "    # Check if expected tools were called\n",
    "    accuracy = 0.0\n",
    "    matches = 0\n",
    "    if expected_tools:\n",
    "        matches = sum(1 for tool in expected_tools if tool in tool_calls_found)\n",
    "        accuracy = matches / len(expected_tools)\n",
    "    \n",
    "    return {\n",
    "        'expected_tools': expected_tools,\n",
    "        'found_tools': tool_calls_found,\n",
    "        'matches': matches,\n",
    "        'total_expected': len(expected_tools) if expected_tools else 0,\n",
    "        'accuracy': accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45472c",
   "metadata": {},
   "source": [
    "Now we'll define the main test function that uses the helper function to evaluate tool selection accuracy across all test cases in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vm.test(\"my_custom_tests.BankingToolCallAccuracy\")\n",
    "def BankingToolCallAccuracy(dataset, agent_output_column, expected_tools_column):\n",
    "    \"\"\"\n",
    "    Evaluates the tool selection accuracy of a LangGraph-powered banking agent.\n",
    "\n",
    "    This test measures whether the agent correctly identifies and invokes the required banking tools\n",
    "    for each user query scenario.\n",
    "    For each case, the outputs generated by the agent (including its tool calls) are compared against an\n",
    "    expected set of tools. The test considers both coverage and exactness: it computes the proportion of\n",
    "    expected tools correctly called by the agent for each instance.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (VMDataset): The dataset containing user queries, agent outputs, and ground-truth tool expectations.\n",
    "        agent_output_column (str): Dataset column name containing agent outputs (should include tool call details in 'messages').\n",
    "        expected_tools_column (str): Dataset column specifying the true expected tools (as lists).\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Per-row dictionaries with details: expected tools, found tools, match count, total expected, and accuracy score.\n",
    "\n",
    "    Purpose:\n",
    "        Provides diagnostic evidence of the banking agent's core reasoning ability—specifically, its capacity to\n",
    "        interpret user needs and select the correct banking actions. Useful for diagnosing gaps in tool coverage,\n",
    "        misclassifications, or breakdowns in agent logic.\n",
    "\n",
    "    Interpretation:\n",
    "        - An accuracy of 1.0 signals perfect tool selection for that example.\n",
    "        - Lower scores may indicate partial or complete failures to invoke required tools.\n",
    "        - Review 'found_tools' vs. 'expected_tools' to understand the source of discrepancies.\n",
    "\n",
    "    Strengths:\n",
    "        - Directly tests a core capability of compositional tool-use agents.\n",
    "        - Framework-agnostic; robust to tool call output format (object or dict).\n",
    "        - Supports batch validation and result logging for systematic documentation.\n",
    "\n",
    "    Limitations:\n",
    "        - Does not penalize extra, unnecessary tool calls.\n",
    "        - Does not assess result quality—only correct invocation.\n",
    "\n",
    "    \"\"\"\n",
    "    df = dataset._df\n",
    "    \n",
    "    results = []\n",
    "    for i, row in df.iterrows():\n",
    "        result = validate_tool_calls_simple(row[agent_output_column]['messages'], row[expected_tools_column])\n",
    "        results.append(result)\n",
    "         \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594c973",
   "metadata": {},
   "source": [
    "Finally, we can call our function with `run_test()` and log the test results to the ValidMind Platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"my_custom_tests.BankingToolCallAccuracy\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_dataset,\n",
    "    },\n",
    "    params={\n",
    "        \"agent_output_column\": \"banking_agent_model_output\",\n",
    "        \"expected_tools_column\": \"expected_tools\"\n",
    "    }\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f4107",
   "metadata": {},
   "source": [
    "<a id='toc6__'></a>\n",
    "\n",
    "## Assigning AI evaluation metric scores\n",
    "\n",
    "*AI agent evaluation metrics* are specialized measurements designed to assess how well autonomous LLM-based agents reason, plan, select and execute tools, and ultimately complete user tasks by analyzing the *full execution trace* — including reasoning steps, tool calls, intermediate decisions, and outcomes, rather than just single input–output pairs. These metrics are essential because agent failures often occur in ways traditional LLM metrics miss — for example, choosing the right tool with wrong arguments, creating a good plan but not following it, or completing a task inefficiently.\n",
    "\n",
    "In this section, we'll evaluate our banking agent's outputs and add scoring to our sample dataset against metrics defined in [DeepEval’s AI agent evaluation framework](https://deepeval.com/guides/guides-ai-agent-evaluation-metrics) which breaks down AI agent evaluation into three layers with corresponding subcategories: **reasoning**, **action**, and **execution**.\n",
    "\n",
    "Together, these three metrics enable granular diagnosis of agent behavior, help pinpoint where failures occur (reasoning, action, or execution), and support both development benchmarking and production monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c853a",
   "metadata": {},
   "source": [
    "<a id='toc6_1__'></a>\n",
    "\n",
    "### Identify relevant DeepEval scorers\n",
    "\n",
    "*Scorers* are evaluation metrics that analyze model outputs and store their results in the dataset:\n",
    "\n",
    "- Each scorer adds a new column to the dataset with format: `{scorer_name}_{metric_name}`\n",
    "- The column contains the numeric score (typically `0`-`1`) for each example\n",
    "- Multiple scorers can be run on the same dataset, each adding their own column\n",
    "- Scores are persisted in the dataset for later analysis and visualization\n",
    "- Common scorer patterns include:\n",
    "  - Model performance metrics (accuracy, F1, etc.)\n",
    "  - Output quality metrics (relevance, faithfulness)\n",
    "  - Task-specific metrics (completion, correctness)\n",
    "\n",
    "Use `list_scorers()` from [`validmind.scorers`](https://docs.validmind.ai/validmind/validmind/tests.html#scorer) to discover all available scoring methods and their IDs that can be used with `assign_scores()`. We'll filter these results to return only DeepEval scorers for our desired three metrics in a formatted table with descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all DeepEval scorers\n",
    "llm_scorers_dict = vm.tests.load._load_tests([s for s in vm.scorer.list_scorers() if \"deepeval\" in s.lower()])\n",
    "\n",
    "# Categorize scorers by metric layer\n",
    "reasoning_scorers = {}\n",
    "action_scorers = {}\n",
    "execution_scorers = {}\n",
    "\n",
    "for scorer_id, scorer_func in llm_scorers_dict.items():\n",
    "    tags = getattr(scorer_func, \"__tags__\", [])\n",
    "    scorer_name = scorer_id.split(\".\")[-1]\n",
    "\n",
    "    if \"reasoning_layer\" in tags:\n",
    "        reasoning_scorers[scorer_id] = scorer_func\n",
    "    elif \"action_layer\" in tags:\n",
    "        action_scorers[scorer_id] = scorer_func\n",
    "    elif \"TaskCompletion\" in scorer_name:\n",
    "        execution_scorers[scorer_id] = scorer_func\n",
    "\n",
    "# Display scorers by category\n",
    "print(\"=\" * 80)\n",
    "print(\"REASONING LAYER\")\n",
    "print(\"=\" * 80)\n",
    "if reasoning_scorers:\n",
    "    reasoning_df = vm.tests.load._pretty_list_tests(reasoning_scorers, truncate=True)\n",
    "    display(reasoning_df)\n",
    "else:\n",
    "    print(\"No reasoning layer scorers found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ACTION LAYER\")\n",
    "print(\"=\" * 80)\n",
    "if action_scorers:\n",
    "    action_df = vm.tests.load._pretty_list_tests(action_scorers, truncate=True)\n",
    "    display(action_df)\n",
    "else:\n",
    "    print(\"No action layer scorers found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTION LAYER\")\n",
    "print(\"=\" * 80)\n",
    "if execution_scorers:\n",
    "    execution_df = vm.tests.load._pretty_list_tests(execution_scorers, truncate=True)\n",
    "    display(execution_df)\n",
    "else:\n",
    "    print(\"No execution layer scorers found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd73d0d",
   "metadata": {},
   "source": [
    "<a id='toc6_2__'></a>\n",
    "\n",
    "### Assign reasoning scores\n",
    "\n",
    "*Reasoning* evaluates planning and strategy generation:\n",
    "\n",
    "- **Plan quality** – How logical, complete, and efficient the agent’s plan is.\n",
    "- **Plan adherence** – Whether the agent follows its own plan during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccae28",
   "metadata": {},
   "source": [
    "<a id='toc6_2_1__'></a>\n",
    "\n",
    "#### Plan quality score\n",
    "\n",
    "Let's measure how well our banking agent generates a plan before acting. A high score means the plan is logical, complete, and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f362ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.PlanQuality\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdc88f",
   "metadata": {},
   "source": [
    "<a id='toc6_2_2__'></a>\n",
    "\n",
    "#### Plan adherence score\n",
    "\n",
    "Let's check whether our banking agent follows the plan it created. Deviations lower this score and indicate gaps between reasoning and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.PlanAdherence\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    expected_output_column = \"expected_output\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1ac95",
   "metadata": {},
   "source": [
    "<a id='toc6_3__'></a>\n",
    "\n",
    "### Assign action scores\n",
    "\n",
    "*Action* assesses tool usage and argument generation:\n",
    "\n",
    "- **Tool correctness** – Whether the agent selects and calls the right tools.\n",
    "- **Argument correctness** – Whether the agent generates correct tool arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db8270",
   "metadata": {},
   "source": [
    "<a id='toc6_3_1__'></a>\n",
    "\n",
    "#### Tool correctness score\n",
    "\n",
    "Let's evaluate if our banking agent selects the appropriate tool for the task. Choosing the wrong tool reduces performance even if reasoning was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.ToolCorrectness\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    expected_tools_column = \"expected_tools\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa50b05",
   "metadata": {},
   "source": [
    "<a id='toc6_3_2__'></a>\n",
    "\n",
    "#### Argument correctness score\n",
    "\n",
    "Let's assesses whether our banking agent provides correct inputs or arguments to the selected tool. Incorrect arguments can lead to failed or unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f90489",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.ArgumentCorrectness\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e5595",
   "metadata": {},
   "source": [
    "<a id='toc6_4__'></a>\n",
    "\n",
    "### Assign execution score\n",
    "\n",
    "*Execution* measures end-to-end performance:\n",
    "\n",
    "- **Task completion** – Whether the agent successfully completes the intended task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64600ca",
   "metadata": {},
   "source": [
    "<a id='toc6_4_1__'></a>\n",
    "\n",
    "#### Task completion score\n",
    "\n",
    "Let's evaluate whether our banking agent successfully completes the requested tasks. Incomplete task execution can lead to user dissatisfaction and failed banking operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05024f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.TaskCompletion\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa9b0d",
   "metadata": {},
   "source": [
    "As you recall from the beginning of this section, when we run scorers through `assign_scores()`, the return values are automatically processed and added as new columns with the format `{scorer_name}_{metric_name}`. Note that the task completion scorer has added a new column `TaskCompletion_score` to our dataset.\n",
    "\n",
    "We'll use this column to visualize the distribution of task completion scores across our test cases through the [BoxPlot test](https://docs.validmind.ai/validmind/validmind/tests/plots/BoxPlot.html#boxplot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.plots.BoxPlot\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    params={\n",
    "        \"columns\": \"TaskCompletion_score\",\n",
    "        \"title\": \"Distribution of Task Completion Scores\",\n",
    "        \"ylabel\": \"Score\",\n",
    "        \"figsize\": (8, 6)\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012bbcb8",
   "metadata": {},
   "source": [
    "<a id='toc7__'></a>\n",
    "\n",
    "## Running RAGAS tests\n",
    "\n",
    "Next, let's run some out-of-the-box *Retrieval-Augmented Generation Assessment* (RAGAS) tests available in the ValidMind Library. RAGAS provides specialized metrics for evaluating retrieval-augmented generation systems and conversational AI agents. These metrics analyze different aspects of agent performance by assessing how well systems integrate retrieved information with generated responses.\n",
    "\n",
    "Our banking agent uses tools to retrieve information and generates responses based on that context, making it similar to a RAG system. RAGAS metrics help evaluate the quality of this integration by analyzing the relationship between retrieved tool outputs, user queries, and generated responses.\n",
    "\n",
    "These tests provide insights into how well our banking agent integrates tool usage with conversational abilities, ensuring it provides accurate, relevant, and helpful responses to banking users while maintaining fidelity to retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036afba",
   "metadata": {},
   "source": [
    "<a id='toc7_1__'></a>\n",
    "\n",
    "### Identify relevant RAGAS tests\n",
    "\n",
    "Let's explore some of ValidMind's available tests. Using ValidMind’s repository of tests streamlines your development testing, and helps you ensure that your models are being documented and evaluated appropriately.\n",
    "\n",
    "You can pass `tasks` and `tags` as parameters to the [`vm.tests.list_tests()` function](https://docs.validmind.ai/validmind/validmind/tests.html#list_tests) to filter the tests based on the tags and task types:\n",
    "\n",
    "- **`tasks`** represent the kind of modeling task associated with a test. Here we'll focus on `text_qa` tasks.\n",
    "- **`tags`** are free-form descriptions providing more details about the test, for example, what category the test falls into. Here we'll focus on the `ragas` tag.\n",
    "\n",
    "We'll then run three of these tests returned as examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests(task=\"text_qa\", tags=[\"ragas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1741ffc",
   "metadata": {},
   "source": [
    "<a id='toc7_1_1__'></a>\n",
    "\n",
    "#### Faithfulness\n",
    "\n",
    "Let's evaluate whether the banking agent's responses accurately reflect the information retrieved from tools. Unfaithful responses can misreport credit analysis, financial calculations, and compliance results—undermining user trust in the banking agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92044533",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.ragas.Faithfulness\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    param_grid={\n",
    "        \"user_input_column\": [\"input\"],\n",
    "        \"response_column\": [\"banking_agent_model_prediction\"],\n",
    "        \"retrieved_contexts_column\": [\"banking_agent_model_tool_messages\"],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b71ccc",
   "metadata": {},
   "source": [
    "<a id='toc7_1_2__'></a>\n",
    "\n",
    "#### Response Relevancy\n",
    "\n",
    "Let's evaluate whether the banking agent's answers address the user's original question or request. Irrelevant or off-topic responses can frustrate users and fail to deliver the banking information they need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7483bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.ragas.ResponseRelevancy\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    params={\n",
    "        \"user_input_column\": \"input\",\n",
    "        \"response_column\": \"banking_agent_model_prediction\",\n",
    "        \"retrieved_contexts_column\": \"banking_agent_model_tool_messages\",\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d0569",
   "metadata": {},
   "source": [
    "<a id='toc7_1_3__'></a>\n",
    "\n",
    "#### Context Recall\n",
    "\n",
    "Let's evaluate how well the banking agent uses the information retrieved from tools when generating its responses. Poor context recall can lead to incomplete or underinformed answers even when the right tools were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRecall\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    param_grid={\n",
    "        \"user_input_column\": [\"input\"],\n",
    "        \"retrieved_contexts_column\": [\"banking_agent_model_tool_messages\"],\n",
    "        \"reference_column\": [\"banking_agent_model_prediction\"],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987b00e",
   "metadata": {},
   "source": [
    "<a id='toc8__'></a>\n",
    "\n",
    "## Running safety tests\n",
    "\n",
    "Finally, let's run some out-of-the-box *safety* tests available in the ValidMind Library. Safety tests provide specialized metrics for evaluating whether AI agents operate reliably and securely. These metrics analyze different aspects of agent behavior by assessing adherence to safety guidelines, consistency of outputs, and resistance to harmful or inappropriate requests.\n",
    "\n",
    "Our banking agent handles sensitive financial information and user requests, making safety and reliability essential. Safety tests help evaluate whether the agent maintains appropriate boundaries, responds consistently and correctly to inputs, and avoids generating harmful, biased, or unprofessional content.\n",
    "\n",
    "These tests provide insights into how well our banking agent upholds standards of fairness and professionalism, ensuring it operates reliably and securely for banking users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754cca3",
   "metadata": {},
   "source": [
    "<a id='toc8_1_1__'></a>\n",
    "\n",
    "#### AspectCritic\n",
    "\n",
    "Let's evaluate our banking agent's responses across multiple quality dimensions — conciseness, coherence, correctness, harmfulness, and maliciousness. Weak performance on these dimensions can degrade user experience, fall short of professional banking standards, or introduce safety risks. \n",
    "\n",
    "We'll use the `AspectCritic` we identified earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148daa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.ragas.AspectCritic\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    param_grid={\n",
    "        \"user_input_column\": [\"input\"],\n",
    "        \"response_column\": [\"banking_agent_model_prediction\"],\n",
    "        \"retrieved_contexts_column\": [\"banking_agent_model_tool_messages\"],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5b1f6",
   "metadata": {},
   "source": [
    "<a id='toc8_1_2__'></a>\n",
    "\n",
    "#### Bias\n",
    "\n",
    "Let's evaluate whether our banking agent's prompts contain unintended biases that could affect banking decisions. Biased prompts can lead to unfair or discriminatory outcomes — undermining customer trust and exposing the institution to compliance risk.\n",
    "\n",
    "We'll first use `list_tests()` again to filter for tests relating to `prompt_validation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eba86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests(filter=\"prompt_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc66b65",
   "metadata": {},
   "source": [
    "And then run the identified `Bias` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062cf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Bias\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2832750",
   "metadata": {},
   "source": [
    "<a id='toc9__'></a>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "You can look at the output produced by the ValidMind Library right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind Platform to work with your model documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb1a58",
   "metadata": {},
   "source": [
    "<a id='toc9_1__'></a>\n",
    "\n",
    "### Work with your model documentation\n",
    "\n",
    "1. From the **Inventory** in the ValidMind Platform, go to the model you registered earlier. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/working-with-model-inventory.html))\n",
    "\n",
    "2. In the left sidebar that appears for your model, click **Documentation** under Documents.\n",
    "\n",
    "    What you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it's ready. [Learn more ...](https://docs.validmind.ai/guide/working-with-model-documentation.html)\n",
    "\n",
    "3. Click into any section related to the tests we ran in this notebook, for example: **4.3. Prompt Evaluation** to review the results of the tests we logged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef26be",
   "metadata": {},
   "source": [
    "<a id='toc9_2__'></a>\n",
    "\n",
    "### Customize the banking agent for your use case\n",
    "\n",
    "You've now built an agentic AI system designed for banking use cases that supports compliance with supervisory guidance such as SR 11-7 and SS1/23, covering credit and fraud risk assessment for both retail and commercial banking. Extend this example agent to real-world banking scenarios and production deployment by:\n",
    "\n",
    "- Adapting the banking tools to your organization's specific requirements\n",
    "- Adding more banking scenarios and edge cases to your test set\n",
    "- Connecting the agent to your banking systems and databases\n",
    "- Implementing additional banking-specific tools and workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681e49c",
   "metadata": {},
   "source": [
    "<a id='toc9_3__'></a>\n",
    "\n",
    "### Discover more learning resources\n",
    "\n",
    "Learn more about the ValidMind Library tools we used in this notebook:\n",
    "\n",
    "- [Custom prompts](https://docs.validmind.ai/notebooks/how_to/tests/run_tests/configure_tests/customize_test_result_descriptions.html)\n",
    "- [Custom tests](https://docs.validmind.ai/notebooks/how_to/tests/custom_tests/implement_custom_tests.html)\n",
    "- [ValidMind scorers](https://docs.validmind.ai/notebooks/how_to/scoring/assign_scores_complete_tutorial.html)\n",
    "\n",
    "We also offer many more interactive notebooks to help you document models:\n",
    "\n",
    "- [Run tests & test suites](https://docs.validmind.ai/guide/testing-overview.html)\n",
    "- [Code samples](https://docs.validmind.ai/guide/samples-jupyter-notebooks.html)\n",
    "\n",
    "Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c1b6e",
   "metadata": {},
   "source": [
    "<a id='toc10__'></a>\n",
    "\n",
    "## Upgrade ValidMind\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\">After installing ValidMind, you’ll want to periodically make sure you are on the latest version to access any new features and other enhancements.</div>\n",
    "\n",
    "Retrieve the information for the currently installed version of ValidMind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0b646",
   "metadata": {},
   "source": [
    "If the version returned is lower than the version indicated in our [production open-source code](https://github.com/validmind/validmind-library/blob/prod/validmind/__version__.py), restart your notebook and run:\n",
    "\n",
    "```bash\n",
    "%pip install --upgrade validmind\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fa7f1",
   "metadata": {},
   "source": [
    "You may need to restart your kernel after running the upgrade package for changes to be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyright-de4baf0f42ba4a37946d52586dff1049",
   "metadata": {},
   "source": [
    "<!-- VALIDMIND COPYRIGHT -->\n",
    "\n",
    "<small>\n",
    "\n",
    "***\n",
    "\n",
    "Copyright © 2023-2026 ValidMind Inc. All rights reserved.<br>\n",
    "Refer to [LICENSE](https://github.com/validmind/validmind-library/blob/main/LICENSE) for details.<br>\n",
    "SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-1QuffXMV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
