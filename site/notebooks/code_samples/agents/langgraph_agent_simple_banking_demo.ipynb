{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Validation with ValidMind - Banking Demo\n",
    "\n",
    "This notebook shows how to document and evaluate an agentic AI system with the ValidMind Library. Using a small banking agent built in LangGraph as an example, you will run ValidMind’s built-in and custom tests and produce the artifacts needed to create evidence-backed documentation.\n",
    "\n",
    "An AI agent is an autonomous system that interprets inputs, selects from available tools or actions, and carries out multi-step behaviors to achieve user goals. In this example, our agent acts as a professional banking assistant that analyzes user requests and automatically selects and invokes the most appropriate specialized banking tool (credit, account, or fraud) to deliver accurate, compliant, and actionable responses.\n",
    "\n",
    "However, agentic capabilities bring concrete risks. The agent may misinterpret user inputs or fail to extract required parameters, producing incorrect credit assessments or inappropriate account actions; it can select the wrong tool (for example, invoking account management instead of fraud detection), which may cause unsafe, non-compliant, or customer-impacting behaviour.\n",
    "\n",
    "This interactive notebook guides you step-by-step through building a demo LangGraph banking agent, preparing an evaluation dataset, initializing the ValidMind Library and required objects, writing custom tests for tool-selection accuracy and entity extraction, running ValidMind’s built-in and custom test suites, and logging documentation artifacts to ValidMind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.content-hidden when-format=\"html\"}\n",
    "## Contents    \n",
    "- [About ValidMind](#toc1__)    \n",
    "  - [Before you begin](#toc1_1__)    \n",
    "  - [New to ValidMind?](#toc1_2__)    \n",
    "  - [Key concepts](#toc1_3__)    \n",
    "- [Setting up](#toc2__)    \n",
    "  - [Install the ValidMind Library](#toc2_1__)    \n",
    "  - [Initialize the ValidMind Library](#toc2_2__)    \n",
    "    - [Register sample model](#toc2_2_1__)    \n",
    "    - [Apply documentation template](#toc2_2_2__)    \n",
    "    - [Get your code snippet](#toc2_2_3__)    \n",
    "  - [Initialize the Python environment](#toc2_3__)    \n",
    "- [Banking Tools](#toc3__)    \n",
    "  - [Tool Overview](#toc3_1__)    \n",
    "  - [Test Banking Tools Individually](#toc3_2__)    \n",
    "- [Complete LangGraph Banking Agent](#toc4__)    \n",
    "- [ValidMind Model Integration](#toc5__)    \n",
    "- [Prompt Validation](#toc6__)    \n",
    "- [Banking Test Dataset](#toc7__)    \n",
    "  - [Initialize ValidMind Dataset](#toc7_1__)    \n",
    "  - [Run the Agent and capture result through assign predictions](#toc7_2__)    \n",
    "    - [Dataframe Display Settings](#toc7_2_1__)    \n",
    "- [Banking Accuracy Test](#toc8__)    \n",
    "- [Banking Tool Call Accuracy Test](#toc9__)    \n",
    "- [Scorers in ValidMind](#toc10__)\n",
    "  - [Plan Quality Metric scorer](#toc10_1)    \n",
    "  - [Plan Adherence Metric scorer](#toc10_2)    \n",
    "  - [Tool Correctness Metric scorer](#toc10_3)    \n",
    "  - [Argument Correctness Metric scorer](#toc10_4)    \n",
    "  - [Task Completion scorer](#toc10_5)    \n",
    "- [RAGAS Tests for an Agent Evaluation](#toc12__)    \n",
    "  - [Faithfulness](#toc12_1__)    \n",
    "  - [Response Relevancy](#toc12_2__)    \n",
    "  - [Context Recall](#toc12_3__)    \n",
    "- [Safety](#toc13__)    \n",
    "  - [AspectCritic](#toc13_1__)    \n",
    "  - [Prompt bias](#toc13_2__)    \n",
    "  - [Toxicity](#toc13_3__)    \n",
    "- [Demo Summary and Next Steps](#toc14__)    \n",
    "  - [What We Built](#toc14_1__)    \n",
    "  - [Next Steps](#toc14_2__)    \n",
    "  - [Key Benefits](#toc14_3__)    \n",
    "\n",
    ":::\n",
    "<!-- jn-toc-notebook-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=4\n",
    "\t/jn-toc-notebook-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1__'></a>\n",
    "\n",
    "## About ValidMind\n",
    "ValidMind is a suite of tools for managing model risk, including risk associated with AI and statistical models.\n",
    "\n",
    "You use the ValidMind Library to automate documentation and validation tests, and then use the ValidMind Platform to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "<a id='toc1_1__'></a>\n",
    "\n",
    "### Before you begin\n",
    "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "<a id='toc1_2__'></a>\n",
    "\n",
    "### New to ValidMind?\n",
    "If you haven't already seen our documentation on the [ValidMind Library](https://docs.validmind.ai/developer/validmind-library.html), we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models and running tests, as well as find code samples and our Python Library API reference.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
    "<br></br>\n",
    "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_3__'></a>\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n",
    "\n",
    "**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n",
    "\n",
    "**Tests**: A function contained in the ValidMind Library, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n",
    "\n",
    "**Custom tests**: Custom tests are functions that you define to evaluate your model or dataset. These functions can be registered via the ValidMind Library to be used with the ValidMind Platform.\n",
    "\n",
    "**Inputs**: Objects to be evaluated and documented in the ValidMind Library. They can be any of the following:\n",
    "\n",
    "- **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\n",
    "- **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\n",
    "- **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom test.\n",
    "- **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom test. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\n",
    "\n",
    "**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a test, customize its behavior, or provide additional context.\n",
    "\n",
    "**Outputs**: Custom tests can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n",
    "\n",
    "**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2__'></a>\n",
    "\n",
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_1__'></a>\n",
    "\n",
    "### Install the ValidMind Library\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2__'></a>\n",
    "\n",
    "### Initialize the ValidMind Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2_1__'></a>\n",
    "\n",
    "#### Register sample model\n",
    "\n",
    "Let's first register a sample model for use with this notebook:\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Inventory** and click **+ Register Model**.\n",
    "\n",
    "3. Enter the model details and click **Next >** to continue to assignment of model stakeholders. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
    "\n",
    "4. Select your own name under the **MODEL OWNER** drop-down.\n",
    "\n",
    "5. Click **Register Model** to add the model to your inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2_2__'></a>\n",
    "\n",
    "#### Apply documentation template\n",
    "\n",
    "Once you've registered your model, let's select a documentation template. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "1. In the left sidebar that appears for your model, click **Documents** and select **Documentation**.\n",
    "\n",
    "2. Under **TEMPLATE**, select `Agentic AI System`.\n",
    "\n",
    "3. Click **Use Template** to apply the template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2_3__'></a>\n",
    "\n",
    "#### Get your code snippet\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "1. On the left sidebar that appears for your model, select **Getting Started** and click **Copy snippet to clipboard**.\n",
    "2. Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"...\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_3__'></a>\n",
    "\n",
    "### Initialize the Python environment\n",
    "\n",
    "Next, let's import all the necessary libraries for building our banking LangGraph agent system:\n",
    "\n",
    "- **LangChain components** for LLM integration and tool management\n",
    "- **LangGraph** for building stateful, multi-step agent workflows\n",
    "- **ValidMind** for model validation and testing\n",
    "- **Banking tools** for specialized financial services\n",
    "- **Standard libraries** for data handling and environment management\n",
    "\n",
    "The setup includes loading environment variables (like OpenAI API keys) needed for the LLM components to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "# Third party imports\n",
    "import pandas as pd\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Local imports\n",
    "from banking_tools import AVAILABLE_TOOLS\n",
    "from validmind.tests import run_test\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Load environment variables if using .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed. Make sure OPENAI_API_KEY is set in your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3__'></a>\n",
    "\n",
    "## Banking Tools\n",
    "\n",
    "Now let's use the following banking demo tools that provide use cases of the financial services:\n",
    "\n",
    "<a id='toc3_1__'></a>\n",
    "\n",
    "### Tool Overview\n",
    "1. **Credit Risk Analyzer** - Loan applications and credit decisions\n",
    "2. **Customer Account Manager** - Account services and customer support\n",
    "3. **Fraud Detection System** - Security and fraud prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available tools: {len(AVAILABLE_TOOLS)}\")\n",
    "print(\"\\nTool Details:\")\n",
    "for i, tool in enumerate(AVAILABLE_TOOLS, 1):\n",
    "    print(f\"   - {tool.name}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3_2__'></a>\n",
    "\n",
    "### Test Banking Tools Individually\n",
    "\n",
    "Let's test each banking tool individually to ensure they're working correctly before integrating them into our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Individual Banking Tools\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Credit Risk Analyzer\n",
    "print(\"TEST 1: Credit Risk Analyzer\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    # Access the underlying function using .func\n",
    "    credit_result = AVAILABLE_TOOLS[0].func(\n",
    "        customer_income=75000,\n",
    "        customer_debt=1200,\n",
    "        credit_score=720,\n",
    "        loan_amount=50000,\n",
    "        loan_type=\"personal\"\n",
    "    )\n",
    "    print(credit_result)\n",
    "    print(\"Credit Risk Analyzer test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Credit Risk Analyzer test FAILED: {e}\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "\n",
    "# Test 2: Customer Account Manager\n",
    "print(\"TEST 2: Customer Account Manager\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    # Test checking balance\n",
    "    account_result = AVAILABLE_TOOLS[1].func(\n",
    "        account_type=\"checking\",\n",
    "        customer_id=\"12345\",\n",
    "        action=\"check_balance\"\n",
    "    )\n",
    "    print(account_result)\n",
    "    \n",
    "    # Test getting account info\n",
    "    info_result = AVAILABLE_TOOLS[1].func(\n",
    "        account_type=\"all\",\n",
    "        customer_id=\"12345\", \n",
    "        action=\"get_info\"\n",
    "    )\n",
    "    print(info_result)\n",
    "    print(\"Customer Account Manager test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Customer Account Manager test FAILED: {e}\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "\n",
    "# Test 3: Fraud Detection System\n",
    "print(\"TEST 3: Fraud Detection System\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    fraud_result = AVAILABLE_TOOLS[2].func(\n",
    "        transaction_id=\"TX123\",\n",
    "        customer_id=\"12345\",\n",
    "        transaction_amount=500.00,\n",
    "        transaction_type=\"withdrawal\",\n",
    "        location=\"Miami, FL\",\n",
    "        device_id=\"DEVICE_001\"\n",
    "    )\n",
    "    print(fraud_result)\n",
    "    print(\"Fraud Detection System test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Fraud Detection System test FAILED: {e}\")\n",
    "\n",
    "print(\"\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4__'></a>\n",
    "\n",
    "## Complete LangGraph Banking Agent\n",
    "\n",
    "Now we'll create our intelligent banking agent with LangGraph that can automatically select and use the appropriate banking tools based on user requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced banking system prompt with tool selection guidance\n",
    "system_context = \"\"\"You are a professional banking AI assistant with access to specialized banking tools.\n",
    "            Analyze the user's banking request and directly use the most appropriate tools to help them.\n",
    "            \n",
    "            AVAILABLE BANKING TOOLS:\n",
    "            \n",
    "            credit_risk_analyzer - Analyze credit risk for loan applications and credit decisions\n",
    "            - Use for: loan applications, credit assessments, risk analysis, mortgage eligibility\n",
    "            - Examples: \"Analyze credit risk for $50k personal loan\", \"Assess mortgage eligibility for $300k home purchase\"\n",
    "            - Parameters: customer_income, customer_debt, credit_score, loan_amount, loan_type\n",
    "\n",
    "            customer_account_manager - Manage customer accounts and provide banking services\n",
    "            - Use for: account information, transaction processing, product recommendations, customer service\n",
    "            - Examples: \"Check balance for checking account 12345\", \"Recommend products for customer with high balance\"\n",
    "            - Parameters: account_type, customer_id, action, amount, account_details\n",
    "\n",
    "            fraud_detection_system - Analyze transactions for potential fraud and security risks\n",
    "            - Use for: transaction monitoring, fraud prevention, risk assessment, security alerts\n",
    "            - Examples: \"Analyze fraud risk for $500 ATM withdrawal in Miami\", \"Check security for $2000 online purchase\"\n",
    "            - Parameters: transaction_id, customer_id, transaction_amount, transaction_type, location, device_id\n",
    "\n",
    "            BANKING INSTRUCTIONS:\n",
    "            - Analyze the user's banking request carefully and identify the primary need\n",
    "            - If they need credit analysis → use credit_risk_analyzer\n",
    "            - If they need financial calculations → use financial_calculator\n",
    "            - If they need account services → use customer_account_manager\n",
    "            - If they need security analysis → use fraud_detection_system\n",
    "            - Extract relevant parameters from the user's request\n",
    "            - Provide helpful, accurate banking responses based on tool outputs\n",
    "            - Always consider banking regulations, risk management, and best practices\n",
    "            - Be professional and thorough in your analysis\n",
    "\n",
    "            Choose and use tools wisely to provide the most helpful banking assistance.\n",
    "        \"\"\"\n",
    "# Initialize the main LLM for banking responses\n",
    "main_llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",\n",
    "    reasoning={\n",
    "        \"effort\": \"low\",\n",
    "        \"summary\": \"auto\"\n",
    "    }\n",
    ")\n",
    "# Bind all banking tools to the main LLM\n",
    "llm_with_tools = main_llm.bind_tools(AVAILABLE_TOOLS)\n",
    "\n",
    "# Banking Agent State Definition\n",
    "class BankingAgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    user_input: str\n",
    "    session_id: str\n",
    "    context: dict\n",
    "\n",
    "def create_banking_langgraph_agent():\n",
    "    \"\"\"Create a comprehensive LangGraph banking agent with intelligent tool selection.\"\"\"\n",
    "    def llm_node(state: BankingAgentState) -> BankingAgentState:\n",
    "        \"\"\"Main LLM node that processes banking requests and selects appropriate tools.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        # Add system context to messages\n",
    "        enhanced_messages = [SystemMessage(content=system_context)] + list(messages)\n",
    "        # Get LLM response with tool selection\n",
    "        response = llm_with_tools.invoke(enhanced_messages)\n",
    "        return {\n",
    "            **state,\n",
    "            \"messages\": messages + [response]\n",
    "        }\n",
    "    \n",
    "    def should_continue(state: BankingAgentState) -> str:\n",
    "        \"\"\"Decide whether to use tools or end the conversation.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        # Check if the LLM wants to use tools\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "        \n",
    "    # Create the banking state graph\n",
    "    workflow = StateGraph(BankingAgentState)\n",
    "    # Add nodes\n",
    "    workflow.add_node(\"llm\", llm_node)\n",
    "    workflow.add_node(\"tools\", ToolNode(AVAILABLE_TOOLS))\n",
    "    # Simplified entry point - go directly to LLM\n",
    "    workflow.add_edge(START, \"llm\")\n",
    "    # From LLM, decide whether to use tools or end\n",
    "    workflow.add_conditional_edges(\n",
    "        \"llm\",\n",
    "        should_continue,\n",
    "        {\"tools\": \"tools\", END: END}\n",
    "    )\n",
    "    # Tool execution flows back to LLM for final response\n",
    "    workflow.add_edge(\"tools\", \"llm\")\n",
    "    # Set up memory\n",
    "    memory = MemorySaver()\n",
    "    # Compile the graph\n",
    "    agent = workflow.compile(checkpointer=memory)\n",
    "    return agent\n",
    "\n",
    "# Create the banking intelligent agent\n",
    "banking_agent = create_banking_langgraph_agent()\n",
    "\n",
    "print(\"Banking LangGraph Agent Created Successfully!\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"   - Intelligent banking tool selection\")\n",
    "print(\"   - Comprehensive banking system prompt\")\n",
    "print(\"   - Streamlined workflow: LLM → Tools → Response\")\n",
    "print(\"   - Automatic tool parameter extraction\")\n",
    "print(\"   - Professional banking assistance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5__'></a>\n",
    "\n",
    "## ValidMind Model Integration\n",
    "\n",
    "Now we'll integrate our banking LangGraph agent with ValidMind for comprehensive testing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import Prompt\n",
    "from validmind.scorer.llm.deepeval import extract_tool_calls_from_agent_output, _convert_to_tool_call_list\n",
    "def banking_agent_fn(input):\n",
    "    \"\"\"\n",
    "    Invoke the banking agent with the given input.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initial state for banking agent\n",
    "        initial_state = {\n",
    "            \"user_input\": input[\"input\"],\n",
    "            \"messages\": [HumanMessage(content=input[\"input\"])],\n",
    "            \"session_id\": input[\"session_id\"],\n",
    "            \"context\": {}\n",
    "        }\n",
    "        session_config = {\"configurable\": {\"thread_id\": input[\"session_id\"]}}\n",
    "        result = banking_agent.invoke(initial_state, config=session_config)\n",
    "\n",
    "        from utils import capture_tool_output_messages\n",
    "\n",
    "        # Capture all tool outputs and metadata\n",
    "        captured_data = capture_tool_output_messages(result)\n",
    "    \n",
    "        # Access specific tool outputs, this will be used for RAGAS tests\n",
    "        tool_message = \"\"\n",
    "        for output in captured_data[\"tool_outputs\"]:\n",
    "            tool_message += output['content']\n",
    "        \n",
    "        tool_calls_found = []\n",
    "        messages = result['messages']\n",
    "        for message in messages:\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    # Handle both dictionary and object formats\n",
    "                    if isinstance(tool_call, dict):\n",
    "                        tool_calls_found.append(tool_call['name'])\n",
    "                    else:\n",
    "                        # ToolCall object - use attribute access\n",
    "                        tool_calls_found.append(tool_call.name)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"prediction\": result['messages'][-1].content[0]['text'],\n",
    "            \"output\": result,\n",
    "            \"tool_messages\": [tool_message],\n",
    "            # \"tool_calls\": tool_calls_found,\n",
    "            \"tool_called\": _convert_to_tool_call_list(extract_tool_calls_from_agent_output(result))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Return a fallback response if the agent fails\n",
    "        error_message = f\"\"\"I apologize, but I encountered an error while processing your banking request: {str(e)}.\n",
    "        Please try rephrasing your question or contact support if the issue persists.\"\"\"\n",
    "        return {\n",
    "            \"prediction\": error_message, \n",
    "            \"output\": {\n",
    "                \"messages\": [HumanMessage(content=input[\"input\"]), SystemMessage(content=error_message)],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        }\n",
    "\n",
    "## Initialize the model\n",
    "vm_banking_model = vm.init_model(\n",
    "    input_id=\"banking_agent_model\",\n",
    "    predict_fn=banking_agent_fn,\n",
    "    prompt=Prompt(template=system_context)\n",
    ")\n",
    "\n",
    "# Add the banking agent to the vm model\n",
    "vm_banking_model.model = banking_agent\n",
    "\n",
    "print(\"Banking Agent Successfully Integrated with ValidMind!\")\n",
    "print(f\"Model ID: {vm_banking_model.input_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6__'></a>\n",
    "\n",
    "## Prompt Validation\n",
    "\n",
    "Let's get an initial sense of how well the prompt meets a few best practices for prompt engineering. These tests use an LLM to rate the prompt on a scale of 1-10 against the following criteria:\n",
    "\n",
    "- **Clarity**: How clearly the prompt states the task.\n",
    "- **Conciseness**: How succinctly the prompt states the task.\n",
    "- **Delimitation**: When using complex prompts containing examples, contextual information, or other elements, is the prompt formatted in such a way that each element is clearly separated?\n",
    "- **NegativeInstruction**: Whether the prompt contains negative instructions.\n",
    "- **Specificity**: How specific the prompt defines the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.prompt_validation.Clarity\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.prompt_validation.Conciseness\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.prompt_validation.Delimitation\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.prompt_validation.NegativeInstruction\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.prompt_validation.Specificity\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7__'></a>\n",
    "\n",
    "## Banking Test Dataset\n",
    "\n",
    "We'll use a sample test dataset to evaluate our agent's performance across different banking scenarios.\n",
    "\n",
    "<a id='toc7_1__'></a>\n",
    "\n",
    "### Initialize ValidMind Dataset\n",
    "\n",
    "Before we can run tests and evaluations, we need to initialize our banking test dataset as a ValidMind dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our banking-specific test dataset\n",
    "from banking_test_dataset import banking_test_dataset\n",
    "\n",
    "vm_test_dataset = vm.init_dataset(\n",
    "    input_id=\"banking_test_dataset\",\n",
    "    dataset=banking_test_dataset.sample(2),\n",
    "    text_column=\"input\",\n",
    "    target_column=\"possible_outputs\",\n",
    ")\n",
    "\n",
    "print(\"Banking Test Dataset Initialized in ValidMind!\")\n",
    "print(f\"Dataset ID: {vm_test_dataset.input_id}\")\n",
    "print(f\"Dataset columns: {vm_test_dataset._df.columns}\")\n",
    "vm_test_dataset._df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7_2__'></a>\n",
    "\n",
    "### Run the Agent and capture result through assign predictions\n",
    "\n",
    "Now we'll execute our banking agent on the test dataset and capture its responses for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_predictions(vm_banking_model)\n",
    "\n",
    "print(\"Banking Agent Predictions Generated Successfully!\")\n",
    "print(f\"Predictions assigned to {len(vm_test_dataset._df)} test cases\")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc8__'></a>\n",
    "\n",
    "## Banking Accuracy Test\n",
    "\n",
    "This test evaluates the banking agent's ability to provide accurate responses by:\n",
    "- Testing against a dataset of predefined banking questions and expected answers\n",
    "- Checking if responses contain expected keywords and banking terminology\n",
    "- Providing detailed test results including pass/fail status\n",
    "- Helping identify any gaps in the agent's banking knowledge or response quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@vm.test(\"my_custom_tests.banking_accuracy_test\")\n",
    "def banking_accuracy_test(model, dataset, list_of_columns):\n",
    "    \"\"\"\n",
    "    The Banking Accuracy Test evaluates whether the agent’s responses include \n",
    "    critical domain-specific keywords and phrases that indicate accurate, compliant,\n",
    "    and contextually appropriate banking information. This test ensures that the agent\n",
    "    provides responses containing the expected banking terminology, risk classifications,\n",
    "    account details, or other domain-relevant information required for regulatory compliance,\n",
    "    customer safety, and operational accuracy.\n",
    "    \"\"\"\n",
    "    df = dataset._df\n",
    "    \n",
    "    # Pre-compute responses for all tests\n",
    "    y_true = dataset.y.tolist()\n",
    "    y_pred = dataset.y_pred(model).tolist()\n",
    "\n",
    "    # Vectorized test results\n",
    "    test_results = []\n",
    "    for response, keywords in zip(y_pred, y_true):\n",
    "        # Convert keywords to list if not already a list\n",
    "        if not isinstance(keywords, list):\n",
    "            keywords = [keywords]\n",
    "        test_results.append(any(str(keyword).lower() in str(response).lower() for keyword in keywords))\n",
    "        \n",
    "    results = pd.DataFrame()\n",
    "    column_names = [col + \"_details\" for col in list_of_columns]\n",
    "    results[column_names] = df[list_of_columns]\n",
    "    results[\"actual\"] = y_pred\n",
    "    results[\"expected\"] = y_true\n",
    "    results[\"passed\"] = test_results\n",
    "    results[\"error\"] = None if test_results else f'Response did not contain any expected keywords: {y_true}'\n",
    "    \n",
    "    return results\n",
    "   \n",
    "result = run_test(\n",
    "    \"my_custom_tests.banking_accuracy_test\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_dataset,\n",
    "        \"model\": vm_banking_model\n",
    "    },\n",
    "    params={\n",
    "        \"list_of_columns\": [\"input\"]\n",
    "    }\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc9__'></a>\n",
    "\n",
    "## Banking Tool Call Accuracy Test\n",
    "\n",
    "This test evaluates how accurately our intelligent banking router selects the correct tools for different banking requests. This test provides quantitative feedback on the agent's core intelligence - its ability to understand what users need and select the right banking tools to help them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vm.test(\"my_custom_tests.BankingToolCallAccuracy\")\n",
    "def BankingToolCallAccuracy(dataset, agent_output_column, expected_tools_column):\n",
    "    \"\"\"\n",
    "    Evaluates the tool selection accuracy of a LangGraph-powered banking agent.\n",
    "\n",
    "    This test measures whether the agent correctly identifies and invokes the required banking tools\n",
    "    for each user query scenario.\n",
    "    For each case, the outputs generated by the agent (including its tool calls) are compared against an\n",
    "    expected set of tools. The test considers both coverage and exactness: it computes the proportion of\n",
    "    expected tools correctly called by the agent for each instance.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (VMDataset): The dataset containing user queries, agent outputs, and ground-truth tool expectations.\n",
    "        agent_output_column (str): Dataset column name containing agent outputs (should include tool call details in 'messages').\n",
    "        expected_tools_column (str): Dataset column specifying the true expected tools (as lists).\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Per-row dictionaries with details: expected tools, found tools, match count, total expected, and accuracy score.\n",
    "\n",
    "    Purpose:\n",
    "        Provides diagnostic evidence of the banking agent's core reasoning ability—specifically, its capacity to\n",
    "        interpret user needs and select the correct banking actions. Useful for diagnosing gaps in tool coverage,\n",
    "        misclassifications, or breakdowns in agent logic.\n",
    "\n",
    "    Interpretation:\n",
    "        - An accuracy of 1.0 signals perfect tool selection for that example.\n",
    "        - Lower scores may indicate partial or complete failures to invoke required tools.\n",
    "        - Review 'found_tools' vs. 'expected_tools' to understand the source of discrepancies.\n",
    "\n",
    "    Strengths:\n",
    "        - Directly tests a core capability of compositional tool-use agents.\n",
    "        - Framework-agnostic; robust to tool call output format (object or dict).\n",
    "        - Supports batch validation and result logging for systematic documentation.\n",
    "\n",
    "    Limitations:\n",
    "        - Does not penalize extra, unnecessary tool calls.\n",
    "        - Does not assess result quality—only correct invocation.\n",
    "\n",
    "    \"\"\"\n",
    "    def validate_tool_calls_simple(messages, expected_tools):\n",
    "        \"\"\"Simple validation of tool calls without RAGAS dependency issues.\"\"\"\n",
    "        \n",
    "        tool_calls_found = []\n",
    "        \n",
    "        for message in messages:\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    # Handle both dictionary and object formats\n",
    "                    if isinstance(tool_call, dict):\n",
    "                        tool_calls_found.append(tool_call['name'])\n",
    "                    else:\n",
    "                        # ToolCall object - use attribute access\n",
    "                        tool_calls_found.append(tool_call.name)\n",
    "        \n",
    "        # Check if expected tools were called\n",
    "        accuracy = 0.0\n",
    "        matches = 0\n",
    "        if expected_tools:\n",
    "            matches = sum(1 for tool in expected_tools if tool in tool_calls_found)\n",
    "            accuracy = matches / len(expected_tools)\n",
    "        \n",
    "        return {\n",
    "            'expected_tools': expected_tools,\n",
    "            'found_tools': tool_calls_found,\n",
    "            'matches': matches,\n",
    "            'total_expected': len(expected_tools) if expected_tools else 0,\n",
    "            'accuracy': accuracy,\n",
    "        }\n",
    "\n",
    "    df = dataset._df\n",
    "    \n",
    "    results = []\n",
    "    for i, row in df.iterrows():\n",
    "        result = validate_tool_calls_simple(row[agent_output_column]['messages'], row[expected_tools_column])\n",
    "        results.append(result)\n",
    "         \n",
    "    return results\n",
    "\n",
    "run_test(\n",
    "    \"my_custom_tests.BankingToolCallAccuracy\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_dataset,\n",
    "    },\n",
    "    params = {\n",
    "        \"agent_output_column\": \"banking_agent_model_output\",\n",
    "        \"expected_tools_column\": \"expected_tools\"\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc10__'></a>\n",
    "\n",
    "## Scorers in ValidMind\n",
    "\n",
    "Scorers are evaluation metrics that analyze model outputs and store their results in the dataset. When using `assign_scores()`:\n",
    "\n",
    "- Each scorer adds a new column to the dataset with format: {scorer_name}_{metric_name}\n",
    "- The column contains the numeric score (typically 0-1) for each example\n",
    "- Multiple scorers can be run on the same dataset, each adding their own column\n",
    "- Scores are persisted in the dataset for later analysis and visualization\n",
    "- Common scorer patterns include:\n",
    "  - Model performance metrics (accuracy, F1, etc)\n",
    "  - Output quality metrics (relevance, faithfulness)\n",
    "  - Task-specific metrics (completion, correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_3_4_\"></a>\n",
    "\n",
    "### AI Agent Evaluation Metrics\n",
    "\n",
    "AI agent evaluation metrics are specialized measurements designed to assess how well autonomous LLM-based agents reason, plan, select and execute tools, and ultimately complete user tasks by analyzing the **full execution trace**—including reasoning steps, tool calls, intermediate decisions, and outcomes—rather than just single input–output pairs.\n",
    "\n",
    "These metrics are essential because agent failures often occur in ways traditional LLM metrics miss (e.g., choosing the right tool with wrong arguments, creating a good plan but not following it, or completing a task inefficiently).\n",
    "\n",
    "**DeepEval’s AI agent evaluation framework** breaks evaluation into three layers with corresponding metric categories:\n",
    "\n",
    "1. **Reasoning Layer** – Evaluates planning and strategy generation:\n",
    "\n",
    "   * *PlanQualityMetric* – how logical, complete, and efficient the agent’s plan is\n",
    "   * *PlanAdherenceMetric* – whether the agent follows its own plan during execution \n",
    "\n",
    "2. **Action Layer** – Assesses tool usage and argument generation:\n",
    "\n",
    "   * *ToolCorrectnessMetric* – whether the agent selects and calls the right tools\n",
    "   * *ArgumentCorrectnessMetric* – whether the agent generates correct tool arguments\n",
    "\n",
    "3. **Execution Layer** – Measures end-to-end performance:\n",
    "\n",
    "   * *TaskCompletionMetric* – whether the agent successfully completes the intended task\n",
    "   * *StepEfficiencyMetric* – whether the agent avoids unnecessary or redundant steps\n",
    "\n",
    "Together, these metrics enable granular diagnosis of agent behavior, help pinpoint where failures occur (reasoning, action, or execution), and support both development benchmarking and production monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc10_1'></a>\n",
    "\n",
    "#### **Reasoning Layer**\n",
    "#### PlanQualityMetric\n",
    "Let's measures how well the agent generates a plan before acting. A high score means the plan is logical, complete, and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorer.llm.deepeval.PlanQuality\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc10_2'></a>\n",
    "\n",
    "#### PlanAdherenceMetric\n",
    "Let's checks whether the agent follows the plan it created. Deviations lower this score and indicate gaps between reasoning and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorer.llm.deepeval.PlanAdherence\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    expected_output_column = \"expected_output\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc10_3'></a>\n",
    "\n",
    "#### **Action Layer**\n",
    "#### ToolCorrectnessMetric\n",
    "Let's evaluates if the agent selects the appropriate tool for the task. Choosing the wrong tool reduces performance even if reasoning was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorer.llm.deepeval.ToolCorrectness\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    expected_tools_column = \"expected_tools\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc10_4'></a>\n",
    "\n",
    "#### ArgumentCorrectnessMetric\n",
    "Let's assesses whether the agent provides correct inputs or arguments to the selected tool. Incorrect arguments can lead to failed or unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorer.llm.deepeval.ArgumentCorrectness\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc10_5'></a>\n",
    "\n",
    "#### **Execution Layer**\n",
    "#### TaskCompletionMetric\n",
    "The TaskCompletion test evaluates whether our banking agent successfully completes the requested tasks by analyzing its outputs and tool usage. This metric assesses the agent's ability to understand user requests, execute appropriate actions, and provide complete responses that address the original query. The test provides a score between 0-1 along with detailed feedback on task completion quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorer.llm.deepeval.TaskCompletion\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"banking_agent_model_prediction\",\n",
    "    agent_output_column = \"banking_agent_model_output\",\n",
    "    tools_called_column = \"banking_agent_model_tool_called\",\n",
    "\n",
    ")\n",
    "vm_test_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TaskCompletion scorer has added a new column 'TaskCompletion_score' to our dataset. This is because when we run scorers through assign_scores(), the return values are automatically processed and added as new columns with the format {scorer_name}_{metric_name}. We'll use this column to visualize the distribution of task completion scores across our test cases. Let's visualize the distribution through the box plot test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.plots.BoxPlot\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    params={\n",
    "        \"columns\": \"TaskCompletion_score\",\n",
    "        \"title\": \"Distribution of Task Completion Scores\",\n",
    "        \"ylabel\": \"Score\",\n",
    "        \"figsize\": (8, 6)\n",
    "    }\n",
    ").log()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc12__'></a>\n",
    "\n",
    "## RAGAS Tests for an Agent Evaluation\n",
    "\n",
    "RAGAS (Retrieval-Augmented Generation Assessment) provides specialized metrics for evaluating conversational AI systems like our banking agent. These tests analyze different aspects of agent performance:\n",
    "\n",
    "Our banking agent uses tools to retrieve information and generates responses based on that context, making it similar to a RAG system. RAGAS metrics help evaluate:\n",
    "\n",
    "- **Response Quality**: How well the agent uses retrieved tool outputs to generate helpful banking responses\n",
    "- **Information Faithfulness**: Whether agent responses accurately reflect tool outputs  \n",
    "- **Relevance Assessment**: How well responses address the original banking query\n",
    "- **Context Utilization**: How effectively the agent incorporates tool results into final answers\n",
    "\n",
    "These tests provide insights into how well our banking agent integrates tool usage with conversational abilities, ensuring it provides accurate, relevant, and helpful responses to banking users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc12_1__'></a>\n",
    "\n",
    "### Faithfulness\n",
    "\n",
    "Faithfulness measures how accurately the banking agent's responses reflect the information retrieved from tools. This metric evaluates:\n",
    "\n",
    "**Information Accuracy**: Whether the agent correctly uses tool outputs in its responses\n",
    "- **Fact Preservation**: Ensuring credit scores, loan calculations, compliance results are accurately reported\n",
    "- **No Hallucination**: Verifying the agent doesn't invent banking information not provided by tools\n",
    "- **Source Attribution**: Checking that responses align with actual tool outputs\n",
    "\n",
    "**Critical for Banking Trust**: Faithfulness is essential for banking agent reliability because users need to trust that:\n",
    "- Credit analysis results are reported correctly\n",
    "- Financial calculations are accurate  \n",
    "- Compliance checks return real information\n",
    "- Risk assessments are properly communicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.model_validation.ragas.Faithfulness\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    param_grid={\n",
    "        \"user_input_column\": [\"input\"],\n",
    "        \"response_column\": [\"banking_agent_model_prediction\"],\n",
    "        \"retrieved_contexts_column\": [\"banking_agent_model_tool_messages\"],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc12_2__'></a>\n",
    "\n",
    "### Response Relevancy\n",
    "\n",
    "Response Relevancy evaluates how well the banking agent's answers address the user's original banking question or request. This metric assesses:\n",
    "\n",
    "**Query Alignment**: Whether responses directly answer what users asked for\n",
    "- **Intent Fulfillment**: Checking if the agent understood and addressed the user's actual banking need\n",
    "- **Completeness**: Ensuring responses provide sufficient information to satisfy the banking query\n",
    "- **Focus**: Avoiding irrelevant information that doesn't help the banking user\n",
    "\n",
    "**Banking Quality**: Measures the agent's ability to maintain relevant, helpful banking dialogue\n",
    "- **Context Awareness**: Responses should be appropriate for the banking conversation context\n",
    "- **User Satisfaction**: Answers should be useful and actionable for banking users\n",
    "- **Clarity**: Banking information should be presented in a way that directly helps the user\n",
    "\n",
    "High relevancy indicates the banking agent successfully understands user needs and provides targeted, helpful banking responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.model_validation.ragas.ResponseRelevancy\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    params={\n",
    "        \"user_input_column\": \"input\",\n",
    "        \"response_column\": \"banking_agent_model_prediction\",\n",
    "        \"retrieved_contexts_column\": \"banking_agent_model_tool_messages\",\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc12_3__'></a>\n",
    "\n",
    "### Context Recall\n",
    "\n",
    "Context Recall measures how well the banking agent utilizes the information retrieved from tools when generating its responses. This metric evaluates:\n",
    "\n",
    "**Information Utilization**: Whether the agent effectively incorporates tool outputs into its responses\n",
    "- **Coverage**: How much of the available tool information is used in the response\n",
    "- **Integration**: How well tool outputs are woven into coherent, natural banking responses\n",
    "- **Completeness**: Whether all relevant information from tools is considered\n",
    "\n",
    "**Tool Effectiveness**: Assesses whether selected banking tools provide useful context for responses\n",
    "- **Relevance**: Whether tool outputs actually help answer the user's banking question\n",
    "- **Sufficiency**: Whether enough information was retrieved to generate good banking responses\n",
    "- **Quality**: Whether the tools provided accurate, helpful banking information\n",
    "\n",
    "High context recall indicates the banking agent not only selects the right tools but also effectively uses their outputs to create comprehensive, well-informed banking responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRecall\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    param_grid={\n",
    "        \"user_input_column\": [\"input\"],\n",
    "        \"retrieved_contexts_column\": [\"banking_agent_model_tool_messages\"],\n",
    "        \"reference_column\": [\"banking_agent_model_prediction\"],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc13__'></a>\n",
    "\n",
    "## Safety\n",
    "\n",
    "Safety testing is critical for banking AI agents to ensure they operate reliably and securely.\n",
    "These tests help validate that our banking agent maintains high standards of fairness and professionalism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc13_1__'></a>\n",
    "\n",
    "### AspectCritic\n",
    "\n",
    "AspectCritic provides comprehensive evaluation across multiple dimensions of banking agent performance. This metric analyzes various aspects of response quality:\n",
    "\n",
    "**Multi-Dimensional Assessment**: Evaluates responses across different quality criteria:\n",
    "  - **Conciseness**: Whether responses are clear and to-the-point without unnecessary details\n",
    "  - **Coherence**: Whether responses are logically structured and easy to follow\n",
    "  - **Correctness**: Accuracy of banking information and appropriateness of recommendations\n",
    "  - **Harmfulness**: Whether responses could cause harm or damage to users or systems\n",
    "  - **Maliciousness**: Whether responses contain malicious content or intent\n",
    "\n",
    "**Holistic Quality Scoring**: Provides an overall assessment that considers:\n",
    "- **User Experience**: How satisfying and useful the banking interaction would be for real users\n",
    "- **Professional Standards**: Whether responses meet quality expectations for production banking systems\n",
    "- **Consistency**: Whether the banking agent maintains quality across different types of requests\n",
    "\n",
    "AspectCritic helps identify specific areas where the banking agent excels or needs improvement, providing actionable insights for enhancing overall performance and user satisfaction in banking scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.model_validation.ragas.AspectCritic\",\n",
    "    inputs={\"dataset\": vm_test_dataset},\n",
    "    param_grid={\n",
    "        \"user_input_column\": [\"input\"],\n",
    "        \"response_column\": [\"banking_agent_model_prediction\"],\n",
    "        \"retrieved_contexts_column\": [\"banking_agent_model_tool_messages\"],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc13_2__'></a>\n",
    "\n",
    "### Prompt bias\n",
    "\n",
    "Let's check if the agent's prompts contain unintended biases that could affect banking decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.prompt_validation.Bias\",\n",
    "    inputs={\n",
    "        \"model\": vm_banking_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc13_3__'></a>\n",
    "\n",
    "### Toxicity\n",
    "\n",
    "Let's ensure responses are professional and appropriate for banking contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\n",
    "    \"validmind.data_validation.nlp.Toxicity\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_dataset,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc14__'></a>\n",
    "\n",
    "## Demo Summary and Next Steps\n",
    "\n",
    "We have successfully built and tested a comprehensive **Banking AI Agent** using LangGraph and ValidMind. Here's what we've accomplished:\n",
    "\n",
    "<a id='toc14_1__'></a>\n",
    "\n",
    "### What We Built\n",
    "\n",
    "1. **5 Specialized Banking Tools**\n",
    "   - Credit Risk Analyzer for loan assessments\n",
    "   - Customer Account Manager for account services\n",
    "   - Fraud Detection System for security monitoring\n",
    "\n",
    "2. **Intelligent LangGraph Agent**\n",
    "   - Automatic tool selection based on user requests\n",
    "   - Banking-specific system prompts and guidance\n",
    "   - Professional banking assistance and responses\n",
    "\n",
    "3. **Comprehensive Testing Framework**\n",
    "   - banking-specific test cases\n",
    "   - ValidMind integration for validation\n",
    "   - Performance analysis across banking domains\n",
    "\n",
    "<a id='toc14_2__'></a>\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Customize Tools**: Adapt the banking tools to your specific banking requirements\n",
    "2. **Expand Test Cases**: Add more banking scenarios and edge cases\n",
    "3. **Integrate with Real Data**: Connect to actual banking systems and databases\n",
    "4. **Add More Tools**: Implement additional banking-specific functionality\n",
    "5. **Production Deployment**: Deploy the agent in a production banking environment\n",
    "\n",
    "<a id='toc14_3__'></a>\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Industry-Specific**: Designed specifically for banking operations\n",
    "- **Regulatory Compliance**: Built-in SR 11-7 and SS 1-23 compliance checks\n",
    "- **Risk Management**: Comprehensive credit and fraud risk assessment\n",
    "- **Customer Focus**: Tools for both retail and commercial banking needs\n",
    "- **Real-World Applicability**: Addresses actual banking use cases and challenges\n",
    "\n",
    "Your banking AI agent is now ready to handle real-world banking scenarios while maintaining regulatory compliance and risk management best practices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "***\n",
    "\n",
    "Copyright © 2023-2026 ValidMind Inc. All rights reserved.<br>\n",
    "Refer to the [LICENSE file in the root of the GitHub `validmind-library` repository](https://github.com/validmind/validmind-library/blob/main/LICENSE) for details.<br>\n",
    "SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ValidMind (Poetry)",
   "language": "python",
   "name": "validmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
