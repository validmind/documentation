{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document an Excel-based application scorecard model\n",
    "\n",
    "Build and document an Excel-based application scorecard model with the ValidMind Library. Learn how to load an Excel-based model, prepare your datasets and model for testing, run tests and log those test results to the ValidMind Platform.\n",
    "\n",
    "An *application scorecard model* is a type of statistical model used in credit scoring to evaluate the creditworthiness of potential borrowers by generating a score based on various characteristics of an applicant such as credit history, income, employment status, and other relevant financial data.\n",
    "\n",
    " - This score assists lenders in making informed decisions about whether to approve or reject loan applications, as well as in determining the terms of the loan, including interest rates and credit limits.\n",
    " - Effective validation of application scorecard models ensures that lenders can manage risk efficiently while maintaining a fast and transparent loan application process for applicants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.content-hidden when-format=\"html\"}\n",
    "## Contents    \n",
    "- [About ValidMind](#toc1__)    \n",
    "  - [Before you begin](#toc1_1__)    \n",
    "  - [New to ValidMind?](#toc1_2__)    \n",
    "  - [Key concepts](#toc1_3__)    \n",
    "- [Setting up](#toc2__)    \n",
    "  - [Install the ValidMind Library](#toc2_1__)    \n",
    "  - [Initialize the ValidMind Library](#toc2_2__)    \n",
    "    - [Register sample model](#toc2_2_1__)    \n",
    "    - [Apply documentation template](#toc2_2_2__)    \n",
    "    - [Get your code snippet](#toc2_2_3__)    \n",
    "  - [Initialize the Python environment](#toc2_3__)    \n",
    "  - [Preview the documentation template](#toc2_4__)    \n",
    "- [Load the sample datasets](#toc3__)    \n",
    "  - [Load the raw dataset](#toc3_1__)    \n",
    "  - [Load the preprocessed dataset](#toc3_2__)    \n",
    "  - [Load the training and test datasets](#toc3_3__)    \n",
    "- [Initialize the ValidMind datasets](#toc4__)    \n",
    "- [Initialize a model object](#toc5__)    \n",
    "  - [Link predictions](#toc5_1__)    \n",
    "- [Running tests](#toc6__)    \n",
    "  - [Enable custom context for test descriptions](#toc6_1__)    \n",
    "  - [Define tests to run](#toc6_2__)    \n",
    "  - [Run defined tests](#toc6_3__)    \n",
    "- [Next steps](#toc7__)    \n",
    "  - [Work with your model documentation](#toc7_1__)    \n",
    "    - [Add individual test results to documentation](#toc7_1_1__)    \n",
    "  - [Discover more learning resources](#toc7_2__)    \n",
    "- [Upgrade ValidMind](#toc8__)    \n",
    "\n",
    ":::\n",
    "<!-- jn-toc-notebook-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=4\n",
    "\t/jn-toc-notebook-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1__'></a>\n",
    "\n",
    "## About ValidMind\n",
    "ValidMind is a suite of tools for managing model risk, including risk associated with AI and statistical models.\n",
    "\n",
    "You use the ValidMind Library to automate documentation and validation tests, and then use the ValidMind Platform to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "<a id='toc1_1__'></a>\n",
    "\n",
    "### Before you begin\n",
    "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "<a id='toc1_2__'></a>\n",
    "\n",
    "### New to ValidMind?\n",
    "If you haven't already seen our [Get started with the ValidMind Library](https://docs.validmind.ai/developer/get-started-validmind-library.html), we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models, find code samples, or read our developer reference.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
    "<br></br>\n",
    "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_3__'></a>\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n",
    "\n",
    "**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n",
    "\n",
    "**Tests**: A function contained in the ValidMind Library, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n",
    "\n",
    "**Custom tests**: Custom tests are functions that you define to evaluate your model or dataset. These functions can be registered via the ValidMind Library to be used with the ValidMind Platform.\n",
    "\n",
    "**Inputs**: Objects to be evaluated and documented in the ValidMind Library. They can be any of the following:\n",
    "\n",
    "- **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\n",
    "- **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\n",
    "- **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom test.\n",
    "- **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom test. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\n",
    "\n",
    "**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a test, customize its behavior, or provide additional context.\n",
    "\n",
    "**Outputs**: Custom tests can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n",
    "\n",
    "**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\n",
    "\n",
    "Example: The [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2__'></a>\n",
    "\n",
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_1__'></a>\n",
    "\n",
    "### Install the ValidMind Library\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2); border-radius: 5px;\">\n",
    "  <span style=\"color: #083E44;\"><b>Recommended Python versions</b></span><br />\n",
    "  Python 3.8 ≤ x ≤ 3.11\n",
    "</div>\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2__'></a>\n",
    "\n",
    "### Initialize the ValidMind Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2_1__'></a>\n",
    "\n",
    "#### Register sample model\n",
    "\n",
    "Let's first register a sample model for use with this notebook:\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Inventory** and click **+ Register Model**.\n",
    "\n",
    "3. Enter the model details and click **Next >** to continue to assignment of model stakeholders. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
    "\n",
    "   For example, to register a model for use with this notebook, select the following use case: `Credit Risk - CECL`\n",
    "\n",
    "4. Select your own name under the **MODEL OWNER** drop-down.\n",
    "\n",
    "5. Click **Register Model** to add the model to your inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2_2__'></a>\n",
    "\n",
    "#### Apply documentation template\n",
    "\n",
    "Once you've registered your model, let's select a documentation template. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "1. In the left sidebar that appears for your model, click **Documents** and select **Documentation**.\n",
    "\n",
    "2. Under **TEMPLATE**, select `Credit Risk Scorecard`.\n",
    "\n",
    "3. Click **Use Template** to apply the template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2_3__'></a>\n",
    "\n",
    "#### Get your code snippet\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "1. On the left sidebar that appears for your model, select **Getting Started** and click **Copy snippet to clipboard**.\n",
    "2. Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model identifier credentials from an `.env` file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    # api_host=\"...\",\n",
    "    # api_key=\"...\",\n",
    "    # api_secret=\"...\",\n",
    "    # model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_3__'></a>\n",
    "\n",
    "### Initialize the Python environment\n",
    "\n",
    "Then, let's import the necessary libraries and set up your Python environment for data analysis:\n",
    "\n",
    "- Install **OpenPyPL** (openpyxl) which will allow us to read and write `.xlsx` files.\n",
    "- Import `pandas`, a Python library for data manipulation and analytics, as an alias.\n",
    "- Enable `matplotlib`, a plotting library used for visualizing data. Ensures that any plots you generate will render inline in our notebook output rather than opening in a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_4__'></a>\n",
    "\n",
    "### Preview the documentation template\n",
    "\n",
    "Let's verify that you have connected the ValidMind Library to the ValidMind Platform and that the appropriate *template* is selected for your model.\n",
    "\n",
    "You will upload documentation and test results unique to your model based on this template later on. For now, **take a look at the default structure that the template provides with [the `vm.preview_template()` function](https://docs.validmind.ai/validmind/validmind.html#preview_template)** from the ValidMind library and note the empty sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3__'></a>\n",
    "\n",
    "## Load the sample datasets\n",
    "\n",
    "Let's import our sample dataset in the form of an Excel workbook ([CreditRiskData.xlsx](CreditRiskData.xlsx)) with five sheets indexed 0 to 3, each representing a different stage of data preparation:\n",
    "\n",
    "0. **Raw Data** – The original unprocessed dataset.\n",
    "1. **Preprocessed Data** – A cleaned and prepared version of the raw data.\n",
    "2. **Train Data** – A training subset used to fit your model.\n",
    "3. **Test Data** – A testing subset used to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3_1__'></a>\n",
    "\n",
    "### Load the raw dataset\n",
    "\n",
    "We'll start by loading the **Raw Data** sheet (index `0`) into a [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('CreditRiskData.xlsx', sheet_name=0,engine='openpyxl')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3_2__'></a>\n",
    "\n",
    "### Load the preprocessed dataset\n",
    "\n",
    "Next, load the **Preprocessed Data** sheet (index `1`), containing cleaned inputs ready for scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = pd.read_excel('CreditRiskData.xlsx', sheet_name=1,engine='openpyxl')\n",
    "preprocess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3_3__'></a>\n",
    "\n",
    "### Load the training and test datasets\n",
    "\n",
    "Finally, load the split training (**Train Data**, index `2`) and testing (**Test Data**, index `3`) sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('CreditRiskData.xlsx', sheet_name=2,engine='openpyxl')\n",
    "test_df = pd.read_excel('CreditRiskData.xlsx', sheet_name=3,engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4__'></a>\n",
    "\n",
    "## Initialize the ValidMind datasets\n",
    "\n",
    "Before you can run tests with your loaded datasets, you must first initialize a ValidMind `Dataset` object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module. **This step is always necessary every time you want to connect a dataset to documentation and produce test results through ValidMind,** but you only need to do it once per dataset.\n",
    "\n",
    "For this example, we'll pass in the following arguments:\n",
    "\n",
    "- **`dataset`:** The input DataFrame to test.\n",
    "- **`input_id`:** A unique identifier for tracking test inputs.\n",
    "- **`target_column`:** Required for tests that compare predictions to actual outcomes; specify the name of the column with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the raw dataset\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column='loan_status',\n",
    ")\n",
    "\n",
    "# Initialize the preprocessed dataset\n",
    "vm_preprocess_dataset = vm.init_dataset(\n",
    "    dataset=preprocess_df,\n",
    "    input_id=\"preprocess_dataset\",\n",
    "    target_column='loan_status',\n",
    ")\n",
    "\n",
    "# Initialize the training dataset\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column='loan_status',\n",
    ")\n",
    "\n",
    "# Initialize the testing dataset\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    input_id=\"test_dataset\",\n",
    "    target_column='loan_status',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5__'></a>\n",
    "\n",
    "## Initialize a model object\n",
    "\n",
    "In this Excel-based use case, predictions are precomputed and included in the Excel file. While there's no model logic to run, a ValidMind model object (`vm_model`) is still required for passing to other functions for analysis and tests on the data.\n",
    "\n",
    "Simply define a placeholder model using [`init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction logic placeholder\n",
    "def dummy(X, **kwargs):\n",
    "    return None\n",
    "\n",
    "xgb_model = vm.init_model(\n",
    "        input_id=\"xgb_model\",\n",
    "        predict_fn=dummy\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_1__'></a>\n",
    "\n",
    "### Link predictions\n",
    "\n",
    "Once the model has been registered, you can assign model predictions to the training and testing datasets.\n",
    "\n",
    "Use the [`assign_predictions()` method](https://docs.validmind.ai/validmind/validmind/vm_models.html#assign_predictions) from the `Dataset` object to link the prediction values and probabilities from the relevant columns on our Excel spreadsheet to the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(model=xgb_model, prediction_column=\"xgb_model_prediction\",probability_column='xgb_model_probabilities')\n",
    "vm_test_ds.assign_predictions(model=xgb_model, prediction_column=\"xgb_model_prediction\",probability_column='xgb_model_probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6__'></a>\n",
    "\n",
    "## Running tests\n",
    "\n",
    "This is where it all comes together — we'll use our previously initialized datasets as inputs to run tests, then log the results to the ValidMind Platform.\n",
    "\n",
    "We'll run some tests that are defined out-of-the-box by the template we previewed earlier in this notebook, as well as some additional tests for more evidence. For the example in this section, we've selected and defined the tests for you.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Want to learn more about navigating ValidMind tests?</b></span>\n",
    "<br></br>\n",
    "Refer to our notebook outlining the utilities available for viewing and understanding available ValidMind tests: <a href=\"https://docs.validmind.ai/notebooks/how_to/explore_tests.html\" style=\"color: #DE257E;\"><b>Explore tests</b></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_1__'></a>\n",
    "\n",
    "### Enable custom context for test descriptions\n",
    "\n",
    "When you run ValidMind tests, test descriptions are automatically generated with LLM using the test results, the test name, and the static test definitions provided in the test’s docstring. While this metadata offers valuable high-level overviews of tests, insights produced by the LLM-based descriptions may not always align with your specific use cases or incorporate organizational policy requirements.\n",
    "\n",
    "Before we run our tests, we'll include some custom use case context to improve the clarity, structure, and interpretability of the test descriptions returned. By default, custom context for LLM-generated descriptions is disabled, meaning that the output will not include any additional context. To enable custom use case context, set the `VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED` environment variable to `1`.\n",
    "\n",
    "This is a global setting that will affect all tests for your linked model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\"\n",
    "\n",
    "context = \"\"\"\n",
    "FORMAT FOR THE LLM DESCRIPTIONS: \n",
    "    **<Test Name>** is designed to <begin with a concise overview of what the test does and its primary purpose, \n",
    "    extracted from the test description>.\n",
    "\n",
    "    The test operates by <write a paragraph about the test mechanism, explaining how it works and what it measures. \n",
    "    Include any relevant formulas or methodologies mentioned in the test description.>\n",
    "\n",
    "    The primary advantages of this test include <write a paragraph about the test's strengths and capabilities, \n",
    "    highlighting what makes it particularly useful for specific scenarios.>\n",
    "\n",
    "    Users should be aware that <write a paragraph about the test's limitations and potential risks. \n",
    "    Include both technical limitations and interpretation challenges. \n",
    "    If the test description includes specific signs of high risk, incorporate these here.>\n",
    "\n",
    "    **Key Insights:**\n",
    "\n",
    "    The test results reveal:\n",
    "\n",
    "    - **<insight title>**: <comprehensive description of one aspect of the results>\n",
    "    - **<insight title>**: <comprehensive description of another aspect>\n",
    "    ...\n",
    "\n",
    "    Based on these results, <conclude with a brief paragraph that ties together the test results with the test's \n",
    "    purpose and provides any final recommendations or considerations.>\n",
    "\n",
    "ADDITIONAL INSTRUCTIONS:\n",
    "    Present insights in order from general to specific, with each insight as a single bullet point with bold title.\n",
    "\n",
    "    For each metric in the test results, include in the test overview:\n",
    "    - The metric's purpose and what it measures\n",
    "    - Its mathematical formula\n",
    "    - The range of possible values\n",
    "    - What constitutes good/bad performance\n",
    "    - How to interpret different values\n",
    "\n",
    "    Each insight should progressively cover:\n",
    "    1. Overall scope and distribution\n",
    "    2. Complete breakdown of all elements with specific values\n",
    "    3. Natural groupings and patterns\n",
    "    4. Comparative analysis between datasets/categories\n",
    "    5. Stability and variations\n",
    "    6. Notable relationships or dependencies\n",
    "\n",
    "    Remember:\n",
    "    - Keep all insights at the same level (no sub-bullets or nested structures)\n",
    "    - Make each insight complete and self-contained\n",
    "    - Include specific numerical values and ranges\n",
    "    - Cover all elements in the results comprehensively\n",
    "    - Maintain clear, concise language\n",
    "    - Use only \"- **Title**: Description\" format for insights\n",
    "    - Progress naturally from general to specific observations\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_2__'></a>\n",
    "\n",
    "### Define tests to run\n",
    "\n",
    "First, we'll specify all the tests we'd like to independently run in a dictionary called `test_config`, including information about the `params` and `inputs` that each test requires.\n",
    "\n",
    "-  Note here that `inputs` and `input_grid` expect the `input_id` of the dataset or model as the value rather than the variable name we specified**.\n",
    "- When running individual tests, you can use a custom `result_id` to tag the individual result with a unique identifier by appending this `result_id` to the `test_id` with a `:` separator. (Example: `:raw_data` for tests run with our raw dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "\n",
    "    # Data validation tests run with raw dataset\n",
    "    'validmind.data_validation.DatasetDescription:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'}\n",
    "    },\n",
    "    'validmind.data_validation.DescriptiveStatistics:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'}\n",
    "    },\n",
    "    'validmind.data_validation.MissingValues:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'min_threshold': 1}\n",
    "    },\n",
    "    'validmind.data_validation.ClassImbalance:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'min_percent_threshold': 10}\n",
    "    },\n",
    "    'validmind.data_validation.Duplicates:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'min_threshold': 1}\n",
    "    },\n",
    "    'validmind.data_validation.HighCardinality:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {\n",
    "            'num_threshold': 100,\n",
    "            'percent_threshold': 0.1,\n",
    "            'threshold_type': 'percent'\n",
    "        }\n",
    "    },\n",
    "    'validmind.data_validation.Skewness:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'max_threshold': 1}\n",
    "    },\n",
    "    'validmind.data_validation.UniqueRows:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'min_percent_threshold': 1}\n",
    "    },\n",
    "    'validmind.data_validation.TooManyZeroValues:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'max_percent_threshold': 0.03}\n",
    "    },\n",
    "    'validmind.data_validation.IQROutliersTable:raw_data': {\n",
    "        'inputs': {'dataset': 'raw_dataset'},\n",
    "        'params': {'threshold': 5}\n",
    "    },\n",
    "\n",
    "    # Data validation tests run with preprocessed dataset\n",
    "    'validmind.data_validation.DescriptiveStatistics:preprocessed_data': {\n",
    "        'inputs': {'dataset': 'preprocess_dataset'}\n",
    "    },\n",
    "    'validmind.data_validation.TabularDescriptionTables:preprocessed_data': {\n",
    "        'inputs': {'dataset': 'preprocess_dataset'}\n",
    "    },\n",
    "    'validmind.data_validation.MissingValues:preprocessed_data': {\n",
    "        'inputs': {'dataset': 'preprocess_dataset'},\n",
    "        'params': {'min_threshold': 1}\n",
    "    },\n",
    "    'validmind.data_validation.TabularNumericalHistograms:preprocessed_data': {\n",
    "        'inputs': {'dataset': 'preprocess_dataset'}\n",
    "    },\n",
    "    'validmind.data_validation.TabularCategoricalBarPlots:preprocessed_data': {\n",
    "        'inputs': {'dataset': 'preprocess_dataset'}\n",
    "    },\n",
    "    'validmind.data_validation.TargetRateBarPlots:preprocessed_data': {\n",
    "        'inputs': {'dataset': 'preprocess_dataset'},\n",
    "        'params': {'default_column': 'loan_status'}\n",
    "    },\n",
    "\n",
    "    'validmind.data_validation.WOEBinTable': {\n",
    "        'input_grid': {'dataset': ['preprocess_dataset']},\n",
    "        'params': {\n",
    "            'breaks_adj': {\n",
    "                'loan_amnt': [5000, 10000, 15000, 20000, 25000],\n",
    "                'int_rate': [10, 15, 20],\n",
    "                'annual_inc': [50000, 100000, 150000]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'validmind.data_validation.WOEBinPlots': {\n",
    "        'input_grid': {'dataset': ['preprocess_dataset']},\n",
    "        'params': {\n",
    "            'breaks_adj': {\n",
    "                'loan_amnt': [5000, 10000, 15000, 20000, 25000],\n",
    "                'int_rate': [10, 15, 20],\n",
    "                'annual_inc': [50000, 100000, 150000]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Data validation tests run with training & testing datasets\n",
    "    'validmind.data_validation.DescriptiveStatistics:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']}\n",
    "    },\n",
    "    'validmind.data_validation.TabularDescriptionTables:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']}\n",
    "    },\n",
    "    'validmind.data_validation.ClassImbalance:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']},\n",
    "        'params': {'min_percent_threshold': 10}\n",
    "    },\n",
    "    'validmind.data_validation.UniqueRows:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']},\n",
    "        'params': {'min_percent_threshold': 1}\n",
    "    },\n",
    "    'validmind.data_validation.TabularNumericalHistograms:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']}\n",
    "    },\n",
    "    'validmind.data_validation.MutualInformation:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']},\n",
    "        'params': {'min_threshold': 0.01}\n",
    "    },\n",
    "    'validmind.data_validation.PearsonCorrelationMatrix:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']}\n",
    "    },\n",
    "    'validmind.data_validation.HighPearsonCorrelation:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']},\n",
    "        'params': {'max_threshold': 0.3, 'top_n_correlations': 10}\n",
    "    },\n",
    "    'validmind.data_validation.ScoreBandDefaultRates:development_data': {\n",
    "        'input_grid': {'dataset': ['train_dataset'], 'model': ['xgb_model']},\n",
    "        'params': {'score_column': 'xgb_scores', 'score_bands': [504, 537, 570]}\n",
    "    },\n",
    "    'validmind.data_validation.DatasetSplit:development_data': {\n",
    "        'inputs': {'datasets': ['train_dataset', 'test_dataset']}\n",
    "    },\n",
    "\n",
    "    # Model validation tests\n",
    "    'validmind.model_validation.statsmodels.GINITable': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.ClassifierPerformance': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.TrainingTestDegradation:XGBoost': {\n",
    "        'inputs': {\n",
    "            'datasets': ['train_dataset', 'test_dataset'],\n",
    "            'model': 'xgb_model'\n",
    "        },\n",
    "        'params': {'max_threshold': 0.1}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.ROCCurve': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.MinimumROCAUCScore': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']},\n",
    "        'params': {'min_threshold': 0.5}\n",
    "    },\n",
    "    'validmind.model_validation.statsmodels.PredictionProbabilitiesHistogram': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.statsmodels.CumulativePredictionProbabilities': {\n",
    "        'input_grid': {'model': ['xgb_model'], 'dataset': ['train_dataset', 'test_dataset']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.PopulationStabilityIndex': {\n",
    "        'inputs': {\n",
    "            'datasets': ['train_dataset', 'test_dataset'],\n",
    "            'model': 'xgb_model'\n",
    "        },\n",
    "        'params': {'num_bins': 10, 'mode': 'fixed'}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.ConfusionMatrix': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.MinimumAccuracy': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']},\n",
    "        'params': {'min_threshold': 0.7}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.MinimumF1Score': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']},\n",
    "        'params': {'min_threshold': 0.5}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.PrecisionRecallCurve': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.CalibrationCurve': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset'], 'model': ['xgb_model']}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.ClassifierThresholdOptimization': {\n",
    "        'inputs': {'dataset': 'train_dataset', 'model': 'xgb_model'},\n",
    "        'params': {'target_recall': 0.8}\n",
    "    },\n",
    "    'validmind.model_validation.statsmodels.ScorecardHistogram': {\n",
    "        'input_grid': {'dataset': ['train_dataset', 'test_dataset']},\n",
    "        'params': {'score_column': 'xgb_scores'}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.ScoreProbabilityAlignment': {\n",
    "        'input_grid': {'dataset': ['train_dataset'], 'model': ['xgb_model']},\n",
    "        'params': {'score_column': 'xgb_scores'}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.WeakspotsDiagnosis': {\n",
    "        'inputs': {'datasets': ['train_dataset', 'test_dataset'], 'model': 'xgb_model'}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.OverfitDiagnosis': {\n",
    "        'inputs': {'model': 'xgb_model', 'datasets': ['train_dataset', 'test_dataset']},\n",
    "        'params': {'cut_off_threshold': 0.04}\n",
    "    },\n",
    "    'validmind.model_validation.sklearn.RobustnessDiagnosis': {\n",
    "        'inputs': {'datasets': ['train_dataset', 'test_dataset'], 'model': 'xgb_model'},\n",
    "        'params': {\n",
    "            'scaling_factor_std_dev_list': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "            'performance_decay_threshold': 0.05\n",
    "        }\n",
    "    },\n",
    "    'validmind.model_validation.FeaturesAUC': {\n",
    "        'input_grid': {'model': ['xgb_model'], 'dataset': ['train_dataset', 'test_dataset']}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_3__'></a>\n",
    "\n",
    "### Run defined tests\n",
    "\n",
    "Then, we'll define a utility wrapper around [the `run_test` function](https://docs.validmind.ai/validmind/validmind/tests.html#run_test) provided by the `validmind.tests` module in a function called `run_doc_tests`.\n",
    "\n",
    "- Every test result returned by the `run_test()` function has a [`.log()` method](https://docs.validmind.ai/validmind/validmind/vm_models.html#TestResult.log) that can be used to send the test results to the ValidMind Platform.\n",
    "- Our function requires information about the inputs to use on every test — which is why we specified these inputs above in `test_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_doc_tests(test_config):\n",
    "    for test_name, test_cfg in test_config.items():\n",
    "        print(test_name)\n",
    "        try:\n",
    "            # Collect available keyword arguments\n",
    "            kwargs = {\n",
    "                key: test_cfg[key]\n",
    "                for key in (\"params\", \"input_grid\", \"inputs\")\n",
    "                if key in test_cfg\n",
    "            }\n",
    "            kwargs[\"show\"] = False\n",
    "\n",
    "            # Execute the test and log the results\n",
    "            vm.tests.run_test(test_name, **kwargs).log()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error running test {test_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pass the input configuration to `run_doc_tests` and run the full suite of tests!\n",
    "\n",
    "The variable `full_suite` then holds the result of these tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_suite = run_doc_tests(test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Note the outputs returned indicating that certain test-driven blocks don't currently exist in your model's documentation for this particular test ID. </b></span>\n",
    "<br></br>\n",
    "That's expected, as when we run individual tests not defined by the documentation template out-of-the-box, the results logged need to be manually added to your documentation within the ValidMind Platform.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7__'></a>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "You can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way, use the ValidMind Platform to work with your model documentation.\n",
    "\n",
    "<a id='toc7_1__'></a>\n",
    "\n",
    "### Work with your model documentation\n",
    "\n",
    "1. From the **Inventory** in the ValidMind Platform, go to the model you registered earlier. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/working-with-model-inventory.html))\n",
    "\n",
    "2. In the left sidebar that appears for your model, click **Documentation** under Documents.\n",
    "\n",
    "   What you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it's ready.\n",
    "\n",
    "3. Expand the following section to review tests automatically inserted into your documentation template: **2.3. Feature Selection and Engineering**\n",
    "\n",
    "<a id='toc7_1_1__'></a>\n",
    "\n",
    "#### Add individual test results to documentation\n",
    "\n",
    "Let's also add our additional test results into the documentation. These were results sent by individual tests not defined out-of-the-box by our template. For example ([Need more help?](https://docs.validmind.ai/developer/model-documentation/work-with-test-results.html)):\n",
    "\n",
    "1. Locate the Data Preparation section of your documentation and click on **2.2. Correlations and Interactions** to expand that section.\n",
    "\n",
    "4. Hover under the Pearson Correlation Matrix content block until a horizontal dashed line with a **+** button appears, indicating that you can insert a new block.\n",
    "\n",
    "    <img src= \"../../tutorials/model_development/add-content-block.gif\" alt=\"Screenshot showing insert block button in model documentation\" style=\"border: 2px solid #083E44; border-radius: 8px; border-right-width: 2px; border-bottom-width: 3px;\">\n",
    "    <br><br>\n",
    "\n",
    "5. Click **+** and then select **Test-Driven Block** under FROM LIBRARY:\n",
    "\n",
    "    - Click on **VM Library** under TEST-DRIVEN in the left sidebar.\n",
    "    - In the search bar, type in `HighPearsonCorrelation`.\n",
    "    - Select `HighPearsonCorrelation:development_data` as the test.\n",
    "\n",
    "6. Finally, click **Insert 1 Test Result to Document** to add the test result to the documentation.\n",
    "\n",
    "    Confirm that the individual results for the high correlation test has been correctly inserted into section **2.3. Correlations and Interactions** of the documentation.\n",
    "\n",
    "<a id='toc7_2__'></a>\n",
    "\n",
    "### Discover more learning resources\n",
    "\n",
    "We offer many interactive notebooks to help you document models:\n",
    "\n",
    "- [Run tests & test suites](https://docs.validmind.ai/developer/model-testing/testing-overview.html)\n",
    "- [Code samples](https://docs.validmind.ai/developer/samples-jupyter-notebooks.html)\n",
    "\n",
    "Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc8__'></a>\n",
    "\n",
    "## Upgrade ValidMind\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\">After installing ValidMind, you’ll want to periodically make sure you are on the latest version to access any new features and other enhancements.</div>\n",
    "\n",
    "Retrieve the information for the currently installed version of ValidMind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the version returned is lower than the version indicated in our [production open-source code](https://github.com/validmind/validmind-library/blob/prod/validmind/__version__.py), restart your notebook and run:\n",
    "\n",
    "```bash\n",
    "%pip install --upgrade validmind\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to restart your kernel after running the upgrade package for changes to be applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ValidMind Library",
   "language": "python",
   "name": "validmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
