{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Simplified LangGraph Agent Model Documentation\n",
        "\n",
        "This notebook demonstrates how to build and validate a simplified AI agent using LangGraph integrated with ValidMind for comprehensive testing and monitoring.\n",
        "\n",
        "Learn how to create intelligent agents that can:\n",
        "- **Automatically select appropriate tools** based on user queries using LLM-powered routing\n",
        "- **Manage workflows** with state management and memory\n",
        "- **Handle two specialized tools** with smart decision-making\n",
        "- **Provide validation and testing** through ValidMind integration\n",
        "\n",
        "We'll build a simplified agent system that intelligently routes user requests to two specialized tools: **search_engine** for document search and **task_assistant** for general assistance, then validate its performance using ValidMind's testing framework.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's import all the necessary libraries for building our LangGraph agent system:\n",
        "\n",
        "- **LangChain components** for LLM integration and tool management\n",
        "- **LangGraph** for building stateful, multi-step agent workflows  \n",
        "- **ValidMind** for model validation and testing\n",
        "- **Standard libraries** for data handling and environment management\n",
        "\n",
        "The setup includes loading environment variables (like OpenAI API keys) needed for the LLM components to function properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q langgraph langchain validmind openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict,  Annotated, Sequence, Optional\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph.message import add_messages\n",
        "import pandas as pd\n",
        "\n",
        "# Load environment variables if using .env file\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    print(\"dotenv not installed. Make sure OPENAI_API_KEY is set in your environment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"...\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    model=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simplified Tools with Rich Docstrings\n",
        "\n",
        "We've simplified the agent to use only two core tools:\n",
        "- **search_engine**: For searching through documents, policies, and knowledge base  \n",
        "- **task_assistant**: For general-purpose task assistance and problem-solving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search Engine Tool\n",
        "@tool\n",
        "def search_engine(query: str, document_type: Optional[str] = \"all\") -> str:\n",
        "    \"\"\"\n",
        "    Search through internal documents, policies, and knowledge base.\n",
        "    \n",
        "    This tool can search for:\n",
        "    - Company policies and procedures\n",
        "    - Technical documentation and manuals\n",
        "    - Compliance and regulatory documents\n",
        "    - Historical records and reports\n",
        "    - Product specifications and requirements\n",
        "    - Legal documents and contracts\n",
        "    \n",
        "    Args:\n",
        "        query (str): Search terms or questions about documents\n",
        "        document_type (str, optional): Type of document to search (\"policy\", \"technical\", \"legal\", \"all\")\n",
        "    \n",
        "    Returns:\n",
        "        str: Relevant document excerpts and references\n",
        "        \n",
        "    Examples:\n",
        "        - \"Find our data privacy policy\"\n",
        "        - \"Search for loan approval procedures\"\n",
        "        - \"What are the security guidelines for API access?\"\n",
        "        - \"Show me compliance requirements for financial reporting\"\n",
        "    \"\"\"\n",
        "    document_db = {\n",
        "        \"policy\": [\n",
        "            \"Data Privacy Policy: All personal data must be encrypted...\",\n",
        "            \"Remote Work Policy: Employees may work remotely up to 3 days...\",\n",
        "            \"Security Policy: All systems require multi-factor authentication...\"\n",
        "        ],\n",
        "        \"technical\": [\n",
        "            \"API Documentation: REST endpoints available at /api/v1/...\",\n",
        "            \"Database Schema: User table contains id, name, email...\",\n",
        "            \"Deployment Guide: Use Docker containers with Kubernetes...\"\n",
        "        ],\n",
        "        \"legal\": [\n",
        "            \"Terms of Service: By using this service, you agree to...\",\n",
        "            \"Privacy Notice: We collect information to provide services...\",\n",
        "            \"Compliance Framework: SOX requirements mandate quarterly audits...\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    results = []\n",
        "    search_types = [document_type] if document_type != \"all\" else document_db.keys()\n",
        "    \n",
        "    for doc_type in search_types:\n",
        "        if doc_type in document_db:\n",
        "            for doc in document_db[doc_type]:\n",
        "                if any(term.lower() in doc.lower() for term in query.split()):\n",
        "                    results.append(f\"[{doc_type.upper()}] {doc}\")\n",
        "    \n",
        "    if not results:\n",
        "        results.append(f\"No documents found matching '{query}'\")\n",
        "    \n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "# Task Assistant Tool\n",
        "@tool\n",
        "def task_assistant(task_description: str, context: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    General-purpose task assistance and problem-solving tool.\n",
        "    \n",
        "    This tool can help with:\n",
        "    - Breaking down complex tasks into steps\n",
        "    - Providing guidance and recommendations\n",
        "    - Answering questions and explaining concepts\n",
        "    - Suggesting solutions to problems\n",
        "    - Planning and organizing activities\n",
        "    - Research and information gathering\n",
        "    \n",
        "    Args:\n",
        "        task_description (str): Description of the task or question\n",
        "        context (str, optional): Additional context or background information\n",
        "    \n",
        "    Returns:\n",
        "        str: Helpful guidance, steps, or information for the task\n",
        "        \n",
        "    Examples:\n",
        "        - \"How do I prepare for a job interview?\"\n",
        "        - \"What are the steps to deploy a web application?\"\n",
        "        - \"Help me plan a team meeting agenda\"\n",
        "        - \"Explain machine learning concepts for beginners\"\n",
        "    \"\"\"\n",
        "    responses = {\n",
        "        \"meeting\": \"For planning meetings: 1) Define objectives, 2) Create agenda, 3) Invite participants, 4) Prepare materials, 5) Set time limits\",\n",
        "        \"interview\": \"Interview preparation: 1) Research the company, 2) Practice common questions, 3) Prepare examples, 4) Plan your outfit, 5) Arrive early\",\n",
        "        \"deploy\": \"Deployment steps: 1) Test in staging, 2) Backup production, 3) Deploy code, 4) Run health checks, 5) Monitor performance\",\n",
        "        \"learning\": \"Learning approach: 1) Start with basics, 2) Practice regularly, 3) Build projects, 4) Join communities, 5) Stay updated\"\n",
        "    }\n",
        "    \n",
        "    task_lower = task_description.lower()\n",
        "    for key, response in responses.items():\n",
        "        if key in task_lower:\n",
        "            return f\"Task assistance for '{task_description}':\\n\\n{response}\"\n",
        "    \n",
        "    \n",
        "    return f\"\"\"For the task '{task_description}', I recommend: 1) Break it into smaller steps, 2) Gather necessary resources, 3)\n",
        "    Create a timeline, 4) Start with the most critical parts, 5) Review and adjust as needed.\n",
        "        \"\"\"\n",
        "\n",
        "# Collect all tools for the LLM router - SIMPLIFIED TO ONLY 2 TOOLS\n",
        "AVAILABLE_TOOLS = [\n",
        "    search_engine,\n",
        "    task_assistant\n",
        "]\n",
        "\n",
        "print(\"Simplified tools created!\")\n",
        "print(f\"Available tools: {len(AVAILABLE_TOOLS)}\")\n",
        "for tool in AVAILABLE_TOOLS:\n",
        "    print(f\"   - {tool.name}: {tool.description[:50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete LangGraph Agent with Intelligent Router\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Simplified Agent State (removed routing fields)\n",
        "class IntelligentAgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    user_input: str\n",
        "    session_id: str\n",
        "    context: dict\n",
        "\n",
        "def create_intelligent_langgraph_agent():\n",
        "    \"\"\"Create a simplified LangGraph agent with direct LLM tool selection.\"\"\"\n",
        "    \n",
        "    # Initialize the main LLM for responses\n",
        "    main_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "    \n",
        "    # Bind tools to the main LLM\n",
        "    llm_with_tools = main_llm.bind_tools(AVAILABLE_TOOLS)\n",
        "    \n",
        "    def llm_node(state: IntelligentAgentState) -> IntelligentAgentState:\n",
        "        \"\"\"Main LLM node that processes requests and directly selects tools.\"\"\"\n",
        "        \n",
        "        messages = state[\"messages\"]\n",
        "        \n",
        "        # Enhanced system prompt with tool selection guidance\n",
        "        system_context = f\"\"\"You are a helpful AI assistant with access to specialized tools.\n",
        "            Analyze the user's request and directly use the most appropriate tools to help them.\n",
        "            \n",
        "            AVAILABLE TOOLS:\n",
        "            ðŸ” **search_engine** - Search through internal documents, policies, and knowledge base\n",
        "            - Use for: finding company policies, technical documentation, compliance documents\n",
        "            - Examples: \"Find our data privacy policy\", \"Search for API documentation\"\n",
        "\n",
        "            ðŸŽ¯ **task_assistant** - General-purpose task assistance and problem-solving  \n",
        "            - Use for: guidance, recommendations, explaining concepts, planning activities\n",
        "            - Examples: \"How to prepare for an interview\", \"Help plan a meeting\", \"Explain machine learning\"\n",
        "\n",
        "            INSTRUCTIONS:\n",
        "            - Analyze the user's request carefully\n",
        "            - If they need to find documents/policies â†’ use search_engine\n",
        "            - If they need general help/guidance/explanations â†’ use task_assistant  \n",
        "            - If the request needs specific information search, use search_engine first\n",
        "            - You can use tools directly based on the user's needs\n",
        "            - Provide helpful, accurate responses based on tool outputs\n",
        "            - If no tools are needed, respond conversationally\n",
        "\n",
        "            Choose and use tools wisely to provide the most helpful response.\"\"\"\n",
        "        \n",
        "        # Add system context to messages\n",
        "        enhanced_messages = [SystemMessage(content=system_context)] + list(messages)\n",
        "        \n",
        "        # Get LLM response with tool selection\n",
        "        response = llm_with_tools.invoke(enhanced_messages)\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"messages\": messages + [response]\n",
        "        }\n",
        "    \n",
        "    def should_continue(state: IntelligentAgentState) -> str:\n",
        "        \"\"\"Decide whether to use tools or end the conversation.\"\"\"\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        \n",
        "        # Check if the LLM wants to use tools\n",
        "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "            return \"tools\"\n",
        "        \n",
        "        return END\n",
        "        \n",
        "    \n",
        "    # Create the simplified state graph  \n",
        "    workflow = StateGraph(IntelligentAgentState)\n",
        "    \n",
        "    # Add nodes (removed router node)\n",
        "    workflow.add_node(\"llm\", llm_node) \n",
        "    workflow.add_node(\"tools\", ToolNode(AVAILABLE_TOOLS))\n",
        "    \n",
        "    # Simplified entry point - go directly to LLM\n",
        "    workflow.add_edge(START, \"llm\")\n",
        "    \n",
        "    # From LLM, decide whether to use tools or end\n",
        "    workflow.add_conditional_edges(\n",
        "        \"llm\",\n",
        "        should_continue,\n",
        "        {\"tools\": \"tools\", END: END}\n",
        "    )\n",
        "    \n",
        "    # Tool execution flows back to LLM for final response\n",
        "    workflow.add_edge(\"tools\", \"llm\")\n",
        "    \n",
        "    # Set up memory\n",
        "    memory = MemorySaver()\n",
        "    \n",
        "    # Compile the graph\n",
        "    agent = workflow.compile(checkpointer=memory)\n",
        "    \n",
        "    return agent\n",
        "\n",
        "# Create the simplified intelligent agent\n",
        "intelligent_agent = create_intelligent_langgraph_agent()\n",
        "\n",
        "print(\"Simplified LangGraph Agent Created!\")\n",
        "print(\"Features:\")\n",
        "print(\"   - Direct LLM tool selection (no separate router)\")\n",
        "print(\"   - Enhanced system prompt for intelligent tool choice\")\n",
        "print(\"   - Streamlined workflow: LLM -> Tools -> Response\")\n",
        "print(\"   - Automatic tool parameter extraction\")\n",
        "print(\"   - Clean, simplified architecture\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ValidMind Model Integration\n",
        "\n",
        "Now we'll integrate our LangGraph agent with ValidMind for comprehensive testing and validation. This step is crucial for:\n",
        "\n",
        "**Model Wrapping**: We create a wrapper function (`agent_fn`) that standardizes the agent interface for ValidMind\n",
        "- **Input Formatting**: Converts ValidMind inputs to the agent's expected format\n",
        "- **State Management**: Handles session configuration and conversation threads\n",
        "- **Result Processing**: Returns agent responses in a consistent format\n",
        "\n",
        "**ValidMind Agent Initialization**: Using `vm.init_model()` creates a ValidMind model object that:\n",
        "- **Enables Testing**: Allows us to run validation tests on the agent\n",
        "- **Tracks Performance**: Monitors agent behavior and responses  \n",
        "- **Provides Documentation**: Generates documentation and analysis reports\n",
        "- **Supports Evaluation**: Enables quantitative assessment of agent capabilities\n",
        "\n",
        "This integration allows us to treat our LangGraph agent like any other machine learning model in the ValidMind ecosystem, enabling comprehensive testing and validation workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def agent_fn(input):\n",
        "    \"\"\"\n",
        "    Invoke the simplified agent with the given input.\n",
        "    \"\"\"\n",
        "    # Simplified initial state (removed routing fields)\n",
        "    initial_state = {\n",
        "        \"user_input\": input[\"input\"],\n",
        "        \"messages\": [HumanMessage(content=input[\"input\"])],\n",
        "        \"session_id\": input[\"session_id\"],\n",
        "        \"context\": {}\n",
        "    }\n",
        "\n",
        "    session_config = {\"configurable\": {\"thread_id\": input[\"session_id\"]}}\n",
        "\n",
        "    result = intelligent_agent.invoke(initial_state, config=session_config)\n",
        "\n",
        "    return {\"prediction\": result['messages'][-1].content, \"output\": result}\n",
        "\n",
        "\n",
        "vm_intelligent_model = vm.init_model(input_id=\"financial_model\", predict_fn=agent_fn)\n",
        "# add model to the vm agent\n",
        "vm_intelligent_model.model = intelligent_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Sample Test Dataset\n",
        "\n",
        "We'll create a comprehensive test dataset to evaluate our agent's performance across different scenarios. This dataset includes:\n",
        "\n",
        "**Diverse Test Cases**: Various types of user requests that test different agent capabilities:\n",
        "- **Single Tool Requests**: Simple queries that require one specific tool\n",
        "- **Multi-Tool Requests**: Complex queries requiring multiple tools in sequence  \n",
        "- **Validation Tasks**: Requests for data validation and verification\n",
        "- **General Assistance**: Open-ended questions for problem-solving guidance\n",
        "\n",
        "**Expected Outputs**: For each test case, we define:\n",
        "- **Expected Tools**: Which tools should be selected by the router\n",
        "- **Possible Outputs**: Valid response patterns or values\n",
        "- **Session IDs**: Unique identifiers for conversation tracking\n",
        "\n",
        "This structured approach allows us to systematically evaluate both tool selection accuracy and response quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Simplified test dataset with only search_engine and task_assistant tools\n",
        "test_dataset = pd.DataFrame([\n",
        "    {\n",
        "        \"input\": \"Find our company's data privacy policy\",\n",
        "        \"expected_tools\": [\"search_engine\"],\n",
        "        \"possible_outputs\": [\"privacy_policy.pdf\", \"data_protection.doc\", \"company_privacy_guidelines.txt\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Search for loan approval procedures\", \n",
        "        \"expected_tools\": [\"search_engine\"],\n",
        "        \"possible_outputs\": [\"loan_procedures.doc\", \"approval_process.pdf\", \"lending_guidelines.txt\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How should I prepare for a technical interview?\",\n",
        "        \"expected_tools\": [\"task_assistant\"],\n",
        "        \"possible_outputs\": [\"algorithms\", \"data structures\", \"system design\", \"coding practice\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Help me understand machine learning basics\",\n",
        "        \"expected_tools\": [\"task_assistant\"],\n",
        "        \"possible_outputs\": [\"supervised\", \"unsupervised\", \"neural networks\", \"training\", \"testing\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What can you do for me?\",\n",
        "        \"expected_tools\": [\"task_assistant\"],\n",
        "        \"possible_outputs\": [\"search documents\", \"provide assistance\", \"answer questions\", \"help with tasks\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Find technical documentation about API endpoints\",\n",
        "        \"expected_tools\": [\"search_engine\"],\n",
        "        \"possible_outputs\": [\"API_documentation.pdf\", \"REST_endpoints.doc\", \"technical_guide.txt\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Help me plan a team meeting agenda\",\n",
        "        \"expected_tools\": [\"task_assistant\"],\n",
        "        \"possible_outputs\": [\"objectives\", \"agenda\", \"participants\", \"materials\", \"time limits\"],\n",
        "        \"session_id\": str(uuid.uuid4())\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"Simplified test dataset created!\")\n",
        "print(f\"Number of test cases: {len(test_dataset)}\")\n",
        "print(f\"Test tools: {test_dataset['expected_tools'].explode().unique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the simplified test dataset\n",
        "print(\"Using simplified test dataset with only 2 tools:\")\n",
        "print(f\"Number of test cases: {len(test_dataset)}\")\n",
        "print(f\"Available tools being tested: {sorted(test_dataset['expected_tools'].explode().unique())}\")\n",
        "print(\"\\nTest cases preview:\")\n",
        "for i, row in test_dataset.iterrows():\n",
        "    print(f\"{i+1}. {row['input']} -> Expected tool: {row['expected_tools'][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize ValidMind Dataset\n",
        "\n",
        "Before we can run tests and evaluations, we need to initialize our test dataset as a ValidMind dataset object. \n",
        "This step is essential for integrating our agent evaluation into ValidMind's comprehensive testing and validation framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_test_dataset = vm.init_dataset(\n",
        "    input_id=\"test_dataset\",\n",
        "    dataset=test_dataset,\n",
        "    target_column=\"possible_outputs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Agent and Assign Predictions\n",
        "\n",
        "Now we'll execute our agent on the test dataset and capture its responses for evaluation. This process generates the prediction data needed for comprehensive performance evaluation and comparison against expected outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_test_dataset.assign_predictions(vm_intelligent_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataframe display settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 40)\n",
        "pd.set_option('display.width', 120)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "vm_test_dataset._df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "This section visualizes the LangGraph agent's workflow structure using Mermaid diagrams.\n",
        "The test below validates that the agent's architecture is properly structured by:\n",
        "- Checking if the model has a valid LangGraph Graph object\n",
        "- Generating a visual representation of component connections and flow\n",
        "- Ensuring the graph can be properly rendered as a Mermaid diagram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import langgraph\n",
        "\n",
        "@vm.test(\"my_custom_tests.LangGraphVisualization\")\n",
        "def LangGraphVisualization(model):\n",
        "    \"\"\"\n",
        "    Visualizes the LangGraph workflow structure using Mermaid diagrams.\n",
        "    \n",
        "    ### Purpose\n",
        "    Creates a visual representation of the LangGraph agent's workflow using Mermaid diagrams\n",
        "    to show the connections and flow between different components. This helps validate that\n",
        "    the agent's architecture is properly structured.\n",
        "    \n",
        "    ### Test Mechanism\n",
        "    1. Retrieves the graph representation from the model using get_graph()\n",
        "    2. Attempts to render it as a Mermaid diagram\n",
        "    3. Returns the visualization and validation results\n",
        "    \n",
        "    ### Signs of High Risk\n",
        "    - Failure to generate graph visualization indicates potential structural issues\n",
        "    - Missing or broken connections between components\n",
        "    - Invalid graph structure that cannot be rendered\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not hasattr(model, 'model') or not isinstance(model.model, langgraph.graph.state.CompiledStateGraph):\n",
        "            return {\n",
        "                'test_results': False,\n",
        "                'summary': {\n",
        "                    'status': 'FAIL', \n",
        "                    'details': 'Model must have a LangGraph Graph object as model attribute'\n",
        "                }\n",
        "            }\n",
        "        graph = model.model.get_graph(xray=False)\n",
        "        mermaid_png = graph.draw_mermaid_png()\n",
        "        return mermaid_png\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'test_results': False, \n",
        "            'summary': {\n",
        "                'status': 'FAIL',\n",
        "                'details': f'Failed to generate graph visualization: {str(e)}'\n",
        "            }\n",
        "        }\n",
        "\n",
        "vm.tests.run_test(\n",
        "    \"my_custom_tests.LangGraphVisualization\",\n",
        "    inputs = {\n",
        "        \"model\": vm_intelligent_model\n",
        "    }\n",
        ").log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy Test\n",
        "The purpose of this test is to evaluate the agent's ability to provide accurate responses by:\n",
        "- Testing against a dataset of predefined questions and expected answers\n",
        "- Checking if responses contain expected keywords\n",
        "- Providing detailed test results including pass/fail status\n",
        "- Helping identify any gaps in the agent's knowledge or response quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import validmind as vm\n",
        "\n",
        "@vm.test(\"my_custom_tests.accuracy_test\")\n",
        "def accuracy_test(model, dataset, list_of_columns):\n",
        "    \"\"\"\n",
        "    Run tests on a dataset of questions and expected responses.\n",
        "    Optimized version using vectorized operations and list comprehension.\n",
        "    \"\"\"\n",
        "    df = dataset._df\n",
        "    \n",
        "    # Pre-compute responses for all tests\n",
        "    y_true = dataset.y.tolist()\n",
        "    y_pred = dataset.y_pred(model).tolist()\n",
        "\n",
        "    # Vectorized test results\n",
        "    test_results = []\n",
        "    for response, keywords in zip(y_pred, y_true):\n",
        "        test_results.append(any(str(keyword).lower() in str(response).lower() for keyword in keywords))\n",
        "        \n",
        "    results = pd.DataFrame()\n",
        "    column_names = [col + \"_details\" for col in list_of_columns]\n",
        "    results[column_names] = df[list_of_columns]\n",
        "    results[\"actual\"] = y_pred\n",
        "    results[\"expected\"] = y_true\n",
        "    results[\"passed\"] = test_results\n",
        "    results[\"error\"] = None if test_results else f'Response did not contain any expected keywords: {y_true}'\n",
        "    \n",
        "    return results\n",
        "   \n",
        "result = vm.tests.run_test(\n",
        "    \"my_custom_tests.accuracy_test\",\n",
        "    inputs={\n",
        "        \"dataset\": vm_test_dataset,\n",
        "        \"model\": vm_intelligent_model\n",
        "    },\n",
        "    params={\n",
        "        \"list_of_columns\": [\"input\"]\n",
        "    }\n",
        ")\n",
        "result.log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool Call Accuracy Test\n",
        "\n",
        "This test evaluates how accurately our intelligent router selects the correct tools for different user requests. This test provides quantitative feedback on the agent's core intelligence - its ability to understand what users need and select the right tools to help them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import validmind as vm\n",
        "\n",
        "# Test with a real LangGraph result instead of creating mock objects\n",
        "@vm.test(\"my_custom_tests.ToolCallAccuracy\")\n",
        "def ToolCallAccuracy(dataset, agent_output_column, expected_tools_column):\n",
        "    \"\"\"Test validation using actual LangGraph agent results.\"\"\"\n",
        "    # Let's create a simpler validation without the complex RAGAS setup\n",
        "    def validate_tool_calls_simple(messages, expected_tools):\n",
        "        \"\"\"Simple validation of tool calls without RAGAS dependency issues.\"\"\"\n",
        "        \n",
        "        tool_calls_found = []\n",
        "        \n",
        "        for message in messages:\n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                for tool_call in message.tool_calls:\n",
        "                    # Handle both dictionary and object formats\n",
        "                    if isinstance(tool_call, dict):\n",
        "                        tool_calls_found.append(tool_call['name'])\n",
        "                    else:\n",
        "                        # ToolCall object - use attribute access\n",
        "                        tool_calls_found.append(tool_call.name)\n",
        "        \n",
        "        # Check if expected tools were called\n",
        "        accuracy = 0.0\n",
        "        matches = 0\n",
        "        if expected_tools:\n",
        "            matches = sum(1 for tool in expected_tools if tool in tool_calls_found)\n",
        "            accuracy = matches / len(expected_tools)\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'expected_tools': expected_tools,\n",
        "            'found_tools': tool_calls_found,\n",
        "            'matches': matches,\n",
        "            'total_expected': len(expected_tools) if expected_tools else 0\n",
        "        }\n",
        "\n",
        "    df = dataset._df\n",
        "    \n",
        "    results = []\n",
        "    for i, row in df.iterrows():\n",
        "        result = validate_tool_calls_simple(row[agent_output_column]['messages'], row[expected_tools_column])\n",
        "        results.append(result)\n",
        "         \n",
        "    return results\n",
        "\n",
        "vm.tests.run_test(\n",
        "    \"my_custom_tests.ToolCallAccuracy\",\n",
        "    inputs = {\n",
        "        \"dataset\": vm_test_dataset,\n",
        "    },\n",
        "    params = {\n",
        "        \"agent_output_column\": \"output\",\n",
        "        \"expected_tools_column\": \"expected_tools\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAGAS Tests for Agent Evaluation\n",
        "\n",
        "RAGAS (Retrieval-Augmented Generation Assessment) provides specialized metrics for evaluating conversational AI systems like our LangGraph agent. These tests analyze different aspects of agent performance:\n",
        "\n",
        "Our agent uses tools to retrieve information (weather, documents, calculations) and generates responses based on that context, making it similar to a RAG system. RAGAS metrics help evaluate:\n",
        "\n",
        "- **Response Quality**: How well the agent uses retrieved tool outputs to generate helpful responses\n",
        "- **Information Faithfulness**: Whether agent responses accurately reflect tool outputs  \n",
        "- **Relevance Assessment**: How well responses address the original user query\n",
        "- **Context Utilization**: How effectively the agent incorporates tool results into final answers\n",
        "\n",
        "These tests provide insights into how well our agent integrates tool usage with conversational abilities, ensuring it provides accurate, relevant, and helpful responses to users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation - Extract Context from Agent State\n",
        "\n",
        "Before running RAGAS tests, we need to extract and prepare the context information from our agent's execution results. This process:\n",
        "\n",
        "**Tool Output Extraction**: Retrieves the outputs from tools used during agent execution\n",
        "- **Message Parsing**: Analyzes the agent's conversation state to find tool outputs\n",
        "- **Content Aggregation**: Combines outputs from multiple tools when used in sequence\n",
        "- **Context Formatting**: Structures tool outputs as context for RAGAS evaluation\n",
        "\n",
        "**RAGAS Format Preparation**: Converts agent data into the format expected by RAGAS metrics\n",
        "- **User Input**: Original user queries from the test dataset\n",
        "- **Retrieved Context**: Tool outputs treated as \"retrieved\" information  \n",
        "- **Agent Response**: Final responses generated by the agent\n",
        "- **Ground Truth**: Expected outputs for comparison\n",
        "\n",
        "This preparation step is essential because RAGAS metrics were designed for traditional RAG systems, so we need to map our agent's tool-based architecture to the RAG paradigm for meaningful evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import capture_tool_output_messages\n",
        "\n",
        "tool_messages = []\n",
        "for i, row in vm_test_dataset._df.iterrows():\n",
        "    tool_message = \"\"\n",
        "    result = row['output']\n",
        "    # Capture all tool outputs and metadata\n",
        "    captured_data = capture_tool_output_messages(result)\n",
        "   \n",
        "    # Access specific tool outputs\n",
        "    for output in captured_data[\"tool_outputs\"]:\n",
        "        tool_message += output['content']\n",
        "    tool_messages.append([tool_message])\n",
        "\n",
        "vm_test_dataset._df['tool_messages'] = tool_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_test_dataset._df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Faithfulness\n",
        "\n",
        "Faithfulness measures how accurately the agent's responses reflect the information retrieved from tools. This metric evaluates:\n",
        "\n",
        "**Information Accuracy**: Whether the agent correctly uses tool outputs in its responses\n",
        "- **Fact Preservation**: Ensuring numerical results, weather data, and document content are accurately reported\n",
        "- **No Hallucination**: Verifying the agent doesn't invent information not provided by tools\n",
        "- **Source Attribution**: Checking that responses align with actual tool outputs\n",
        "\n",
        "**Critical for Agent Trust**: Faithfulness is essential for agent reliability because users need to trust that:\n",
        "- Calculator results are reported correctly\n",
        "- Weather information is accurate  \n",
        "- Document searches return real information\n",
        "- Validation results are properly communicated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.tests.run_test(\n",
        "    \"validmind.model_validation.ragas.Faithfulness\",\n",
        "    inputs={\"dataset\": vm_test_dataset},\n",
        "    param_grid={\n",
        "        \"user_input_column\": [\"input\"],\n",
        "        \"response_column\": [\"financial_model_prediction\"],\n",
        "        \"retrieved_contexts_column\": [\"tool_messages\"],\n",
        "    },\n",
        ").log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Response Relevancy\n",
        "\n",
        "Response Relevancy evaluates how well the agent's answers address the user's original question or request. This metric assesses:\n",
        "\n",
        "**Query Alignment**: Whether responses directly answer what users asked for\n",
        "- **Intent Fulfillment**: Checking if the agent understood and addressed the user's actual need\n",
        "- **Completeness**: Ensuring responses provide sufficient information to satisfy the query\n",
        "- **Focus**: Avoiding irrelevant information that doesn't help the user\n",
        "\n",
        "**Conversational Quality**: Measures the agent's ability to maintain relevant, helpful dialogue\n",
        "- **Context Awareness**: Responses should be appropriate for the conversation context\n",
        "- **User Satisfaction**: Answers should be useful and actionable for the user\n",
        "- **Clarity**: Information should be presented in a way that directly helps the user\n",
        "\n",
        "High relevancy indicates the agent successfully understands user needs and provides targeted, helpful responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.tests.run_test(\n",
        "    \"validmind.model_validation.ragas.ResponseRelevancy\",\n",
        "    inputs={\"dataset\": vm_test_dataset},\n",
        "    params={\n",
        "        \"user_input_column\": \"input\",\n",
        "        \"response_column\": \"financial_model_prediction\",\n",
        "        \"retrieved_contexts_column\": \"tool_messages\",\n",
        "    }\n",
        ").log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Context Recall\n",
        "\n",
        "Context Recall measures how well the agent utilizes the information retrieved from tools when generating its responses. This metric evaluates:\n",
        "\n",
        "**Information Utilization**: Whether the agent effectively incorporates tool outputs into its responses\n",
        "- **Coverage**: How much of the available tool information is used in the response\n",
        "- **Integration**: How well tool outputs are woven into coherent, natural responses\n",
        "- **Completeness**: Whether all relevant information from tools is considered\n",
        "\n",
        "**Tool Effectiveness**: Assesses whether selected tools provide useful context for responses\n",
        "- **Relevance**: Whether tool outputs actually help answer the user's question\n",
        "- **Sufficiency**: Whether enough information was retrieved to generate good responses\n",
        "- **Quality**: Whether the tools provided accurate, helpful information\n",
        "\n",
        "High context recall indicates the agent not only selects the right tools but also effectively uses their outputs to create comprehensive, well-informed responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.tests.run_test(\n",
        "    \"validmind.model_validation.ragas.ContextRecall\",\n",
        "    inputs={\"dataset\": vm_test_dataset},\n",
        "    param_grid={\n",
        "        \"user_input_column\": [\"input\"],\n",
        "        \"retrieved_contexts_column\": [\"tool_messages\"],\n",
        "        \"reference_column\": [\"financial_model_prediction\"],\n",
        "    },\n",
        ").log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AspectCritic\n",
        "\n",
        "AspectCritic provides comprehensive evaluation across multiple dimensions of agent performance. This metric analyzes various aspects of response quality:\n",
        "\n",
        "**Multi-Dimensional Assessment**: Evaluates responses across different quality criteria\n",
        "- **Helpfulness**: Whether responses genuinely assist users in accomplishing their goals\n",
        "- **Relevance**: How well responses address the specific user query\n",
        "- **Coherence**: Whether responses are logically structured and easy to follow\n",
        "- **Correctness**: Accuracy of information and appropriateness of recommendations\n",
        "\n",
        "**Holistic Quality Scoring**: Provides an overall assessment that considers:\n",
        "- **User Experience**: How satisfying and useful the interaction would be for real users\n",
        "- **Professional Standards**: Whether responses meet quality expectations for production systems\n",
        "- **Consistency**: Whether the agent maintains quality across different types of requests\n",
        "\n",
        "AspectCritic helps identify specific areas where the agent excels or needs improvement, providing actionable insights for enhancing overall performance and user satisfaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.tests.run_test(\n",
        "    \"validmind.model_validation.ragas.AspectCritic\",\n",
        "    inputs={\"dataset\": vm_test_dataset},\n",
        "    param_grid={\n",
        "        \"user_input_column\": [\"input\"],\n",
        "        \"response_column\": [\"financial_model_prediction\"],\n",
        "        \"retrieved_contexts_column\": [\"tool_messages\"],\n",
        "    },\n",
        ").log()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
