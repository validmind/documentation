{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Custom Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing custom metrics and threshold tests\n",
    "\n",
    "Custom metrics allow you to extend the default set of metrics provided by ValidMind and provide full flexibility for documenting any type of model or use case. Metrics and threshold tests are similar in that they both provide a way to evaluate a model. The difference is that metrics capture any arbitrary set of values that measure a behavior in a dataset(s) or model(s), while threshold tests are Boolean tests that evaluate whether a behavior passes or fails a set of criteria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation components of a metric and threshold test\n",
    "\n",
    "A **metric** is composed of the following documentation elements:\n",
    "\n",
    "- Title\n",
    "- Description\n",
    "- Results Table(s)\n",
    "- Plot(s)\n",
    "\n",
    "A **threshold test** is composed of the following documentation elements:\n",
    "\n",
    "- Title\n",
    "- Description\n",
    "- Test Parameters\n",
    "- Results Table(s)\n",
    "- Plot(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\n",
    "\n",
    "If you don't already have one, you should also [create a documentation project](https://docs.validmind.ai/guide/create-your-first-documentation-project.html) on the ValidMind platform. You will use this project to upload your documentation and test results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "In a browser, go to the **Client Integration** page of your documentation project and click **Copy to clipboard** next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n",
    "\n",
    "::: {.column-margin}\n",
    "::: {.callout-tip}\n",
    "This step requires a documentation project. [Learn how you can create one](https://docs.validmind.ai/guide/create-your-first-documentation-project.html).\n",
    ":::\n",
    ":::\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace with code snippet from your documentation project ##\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric class signature\n",
    "\n",
    "In order to implement a custom metric or threshold test, you must create a class that inherits from the `Metric` or `ThresholdTest` class. The class signatures below show the different methods that need to be implemented in order to provide the required documentation elements:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class ExampleMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    # Markdown compatible description of the metric\n",
    "    def description(self):\n",
    "\n",
    "    # Code to compute the metric and cache its results and Figures\n",
    "    def run(self):\n",
    "\n",
    "    # Code to build a list of ResultSummaries that form the results tables\n",
    "    def summary(self, metric_values):\n",
    "```\n",
    "\n",
    "We'll now implement a sample metric to illustrate their different documentation components."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a custom metric\n",
    "\n",
    "The following example shows how to implement a custom metric that calculates the mean of a list of numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic metric implementation\n",
    "\n",
    "At its most basic, a metric implementation requires a `run()` method that computes the metric and caches its results and Figures. The run() method is called by the ValidMind client when the metric is executed. The `run()` should return any value that can be serialized to JSON.\n",
    "\n",
    "In the example below we also provide a simple description for the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Metric\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        \n",
    "        return self.cache_results(mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the custom metric\n",
    "\n",
    "We should run a metric first without running an entire test plan and test its behavior.\n",
    "\n",
    "The only requirement to run a metric is build a `TestContext` object and pass it to the metric initializer. Test context objects allow metrics and tests to access data inside their class methods in a predictable way. By default, ValidMind provides support for the following special keys in a test context objects:\n",
    "\n",
    "- `dataset`\n",
    "- `model`\n",
    "- `models`\n",
    "\n",
    "When a test context object is build with one of these keys, the corresponding value is automatically added to the object as an attribute. For example, if you build a test context object with the `dataset` key, you can access the dataset inside the metric's `run()` method as `self.dataset`. We'll illustrate this in detail in the next section.\n",
    "\n",
    "In our simple example, we don't need to pass any arguments to the `TestContext` initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also inspect the results of the metric by accessing the `result` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a `summary()` method to the custom metric\n",
    "\n",
    "The `summary()` method is used to build a `ResultSummary` object that can display the results of our test as a list of one or more summray tables. The `ResultSummary` class takes a `results` argument that is a list of `ResultTable` objects.\n",
    "\n",
    "Each `ResultTable` object is composed of a `data` and `metadata` attribute. The `data` attribute is any valid Pandas tabular DataFrame and `metadata` is a `ResultTableMetadata` instance that takes `title` as the table description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def summary(self, metric_value):\n",
    "        # Create a dataframe structure that can be rendered as a table\n",
    "        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n",
    "\n",
    "        return ResultSummary(\n",
    "            results=[\n",
    "                ResultTable(\n",
    "                    data=simple_df,\n",
    "                    metadata=ResultTableMetadata(title=\"Example Table\"),\n",
    "                ),                \n",
    "            ]\n",
    "        )        \n",
    "        \n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        return self.cache_results(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add figures to a metric\n",
    "\n",
    "You can also add figures to a metric by passing a `figures` list to `cache_results()`. Each figure is a `Figure` object that takes the following arguments:\n",
    "\n",
    "- `for_object`: The name of the object that the figure is for. Usually defaults to `self`\n",
    "- `figure`: A Matplotlib or Plotly figure object\n",
    "- `key`: A unique key for the figure\n",
    "\n",
    "The developer framework uses `for_object` and `key` to link figures to the corresponding metric or test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from validmind.vm_models import Figure, Metric, ResultSummary, ResultTable, ResultTableMetadata\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def summary(self, metric_value):\n",
    "        # Create a dataframe structure that can be rendered as a table\n",
    "        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n",
    "\n",
    "        return ResultSummary(\n",
    "            results=[\n",
    "                ResultTable(\n",
    "                    data=simple_df,\n",
    "                    metadata=ResultTableMetadata(title=\"Example Table\"),\n",
    "                ),\n",
    "            ]\n",
    "        )        \n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "\n",
    "        # Create a random histogram with matplotlib\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n",
    "        ax.set_title(\"Histogram of random numbers\")\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Do this if you want to prevent the figure from being displayed\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        figure = Figure(\n",
    "            for_object=self,\n",
    "            key=self.key,\n",
    "            figure=fig\n",
    "        )\n",
    "\n",
    "        return self.cache_results(mean, figures=[figure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import TestPlan\n",
    "\n",
    "class MyCustomTestPlan(TestPlan):\n",
    "    \"\"\"\n",
    "    Custom test plan\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"my_custom_test_suite\"\n",
    "    required_inputs = []\n",
    "    tests = [MeanMetric]\n",
    "\n",
    "my_custom_test_suite = MyCustomTestPlan(config={\n",
    "    \"mean_of_values\": {\n",
    "        \"values\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    },\n",
    "})\n",
    "results = my_custom_test_suite.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
