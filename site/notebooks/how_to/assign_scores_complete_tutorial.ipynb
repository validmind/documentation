{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intro to Assign Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `assign_scores()` method is a powerful feature that allows you to compute and add scorer scores as new columns in your dataset. This method takes a model and metric(s) as input, computes the specified metrics from the ValidMind scorer library, and adds them as new columns. The computed metrics provide per-row values, giving you granular insights into model performance at the individual prediction level.\n",
        "\n",
        "In this interactive notebook, we demonstrate how to use the `assign_scores()` method effectively. We'll walk through a complete example using a customer churn dataset, showing how to compute and assign row-level metrics (like Brier Score and Log Loss) that provide detailed performance insights for each prediction. You'll learn how to work with single and multiple scorers, pass custom parameters, and handle different metric types - all while maintaining a clean, organized dataset structure. Currently, assign_scores() supports all metrics available in the validmind.scorer module.\n",
        "\n",
        "**The Power of Row-Level Scoring**\n",
        "\n",
        "Traditional model evaluation workflows often focus on aggregate metrics that provide overall performance summaries. The `assign_scores()` method complements this by providing granular, row-level insights that help you:\n",
        "\n",
        "- **Identify Problematic Predictions**: Spot individual cases where your model performs poorly\n",
        "- **Understand Model Behavior**: Analyze how model performance varies across different types of inputs\n",
        "- **Enable Detailed Analysis**: Perform targeted investigations on specific subsets of your data\n",
        "- **Support Model Debugging**: Pinpoint exactly where and why your model makes errors\n",
        "\n",
        "**Understanding assign_scores()**\n",
        "\n",
        "The `assign_scores()` method computes row metrics for a given model-dataset combination and adds the results as new columns to your dataset. Each new column follows the naming convention: `{model.input_id}_{metric_name}`, ensuring clear identification of which model and metric combination generated each score.\n",
        "\n",
        "Key features:\n",
        "\n",
        "- **Row-Level Focus**: Computes per-prediction metrics rather than aggregate scores\n",
        "- **Flexible Input**: Accepts single metrics or lists of metrics\n",
        "- **Parameter Support**: Allows passing additional parameters to underlying metric implementations\n",
        "- **Multi-Model Support**: Can assign scores from multiple models to the same dataset\n",
        "- **Type Agnostic**: Works with classification, regression, and other model types\n",
        "\n",
        "This approach provides detailed insights into your model's performance at the individual prediction level, enabling more sophisticated analysis and debugging workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-hidden when-format=\"html\"}\n",
        "## Contents    \n",
        "- [About ValidMind](#toc1__)    \n",
        "  - [Before you begin](#toc1_1__)    \n",
        "  - [New to ValidMind?](#toc1_2__)    \n",
        "- [Setting up](#toc2__)    \n",
        "  - [Install the ValidMind Library](#toc2_1__)    \n",
        "  - [Initialize the ValidMind Library](#toc2_2__)    \n",
        "    - [Register sample model](#toc2_2_1__)    \n",
        "    - [Apply documentation template](#toc2_2_2__)    \n",
        "    - [Get your code snippet](#toc2_2_3__)    \n",
        "- [Load the demo dataset](#toc3__)    \n",
        "- [Train models for testing](#toc4__)    \n",
        "- [Initialize ValidMind objects](#toc5__)    \n",
        "- [Assign predictions](#toc6__)    \n",
        "- [Using assign_scores()](#toc7__)    \n",
        "  - [Basic Usage](#toc7_1__)    \n",
        "  - [Single Scorer Assignment](#toc7_2__)    \n",
        "  - [A Scorer returns complex object](#toc7_3__)    \n",
        "  - [Multiple Scorers Assignment](#toc7_4__)    \n",
        "  - [Passing Parameters to Scorer](#toc7_5__)    \n",
        "  - [Multi-Model scorers](#toc7_6__)    \n",
        "  - [Scorer Metrics](#toc7_7__)    \n",
        "  - [Custom Scorer](#toc7_8__)    \n",
        "- [Next steps](#toc8__)    \n",
        "  - [Work with your model documentation](#toc8_1__)    \n",
        "  - [Discover more learning resources](#toc8_2__)    \n",
        "- [Upgrade ValidMind](#toc9__)    \n",
        "\n",
        ":::\n",
        "<!-- jn-toc-notebook-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=2\n",
        "\tmaxLevel=4\n",
        "\t/jn-toc-notebook-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc1__'></a>\n",
        "\n",
        "## About ValidMind\n",
        "\n",
        "ValidMind is a suite of tools for managing model risk, including risk associated with AI and statistical models.\n",
        "\n",
        "You use the ValidMind Library to automate documentation and validation tests, and then use the ValidMind Platform to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
        "\n",
        "<a id='toc1_1__'></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \n",
        "\n",
        "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
        "\n",
        "<a id='toc1_2__'></a>\n",
        "\n",
        "### New to ValidMind?\n",
        "\n",
        "If you haven't already seen our documentation on the [ValidMind Library](https://docs.validmind.ai/developer/validmind-library.html), we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models and running tests, as well as find code samples and our Python Library API reference.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
        "<br></br>\n",
        "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2__'></a>\n",
        "\n",
        "## Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_1__'></a>\n",
        "\n",
        "### Install the ValidMind Library\n",
        "\n",
        "To install the library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_2__'></a>\n",
        "\n",
        "### Initialize the ValidMind Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_2_1__'></a>\n",
        "\n",
        "#### Register sample model\n",
        "\n",
        "Let's first register a sample model for use with this notebook:\n",
        "\n",
        "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
        "\n",
        "2. In the left sidebar, navigate to **Inventory** and click **+ Register Model**.\n",
        "\n",
        "3. Enter the model details and click **Next >** to continue to assignment of model stakeholders. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
        "\n",
        "   For example, to register a model for use with this notebook, select the following use case: `Marketing/Sales - Analytics`\n",
        "\n",
        "4. Select your own name under the **MODEL OWNER** drop-down.\n",
        "\n",
        "5. Click **Register Model** to add the model to your inventory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_2_2__'></a>\n",
        "\n",
        "#### Apply documentation template\n",
        "\n",
        "Once you've registered your model, let's select a documentation template. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
        "\n",
        "1. In the left sidebar that appears for your model, click **Documents** and select **Documentation**.\n",
        "\n",
        "2. Under **TEMPLATE**, select `Binary classification`.\n",
        "\n",
        "3. Click **Use Template** to apply the template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc2_2_3__'></a>\n",
        "\n",
        "#### Get your code snippet\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "1. On the left sidebar that appears for your model, select **Getting Started** and click **Copy snippet to clipboard**.\n",
        "2. Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your model identifier credentials from an `.env` file\n",
        "\n",
        "%load_ext dotenv\n",
        "%dotenv .env\n",
        "\n",
        "# Or replace with your code snippet\n",
        "\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"...\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    model=\"...\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc3__'></a>\n",
        "\n",
        "## Load the demo dataset\n",
        "\n",
        "In this example, we load a demo dataset to demonstrate the assign_scores functionality with customer churn prediction models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.datasets.classification import customer_churn as demo_dataset\n",
        "\n",
        "print(\n",
        "    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n",
        ")\n",
        "\n",
        "raw_df = demo_dataset.load_data()\n",
        "raw_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc4__'></a>\n",
        "\n",
        "## Train models for testing\n",
        "\n",
        "We'll train two different customer churn models to demonstrate the assign_scores functionality with multiple models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Preprocess the data\n",
        "train_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n",
        "\n",
        "# Prepare training data\n",
        "x_train = train_df.drop(demo_dataset.target_column, axis=1)\n",
        "y_train = train_df[demo_dataset.target_column]\n",
        "x_val = validation_df.drop(demo_dataset.target_column, axis=1)\n",
        "y_val = validation_df[demo_dataset.target_column]\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(early_stopping_rounds=10, random_state=42)\n",
        "xgb_model.set_params(\n",
        "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
        ")\n",
        "xgb_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    eval_set=[(x_val, y_val)],\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully!\")\n",
        "print(f\"XGBoost training accuracy: {xgb_model.score(x_train, y_train):.3f}\")\n",
        "print(f\"Random Forest training accuracy: {rf_model.score(x_train, y_train):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc5__'></a>\n",
        "\n",
        "## Initialize ValidMind objects\n",
        "\n",
        "We initialize ValidMind `dataset` and `model` objects. The `input_id` parameter is crucial for the assign_scores functionality as it determines the column naming convention for assigned scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize datasets\n",
        "vm_train_ds = vm.init_dataset(\n",
        "    input_id=\"train_dataset\",\n",
        "    dataset=train_df,\n",
        "    target_column=demo_dataset.target_column,\n",
        ")\n",
        "vm_test_ds = vm.init_dataset(\n",
        "    input_id=\"test_dataset\",\n",
        "    dataset=test_df,\n",
        "    target_column=demo_dataset.target_column,\n",
        ")\n",
        "\n",
        "# Initialize models with descriptive input_ids\n",
        "vm_xgb_model = vm.init_model(model=xgb_model, input_id=\"xgboost_model\")\n",
        "vm_rf_model = vm.init_model(model=rf_model, input_id=\"random_forest_model\")\n",
        "\n",
        "print(\"ValidMind objects initialized successfully!\")\n",
        "print(f\"XGBoost model ID: {vm_xgb_model.input_id}\")\n",
        "print(f\"Random Forest model ID: {vm_rf_model.input_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc6__'></a>\n",
        "\n",
        "## Assign predictions\n",
        "\n",
        "Before we can use assign_scores(), we need to assign predictions to our datasets. This step is essential as many unit metrics require both actual and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign predictions for both models to both datasets\n",
        "vm_train_ds.assign_predictions(model=vm_xgb_model)\n",
        "vm_train_ds.assign_predictions(model=vm_rf_model)\n",
        "\n",
        "vm_test_ds.assign_predictions(model=vm_xgb_model)\n",
        "vm_test_ds.assign_predictions(model=vm_rf_model)\n",
        "\n",
        "print(\"Predictions assigned successfully!\")\n",
        "print(f\"Test dataset now has {len(vm_test_ds.df.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7__'></a>\n",
        "\n",
        "## Using assign_scores()\n",
        "\n",
        "Now we'll explore the various ways to use the assign_scores() method to integrate performance metrics directly into your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_1__'></a>\n",
        "\n",
        "### Basic Usage\n",
        "\n",
        "The assign_scores() method has a simple interface:\n",
        "\n",
        "```python\n",
        "dataset.assign_scores(model, metrics, **kwargs)\n",
        "```\n",
        "\n",
        "- **model**: A ValidMind model object\n",
        "- **metrics**: Single metric ID or list of metric IDs (can use short names or full IDs)\n",
        "- **kwargs**: Additional parameters passed to the underlying metric implementations\n",
        "\n",
        "Let's first check what columns we currently have in our test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Current columns in test dataset:\")\n",
        "for i, col in enumerate(vm_test_ds.df.columns, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "\n",
        "print(f\"\\nDataset shape: {vm_test_ds.df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_2__'></a>\n",
        "\n",
        "### Single Scorer Assignment\n",
        " \n",
        "Let's start by assigning a single Scorer - the Brier Score - for our XGBoost model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign Brier Score for XGBoost model\n",
        "vm_test_ds.assign_scores(metrics = \"validmind.scorer.classification.BrierScore\", model = vm_xgb_model)\n",
        "\n",
        "print(\"After assigning Brier Score:\")\n",
        "print(f\"New column added: {vm_test_ds.df.columns}\")\n",
        "# Display the metric values\n",
        "vm_test_ds.df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_3__'></a>\n",
        "\n",
        "### A Scorer returns complex object\n",
        " The OutlierScore scorer demonstrates how scorers can return complex objects. It returns a dictionary containing per-row outlier detection results. For each row, it includes:\n",
        " - is_outlier: Boolean indicating if the row is an outlier\n",
        " - anomaly_score: Numerical score indicating degree of outlierness\n",
        " - isolation_path: Length of isolation path in the tree\n",
        "\n",
        "When assigned to a dataset, these dictionary values are automatically unpacked into separate columns with appropriate prefixes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign Brier Score for XGBoost model\n",
        "vm_test_ds.assign_scores(metrics = \"validmind.scorer.classification.OutlierScore\", model = vm_xgb_model)\n",
        "\n",
        "print(\"After assigning Score With Confidence:\")\n",
        "print(f\"New column added: {vm_test_ds.df.columns}\")\n",
        "# Display the metric values\n",
        "vm_test_ds.df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign Brier Score for XGBoost model\n",
        "vm_test_ds.assign_scores(\"validmind.scorer.classification.OutlierScore\")\n",
        "\n",
        "print(\"After assigning Score With Confidence:\")\n",
        "print(f\"New column added: {vm_test_ds.df.columns}\")\n",
        "# Display the metric values\n",
        "vm_test_ds.df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_4__'></a>\n",
        "\n",
        "### Multiple Scorers Assignment\n",
        "\n",
        "We can assign multiple metrics at once by passing a list of Scorer names. This is more efficient than calling assign_scores() multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign multiple classification metrics for the Random Forest model\n",
        "scorer = [\n",
        "    \"validmind.scorer.classification.BrierScore\",\n",
        "    \"validmind.scorer.classification.LogLoss\",\n",
        "    \"validmind.scorer.classification.Confidence\"\n",
        "]\n",
        "\n",
        "vm_test_ds.assign_scores(metrics = scorer, model = vm_rf_model)\n",
        "\n",
        "print(\"After assigning multiple row metrics for Random Forest:\")\n",
        "rf_columns = [col for col in vm_test_ds.df.columns if 'random_forest_model' in col]\n",
        "print(f\"Random Forest columns: {rf_columns}\")\n",
        "\n",
        "# Display the metric values\n",
        "vm_test_ds.df[rf_columns].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_5__'></a>\n",
        "\n",
        "### Passing Parameters to Scorer\n",
        "\n",
        "Many row metrics accept additional parameters that are passed through to the underlying implementations. Let's demonstrate this with the LogLoss metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign LogLoss\n",
        "vm_test_ds.assign_scores(metrics = \"validmind.scorer.classification.LogLoss\", model = vm_xgb_model, eps = 1e-16)\n",
        "\n",
        "# We can also assign with different parameters by calling assign_scores again\n",
        "# Note: This will overwrite the previous column with the same name\n",
        "print(\"LogLoss assigned successfully\")\n",
        "\n",
        "# Let's also assign BrierScore and Confidence\n",
        "vm_test_ds.assign_scores(metrics = [\"validmind.scorer.classification.BrierScore\",\"validmind.scorer.classification.Confidence\"], model = vm_xgb_model)\n",
        "\n",
        "print(\"BrierScore and Confidence assigned successfully\")\n",
        "\n",
        "# Display current XGBoost metric columns\n",
        "xgb_columns = [col for col in vm_test_ds.df.columns if 'xgboost_model' in col]\n",
        "print(f\"\\nXGBoost model columns: {xgb_columns}\")\n",
        "\n",
        "vm_test_ds.df[xgb_columns].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_6__'></a>\n",
        "\n",
        "### Multi-Model scorers\n",
        "\n",
        "One of the powerful features of assign_scores() is the ability to assign scores from multiple models to the same dataset, enabling detailed model comparison at the prediction level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's assign a comprehensive set of metrics for both models\n",
        "comprehensive_metrics = [\n",
        "    \"validmind.scorer.classification.BrierScore\",\n",
        "    \"validmind.scorer.classification.LogLoss\",\n",
        "    \"validmind.scorer.classification.Confidence\",\n",
        "    \"validmind.scorer.classification.Correctness\"\n",
        "]\n",
        "\n",
        "# Assign for XGBoost model\n",
        "vm_test_ds.assign_scores(metrics = comprehensive_metrics, model = vm_xgb_model)\n",
        "\n",
        "# Assign for Random Forest model}\n",
        "vm_test_ds.assign_scores(metrics = comprehensive_metrics, model = vm_rf_model)\n",
        "\n",
        "print(\"Row-level metrics assigned for both models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_7__'></a>\n",
        "\n",
        "### Scorer Metrics\n",
        "The next section demonstrates how to assign individual metrics that compute scores per row, rather than aggregate metrics.\n",
        "We'll use several important row metrics:\n",
        " \n",
        "- Brier Score: Measures how well calibrated the model's probability predictions are for each individual prediction\n",
        "- Log Loss: Evaluates how well the predicted probabilities match the true labels on a per-prediction basis\n",
        "- Confidence: Measures the model's confidence in its predictions for each row\n",
        "- Correctness: Indicates whether each prediction is correct (1) or incorrect (0)\n",
        "\n",
        "All these metrics provide granular insights into model performance at the individual prediction level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's add some individual metrics that compute per-row scores\n",
        "print(\"Adding individual metrics...\")\n",
        "\n",
        "# Add Brier Score - measures accuracy of probabilistic predictions per row\n",
        "vm_test_ds.assign_scores(metrics = \"validmind.scorer.classification.BrierScore\", model = vm_xgb_model)\n",
        "print(\"Added Brier Score - lower values indicate better calibrated probabilities\")\n",
        "\n",
        "# Add Log Loss - measures how well the predicted probabilities match true labels per row\n",
        "vm_test_ds.assign_scores(metrics = \"validmind.scorer.classification.LogLoss\", model = vm_xgb_model)\n",
        "print(\"Added Log Loss - lower values indicate better probability estimates\")\n",
        "\n",
        "# Create a comparison summary showing first few rows of individual metrics\n",
        "print(\"\\nFirst few rows of individual metrics:\")\n",
        "individual_metrics = [col for col in vm_test_ds.df.columns if any(m in col for m in ['BrierScore', 'LogLoss', 'Confidence', 'Correctness'])]\n",
        "print(vm_test_ds.df[individual_metrics].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm_test_ds._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc7_8__'></a>\n",
        "\n",
        "### Custom Scorer\n",
        "Let's see how to create your own custom scorers using the `@scorer` decorator.\n",
        " \n",
        "The example below demonstrates a scorer that looks at the class balance in the neighborhood around each data point. For each row, it will give you a score from 0 to 1, where a score closer to 1 means there's a nice even balance of classes in that area of your data. This can help you identify regions where your classes are well-mixed vs regions dominated by a single class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.scorer import scorer\n",
        "import numpy as np\n",
        "\n",
        "@scorer(\"my_scorers.TestScorer\") \n",
        "def test_scorer(model, dataset):\n",
        "    \"\"\"Custom scorer that calculates class balance ratio.\n",
        "    \n",
        "    Args:\n",
        "        model: Not used in this scorer\n",
        "        dataset: The dataset to analyze\n",
        "        \n",
        "    Returns:\n",
        "        numpy.ndarray: Array of class balance ratios between 0 and 1,\n",
        "        where values closer to 1 indicate better class balance in the local neighborhood\n",
        "    \"\"\"\n",
        "    # Get target values\n",
        "    y = dataset.df[dataset.target_column].values\n",
        "    \n",
        "    # Calculate local class balance in sliding windows\n",
        "    window_size = 100\n",
        "    balance_scores = []\n",
        "    \n",
        "    for i in range(len(y)):\n",
        "        start_idx = max(0, i - window_size//2)\n",
        "        end_idx = min(len(y), i + window_size//2)\n",
        "        window = y[start_idx:end_idx]\n",
        "        \n",
        "        # Calculate ratio of minority class\n",
        "        class_ratio = np.mean(window)\n",
        "        # Adjust to be symmetric around 0.5\n",
        "        balance_score = 1 - abs(0.5 - class_ratio) * 2\n",
        "        \n",
        "        balance_scores.append(balance_score)\n",
        "        \n",
        "    return np.array(balance_scores)\n",
        "\n",
        "# Assign the class balance scores to the dataset\n",
        "vm_test_ds.assign_scores(metrics = \"my_scorers.TestScorer\", model = vm_xgb_model)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc8__'></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You can explore the assigned scores right in the notebook as demonstrated above. However, there's even more value in using the ValidMind Platform to work with your model documentation and monitoring.\n",
        "\n",
        "<a id='toc8_1__'></a>\n",
        "\n",
        "### Work with your model documentation\n",
        "\n",
        "1. From the **Model Inventory** in the ValidMind Platform, go to the model you registered earlier. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/working-with-model-inventory.html))\n",
        "\n",
        "2. Click and expand the **Model Development** section.\n",
        "\n",
        "The scores you've assigned using `assign_scores()` become part of your model's documentation and can be used in ongoing monitoring workflows. You can view these metrics over time, set up alerts for performance drift, and compare models systematically. [Learn more ...](https://docs.validmind.ai/guide/model-documentation/working-with-model-documentation.html)\n",
        "\n",
        "<a id='toc8_2__'></a>\n",
        "\n",
        "### Discover more learning resources\n",
        "\n",
        "We offer many interactive notebooks to help you work with model scoring and evaluation:\n",
        "\n",
        "- [Run unit metrics](https://docs.validmind.ai/developer/model-testing/testing-overview.html)\n",
        "- [Assign predictions](https://docs.validmind.ai/developer/samples-jupyter-notebooks.html)\n",
        "- [Model comparison workflows](https://docs.validmind.ai/developer/samples-jupyter-notebooks.html)\n",
        "\n",
        "Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='toc9__'></a>\n",
        "\n",
        "## Upgrade ValidMind\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\">After installing ValidMind, you'll want to periodically make sure you are on the latest version to access any new features and other enhancements.</div>\n",
        "\n",
        "Retrieve the information for the currently installed version of ValidMind:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%pip show validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the version returned is lower than the version indicated in our [production open-source code](https://github.com/validmind/validmind-library/blob/prod/validmind/__version__.py), restart your notebook and run:\n",
        "\n",
        "```bash\n",
        "%pip install --upgrade validmind\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may need to restart your kernel after running the upgrade package for changes to be applied."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
