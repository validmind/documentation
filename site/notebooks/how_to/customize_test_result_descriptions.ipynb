{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize test result descriptions\n",
    "\n",
    "When you run ValidMind tests, test descriptions are automatically generated with LLM using the test results, the test name, and the static test definitions provided in the test's docstring. While this metadata offers valuable high-level overviews of tests, insights produced by the LLM-based descriptions may not always align with your specific use cases or incorporate organizational policy requirements.\n",
    "\n",
    "In this notebook, you'll learn how to take complete control over the context that drives test description generation. ValidMind provides a `context` parameter in `run_test` that accepts a dictionary with three complementary keys for comprehensive context management:\n",
    "\n",
    "- `instructions`: Overwrites ValidMind’s default result description structure. If you provide custom instructions, they take full priority over the built-in ones. This parameter controls how the final description is structured and presented. Use this to specify formatting requirements, target different audiences (executives vs. technical teams), or ensure consistent report styles across your organization.  \n",
    "\n",
    "- `test_description`: Overwrites the test’s built-in docstring if provided. This parameter contains the technical mechanics of how the test works. However, for generic tests where the methodology isn't the focus, you may use this to describe what's actually being analyzed—the specific variables, features, or metrics being plotted and their business meaning rather than the statistical mechanics. You can also override ValidMind's built-in test documentation if you prefer different structure or language. \n",
    "\n",
    "- `additional_context`: Does not overwrite the instructions or test descriptions, but instead adds to them. This parameter provides any background information you want the LLM to consider when analyzing results. It could include business priorities, acceptance thresholds, regulatory requirements, domain expertise, use case details, model purpose, or stakeholder concerns—any information that helps the LLM better understand and interpret your specific situation.\n",
    "\n",
    "Together, these context parameters allow you to manage every aspect of how the LLM interprets and presents your test results. Whether you need to align descriptions with regulatory requirements, target specific audiences, incorporate organizational policies, or ensure consistent reporting standards, this context management approach gives you the flexibility to generate descriptions that perfectly match your needs while still leveraging the analytical power of AI-generated insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.content-hidden when-format=\"html\"}\n",
    "## Contents    \n",
    "- [Setting up](#toc1__)    \n",
    "  - [Install the ValidMind Library](#toc1_1__)    \n",
    "  - [Initialize the ValidMind Library](#toc1_2__)    \n",
    "    - [Register sample model](#toc1_2_1__)    \n",
    "    - [Apply documentation template](#toc1_2_2__)    \n",
    "    - [Get your code snippet](#toc1_2_3__)    \n",
    "  - [Initialize the Python environment](#toc1_3__)    \n",
    "- [Model development](#toc2__)    \n",
    "  - [Load data](#toc2_1__)    \n",
    "  - [Fit the model](#toc2_2__)    \n",
    "  - [Initialize the ValidMind objects](#toc2_3__)    \n",
    "- [Understanding test result descriptions](#toc3__)    \n",
    "  - [Default LLM-generated descriptions](#toc3_1__)    \n",
    "- [Customizing results structure with instructions](#toc4__)    \n",
    "  - [Simple instruction example](#toc4_1__)    \n",
    "  - [Structured format instructions](#toc4_2__)    \n",
    "  - [Template with LLM fill-ins](#toc4_3__)    \n",
    "  - [Mixed static and dynamic content](#toc4_4__)    \n",
    "- [Enriching results with additional context](#toc5__)    \n",
    "  - [Understanding the additional context parameter](#toc5_1__)    \n",
    "  - [Basic additional context usage](#toc5_2__)    \n",
    "  - [Combining instructions and additional context](#toc5_3__)    \n",
    "- [Overriding test documentation with test description parameter](#toc6__)    \n",
    "  - [Structure of ValidMind built-in test docstrings](#toc6_1__)    \n",
    "  - [Understanding the test description parameter](#toc6_2__)    \n",
    "  - [Basic test description usage](#toc6_3__)    \n",
    "  - [Combining test description with instructions and additional context](#toc6_4__)    \n",
    "- [Best practices for managing context](#toc7__)    \n",
    "\n",
    ":::\n",
    "<!-- jn-toc-notebook-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=4\n",
    "\t/jn-toc-notebook-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1__'></a>\n",
    "\n",
    "## Setting up\n",
    "\n",
    "This section covers the basic setup required to run the examples in this notebook. We'll install ValidMind, connect to the platform, and create a customer churn model that we'll use to demonstrate the instructions and knowledge parameters throughout the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_1__'></a>\n",
    "\n",
    "### Install the ValidMind Library\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_2__'></a>\n",
    "\n",
    "### Initialize the ValidMind Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_2_1__'></a>\n",
    "\n",
    "#### Register sample model\n",
    "\n",
    "Let's first register a sample model for use with this notebook:\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Inventory** and click **+ Register Model**.\n",
    "\n",
    "3. Enter the model details and click **Next >** to continue to assignment of model stakeholders. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
    "\n",
    "   For example, to register a model for use with this notebook, select the following use case: `Marketing/Sales - Attrition/Churn Management`\n",
    "\n",
    "4. Select your own name under the **MODEL OWNER** drop-down.\n",
    "\n",
    "5. Click **Register Model** to add the model to your inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_2_2__'></a>\n",
    "\n",
    "#### Apply documentation template\n",
    "\n",
    "Once you've registered your model, let's select a documentation template. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "1. In the left sidebar that appears for your model, click **Documents** and select **Documentation**.\n",
    "\n",
    "2. Under **TEMPLATE**, select `Binary classification`.\n",
    "\n",
    "3. Click **Use Template** to apply the template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_2_3__'></a>\n",
    "\n",
    "#### Get your code snippet\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "1. On the left sidebar that appears for your model, select **Getting Started** and click **Copy snippet to clipboard**.\n",
    "2. Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model identifier credentials from an `.env` file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    # api_host=\"...\",\n",
    "    # api_key=\"...\",\n",
    "    # api_secret=\"...\",\n",
    "    # model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_3__'></a>\n",
    "\n",
    "### Initialize the Python environment\n",
    "\n",
    "After you've connected to your model register in the ValidMind Platform, let's import the necessary libraries and set up your Python environment for data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2__'></a>\n",
    "\n",
    "## Model development\n",
    "\n",
    "Now we'll build the customer churn model using XGBoost and ValidMind's sample dataset. This trained model will generate the test results we'll use to demonstrate the instructions and knowledge parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_1__'></a>\n",
    "\n",
    "### Load data\n",
    "\n",
    "First, we'll import a sample ValidMind dataset and load it into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample dataset from the library\n",
    "\n",
    "from validmind.datasets.classification import customer_churn\n",
    "\n",
    "print(\n",
    "    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{customer_churn.target_column}' \\n\\t• Class labels: {customer_churn.class_labels}\"\n",
    ")\n",
    "\n",
    "raw_df = customer_churn.load_data()\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_2__'></a>\n",
    "\n",
    "### Fit the model\n",
    "\n",
    "Then, we prepare the data and model by first splitting the DataFrame into training, validation, and test sets, then separating features from targets. An XGBoost classifier is initialized with early stopping, evaluation metrics (error, logloss, and auc) are defined, and the model is trained on the training data with validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n",
    "\n",
    "x_train = train_df.drop(customer_churn.target_column, axis=1)\n",
    "y_train = train_df[customer_churn.target_column]\n",
    "x_val = validation_df.drop(customer_churn.target_column, axis=1)\n",
    "y_val = validation_df[customer_churn.target_column]\n",
    "\n",
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_val, y_val)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_3__'></a>\n",
    "\n",
    "### Initialize the ValidMind objects\n",
    "\n",
    "Before you can run tests, you'll need to initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "We'll include the following arguments:\n",
    "\n",
    "- **`dataset`** — the raw dataset that you want to provide as input to tests\n",
    "- **`input_id`** - a unique identifier that allows tracking what inputs are used when running each individual test\n",
    "- **`target_column`** — a required argument if tests require access to true values. This is the name of the target column in the dataset\n",
    "- **`class_labels`** — an optional value to map predicted classes to class labels\n",
    "\n",
    "With all datasets ready, you can now initialize the raw, training, and test datasets (`raw_df`, `train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    class_labels=customer_churn.class_labels,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df, input_id=\"test_dataset\", target_column=customer_churn.target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you'll need to initialize a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data. \n",
    "\n",
    "Simply intialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_model = vm.init_model(\n",
    "    model,\n",
    "    input_id=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `assign_predictions()` method from the Dataset object to link existing predictions to any model.\n",
    "\n",
    "If no prediction values are passed, the method will compute predictions automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3__'></a>\n",
    "\n",
    "## Understanding test result descriptions\n",
    "\n",
    "Before diving into custom instructions, let's understand how ValidMind generates test descriptions by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3_1__'></a>\n",
    "\n",
    "### Default LLM-generated descriptions\n",
    "\n",
    "When you run a test without custom instructions, ValidMind's LLM analyzes:\n",
    "- The test results (tables, figures)\n",
    "- The test's built-in documentation (docstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ValidMind generates test descriptions automatically (without custom instructions), the LLM follows a series of standardized sections designed to provide comprehensive, objective analysis of test results:\n",
    "\n",
    "- **Test purpose:**\n",
    "This section opens with a clear explanation of what the test does and why it exists. It draws from the test’s documentation and presents the purpose in accessible, straightforward language.\n",
    "\n",
    "- **Test mechanism:**\n",
    "Here the description outlines how the test works, including its methodology, what it measures, and how those measurements are derived. For statistical tests, it also explains the meaning of each metric, how values are typically interpreted, and what ranges are expected.\n",
    "\n",
    "- **Test strengths:**\n",
    "This part highlights the value of the test by pointing out its key strengths and the scenarios where it is most useful. It also notes the kinds of insights it can provide that other tests may not capture.\n",
    "\n",
    "- **Test limitations:**\n",
    "Limitations focus on both technical constraints and interpretation challenges. The text notes when results should be treated with caution and highlights specific risk indicators tied to the test type.\n",
    "\n",
    "- **Results interpretation:**\n",
    "The results section explains how to read the outputs, whether tables or figures, and clarifies what each column, axis, or metric means. It also points out key data points, units of measurement, and any notable observations that help frame interpretation.\n",
    "\n",
    "- **Key insights:**\n",
    "Insights are listed in bullet points, moving from broad to specific. Each one has a clear title, includes relevant numbers or ranges, and ensures that all important aspects of the results are addressed.\n",
    "\n",
    "- **Conclusions**:\n",
    "The conclusion ties the insights together into a coherent narrative. It synthesizes the findings into objective technical takeaways and emphasizes what the results reveal about the model or data.\n",
    "\n",
    "Let's see a default description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4__'></a>\n",
    "\n",
    "## Customizing results structure with instructions\n",
    "\n",
    "While the default descriptions are designed to be comprehensive, there are many cases where you might want to tailor them for your specific context. Customizing test results allows you to shape descriptions to fit your organization’s standards and practical needs. This can involve adjusting report formats, applying specific risk rating scales, adding mandatory disclaimer text, or emphasizing particular metrics.\n",
    "\n",
    "The `instructions` parameter is what enables this flexibility by adapting the generated descriptions to different audiences and test types. Executives often need concise summaries that emphasize overall risk, data scientists look for detailed explanations of the methodology behind tests, and compliance teams require precise language that aligns with regulatory expectations. Different test types also demand different emphases: performance metrics may benefit from technical breakdowns, while validation checks might require risk-focused narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_1__'></a>\n",
    "\n",
    "### Simple instruction example\n",
    "\n",
    "Let's start with simple examples of the `instructions` parameter. Here's how to provide basic guidance to the LLM-generated descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_instructions = \"\"\"\n",
    "Please focus on business impact and provide a concise summary. \n",
    "Include specific actionable recommendations.\n",
    "\"\"\"\n",
    "\n",
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    context={\n",
    "        \"instructions\": simple_instructions,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_2__'></a>\n",
    "\n",
    "### Structured format instructions\n",
    "\n",
    "You can request specific formatting and structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_instructions = \"\"\"\n",
    "Please structure your analysis using the following format:\n",
    "\n",
    "### Executive Summary\n",
    "- One sentence overview of the test results\n",
    "\n",
    "### Key Findings\n",
    "- Bullet points with the most important insights\n",
    "- Include specific percentages and thresholds\n",
    "\n",
    "### Risk Assessment\n",
    "- Classify risk level as Low/Medium/High\n",
    "- Explain reasoning for the risk classification\n",
    "\n",
    "### Recommendations\n",
    "- Specific actionable next steps\n",
    "- Priority level for each recommendation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    context={\n",
    "        \"instructions\": structured_instructions,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_3__'></a>\n",
    "\n",
    "### Template with LLM fill-ins\n",
    "\n",
    "One of the most powerful features is combining hardcoded text with LLM-generated content using placeholders. This allows you to ensure specific information is always included while still getting intelligent analysis of the results.\n",
    "\n",
    "Create a template where specific sections are filled by the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_instructions = \"\"\"\n",
    "Please generate the description using this exact template. \n",
    "Fill in the [PLACEHOLDER] sections with your analysis:\n",
    "\n",
    "---\n",
    "**VALIDATION REPORT: CLASSIFIER PERFORMANCE ASSESSMENT**\n",
    "\n",
    "**Dataset ID:** test_dataset\n",
    "**Validation Type:** Classification Performance Analysis\n",
    "**Reviewer:** ValidMind AI Analysis\n",
    "\n",
    "**EXECUTIVE SUMMARY:**\n",
    "[PROVIDE_2_SENTENCE_SUMMARY_OF_RESULTS]\n",
    "\n",
    "**KEY FINDINGS:**\n",
    "[ANALYZE_AND_LIST_TOP_3_MOST_IMPORTANT_FINDINGS_WITH_VALUES]\n",
    "\n",
    "**CLASSIFICATION PERFORMANCE ASSESSMENT:**\n",
    "[DETAILED_ANALYSIS_OF_CLASSIFICATION_PERFORMANCE_PATTERNS_AND_IMPACT]\n",
    "\n",
    "**RISK RATING:** [ASSIGN_LOW_MEDIUM_HIGH_RISK_WITH_JUSTIFICATION]\n",
    "\n",
    "**RECOMMENDATIONS:**\n",
    "[PROVIDE_SPECIFIC_ACTIONABLE_RECOMMENDATIONS_NUMBERED_LIST]\n",
    "\n",
    "**VALIDATION STATUS:** [PASS_CONDITIONAL_PASS_OR_FAIL_WITH_REASONING]\n",
    "\n",
    "---\n",
    "*This report was generated using ValidMind's automated validation platform.*\n",
    "*For questions about this analysis, contact the Data Science team.*\n",
    "---\n",
    "\n",
    "Important: Use the exact template structure above and fill in each [PLACEHOLDER] section.\n",
    "\"\"\"\n",
    "\n",
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    context={\n",
    "        \"instructions\": template_instructions,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_4__'></a>\n",
    "\n",
    "### Mixed static and dynamic content\n",
    "\n",
    "Combine mandatory text with intelligent analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed static and dynamic content\n",
    "mixed_content_instructions =\"\"\"\n",
    "Return ONLY the assembled content in plain Markdown paragraphs and lists.\n",
    "Do NOT include any headings or titles (no lines starting with '#'), labels,\n",
    "XML-like tags (<MANDATORY>, <PLACEHOLDER>), variable names, or code fences.\n",
    "Do NOT repeat or paraphrase these instructions. Start the first line with the\n",
    "first mandatory sentence below—no preface.\n",
    "\n",
    "You MUST include all MANDATORY blocks verbatim (exact characters, spacing, and punctuation).\n",
    "You MUST replace PLACEHOLDER blocks with the requested content.\n",
    "Between blocks, include exactly ONE blank line.\n",
    "\n",
    "MANDATORY BLOCK A (include verbatim):\n",
    "This data validation assessment was conducted in accordance with the \n",
    "XYZ Bank Model Risk Management Policy (Document ID: MRM-2024-001). \n",
    "All findings must be reviewed by the Model Validation Team before \n",
    "model deployment.\n",
    "\n",
    "PLACEHOLDER BLOCK B (replace with prose paragraphs; no headings):\n",
    "[Provide detailed analysis of the test results, including specific values, \n",
    "interpretations, and implications for model quality. Focus on classification performance quality \n",
    "aspects and potential issues that could affect model performance.]\n",
    "\n",
    "MANDATORY BLOCK C (include verbatim):\n",
    "IMPORTANT: This automated analysis is supplementary to human expert review. \n",
    "All high-risk findings require immediate escalation to the Chief Risk Officer. \n",
    "Model deployment is prohibited until all Medium and High risk items are resolved.\n",
    "\n",
    "PLACEHOLDER BLOCK D (replace with a numbered list only):\n",
    "[Create a numbered list of specific action items with responsible parties \n",
    "and suggested timelines for resolution.]\n",
    "\n",
    "MANDATORY BLOCK E (include verbatim):\n",
    "Validation performed using ValidMind Platform v2.0 | \n",
    "Next review required: [30 days from test date] | \n",
    "Contact: model-risk@xyzbank.com\n",
    "\n",
    "Compliance checks BEFORE you finalize your answer:\n",
    "- No headings or titles present (no '#' anywhere).\n",
    "- No tags (<MANDATORY>, <PLACEHOLDER>) or labels (e.g., \"BLOCK A\") in the output.\n",
    "- All three MANDATORY blocks included exactly as written.\n",
    "- PLACEHOLDER B replaced with prose; PLACEHOLDER D replaced with a numbered list.\n",
    "- Exactly one blank line between each block.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    context={\n",
    "        \"instructions\": mixed_content_instructions,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5__'></a>\n",
    "\n",
    "## Enriching results with additional context\n",
    "\n",
    "While the `instructions` parameter controls how your test descriptions are formatted and structured, the `additional_context` parameter provides *background information* about what the results mean for your specific business situation. Think of `instructions` as the \"presentation guide\" and `additional_context` as the \"business background\" that helps the LLM understand what matters most in your organization and how to interpret the results in your specific context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_1__'></a>\n",
    "\n",
    "### Understanding the additional context parameter\n",
    "\n",
    "The `additional_context` parameter can be used to add any background information that helps put the test results into context. For example, you might include business priorities and constraints that shape how results are interpreted, risk tolerance levels or acceptance criteria specific to your organization, regulatory requirements that influence what counts as acceptable performance, or details about the intended use case of the model in production. These are just examples—the parameter is flexible and can capture whatever context is most relevant to your needs.\n",
    "\n",
    "**Key difference:**\n",
    "- `instructions`: \"Write a 3-paragraph executive summary\"\n",
    "\n",
    "- `additional_context`: \"If Accuracy is above 0.85 but Class 1 Recall falls below 0.60, the model should be considered high risk\"\n",
    "\n",
    "When used together, these parameters create descriptions that don’t just report the Recall or Accuracy measures for Class 1, but explain that because Accuracy is above 0.85 while Recall falls below 0.60, the model should be treated as high risk for your business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_2__'></a>\n",
    "\n",
    "### Basic additional context usage\n",
    "\n",
    "Here's how business context transforms the interpretation of our classifier results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_context = \"\"\"\n",
    "MODEL CONTEXT:\n",
    "- Class 0 = Customer stays (retains banking relationship)\n",
    "- Class 1 = Customer churns (closes accounts, leaves bank)\n",
    "\n",
    "DECISION RULES:\n",
    "- ROC AUC >0.9: APPROVE deployment\n",
    "- ROC AUC <0.9: REJECT model\n",
    "\n",
    "CHURN DETECTION RULES:\n",
    "- Recall >50% for churning customers: Good - use high-touch retention  \n",
    "- Recall <50% for churning customers: Poor - retention program will fail\n",
    "\"\"\"\n",
    "\n",
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    context={\n",
    "        \"additional_context\": simple_context,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_3__'></a>\n",
    "\n",
    "### Combining instructions and additional context\n",
    "\n",
    "Here's how combining both parameters creates targeted analysis of our churn model performance, using additional_context to pass both static business rules and dynamic real-time information like analysis dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Executive decision instructions with date placeholder\n",
    "executive_instructions = \"\"\"\n",
    "Create a GO/NO-GO decision memo following this template:\n",
    "\n",
    "<TEMPLATE>\n",
    "**DATE:** [Use analysis date from context]\n",
    "**THRESHOLD ANALYSIS:** [Pass/Fail against specific thresholds]\n",
    "**BUSINESS IMPACT:** [Revenue impact of current performance]  \n",
    "**DEPLOYMENT DECISION:** [APPROVE/CONDITIONAL/REJECT]\n",
    "**REQUIRED ACTIONS:** [Specific next steps with timelines]\n",
    "</TEMPLATE>\n",
    "\n",
    "Be definitive - use the thresholds to make clear recommendations.\n",
    "\"\"\"\n",
    "\n",
    "# Retail banking with hard thresholds including date\n",
    "retail_thresholds = f\"\"\"\n",
    "RETAIL BANKING CONTEXT (Analysis Date: {today}):\n",
    "- Class 0 = Customer retention (keeps checking/savings accounts)\n",
    "- Class 1 = Customer churn (closes accounts, switches banks)\n",
    "\n",
    "REGULATORY THRESHOLDS:\n",
    "- AUC >0.80: Meets regulatory model standards\n",
    "- Churn Recall >55%: Adequate churn detection \n",
    "- Churn Precision >65%: Cost-effective targeting \n",
    "\n",
    "DEPLOYMENT CRITERIA:\n",
    "- All 3 Pass: FULL DEPLOYMENT\n",
    "- 2 Pass: CONDITIONAL DEPLOYMENT\n",
    "- <2 Pass: REJECT MODEL\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    context={\n",
    "        \"instructions\": executive_instructions,\n",
    "        \"additional_context\": retail_thresholds,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6__'></a>\n",
    "\n",
    "## Overriding test documentation with test description parameter\n",
    "\n",
    "Each test, whether built-in or customized, includes a built-in docstring that serves as its default documentation. This docstring usually explains what the test does and what it outputs. In many cases, especially for specialized tests with well-defined purposes—the default docstring is already useful and sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_1__'></a>\n",
    "\n",
    "### Structure of ValidMind built-in test docstrings\n",
    "\n",
    "Every ValidMind built-in test includes a docstring that serves as its default documentation. This docstring follows a consistent structure so that both users and the LLM can rely on a predictable format. While the content varies depending on the type of test—for example, highly specific tests like SHAP values or PSI provide technical detail, whereas generic tests like descriptive statistics or histograms are more general—the overall layout remains the same.\n",
    "\n",
    "A typical docstring contains the following sections:\n",
    "\n",
    "- **Overview:**\n",
    "A short description of what the test does and what kind of output it generates.\n",
    "\n",
    "- **Purpose:**\n",
    "Explains why the test exists and what it is designed to evaluate. This section provides the context for the test’s role in model documentation, often describing the intended use cases or the kind of insights it supports.\n",
    "\n",
    "- **Test mechanism**:\n",
    "Describes how the test works internally. This includes the approach or methodology, what inputs are used, how results are calculated or visualized, and the logic behind the test’s implementation.\n",
    "\n",
    "- **Signs of high risks:**\n",
    "Outlines risk indicators that are specific to the test. These highlight situations where results should be interpreted with caution—for example, imbalances in distributions or errors in processing steps.\n",
    "\n",
    "- **Strengths:**\n",
    "Highlights the capabilities and benefits of the test, explaining what makes it particularly useful and what kinds of insights it provides that may not be captured elsewhere.\n",
    "\n",
    "- **Limitations:**\n",
    "Discusses the constraints of the test, including technical shortcomings, interpretive challenges, and situations where the results might be misleading or incomplete.\n",
    "\n",
    "This structure ensures that all built-in tests provide a comprehensive explanation of their purpose, mechanics, strengths, and limitations. For more generic tests, the docstring may read as boilerplate information about the test’s mechanics. In these cases, the `doc` parameter can be used to override the docstring with context that is more relevant to the dataset, feature, or business use case under analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_2__'></a>\n",
    "\n",
    "### Understanding the test description parameter\n",
    "\n",
    "Overriding the docstring with the `test_description` parameter is particularly valuable for more generic tests, where the default text often focuses on the mechanics of producing an output rather than the data or variable being analyzed. For example, instead of including documentation about the details about the methodology used to compute an histogram, you may want to document the business meaning of the feature being visualized, its expected distribution, or what to pay attention to. Similarly, when generating a descriptive statistics table, you may prefer documentation that describes the dataset under review. \n",
    "\n",
    "Customizing the doc, allows you to shift the focus of the explanation from the test machinery to the aspects of the data that matter most for your audience, while still relying on the built-in docstring for cases where the default detail is already fit for purpose.\n",
    "\n",
    "**When to override**\n",
    "\n",
    "For tests like histograms or descriptive statistics where the statistical methodology is standard and uninteresting, replace the generic documentation with meaningful descriptions of the variables being analyzed. Also use this to customize ValidMind's built-in test documentation when you want different terminology, structure, or emphasis than what's provided by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_3__'></a>\n",
    "\n",
    "### Basic test description usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_description = \"\"\"\n",
    "This test evaluates customer churn prediction model performance specifically \n",
    "for retail banking applications. The analysis focuses on classification \n",
    "metrics relevant to customer retention programs and regulatory compliance \n",
    "requirements under our internal Model Risk Management framework.\n",
    "\n",
    "Key metrics analyzed:\n",
    "- Precision: Accuracy of churn predictions to minimize wasted retention costs\n",
    "- Recall: Coverage of actual churners to maximize retention program effectiveness  \n",
    "- F1-Score: Balanced measure considering both precision and recall\n",
    "- ROC AUC: Overall discriminatory power for regulatory model approval\n",
    "\n",
    "Results inform deployment decisions for automated retention campaigns.\n",
    "\"\"\"\n",
    "\n",
    "result = vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"model\": vm_model,\n",
    "        \"dataset\": vm_test_ds\n",
    "    },\n",
    "    context={\n",
    "        \"test_description\": custom_description,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_4__'></a>\n",
    "\n",
    "### Combining test description with instructions and additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All three parameters working together\n",
    "banking_test_description = \"\"\"\n",
    "Customer Churn Risk Assessment Test for Retail Banking.\n",
    "Evaluates model's ability to identify customers likely to close accounts \n",
    "and switch to competitor banks within 12 months.\n",
    "- Class 0 = Customer retention (maintains banking relationship)\n",
    "- Class 1 = Customer churn (closes primary accounts)\n",
    "\"\"\"\n",
    "\n",
    "executive_instructions = \"\"\"\n",
    "Format as a risk committee briefing:\n",
    "**TEST DESCRIPTION:** [Test description]\n",
    "**RISK ASSESSMENT:** [Model risk level]\n",
    "**REGULATORY STATUS:** [Compliance with banking regulations]\n",
    "**BUSINESS RECOMMENDATION:** [Deploy/Hold/Reject with rationale]\n",
    "\"\"\"\n",
    "\n",
    "banking_contetx = \"\"\"\n",
    "REGULATORY CONTEXT:\n",
    "- OCC guidance requires AUC >0.80 for model approval\n",
    "- Our threshold: Churn recall >50% for retention program viability\n",
    "\"\"\"\n",
    "\n",
    "result = vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\n",
    "        \"model\": vm_model,\n",
    "        \"dataset\": vm_test_ds\n",
    "    },\n",
    "    context={\n",
    "        \"test_description\": banking_test_description,\n",
    "        \"instructions\": executive_instructions,\n",
    "        \"additional_context\": banking_contetx,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7__'></a>\n",
    "\n",
    "## Best practices for managing context\n",
    "\n",
    "When using `instructions`, `additional_context`, and `test_description` parameters together, follow these guidelines to create effective, consistent, and maintainable test descriptions.\n",
    "\n",
    "**Choose the right parameter for each need:**\n",
    "\n",
    "- Use `test_description` for technical corrections when you need to fix or clarify test methodology, override ValidMind's built-in documentation with your preferred structure or terminology, replace generic test mechanics with meaningful descriptions of variables and features being analyzed, or provide domain-specific context for regulatory compliance. \n",
    "\n",
    "- Apply `additional_context` for business rules like performance thresholds and decision criteria, business context such as customer economics and operational constraints, threshold-driven decision logic, regulatory requirements, real-time information like dates or risk indicators, stakeholder priorities, or any background information that helps the LLM interpret results in your specific context\n",
    "\n",
    "- Leverage `instructions` for audience targeting to control format and presentation style, create structured templates with specific sections and placeholders for LLM fill-ins, combine hardcoded mandatory text with dynamic analysis, and ensure consistent organizational reporting standards across different stakeholder groups.\n",
    "\n",
    "**Avoid redundancy:**\n",
    "\n",
    "Don't repeat the same information across multiple parameters, as each parameter should add unique value to the description generation. If content overlaps, choose the most appropriate parameter for that information to maintain clarity and prevent conflicting or duplicate guidance in your test descriptions.\n",
    "\n",
    "**Increasing consistency and grounding:**\n",
    "\n",
    "Since LLMs can produce variable responses, use hardcoded sections in your instructions for content that requires no variability, combined with specific placeholders for data you trust the LLM to generate. For example, include mandatory disclaimers, policy references, and fixed formatting exactly as written, while using placeholders like `[ANALYZE_PERFORMANCE_METRICS]` for dynamic content. This approach ensures critical information appears consistently while still leveraging the LLM's analytical capabilities.\n",
    "\n",
    "Use `test_description` and `additional_context` parameters to anchor test results descriptions in your specific domain and business context, preventing the LLM from generating generic or inappropriate interpretations. Then use `instructions` to explicitly direct the LLM to ground its analysis in this provided context, such as \"Base all recommendations on the thresholds specified in the additional context section\" or \"Interpret all metrics according to the test description provided.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
