{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-embeddings-huggingface llama-index-vector-stores-chroma -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample dataset from the library\n",
    "from validmind.datasets.llm.rag import rfp\n",
    "\n",
    "raw_df = rfp.load_data()\n",
    "train_df, test_df = rfp.preprocess(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First, we test both the `sentence-transformers` and `openai` embedding models using their native interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "embeddings = client.embeddings.create(\n",
    "    input=\"Hello World!\", \n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validmind` embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import FunctionModel\n",
    "\n",
    "def embed(input):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    \n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return embed_model.get_text_embedding(input[\"question\"])\n",
    "\n",
    "\n",
    "vm_embedder_st = FunctionModel(input_id=\"embedding_model_st\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(question):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=question,\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "\n",
    "vm_embedder_openai = FunctionModel(\n",
    "    input_id=\"embedding_model_openai\", \n",
    "    predict_fn=embed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validmind` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm_train_ds = vm.init_dataset(train_df, text_column=\"question\", __log=False)\n",
    "vm_test_ds = vm.init_dataset(test_df, text_column=\"question\", __log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings from the text in the `question` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(vm_embedder_st)\n",
    "vm_test_ds.assign_predictions(vm_embedder_openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an embedding test for both models to ensure that the embedding models function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise:HFEmbeddingModel\",\n",
    "        inputs={\"model\": vm_embedder_st, \"dataset\": vm_test_ds},\n",
    "        params={\"probability\": 0.3},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise:OpenAIEmbeddingModel\",\n",
    "        inputs={\"model\": vm_embedder_openai, \"dataset\": vm_test_ds},\n",
    "        params={\"probability\": 0.3},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"embedding\"] = vm_embedder_st.predict(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"rfp_rag_collection\")\n",
    "\n",
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in train_df.iterrows():\n",
    "\n",
    "    all_embeddings.append(row[\"embedding\"])\n",
    "    all_metadatas.append({\n",
    "        'ground_truth': row['ground_truth'],\n",
    "        'hnsw:space': 'cosine'\n",
    "    })\n",
    "    all_documents.append(row['question'])\n",
    "    all_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the retriever by directly querying using the pre-computed embedding corresponding to the question. We expect the vector store to return the top k most similar questions, along with the metadata associated with each of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "query = VectorStoreQuery(query_embedding=list(vm_test_ds.y_pred(vm_embedder_st)[0]), similarity_top_k=10)\n",
    "result = chroma_vector_store.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "    print(\"Node ID:\", id_)\n",
    "    print(\"Question:\", node.text)\n",
    "    print(\"Answer:\", node.metadata['ground_truth'])\n",
    "    print(\"Similarity:\", similarity)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(input):\n",
    "\n",
    "    contexts = []\n",
    "    \n",
    "    query = VectorStoreQuery(query_embedding=input[\"embedding_model_st\"], similarity_top_k=10)\n",
    "\n",
    "    result = chroma_vector_store.query(query)\n",
    "\n",
    "    for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "\n",
    "        context = f\"Node ID: {id_}\\n\"\n",
    "        context = f\"Question: {node.text}\\n\"\n",
    "        context += f\"Answer: {node.metadata['ground_truth']}\\n\"\n",
    "        context += f\"Similarity: {similarity}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "vm_retriever = FunctionModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import PipelineModel\n",
    "\n",
    "embed_retrieve_pipeline = PipelineModel(vm_embedder_st | vm_retriever, input_id=\"embed_retrieve_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(embed_retrieve_pipeline)\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = Prompt(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(\n",
    "    context=vm_test_ds.df.iloc[0]['embed_retrieve_pipeline_prediction'][0], \n",
    "    question=vm_test_ds.df.iloc[0]['question'],\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate(input):\n",
    "\n",
    "    formatted_prompt = prompt.format(\n",
    "        context=input[vm_retriever.input_id], \n",
    "        question=input[\"question\"],\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = FunctionModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a ValidMind RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_rag_model = PipelineModel(vm_embedder_st | vm_retriever | vm_generator, input_id=\"rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(vm_rag_model)\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = run_test(\n",
    "#     \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "#     inputs={\"dataset\": vm_ragas_ds},\n",
    "#     show=False,\n",
    "# )\n",
    "# plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
