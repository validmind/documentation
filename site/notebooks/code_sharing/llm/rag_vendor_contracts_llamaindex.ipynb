{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP: RAG Pipeline using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-embeddings-huggingface llama-index-vector-stores-chroma -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def extract_metadata_from_csv(file_path, metadata_columns):\n",
    "    metadata = {}\n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            for column in metadata_columns:\n",
    "                if column in row:\n",
    "                    # Assuming you want to collect unique values from each column\n",
    "                    if column not in metadata:\n",
    "                        metadata[column] = set()\n",
    "                    metadata[column].add(row[column])\n",
    "    # Convert sets to lists for JSON serializability\n",
    "    for key in metadata:\n",
    "        metadata[key] = list(metadata[key])\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vendor contracts dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the contracts by first loading them as pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CONTRACT_FILES = [\n",
    "    \"datasets/rag/vendor_contracts_001_020.csv\",\n",
    "    \"datasets/rag/vendor_contracts_021_040.csv\",\n",
    "    \"datasets/rag/vendor_contracts_041_060.csv\",\n",
    "]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "contracts_df = pd.concat(\n",
    "    [pd.read_csv(file) for file in CONTRACT_FILES], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "contracts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the questions and answers dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we explore the questions and answers dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_FILES = [\n",
    "    \"datasets/rag/vendor_contracts_questions.csv\",\n",
    "]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "questions_df = pd.concat(\n",
    "    [pd.read_csv(file) for file in QUESTION_FILES], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_questions_df`, which contains questions and answers, will be stored in the vector store. This will simulate questions that have already been answered and are stored in the database. The `test_questions_df`, on the other hand, will act as a set of new questions posed by the user. The answers in this dataset will be used as ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "questions_df = pd.read_csv(\"datasets/rag/vendor_contracts_questions.csv\")\n",
    "train_questions_df, test_questions_df = train_test_split(questions_df, test_size=0.20, random_state=42)\n",
    "\n",
    "# Rename columns \n",
    "train_questions_df = train_questions_df.rename(columns={\n",
    "    'Question #': 'question_id',\n",
    "    'Question': 'question', \n",
    "    'Answer': 'answer'})\n",
    "test_questions_df = test_questions_df.rename(columns={\n",
    "    'Question #': 'question_id',\n",
    "    'Question': 'question', \n",
    "    'Answer': 'ground_truth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm_train_questions_ds = vm.init_dataset(\n",
    "    input_id=\"train_questions\",\n",
    "    dataset=train_questions_df,\n",
    "    text_column=\"question\",\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_test_questions_ds = vm.init_dataset(\n",
    "    input_id=\"test_questions\",\n",
    "    dataset=test_questions_df,\n",
    "    text_column=\"question\",\n",
    "    __log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_questions_ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_questions_ds.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.Duplicates\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.StopWords\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.Punctuations\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.CommonWords\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.LanguageDetection\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Toxicity\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.PolarityAndSubjectivity\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Sentiment\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_questions_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `CSVLoader` from LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try the `CSVLoader` from `langchain` and check the document properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "lc_documents = [] \n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in CONTRACT_FILES:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    lc_documents.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_print = 2\n",
    "\n",
    "for index, doc in enumerate(lc_documents[:number_to_print]):\n",
    "    print(f\"Document {index + 1}:\")\n",
    "    print(\"Page Content:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling what is stored as metadata and what is stored as page content, which will be converted into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_documents = [] \n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in CONTRACT_FILES:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "        metadata_columns=[\"Contract ID\",\"Supported BSLs\", \"Engagement Terms\"]\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    lc_documents.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_print = 2\n",
    "\n",
    "for index, doc in enumerate(lc_documents[:number_to_print]):\n",
    "    print(f\"Document {index + 1}:\")\n",
    "    print(\"Page Content:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `CSVReader` from LlamaIndex\n",
    "\n",
    "We will test the `CSVReader` from `llama_index` and review the properties of the documents it loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import CSVReader\n",
    "from pathlib import Path\n",
    "\n",
    "reader = CSVReader(concat_rows=False)\n",
    "llama_documents = []\n",
    "metadata_columns = contracts_df.columns.tolist()\n",
    "print(f\"Metadata columns: {metadata_columns}\")\n",
    "\n",
    "# Iterate over each file path in the FILES list\n",
    "for file_path in CONTRACT_FILES:\n",
    "    \n",
    "    # Convert string file path to Path object\n",
    "    path_obj = Path(file_path)\n",
    "    \n",
    "    # Load data from each file and append to contract_docs list\n",
    "    documents = reader.load_data(\n",
    "        file=path_obj,\n",
    "    )\n",
    "\n",
    "    # Remove firs document corresponding to the header \n",
    "    documents = documents[1:]\n",
    "\n",
    "    llama_documents.extend(documents)\n",
    "\n",
    "print(f\"Loaded {len(llama_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert metadata in each document created by `CVSReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(llama_documents):\n",
    "    # Check if metadata already exists and is a dictionary; if not, initialize it\n",
    "    if not hasattr(doc, 'metadata') or not isinstance(doc.metadata, dict):\n",
    "        doc.metadata = {}\n",
    "\n",
    "    # Existing metadata is preserved and new keys are added or updated\n",
    "    doc.metadata.update({\n",
    "        column: contracts_df.iloc[i][column] for column in metadata_columns if column in contracts_df.columns\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_print = 2\n",
    "\n",
    "for i, doc in enumerate(llama_documents[:number_to_print]):\n",
    "    print(\"Document ID:\", doc.id_)\n",
    "    print(\"Text Content:\", doc.text)\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents into chunks\n",
    "\n",
    "We use `SentenceSplitter`, which aims to keep sentences and paragraphs together. This reduces the likelihood of hanging sentences or sentence fragments at the end of the node chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the llama documents into chunk nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter()\n",
    "nodes = splitter.get_nodes_from_documents(llama_documents)\n",
    "nodes[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the chunk nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_print = 2\n",
    "\n",
    "for index, node in enumerate(nodes[:num_to_print]):\n",
    "    print(f\"Node ID: {node.id_}\")\n",
    "    print(f\"Text Content: {node.text}\")\n",
    "    print(f\"Start Char IDX: {node.start_char_idx}\")\n",
    "    print(f\"End Char IDX: {node.end_char_idx}\")\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in node.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(f\"Embeddings: {node.embedding}\")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the chunk nodes into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, node in enumerate(nodes):\n",
    "    # Start with non-metadata fields\n",
    "    node_data = {\n",
    "        \"Node ID\": node.id_,\n",
    "        \"Text Content\": node.text\n",
    "    }\n",
    "    \n",
    "    # Add metadata fields dynamically if they exist in the node's metadata\n",
    "    node_data.update({\n",
    "        key: node.metadata[key]  # Use keys directly without removing spaces\n",
    "        for key in metadata_columns\n",
    "        if key in node.metadata\n",
    "    })\n",
    "    \n",
    "    data.append(node_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "contract_chunk_nodes_df = pd.DataFrame(data)\n",
    "contract_chunk_nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_contracts_ds = vm.init_dataset(\n",
    "    input_id=\"contracts_nodes\",\n",
    "    dataset=contract_chunk_nodes_df,\n",
    "    text_column=\"Text Content\", \n",
    "    __log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute embeddings on contract chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the embed function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from validmind.models import FunctionModel\n",
    "\n",
    "client = OpenAIEmbedding()\n",
    "\n",
    "def embed_contracts(input):\n",
    "    model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "    return model.get_text_embedding(input[\"Text Content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding model using the ValidMind `FunctionModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_embedder_contracts = FunctionModel(input_id=\"contracts_openai_embedding\", predict_fn=embed_contracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute embeddings by assigning predictions from the `vm_embedder` model to the `Text Content` column in the `vm_contracts_ds` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_contracts_ds.assign_predictions(vm_embedder_contracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the embeddings corresponding to the `Text Content` column have been correctly assigned to the dataset in a column named `<input_id_predictions>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_contracts_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False\n",
    "if run:\n",
    "    \n",
    "    test= run_test(\n",
    "        \"validmind.model_validation.embeddings.CosineSimilarityHeatmap\",\n",
    "        inputs = {\n",
    "            \"dataset\": vm_contracts_ds,\n",
    "            \"model\": vm_embedder_contracts,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False\n",
    "if run:\n",
    "    \n",
    "    test= run_test(\n",
    "        \"validmind.model_validation.embeddings.EuclideanDistanceHeatmap\",\n",
    "        inputs = {\n",
    "            \"dataset\": vm_contracts_ds,\n",
    "            \"model\": vm_embedder_contracts,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False\n",
    "if run:\n",
    "\n",
    "    test= run_test(\n",
    "        \"validmind.model_validation.embeddings.PCAComponentsPairwisePlots\",\n",
    "        inputs = {\n",
    "            \"dataset\": vm_contracts_ds,\n",
    "            \"model\": vm_embedder_contracts,\n",
    "        },\n",
    "        params = {\n",
    "            \"n_components\": 3\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False\n",
    "if run:\n",
    "\n",
    "    test= run_test(\n",
    "        \"validmind.model_validation.embeddings.TSNEComponentsPairwisePlots\",\n",
    "        inputs = {\n",
    "            \"dataset\": vm_contracts_ds,\n",
    "            \"model\": vm_embedder_contracts,\n",
    "        },\n",
    "        params = {\n",
    "            \"n_components\": 3\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert contract embeddings into the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "\n",
    "# Create or get a ChromaDB collection\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"contracts_collection\")\n",
    "\n",
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in vm_contracts_ds.df.iterrows():\n",
    "\n",
    "    # Append document-specific data\n",
    "    all_embeddings.append(row[vm_embedder_contracts.input_id + '_prediction'])\n",
    "    all_documents.append(row['Text Content'])\n",
    "    all_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "    # Prepare metadata dictionary dynamically\n",
    "    metadata = {\n",
    "        'hnsw:space': 'cosine'\n",
    "    }\n",
    "\n",
    "    # Dynamically add additional metadata from the defined list\n",
    "    metadata.update({\n",
    "        key: row[key] for key in metadata_columns if key in row\n",
    "    })\n",
    "    \n",
    "    all_metadatas.append(metadata)\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_question(input):\n",
    "    model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "    return model.get_text_embedding(input[\"question\"])\n",
    "\n",
    "vm_embedder_question = FunctionModel(input_id=\"question_openai_embedding\", predict_fn=embed_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_questions_ds.assign_predictions(vm_embedder_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_questions_ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_questions_ds.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "question_id = 4\n",
    "\n",
    "print(f\"Question: {vm_train_questions_ds.df['question'][question_id-1]}\")\n",
    "question_embedding = vm_train_questions_ds.df[vm_embedder_question.input_id + '_prediction'][question_id-1]\n",
    "\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "query = VectorStoreQuery(query_embedding=question_embedding, similarity_top_k=10)\n",
    "result = chroma_vector_store.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_print = 2  \n",
    "\n",
    "for node, similarity, id_ in zip(result.nodes[:num_to_print], result.similarities[:num_to_print], result.ids[:num_to_print]):\n",
    "    print(\"Node ID:\", id_)\n",
    "    print(\"Text Content:\")\n",
    "    print(node.text)\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in node.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"Similarity:\", similarity)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a dataframe to evaluate the relevance of the retrieved context. This dataframe will contain the query question and the retrieved text content for each node, allowing us to check the similarities between the question and the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "questions = []\n",
    "contexts = []\n",
    "similarities = []\n",
    "\n",
    "# Get the specific question you want to use for all entries (assuming index 3 is the question you want)\n",
    "constant_question = vm_train_questions_ds.df['question'][question_id-1]\n",
    "\n",
    "# Assuming 'result' is your object containing nodes and similarities\n",
    "for node, similarity in zip(result.nodes, result.similarities):\n",
    "    # Append the same question to the list for each node\n",
    "    questions.append(constant_question)\n",
    "    # Append the text content of each node to the contexts list\n",
    "    contexts.append(node.text)\n",
    "    # Append similarity score\n",
    "    similarities.append(similarity)\n",
    "\n",
    "# Create a DataFrame\n",
    "question_context_df = pd.DataFrame({\n",
    "    'Question': questions,\n",
    "    'Retrieved Context': contexts,\n",
    "    'Retrieved Similarity': similarities\n",
    "})\n",
    "\n",
    "question_context_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute embeddings for question and retrieved context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we rename the columns to enable the use of the predefined embedding functions for questions and contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_context_df = question_context_df.rename(columns={\n",
    "    'Question': 'question', \n",
    "    'Retrieved Context': 'Text Content'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert this dataframe into a ValidMind dataset to enable the computation of embeddings using assigned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_question_context_ds = vm.init_dataset(\n",
    "    input_id=\"question_context\",\n",
    "    dataset=question_context_df,\n",
    "    __log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_question_context_ds.assign_predictions(vm_embedder_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_question_context_ds.assign_predictions(vm_embedder_contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_question_context_ds.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    \n",
    "    test= run_test(\n",
    "        \"validmind.model_validation.embeddings.CosineSimilarityComparison\",\n",
    "        inputs = {\n",
    "            \"dataset\": vm_question_context_ds,\n",
    "            \"models\": [vm_embedder_contracts,vm_embedder_question],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    \n",
    "    test= run_test(\n",
    "        \"validmind.model_validation.embeddings.EuclideanDistanceComparison\",\n",
    "        inputs = {\n",
    "            \"dataset\": vm_question_context_ds,\n",
    "            \"models\": [vm_embedder_contracts,vm_embedder_question],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_questions_ds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_questions_ds.assign_predictions(vm_embedder_question)\n",
    "vm_test_questions_ds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_embedder_question.input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(input):\n",
    "    contexts = []\n",
    "    \n",
    "    # Assuming VectorStoreQuery and chroma_vector_store are predefined elsewhere in the application\n",
    "    query = VectorStoreQuery(query_embedding=input[\"question_openai_embedding\"], similarity_top_k=10)\n",
    "    result = chroma_vector_store.query(query)\n",
    "\n",
    "    for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "        # Initialize the context string with the Node ID\n",
    "        context = f\"Node ID: {id_}\\n\"\n",
    "        \n",
    "        # Append the contract text from the node\n",
    "        context += f\"Contract: {node.text}\\n\"\n",
    "        \n",
    "        # Append each metadata key-value pair to the context string\n",
    "        for key, value in node.metadata.items():\n",
    "            context += f\"{key}: {value}\\n\"\n",
    "        \n",
    "        # Append the similarity score\n",
    "        context += f\"Similarity: {similarity:.2f}\\n\"  # Formatting the similarity to two decimal places\n",
    "\n",
    "        # Add the fully constructed context for this node to the list\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "vm_retriever = FunctionModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import PipelineModel\n",
    "\n",
    "embed_retrieve_pipeline = PipelineModel(vm_embedder_question | vm_retriever, input_id=\"embed_retrieve_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_questions_ds.assign_predictions(embed_retrieve_pipeline)\n",
    "vm_test_questions_ds.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = Prompt(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(\n",
    "    context=vm_test_questions_ds.df.iloc[0]['embed_retrieve_pipeline_prediction'][0], \n",
    "    question=vm_test_questions_ds.df.iloc[0]['question'],\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate(input):\n",
    "\n",
    "    formatted_prompt = prompt.format(\n",
    "        context=input[vm_retriever.input_id], \n",
    "        question=input[\"question\"],\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = FunctionModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a ValidMind RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_rag_model = PipelineModel(vm_embedder_question | vm_retriever | vm_generator, input_id=\"rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_questions_ds.assign_predictions(vm_rag_model)\n",
    "vm_test_questions_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_params= {\n",
    "    \"question_column\":\"question\",\n",
    "    \"answer_column\":\"rag_model_prediction\",\n",
    "    \"ground_truth_column\":\"ground_truth\",\n",
    "    \"contexts_column\":\"embed_retrieve_pipeline_prediction\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_test_questions_ds},\n",
    "    params= ragas_params,\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_test_questions_ds},\n",
    "    params=ragas_params,\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_test_questions_ds},\n",
    "    params=ragas_params,\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_test_questions_ds},\n",
    "    params=ragas_params,\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-eEL8LtKG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
