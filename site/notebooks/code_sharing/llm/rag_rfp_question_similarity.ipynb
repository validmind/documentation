{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of a RAG Model for RFP Question Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise Exception(\"OPENAI_API_KEY not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import HTML, display\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def _format_cell_text(text, width=50):\n",
    "    \"\"\"Private function to format a cell's text.\"\"\"\n",
    "    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n",
    "\n",
    "\n",
    "def _format_dataframe_for_tabulate(df):\n",
    "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Format all string columns\n",
    "    for column in df_out.columns:\n",
    "        # Check if column is of type object (likely strings)\n",
    "        if df_out[column].dtype == object:\n",
    "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def _dataframe_to_html_table(df):\n",
    "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
    "    headers = df.columns.tolist()\n",
    "    table_data = df.values.tolist()\n",
    "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
    "\n",
    "\n",
    "def display_nice(df, num_rows=None):\n",
    "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    formatted_df = _format_dataframe_for_tabulate(df)\n",
    "    html_table = _dataframe_to_html_table(formatted_df)\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_keys(data, indent=0):\n",
    "    for key, value in data.items():\n",
    "        print(' ' * indent + str(key))\n",
    "        if isinstance(value, dict):  # if the value is another dictionary, recurse\n",
    "            print_dict_keys(value, indent + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file paths\n",
    "existing_rfp_paths = [\n",
    "    \"datasets/rag/rfp_existing_questions_client_2.csv\",\n",
    "]\n",
    "\n",
    "existing_rfp_df = [pd.read_csv(file_path) for file_path in existing_rfp_paths]\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "existing_rfp_df = pd.concat(existing_rfp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rfp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Documents from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in existing_rfp_paths:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "        metadata_columns=[\"Area\"]\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    documents.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `CSVLoader`, each document represents a single row and includes its respective contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 5\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Document {i + 1}: {document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the page content of each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 2\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Page content for document {i + 1}:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when adding metadata, it is appended to the default metadata, which consists of the `row` number and the `source`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 5\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Metadata for document {i + 1}: {document.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=10, add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some general information about the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the length of the bigger and smaller chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunk_length = max([len(chunk.page_content) for chunk in chunks])\n",
    "min_chunk_length = min([len(chunk.page_content) for chunk in chunks])\n",
    "mean_chunk_length = sum([len(chunk.page_content) for chunk in chunks]) / len(chunks)\n",
    "\n",
    "print(f\"Maximum chunk length: {max_chunk_length}\")\n",
    "print(f\"Minimum chunk length: {min_chunk_length}\")\n",
    "print(f\"Mean chunk length: {mean_chunk_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Calculate lengths of each chunk's page_content\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# Creating a histogram of chunk lengths\n",
    "fig = px.histogram(chunk_lengths, nbins=50, title=\"Distribution of Chunk Lengths\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Chunk Length\",\n",
    "    yaxis_title=\"Count\",\n",
    "    bargap=0.2,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add summary statistics as text on the plot\n",
    "fig.add_annotation(\n",
    "    x=max(chunk_lengths),\n",
    "    y=0,\n",
    "    showarrow=False,\n",
    "    yshift=10\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for index, chunk in enumerate(chunks[:i]):\n",
    "    print(f\"Chunk {index + 1}: {chunk}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the page content of each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5\n",
    "\n",
    "for i, document in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Page content for chunk {i + 1}:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the metadata for individual chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Metadata for chunk {i + 1}: {chunk.metadata}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the source of each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Source for chunk {i + 1}: {chunk.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store chunks into a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new RFP questions \n",
    "rag_evaluation_df = pd.read_csv(\"datasets/rag/rag_evaluation_dataset_01.csv\")\n",
    "\n",
    "# Set the constant variable to the number of rows in the DataFrame\n",
    "NUM_OF_NEW_RFP_QUESTIONS = len(rag_evaluation_df)\n",
    "\n",
    "print(\"Number of New RFP Questions:\", NUM_OF_NEW_RFP_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Question-Answer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG Chain\n",
    "\n",
    "1. Get the question from the user\n",
    "2. Pass the question to the retriever \n",
    "3. Get the context from the retriever \n",
    "4. Combine the context and the question to format the prompt\n",
    "5. Pass the prompt to the LLM to get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Step 1: \"question\": Retrieved from the \"question\" key.\n",
    "# Step 2: \"context\": Retrieved from the \"question\" key and fed into the retriever.\n",
    "# Step 3: \"context\": Assigned to a RunnablePassthrough object using the \"context\" key from the previous step.\n",
    "# Step 4: \"answer\": \"context\" and \"question\" are combined to format the prompt, then sent to the LLM and stored under the \"answer\" key.\n",
    "# Step 5: \"context\": Repopulated using the \"context\" key from the previous step.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    \n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"answer\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question to test the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find a similar question as this one: 'What is your experience in developing AI-based applications?'\"\n",
    "response = rag_chain.invoke({\"question\" : question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined in the earlier chat prompt, the RAG response includes two fields: `answer` and `context`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the answer, we see that the `rag_chain` is functioning correctly and identifies the most similar question in the `vectorstore`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question:\")\n",
    "print(question)\n",
    "print()\n",
    "print(f\"Answer:\")\n",
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inspect the content of the `answer` and the `context` retrieved based on the `question`. The context should contain `k` chunks, the most relevant based on the question. Remember that we set`k` in the `retriever` earlier. These `k` chunks are pasted into the prompt as text, informing the LLM to generate an answer that is closer in the embedding space to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(response[\"context\"][:number_of_chunks]):\n",
    "    print(f\"Content for chunk {i + 1}:\")  # i + 1 to start counting from 1 instead of 0\n",
    "    print(chunk.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now inspect the `response_metadata` object to understand its contents and identify what could be useful to incorporate in our RAG evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"].response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response[\"answer\"].response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the LLM used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {response['answer'].response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed earlier, we can also extract some token usage statistics that can help us understand and optimize our interactions with the language model for cost-effectiveness and efficiency.\n",
    "\n",
    "- **Prompt tokens**: tokens that form the input text sent to the language model. This includes all the text provided to the LLM to generate a response.\n",
    "- **Completion tokens**: number of tokens in the generated text or output from the model.\n",
    "- **Total tokens**: total number of tokens processed by the model. It is the sum of both `prompt_tokens` and `completion_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Completion tokens: {response['answer'].response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"Prompt tokens: {response['answer'].response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"Total tokens: {response['answer'].response_metadata['token_usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now expand our evaluation dataset to capture some metadata generated by the LLM, which will be used later when validating our RAG pipeline. We will add the following additional columns to our dataframe: `context`, `model_name`, `completion_tokens`, prompt_tokens, and `total_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df['context'] = ''\n",
    "\n",
    "rag_evaluation_df['question_embeddings'] = ''\n",
    "rag_evaluation_df['answer_embeddings'] = ''\n",
    "rag_evaluation_df['context_embeddings'] = ''\n",
    "\n",
    "rag_evaluation_df['similarity_score_question_vs_context'] = ''\n",
    "rag_evaluation_df['similarity_score_question_vs_answer'] = ''\n",
    "rag_evaluation_df['similarity_score_context_vs_answer'] = ''\n",
    "\n",
    "rag_evaluation_df['model'] = ''\n",
    "\n",
    "rag_evaluation_df['completion_tokens'] = ''\n",
    "rag_evaluation_df['prompt_tokens'] = ''\n",
    "rag_evaluation_df['total_tokens'] = ''\n",
    "\n",
    "rag_evaluation_df['response_time'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to also compute few similarity metrics between embeddings such as cosine similaruty or euclidean distance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_score(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding1 (array-like): Embedding of the first entity.\n",
    "    - embedding2 (array-like): Embedding of the second entity.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cosine similarity score between the two embeddings.\n",
    "\n",
    "    Note: The order of the embeddings does not affect the result as cosine similarity is symmetric.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings are reshaped to 2D arrays for sklearn's cosine_similarity\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # Calculate and return the cosine similarity\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between two embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding1 (array-like): First embedding vector.\n",
    "    - embedding2 (array-like): Second embedding vector.\n",
    "\n",
    "    Returns:\n",
    "    - float: Euclidean distance between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays if they aren't already\n",
    "    embedding1 = np.array(embedding1)\n",
    "    embedding2 = np.array(embedding2)\n",
    "    \n",
    "    # Calculate and return the Euclidean distance\n",
    "    return np.linalg.norm(embedding1 - embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Number of questions to process by the RAG model\n",
    "number_of_rows_to_process = NUM_OF_NEW_RFP_QUESTIONS\n",
    "\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing row {i}...\")\n",
    "\n",
    "    # Check if the 'answer' field is 'None' (as a string) for the current row\n",
    "    if row[\"answer\"] == \"None\":\n",
    "        print(f\"Answer is 'None' for question ID {index}. Invoking RAG model...\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        # Invoke the RAG model with the question from the current row\n",
    "        response = rag_chain.invoke({\"question\": row[\"question_to_llm\"]})\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "\n",
    "        # Calculate the response time and store it\n",
    "        rag_evaluation_df.at[index, 'response_time'] = round(end_time - start_time, 1)\n",
    "\n",
    "        # Store whatever response comes from the LLM\n",
    "        rag_evaluation_df.at[index, \"answer\"] = response[\"answer\"].content\n",
    "        print(f\"Question ID {index} answer updated with the response from the RAG model.\")\n",
    "    \n",
    "        # Store the context included in the prompt\n",
    "        context = \"\\n\\n\".join(chunk.page_content for chunk in response[\"context\"])\n",
    "        rag_evaluation_df.at[index, \"context\"] = context\n",
    "        \n",
    "        # Compute and store embeddings for the question, context and answer\n",
    "        print(\"Computing embeddings for the question...\")\n",
    "        question_embeddings = np.array(embeddings_model.embed_query(row[\"question_to_llm\"]))\n",
    "        rag_evaluation_df.at[index, 'question_embeddings'] = question_embeddings\n",
    "        \n",
    "        print(\"Computing embeddings for the context...\")\n",
    "        context_embeddings = np.array(embeddings_model.embed_query(context))\n",
    "        rag_evaluation_df.at[index, 'context_embeddings'] = context_embeddings\n",
    "        \n",
    "        print(\"Computing embeddings for the answer...\")\n",
    "        answer_embeddings = np.array(embeddings_model.embed_query(response[\"answer\"].content))\n",
    "        rag_evaluation_df.at[index, 'answer_embeddings'] = answer_embeddings\n",
    "        \n",
    "        # Compute similarity measures between embeddings \n",
    "        print(\"Computing cosine similarity between question and context...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_question_vs_context'] = cosine_similarity_score(question_embeddings, context_embeddings)\n",
    "        \n",
    "        print(\"Computing cosine similarity between question and answer...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_question_vs_answer'] = cosine_similarity_score(question_embeddings, answer_embeddings)\n",
    "\n",
    "        print(\"Computing cosine similarity between context and answer...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_context_vs_answer'] = cosine_similarity_score(context_embeddings, answer_embeddings)\n",
    "        \n",
    "        # Store some metadata such as model name and tokens statistics\n",
    "        rag_evaluation_df.at[index, \"model\"] = response[\"answer\"].response_metadata[\"model_name\"]\n",
    "        rag_evaluation_df.at[index, \"completion_tokens\"] = response['answer'].response_metadata['token_usage']['completion_tokens']\n",
    "        rag_evaluation_df.at[index, \"prompt_tokens\"] = response['answer'].response_metadata['token_usage']['prompt_tokens']\n",
    "        rag_evaluation_df.at[index, \"total_tokens\"] = response['answer'].response_metadata['token_usage']['total_tokens']\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if all responses have been generated by the RAG pipeline or if there are any `None` values in the answers column. If there are any rows with `None` answers, remove these before they are passed to the RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df = rag_evaluation_df[rag_evaluation_df['answer'] != 'None']\n",
    "rag_evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the results in a CSV file for convenience to avoid having to execute the entire RAG pipeline every time we want to test our RAG evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "rag_evaluation_df.to_csv('datasets/rag/rag_evaluation_dataset_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation with RAGAS\n",
    "\n",
    "RETRIEVAL \n",
    "- **Context Precision**:  how relevant is the `context` to the `question`.\n",
    " - **Context Recall**: given the `ground truth`, is the retriever able to retrieve all the relevant `context`.\n",
    "\n",
    "GENERATION\n",
    " - **Answer Relevancy**: how relevant is the generated `answer` to the `question`. \n",
    " - **Faithfulness**: is the `answer` fact-checkable. The number of correct statements from the given `contexts` divided by the total number of statements in the generated answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to evaluate our RAG pipeline using RAGAS metrics from the `ragas` package. The `evaluate()` function expects a Dataset with specific column names: `question`, `contexts`, `ground_truth`, and `answer`. We will now rename these columns to conform to the expected column names in RAGAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for RAGAS evaluation\n",
    "ragas_results_df = rag_evaluation_df.copy()\n",
    "\n",
    "# Rename the columns to match ragas convention\n",
    "ragas_results_df.rename(\n",
    "    columns={\n",
    "        \"question_to_llm\": \"question\",\n",
    "        \"context\": \"contexts\"}, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the 'contexts' column from a string to a list of strings for each row\n",
    "ragas_results_df['contexts'] = ragas_results_df['contexts'].apply(lambda x: [x])\n",
    "\n",
    "ragas_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the RAGAS evaluation metrics row by row, adding the results to corresponding columns for each metric in our evaluation dataset. We first initialize the columns where the evaluation metrics will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df['context_precision'] = ''\n",
    "ragas_results_df['faithfulness'] = ''\n",
    "ragas_results_df['answer_relevancy'] = ''\n",
    "ragas_results_df['context_recall'] = ''\n",
    "ragas_results_df['context_relevancy'] = ''\n",
    "ragas_results_df['answer_correctness'] = ''\n",
    "ragas_results_df['answer_similarity'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "required_fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "metrics = [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"context_relevancy\", \"answer_correctness\", \"answer_similarity\"]\n",
    "\n",
    "# Set the variable to the number of rows, limited to a maximum of NUM_OF_NEW_RFP_QUESTIONS\n",
    "number_of_rows_to_process = min(len(ragas_results_df), NUM_OF_NEW_RFP_QUESTIONS)\n",
    "\n",
    "# Mapping of metric names to their respective functions, assuming these functions are predefined\n",
    "metrics_functions = {\n",
    "    \"context_precision\": context_precision,\n",
    "    \"faithfulness\": faithfulness,\n",
    "    \"answer_relevancy\": answer_relevancy,\n",
    "    \"context_recall\": context_recall,\n",
    "    \"context_relevancy\": context_relevancy,\n",
    "    \"answer_correctness\": answer_correctness,\n",
    "    \"answer_similarity\": answer_similarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This loop processes each row up to a predefined number of rows, evaluating them with specified metrics and storing the results\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing RFP question {i+1}...\")\n",
    "    print(f\"Question: {ragas_results_df.iloc[i]['question']}\")\n",
    "    print(f\"Answer: {ragas_results_df.iloc[i]['answer']}\")\n",
    "\n",
    "    # Create a temporary Dataset for the current row\n",
    "    ragas_dataset = Dataset.from_pandas(ragas_results_df.iloc[i: i + 1][required_fields])\n",
    "\n",
    "    # Evaluate using RAGAS metrics\n",
    "    evaluation_result = evaluate(\n",
    "        ragas_dataset, \n",
    "        [metrics_functions[metric] for metric in metrics if metric in metrics_functions])\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    # Store evaluation results back into the DataFrame\n",
    "    for metric in metrics:\n",
    "        if metric in evaluation_result:\n",
    "            ragas_results_df.at[i, metric] = evaluation_result[metric]\n",
    "            print(f\"{metric}: {evaluation_result[metric]}\")\n",
    "\n",
    "print(\"All RFP questions processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "ragas_results_df.to_csv('datasets/rag/rag_evaluation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
