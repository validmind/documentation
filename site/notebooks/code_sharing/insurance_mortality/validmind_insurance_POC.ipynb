{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be3f7c6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5838f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9833449",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Being able to make accurate and timely estimates of future claims is a fundamental task for\n",
    "actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on\n",
    "understanding future claims, with mortality being one of the central issues facing a life insurer.\n",
    "\n",
    "In this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19189a9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Overview of Mortality Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cf794",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Case Study Data </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6aad3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\n",
    "\n",
    "For the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old. \n",
    "\n",
    "More details on this dataset can be found in Section 2 of the data report  https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c422f70",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Case Study Model </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ce480",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the case study in this paper, we used the `statsmodel`'s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\n",
    "\n",
    "The <b> response variable</b> used in this case study is the `number of deaths`. `Policies exposed` was used as a weight in the model. We also tried to fit the `mortality rate`, which is `number of deaths`/ `policies exposed` using Gaussian distribution with log link, that can be found in the Appendix\n",
    "\n",
    "The <b>features</b> used in the mortality model are:\n",
    "\n",
    "\n",
    "- `Attained Age` – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\n",
    "\n",
    "- `Duration` – the number of years (starting with a value of one) the policyholder has had the policy.\n",
    "\n",
    "- `Smoking Status` – if the policyholder is considered a smoker or not.\n",
    "\n",
    "- `Preferred Class` – an underwriting structure used by insurers to classify and price policyholders.\n",
    "Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\n",
    "\n",
    "- `Gender` – A categorical feature in the model with two levels, male and female.\n",
    "\n",
    "- `Guaranteed Term Period` – the length of the policy at issue during which the premium will remain\n",
    "constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\n",
    "\n",
    "- `Face_Amount_Band`\n",
    "\n",
    "- `Observation Year`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739cfb7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50605742",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import os\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93d87e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fd540",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# directly curl from the SOA website and unzip\n",
    "! echo Working Directory = $(pwd)\n",
    "! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n",
    "! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n",
    "! echo \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290e8f1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Second, sample 5% from the giant file. Another 10 minutes or so the first time you run it :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5cf71",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sample 5% and save it out to a sample file\n",
    "if not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n",
    "    p = 0.02\n",
    "    random.seed(42)\n",
    "    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv',\n",
    "                        skiprows = lambda i: i>0 and random.random() >p)\n",
    "    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aea6b4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58458b95",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load sample file\n",
    "sample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n",
    "                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n",
    "                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n",
    "                               'Face_Amount_Band', 'Preferred_Class',\n",
    "                               'Number_Of_Deaths','Policies_Exposed',\n",
    "                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator',\n",
    "                               'Expected_Death_QX2015VBT_by_Policy',\n",
    "                               'Issue_Age', 'Issue_Year'])\n",
    "\n",
    "# target variable\n",
    "sample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fabc1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# filter pipeline\n",
    "sample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n",
    "               & (sample_df.Smoker_Status != 'Unknown')\n",
    "               & (sample_df.Insurance_Plan == ' Term')\n",
    "               & (-sample_df.Preferred_Class.isna())\n",
    "               & (sample_df.Attained_Age >= 18)\n",
    "               & (sample_df.Issue_Year >= 1980)\n",
    "               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n",
    "               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n",
    "               & (sample_df.mort < 1)]\n",
    "\n",
    "print(f'Count: {sample_df.shape[0]}')\n",
    "print()\n",
    "\n",
    "# describe data\n",
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11524c29",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "cat_vars = ['Observation_Year',\n",
    "     'Gender',\n",
    "     'Smoker_Status',\n",
    "     'Face_Amount_Band',\n",
    "     'Preferred_Class',\n",
    "     'SOA_Anticipated_Level_Term_Period']\n",
    "\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "results = onehot.fit_transform(sample_df[cat_vars]).toarray()\n",
    "cat_vars_encoded = list(onehot.get_feature_names_out())\n",
    "sample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7b7eb",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# categorical variables\n",
    "face_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\n",
    "term_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\n",
    "fig, ax = plt.subplots(4,2, figsize = (20,30))\n",
    "ax = ax.flatten()\n",
    "for i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n",
    "       'Face_Amount_Band', 'Preferred_Class',\n",
    "       'SOA_Guaranteed_Level_Term_Period']):\n",
    "    if column == 'Face_Amount_Band':\n",
    "        order = face_amount_order\n",
    "    elif column == 'SOA_Guaranteed_Level_Term_Period':\n",
    "        order = term_period_order\n",
    "    else:\n",
    "        order = None\n",
    "    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5250f61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# age and duration variables\n",
    "fig, ax = plt.subplots(1,2, figsize = (20,5))\n",
    "sns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n",
    "\n",
    "sns.histplot(x = sample_df['Duration'], ax = ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1cde6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we quickly check for any collinearity\n",
    "fig, ax = plt.subplots(figsize = (20,20))\n",
    "sns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b08a7e",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# log mort by Attained Age\n",
    "\n",
    "def stratify(field):\n",
    "    fig, ax = plt.subplots(figsize = (7,3))\n",
    "    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n",
    "    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n",
    "    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n",
    "    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n",
    "    plt.show()\n",
    "\n",
    "stratify('Smoker_Status')\n",
    "stratify('Preferred_Class')\n",
    "stratify('Gender')\n",
    "stratify('Observation_Year')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c258ea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83caa98",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce49d5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we split the data into 80% for training and 20% for testing. \n",
    "\n",
    "In this context because we don't really need to do hyperparameter tuning so it's not necessary to create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c55095",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create training (80%), validation (5%) and test set (15%)\n",
    "random_seed = 0\n",
    "train_df = sample_df.sample(frac = 0.8, random_state = random_seed)\n",
    "test_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n",
    "\n",
    "# add constant variable\n",
    "train_df['Const'] = 1\n",
    "test_df['Const'] = 1\n",
    "\n",
    "print(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dea1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('train_df.csv', index = False)\n",
    "# test_df.to_csv('test_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8781756",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### GLM modeling 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35510d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, $μ$, of the distribution depends on the independent variables, X, through\n",
    "\n",
    "<center>${\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}$</center>\n",
    "\n",
    "${\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $E(Y|X)$ is the expected value of $Y$ conditional on $X$\n",
    "- $Xβ$ is the linear predictor, a linear combination of unknown parameters $β$\n",
    "- $g$ is the link function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb087f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model 1: Poisson distribution with log link on count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77bb8d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<i> Target Variable </i> = [Number_Of_Deaths]\n",
    "\n",
    "<i> Input Variables </i> =  [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\n",
    "\n",
    "As the <i> target variable</i> is a count measure, we will fit GLM with Poisson distribution and log link. \n",
    "\n",
    "The target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to \n",
    "https://en.wikipedia.org/wiki/Poisson_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae0a86",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n",
    "                                       + Attained_Age + Duration',\n",
    "                data = train_df,\n",
    "                family=sm.families.Poisson(sm.families.links.log()),\n",
    "                freq_weights = train_df['Policies_Exposed'],\n",
    "                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n",
    "              )\n",
    "res1 = model1.fit()\n",
    "res1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579ea6e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we show the <b>lift chart</b> that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we're only look at the high-level average for each decile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af263f27",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# append fitted values for training and predicted values for testing\n",
    "train_df['mort_hat1'] = res1.predict(exog = train_df)\n",
    "train_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\n",
    "test_df['mort_hat1'] = res1.predict(exog = test_df)\n",
    "test_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n",
    "\n",
    "# groupby and aggregate by deciles\n",
    "test_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\n",
    "wm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n",
    "temp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\n",
    "\n",
    "# lift chart\n",
    "fig, ax = plt.subplots(figsize = (7,3))\n",
    "temp.plot(ax = ax)\n",
    "plt.title('Actual vs predicted mortality rate by deciles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec3f55",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Second, we can plot the partial dependency chart between the `log mortality rate` and key covariates like `Attained Age` or `Duration` to see more granular comparisons between actual vs predicted. \n",
    "\n",
    "We can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d524e",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pdp(df, agg_field, title, predict_col = 'death_hat1'):\n",
    "    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n",
    "    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n",
    "    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (7,3))\n",
    "    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n",
    "    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n",
    "    plt.legend(['actual','predicted'])\n",
    "    plt.xlabel(agg_field)\n",
    "    plt.ylabel('log_mort')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "pdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\n",
    "pdp(train_df, 'Duration', 'How well does the model fit the train set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d4698",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Third, we look at Prediction Error by taking the difference between the `Number Of Deaths` (actual) and Predicted Number of Deaths and then normalized by `Policies Exposed`. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0a4ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7,3))\n",
    "train_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\n",
    "agg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\n",
    "plt.legend(['Model 1'])\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Error')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,3))\n",
    "test_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\n",
    "agg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\n",
    "plt.legend(['Model 1'])\n",
    "plt.ylabel('Error')\n",
    "plt.title('Testing error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7d815",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3d94a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1. Goodness of Fit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a9586",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Pseudo R-squared </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d54f1d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\n",
    "\n",
    "For GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n",
    "\n",
    "<center> $R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}$ </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bfc77",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res1.pseudo_rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21246266",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Deviance </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677183c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The (total) deviance for a model M with estimates ${\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}$, based on a dataset y, may be constructed by its likelihood as:\n",
    "\n",
    "<center> ${\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}$ </center>\n",
    "\n",
    "Here $\\hat \\theta_0$ denotes the fitted values of the parameters in the model M, while $\\hat \\theta_s$ denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76080f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012af714",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we divided the `deviance` by the `residual degree of freedom` and observed a ratio much smaller than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd850e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res1.deviance/res1.df_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655df26",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Pearson Statistic and dispersion </i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54185196",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similar to `deviance` test, the `Pearson Statistic` is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefb259",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Additionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. <b>Overdispersion</b> means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6adb3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we divided the `pearson statistic` by the `residual degree of freedom` and observed a value very close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d353bd3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res1.pearson_chi2/res1.df_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14f5ce",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61116004",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i>Confidence intervals and p-values </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eeecd4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Confidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d9a46",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818adeb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the summary, we can see that all of the features other than `SOA_Anticipated_Level_Term_Period` are significant as all p-values are < 5%. \n",
    "\n",
    "Directionally, the coeficients for the main features like `Gender`, `Smoking Status`, `Attained_Age` or `Duration` are all aligned with our intuition and the EDA charts that we created previously:\n",
    "\n",
    "- Mortality rate for Male is higher than Female\n",
    "- Mortality rate for Smoker is higher than non-Smoker\n",
    "- Mortality rate is higher as age is higher\n",
    "- Mortality rate is higher as duration is longer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbc575",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3. Main Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dd561",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5fd4c",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pdp2(df, x, hue, predict_col = 'death_hat1'):\n",
    "    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n",
    "    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (6,3))\n",
    "    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel('log_mort')\n",
    "    plt.title(f'Log mortality by {x} and {hue}')\n",
    "    plt.show()\n",
    "\n",
    "pdp2(train_df, 'Attained_Age', 'Gender')\n",
    "pdp2(train_df, 'Duration', 'Gender')\n",
    "pdp2(train_df, 'Attained_Age', 'Smoker_Status')\n",
    "pdp2(train_df, 'Duration', 'Smoker_Status')\n",
    "pdp2(train_df, 'Attained_Age', 'Preferred_Class')\n",
    "pdp2(train_df, 'Duration', 'Preferred_Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e99cef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\n",
    "\n",
    "Additionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it's almost constant regardless of ages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb90d8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4. Interaction Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89965dda",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features. \n",
    "\n",
    "Here we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc72bf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Model 2: Poisson distribution with log link on Death Count with interactions </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de1581",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n",
    "                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n",
    "                data = train_df,\n",
    "                family=sm.families.Poisson(sm.families.links.log()),\n",
    "                freq_weights = train_df['Policies_Exposed'],\n",
    "                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n",
    "              )\n",
    "res2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n",
    "\n",
    "# append fitted values for training and predicted values for testing\n",
    "train_df['mort_hat2'] = res2.predict(exog = train_df)\n",
    "train_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\n",
    "test_df['mort_hat2'] = res2.predict(exog = test_df)\n",
    "test_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n",
    "\n",
    "res2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2748019",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <i> Compared to the vanilla model </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f85304",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, pearson and deviance are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6866ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n",
    "\n",
    "print(f'deviance/df = {res2.deviance/res2.df_resid}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ce7bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b278b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'AIC for Model 1 - No interaction: {res1.aic}')\n",
    "print(f'AIC for Model 2 - With interactions: {res2.aic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df1e8d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Side note on definition of AIC: </b> A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf6e42",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 5. Correlated Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ec54d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\n",
    "\n",
    "This is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let's show it again below. We don't see severe correlation between any two features that requires dropping one from the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef09a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we quickly check for any collinearity\n",
    "fig, ax = plt.subplots(figsize = (20,20))\n",
    "sns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde58d7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9efee",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model.\n",
    " - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. \n",
    " - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. \n",
    " - We checked for any necessary inclusion of interactions and handling of correlated features.\n",
    " \n",
    "Apparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ab765",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c665d49",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model 1 not using formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222eb98",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the explicit setup where we don't lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f830bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Target Variable\n",
    "Y = ['Number_Of_Deaths']\n",
    "\n",
    "# Predictors (aka Input Variables)\n",
    "X = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const']\n",
    "\n",
    "# Our choice for Link function is the Gaussian distribution for the nature of death frequency\n",
    "model = sm.GLM(endog = train_df[Y],\n",
    "               exog = train_df[X],\n",
    "               family=sm.families.Poisson(sm.families.links.log()),\n",
    "               freq_weights = train_df['Policies_Exposed'],\n",
    "               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n",
    "              )\n",
    "res = model.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c07d4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model 3: Gaussian distribution with log link on mortality rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84c7d1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. `Pseudo R-squared` is far worse than Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba224fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration',\n",
    "                 data = train_df,\n",
    "                 family=sm.families.Gaussian(link = sm.families.links.log()),\n",
    "                 freq_weights = train_df['Policies_Exposed'])\n",
    "res2 = model2.fit()\n",
    "res2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032b513",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model 4: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819f90c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.  \n",
    "\n",
    "Note that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbcf03",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\n",
    "Y = ['mort']#['Number_Of_Deaths']\n",
    "\n",
    "X_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\n",
    "for x in X_cat:\n",
    "    train_df[x] = train_df[x].astype(\"category\")\n",
    "    test_df[x] = test_df[x].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdee9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create model instance\n",
    "bst = xgb.XGBRegressor(n_estimators=50,\n",
    "                   max_depth=4,\n",
    "                   learning_rate=0.5,\n",
    "                   objective='count:poisson',\n",
    "                   enable_categorical = True,\n",
    "                   tree_method = 'approx',\n",
    "                   booster = 'gbtree',\n",
    "                   verbosity = 1)\n",
    "\n",
    "# fit model\n",
    "bst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n",
    "\n",
    "# make predictions\n",
    "preds = bst.predict(test_df[X])\n",
    "\n",
    "# append fitted values for training and predicted values for testing\n",
    "train_df['mort_hat4'] = bst.predict(train_df[X])\n",
    "train_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\n",
    "test_df['mort_hat4'] = bst.predict(test_df[X])\n",
    "test_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5383e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lift chart does not show too much of a difference from Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951b461",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lift chart by deciles\n",
    "test_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\n",
    "wm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n",
    "\n",
    "# groupby and aggregate\n",
    "fig, ax = plt.subplots(figsize = (7,3))\n",
    "temp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\n",
    "temp.plot(ax = ax)\n",
    "plt.title('Actual vs Predicted by deciles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86814b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b7001",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# partial dependence plots\n",
    "pdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\n",
    "pdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\n",
    "pdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\n",
    "pdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e4de0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11343bc",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\n",
    "pdp2(train_df, 'Duration', 'Gender','death_hat4')\n",
    "pdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\n",
    "pdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\n",
    "pdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\n",
    "pdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7aa421",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Compare Model 1, Model 2 and Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93976c73",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7,3))\n",
    "train_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\n",
    "train_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\n",
    "train_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n",
    "\n",
    "agg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\n",
    "plt.legend(['Model 1', 'Model 2', 'Model 4'])\n",
    "# plt.ylim(0,1)\n",
    "# plt.xlim(30,85)\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Error')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,3))\n",
    "test_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\n",
    "test_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\n",
    "test_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n",
    "\n",
    "agg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\n",
    "sns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\n",
    "plt.legend(['Model 1', 'Model 2', 'Model 4'])\n",
    "plt.ylabel('Error')\n",
    "# plt.ylim(0,1)\n",
    "# plt.xlim(30,85)\n",
    "a = plt.title('Testing Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9aacb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res1.save('mortality_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893d34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev Framework 3.9.16",
   "language": "python",
   "name": "dev-framework-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5507f2e99c1cac96073e07e686bb64d511c5f1c7216ba7fc4306f43af6557f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
