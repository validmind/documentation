[
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "Platform Overview",
    "section": "",
    "text": "ValidMind is a flexible, all-in-one solution for model risk management (MRM) tailored for financial institutions. We help you streamline collaboration between model developers, validators, and executive stakeholders, increasing the speed and efficiency of model documentation and validation.\nOur solution comprises two primary product components: the locally-run_ developer framework_ and the cloud-hosted software-as-a-service (SaaS) platform, seamlessly integrating the two with your existing infrastructure and software stack."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "Platform Overview",
    "section": "Related Topics",
    "text": "Related Topics\nGet started with ValidMind"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guide",
    "section": "",
    "text": "How-to instructions for common user tasks for:\n\nPlatform administrators\nModel developers\nModel validators"
  },
  {
    "objectID": "guide/jupyter-notebooks.html",
    "href": "guide/jupyter-notebooks.html",
    "title": "Jupyter notebooks",
    "section": "",
    "text": "Our Jupyter notebooks are designed to showcase the capabilities and features of the ValidMind platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nOur notebooks cover a variety of use cases, ranging from a basic introductory one fpr data analysis and visualization techniques to more advanced machine learning algorithms and data science workflows for model risk management."
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "Learn how model developers and model validators can get started and collaborate with the ValidMind platform.\n\nThese instructions show you how to:\n\nModel developers — Initialize the developer framework, add documentation, run tests and generate metrics, and send the resulting information to the ValidMind SaaS platform.\nModel validators — Review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements."
  },
  {
    "objectID": "guide/get-started-auditor.html",
    "href": "guide/get-started-auditor.html",
    "title": "ValidMind",
    "section": "",
    "text": "title: “Get started as an auditor”\nThe internal or external auditor is responsible for evaluating the effectiveness of the model risk management process. They are typically independent of the Model Developer and Model Validator and are responsible for ensuring that the model risk management process is well-designed, operating effectively, and meeting regulatory requirements. They provide an objective assessment of the model risk management process to senior management and the board of directors."
  },
  {
    "objectID": "guide/get-started-auditor.html#prerequisites",
    "href": "guide/get-started-auditor.html#prerequisites",
    "title": "ValidMind",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/get-started-auditor.html#steps",
    "href": "guide/get-started-auditor.html#steps",
    "title": "ValidMind",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/get-started-auditor.html#troubleshooting",
    "href": "guide/get-started-auditor.html#troubleshooting",
    "title": "ValidMind",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/get-started-auditor.html#conclusion",
    "href": "guide/get-started-auditor.html#conclusion",
    "title": "ValidMind",
    "section": "Conclusion",
    "text": "Conclusion\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/get-started-validator.html",
    "href": "guide/get-started-validator.html",
    "title": "Get started as a validator",
    "section": "",
    "text": "The model validator is responsible for assessing the accuracy and appropriateness of the model. They are typically independent of the Model Developer and are responsible for ensuring that the model is fit-for-purpose, accurate, and consistent with the business requirements. They also ensure that the assumptions and limitations of the model are understood and documented."
  },
  {
    "objectID": "guide/get-started-validator.html#prerequisites",
    "href": "guide/get-started-validator.html#prerequisites",
    "title": "Get started as a validator",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/get-started-validator.html#steps",
    "href": "guide/get-started-validator.html#steps",
    "title": "Get started as a validator",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/get-started-validator.html#troubleshooting",
    "href": "guide/get-started-validator.html#troubleshooting",
    "title": "Get started as a validator",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/get-started-validator.html#whats-next",
    "href": "guide/get-started-validator.html#whats-next",
    "title": "Get started as a validator",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our developer framework, including:\n\nThe ValidMind Python library API\nThe ValidMind model API\nThe ValidMind test plan API"
  },
  {
    "objectID": "guide/release-pilot.html",
    "href": "guide/release-pilot.html",
    "title": "Pilot access",
    "section": "",
    "text": "Welcome to the Pilot Release for ValidMind. On this page you will find important information to help you get started."
  },
  {
    "objectID": "guide/release-pilot.html#access-urls",
    "href": "guide/release-pilot.html#access-urls",
    "title": "Pilot access",
    "section": "Access URLs",
    "text": "Access URLs\n\n\n\nService\nURL\n\n\n\n\nSaaS login\nhttps://app.dev.vm.validmind.ai/"
  },
  {
    "objectID": "guide/release-pilot.html#users-roles",
    "href": "guide/release-pilot.html#users-roles",
    "title": "Pilot access",
    "section": "Users & roles",
    "text": "Users & roles\nTo ensure the security of our services, user names and passwords are supplied separately. If you do not already have this information, please contactsupport@validmind.ai.\nValidMind supports user roles for:\n\nmodel developers\nmodel validators\nplatform administrators\n\nFor the pilot, the initial roles will have been set up for you already. In the future, we will allow you to manage users and roles yourself."
  },
  {
    "objectID": "guide/release-pilot.html#prerequisites",
    "href": "guide/release-pilot.html#prerequisites",
    "title": "Pilot access",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to access ValidMind from your company network, the following prerequisits apply:\n\n\nTo use the developer framework and to access the SaaS user interface, you must add the validmind.ai domain to your allowlist (whitelist).\nIf you require VPC access from your company network to ValidMind, configure AWS PrivateLink."
  },
  {
    "objectID": "guide/release-pilot.html#related-topics",
    "href": "guide/release-pilot.html#related-topics",
    "title": "Pilot access",
    "section": "Related topics",
    "text": "Related topics\nReady to try out ValidMind? Get started."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support page is designed to provide you with quick and easy access to the resources you need to troubleshoot technical issues, find answers to frequently asked questions, and get the most out of our ValidMind platform."
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? You can also email support@validmind.ai to get more help."
  },
  {
    "objectID": "guide/support.html#troubleshooting",
    "href": "guide/support.html#troubleshooting",
    "title": "Support",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n(Future troubleshooting content.)"
  },
  {
    "objectID": "guide/release-notes.html",
    "href": "guide/release-notes.html",
    "title": "Release notes",
    "section": "",
    "text": "(Future curated release highlights)"
  },
  {
    "objectID": "guide/release-notes.html#changelog",
    "href": "guide/release-notes.html#changelog",
    "title": "Release notes",
    "section": "Changelog",
    "text": "Changelog\n(Future generated changelog)"
  },
  {
    "objectID": "guide/release-notes.html#bug-fixes",
    "href": "guide/release-notes.html#bug-fixes",
    "title": "Release notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n(Future bugs we fixed)"
  },
  {
    "objectID": "guide/release-notes.html#known-issues",
    "href": "guide/release-notes.html#known-issues",
    "title": "Release notes",
    "section": "Known issues",
    "text": "Known issues\n(Future known issues our users should hear about)"
  },
  {
    "objectID": "guide/configure-workflows.html",
    "href": "guide/configure-workflows.html",
    "title": "Configure workflows",
    "section": "",
    "text": "As part of the initial setup of the ValidMind solution for your company or organization, you need to configure the validation workflows that model developers and model validators will follow in the web interface."
  },
  {
    "objectID": "guide/configure-workflows.html#prerequisites",
    "href": "guide/configure-workflows.html#prerequisites",
    "title": "Configure workflows",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/configure-workflows.html#steps",
    "href": "guide/configure-workflows.html#steps",
    "title": "Configure workflows",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/configure-workflows.html#troubleshooting",
    "href": "guide/configure-workflows.html#troubleshooting",
    "title": "Configure workflows",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/configure-workflows.html#whats-next",
    "href": "guide/configure-workflows.html#whats-next",
    "title": "Configure workflows",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/create-project.html",
    "href": "guide/create-project.html",
    "title": "Create a project",
    "section": "",
    "text": "These steps show you how you can create a new project in the ValidMind SaaS platforms. You can use this new project to upload tests and documentation to the ValidMind SaaS platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-project.html#prerequisites",
    "href": "guide/create-project.html#prerequisites",
    "title": "Create a project",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/create-project.html#steps",
    "href": "guide/create-project.html#steps",
    "title": "Create a project",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/create-project.html#troubleshooting",
    "href": "guide/create-project.html#troubleshooting",
    "title": "Create a project",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/create-project.html#whats-next",
    "href": "guide/create-project.html#whats-next",
    "title": "Create a project",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/developer-framework.html",
    "href": "guide/developer-framework.html",
    "title": "Developer framework",
    "section": "",
    "text": "The ValidMind developer framework allows model developers and validators to automatically document different aspects of the model development lifecycle.\nThis Python library provides the following high level features:"
  },
  {
    "objectID": "guide/developer-framework.html#installing-the-client-library",
    "href": "guide/developer-framework.html#installing-the-client-library",
    "title": "Developer framework",
    "section": "Installing the client library",
    "text": "Installing the client library\nThe Python library can be installed with the following command:\npip install validmind"
  },
  {
    "objectID": "guide/developer-framework.html#initializing-the-client-library",
    "href": "guide/developer-framework.html#initializing-the-client-library",
    "title": "Developer framework",
    "section": "Initializing the client library",
    "text": "Initializing the client library\nEvery validation project has a project identifier that allows the client library to associate documentation and tests with the appropriate project. In order to initialize the client, we need to provide the following arguments:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nLocation of the ValidMind API.\n\n\napi_key\nAccount API key.\n\n\napi_secret\nAccount Secret key.\n\n\nproject\nThe project identifier.\n\n\n\nThe following code snippet shows how to initialize a ValidMind client instance:\nimport validmind as vm\n\nvm.init(\n  api_host = \"<API_HOST>\",\n  api_key = \"<API_KEY>\",\n  api_secret = \"<API_SECRET>\",\n  project = \"<PROJECT_ID>\"\n)"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\ncom.amazonaws.vpce.us-west-2.vpce-svc-0b956fa3e03afa538\nprivate.prod.vm.validmind.ai"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the developer framework.\n\n\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink."
  },
  {
    "objectID": "guide/glossary.html",
    "href": "guide/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "developer framework \n\nValidMind’s set of tools, guidelines, and standards designed to provide developers with a structured approach to documenting risk models. Includes our Python library, API endpoints, and other software components that simplify the model development process. Also see Saas Platform.\n\nfirst line of defense\n\nResponsible for developing risk models. Includes the model owners, developers, and users who ensure that the models are developed and documented appropriately and that associated risks are identified, assessed, and managed effectively. The first line of defense is accountable for implementing effective model risk management practices at the business unit level and plays a critical role in identifying and mitigating potential risks associated with models.\n\nmodel developer\n\nResponsible for developing, implementing, and maintaining risk models. Model developers are typically subject matter experts in their domain and are responsible for ensuring that the model is fit-for-purpose, accurate, and aligned with the business requirements.\n\n\nThe model developer is responsible for designing, building, and refining models to ensure their accuracy, reliability, and compliance with regulatory requirements in the context of model risk management. This involves selecting appropriate algorithms, managing data quality, addressing biases, and testing model performance across various scenarios. The developer also plays a crucial role in communicating model assumptions, limitations, and validation results to stakeholders, while continuously seeking opportunities for improvement and staying abreast of evolving industry standards and best practices.\n\nmodel risk management (MRM)\n\nThe process of identifying, assessing, and controlling risks associated with models used in financial institutions. Governance and risk management functions oversee the MRM process and ensure compliance with policies.\n\nmodel risk\n\nThe potential for financial loss, incorrect decisions or unintended consequences resulting from errors or inaccuracies in financial models.\n\nmodel governance\n\nThe policies, procedures, and controls put in place by financial institutions to manage and monitor the use of models.\n\nmonitoring\n\nThe process of identifying model performance issues, detecting model drift, and ensuring the continued accuracy and relevance of the model.\n\nproject\n\n\n\nSaaS platform \n\nValidMind’s hosted multi-tenant architecture that includes the cloud-based web interface, APIs, databases, documentation and validation engine, and various internal services. Also see developer framework.\n\nsecond line of defense\n\nResponsible for validating and challenging the risk models developed and used by the first line of defense. Ensures compliance with applicable regulations and guidelines, and monitors the effectiveness of the first line’s model risk management (MRM) practices and provides guidance to enhance the first line’s understanding of model risk management.\n\nvalidation, model validation\n\nThe process of evaluating and testing models to ensure that they are accurate, reliable, and effective with the help of our platform."
  },
  {
    "objectID": "guide/about.html",
    "href": "guide/about.html",
    "title": "About",
    "section": "",
    "text": "This section contains information about product compliance and third-party software dependencies."
  },
  {
    "objectID": "guide/about.html#compliance",
    "href": "guide/about.html#compliance",
    "title": "About",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/about.html#software-dependencies",
    "href": "guide/about.html#software-dependencies",
    "title": "About",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/manage-users-and-roles.html",
    "href": "guide/manage-users-and-roles.html",
    "title": "Manage users and roles",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#prerequisites",
    "href": "guide/manage-users-and-roles.html#prerequisites",
    "title": "Manage users and roles",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#add-users",
    "href": "guide/manage-users-and-roles.html#add-users",
    "title": "Manage users and roles",
    "section": "Add users",
    "text": "Add users\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#delete-users",
    "href": "guide/manage-users-and-roles.html#delete-users",
    "title": "Manage users and roles",
    "section": "Delete users",
    "text": "Delete users\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#update-user-roles",
    "href": "guide/manage-users-and-roles.html#update-user-roles",
    "title": "Manage users and roles",
    "section": "Update user roles",
    "text": "Update user roles\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/get-started-developer.html",
    "href": "guide/get-started-developer.html",
    "title": "Get started as a developer",
    "section": "",
    "text": "Explore our sample Jupyter notebook to learn how a model developer can:"
  },
  {
    "objectID": "guide/get-started-developer.html#whats-next",
    "href": "guide/get-started-developer.html#whats-next",
    "title": "Get started as a developer",
    "section": "What’s Next",
    "text": "What’s Next\n\nWant to try out the model yourself? You can get the source."
  },
  {
    "objectID": "guide/Initialize-developer-framework.html",
    "href": "guide/Initialize-developer-framework.html",
    "title": "Initialize the developer framework",
    "section": "",
    "text": "These steps show how a model developer can:"
  },
  {
    "objectID": "guide/Initialize-developer-framework.html#prerequisites",
    "href": "guide/Initialize-developer-framework.html#prerequisites",
    "title": "Initialize the developer framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to initialize the developer framework and to be able to upload to the ValidMind SaaS platform, you must provide the following information:\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nFor existing projects, this information can be found in the SaaS UI:\n\nGo to the Projects page and select the project.\nClick CLIENT INTEGRATION and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe developer framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind SaaS platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/Initialize-developer-framework.html#initialize-the-developer-framework",
    "href": "guide/Initialize-developer-framework.html#initialize-the-developer-framework",
    "title": "Initialize the developer framework",
    "section": "Initialize the developer framework",
    "text": "Initialize the developer framework\n\nInstall the client library:\npip install validmind\nInitialize the client library:\nPaste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"<project-identifier>\"\n)\n\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the client library should initialize and successfully register with ValidMind when executed the client library can be used for documenting and testing your models, and to upload to the ValidMind SaaS platform."
  },
  {
    "objectID": "guide/Initialize-developer-framework.html#add-documentation",
    "href": "guide/Initialize-developer-framework.html#add-documentation",
    "title": "Initialize the developer framework",
    "section": "Add documentation",
    "text": "Add documentation\nTBD"
  },
  {
    "objectID": "guide/Initialize-developer-framework.html#run-tests-and-generate-metrics",
    "href": "guide/Initialize-developer-framework.html#run-tests-and-generate-metrics",
    "title": "Initialize the developer framework",
    "section": "Run tests and generate metrics",
    "text": "Run tests and generate metrics\nTBD"
  },
  {
    "objectID": "guide/Initialize-developer-framework.html#upload-to-the-validmind-saas-platform",
    "href": "guide/Initialize-developer-framework.html#upload-to-the-validmind-saas-platform",
    "title": "Initialize the developer framework",
    "section": "Upload to the ValidMind SaaS platform",
    "text": "Upload to the ValidMind SaaS platform\nTBD"
  },
  {
    "objectID": "guide/Initialize-developer-framework.html#whats-next",
    "href": "guide/Initialize-developer-framework.html#whats-next",
    "title": "Initialize the developer framework",
    "section": "What’s Next",
    "text": "What’s Next\nTO DO Switch to web user interface to track your first upload to the SaaS platform."
  },
  {
    "objectID": "guide/generate-validation-reports.html",
    "href": "guide/generate-validation-reports.html",
    "title": "Generate validation reports",
    "section": "",
    "text": "These steps show you how to generate final validation reports as part of model lifecycle management."
  },
  {
    "objectID": "guide/generate-validation-reports.html#prerequisites",
    "href": "guide/generate-validation-reports.html#prerequisites",
    "title": "Generate validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/generate-validation-reports.html#steps",
    "href": "guide/generate-validation-reports.html#steps",
    "title": "Generate validation reports",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/generate-validation-reports.html#troubleshooting",
    "href": "guide/generate-validation-reports.html#troubleshooting",
    "title": "Generate validation reports",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/generate-validation-reports.html#whats-next",
    "href": "guide/generate-validation-reports.html#whats-next",
    "title": "Generate validation reports",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "validmind/api.html",
    "href": "validmind/api.html",
    "title": "ValidMind",
    "section": "",
    "text": "Python Library API\nMain entrypoint to the ValidMind Python Library\n\nvalidmind.init(project, api_key=None, api_secret=None, api_host=None)\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\n\nParameters\n\nproject (str) – The project CUID\napi_key (str, optional) – The API key. Defaults to None.\napi_secret (str, optional) – The API secret. Defaults to None.\napi_host (str, optional) – The API host. Defaults to None.\n\nRaises\nValueError – If the API key and secret are not provided\nReturns\nTrue if the ping was successful\nReturn type\nbool\n\n\n\nvalidmind.init_dataset(dataset: DataFrame, type: str = ‘training’, options: dict | None = None, targets: DatasetTargets | None = None, target_column: str | None = None, class_labels: dict | None = None)\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\nRaises\nValueError – If the dataset type is not supported\nReturns\nA VM Dataset instance\nReturn type\nvm.vm.Dataset\n\n\n\nvalidmind.init_model(model: object)\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained sklearn model\nRaises\nValueError – If the model type is not supported\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.init_r_model(model_path: str, model_type: str)\nInitializes a VM Model for an R model\nR models must be saved to disk and the filetype depends on the model type… Currently we support the following model types:\n\n\nLogisticRegression glm model in R: saved as an RDS file with saveRDS\n\n\n\n\nLinearRegression lm model in R: saved as an RDS file with saveRDS\n\n\n\n\nXGBClassifier: saved as a .json or .bin file with xgb.save\n\n\n\n\nXGBRegressor: saved as a .json or .bin file with xgb.save\n\n\nLogisticRegression and LinearRegression models are converted to sklearn models by extracting the coefficients and intercept from the R model. XGB models are loaded using the xgboost since xgb models saved in .json or .bin format can be loaded directly with either Python or R\n\nParameters\n\nmodel_path (str) – The path to the R model saved as an RDS or XGB file\nmodel_type (str) – The type of the model (one of R_MODEL_TYPES)\n\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.run_test_plan(test_plan_name, send=True, **kwargs)\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘sklearn_classifier’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises\nValueError – If the test plan name is not found or if there is an error initializing the test plan\nReturns\nA dictionary of test results\nReturn type\ndict\n\n\n\nvalidmind.log_dataset(vm_dataset)\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\nvm_dataset (validmind.VMDataset) – A VM dataset object\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options (dict, optional) – Additional dataset options for analysis. Defaults to None.\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset. Defaults to None.\nfeatures (list, optional) – Optional. A list of features metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nThe VMDataset object\nReturn type\nvalidmind.VMDataset\n\n\n\nvalidmind.log_figure(data_or_path, key, metadata, run_cuid=None)\nLogs a figure\n\nParameters\n\ndata_or_path (str or matplotlib.figure.Figure) – The path of the image or the data of the plot\nkey (str) – Identifier of the figure\nmetadata (dict) – Python data structure\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metadata(content_id, text=None, extra_json=None)\nLogs free-form metadata to ValidMind API.\n\nParameters\n\ncontent_id (str) – Unique content identifier for the metadata\ntext (str, optional) – Free-form text to assign to the metadata. Defaults to None.\nextra_json (dict, optional) – Free-form key-value pairs to assign to the metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metrics(metrics, run_cuid=None)\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics (list) – A list of Metric objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_model(vm_model)\nLogs model metadata and hyperparameters to ValidMind API.\n\nParameters\nvm_model (validmind.VMModel) – A VM model object\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_test_results(results, run_cuid=None, dataset_type=‘training’)\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults (list) – A list of TestResults objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nclass validmind.Dataset(raw_dataset: object, fields: list, sample: list, shape: dict, correlation_matrix: object | None = None, correlations: dict | None = None, type: str | None = None, options: dict | None = None, statistics: dict | None = None, targets: dict | None = None, target_column: str = ’’, class_labels: dict | None = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: object | None = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.DatasetTargets(target_column: str, description: str | None = None, class_labels: dict | None = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.Figure(key: str, metadata: dict, figure: object, extras: dict | None = None)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.Metric(test_context: TestContext, params: dict | None = None, result: TestPlanMetricResult | None = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.Model(attributes: ModelAttributes | None = None, task: str | None = None, subtask: str | None = None, params: dict | None = None, model_id: str = ‘main’, model: object | None = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.ModelAttributes(architecture: str | None = None, framework: str | None = None, framework_version: str | None = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.TestResult(*, test_name: str | None = None, column: str | None = None, passed: bool | None = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: str | Non )\n\n\ncolumn(: str | Non )\n\n\npassed(: bool | Non )\n\n\nvalues(: dic )\n\n\n\nclass validmind.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.ThresholdTest(test_context: TestContext, params: dict | None = None, test_results: TestResults | None = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/vm_models.html",
    "href": "validmind/vm_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Models\nModels entrypoint\n\nclass validmind.vm_models.Dataset(raw_dataset: object, fields: list, sample: list, shape: dict, correlation_matrix: object | None = None, correlations: dict | None = None, type: str | None = None, options: dict | None = None, statistics: dict | None = None, targets: dict | None = None, target_column: str = ’’, class_labels: dict | None = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: object | None = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.vm_models.DatasetTargets(target_column: str, description: str | None = None, class_labels: dict | None = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.vm_models.Figure(key: str, metadata: dict, figure: object, extras: dict | None = None)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Metric(test_context: TestContext, params: dict | None = None, result: TestPlanMetricResult | None = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.vm_models.MetricResult(type: str, scope: str, key: dict, value: dict | list | DataFrame, value_formatter: str | None = None)\nBases: object\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\ntype(: st )\n\n\nscope(: st )\n\n\nkey(: dic )\n\n\nvalue(: dict | list | DataFram )\n\n\nvalue_formatter(: str | Non _ = Non_ )\n\n\nserialize()\nSerializes the Metric to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Model(attributes: ModelAttributes | None = None, task: str | None = None, subtask: str | None = None, params: dict | None = None, model_id: str = ‘main’, model: object | None = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.vm_models.ModelAttributes(architecture: str | None = None, framework: str | None = None, framework_version: str | None = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.vm_models.TestContext(dataset: Dataset | None = None, model: Model | None = None, train_ds: Dataset | None = None, test_ds: Dataset | None = None, y_train_predict: object | None = None, y_test_predict: object | None = None, context_data: dict | None = None)\nBases: object\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as model predictions, shared dataset metrics, etc.\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\ncontext_data(: dic _ = Non_ )\n\n\nset_context_data(key, value)\n\n\nget_context_data(key)\n\n\n\nclass validmind.vm_models.TestContextUtils()\nBases: object\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\ntest_context(: TestContex )\n\n\nproperty dataset()\n\n\nproperty model()\n\n\nproperty train_ds()\n\n\nproperty test_ds()\n\n\nproperty y_train_predict()\n\n\nproperty y_test_predict()\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\nproperty df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame\n\n\n\nclass validmind.vm_models.TestPlan(config: {} = None, test_context: TestContext = None, dataset: Dataset = None, model: Model = None, train_ds: Dataset = None, test_ds: Dataset = None, pbar: tqdm = None)\nBases: object\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\nname(: ClassVar[str )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\ntests(: ClassVar[List[object] _ = [_ )\n\n\ntest_plans(: ClassVar[List[object] _ = [_ )\n\n\nresults(: ClassVar[List[TestPlanResult] _ = [_ )\n\n\nconfig(: { _ = Non_ )\n\n\ntest_context(: TestContex _ = Non_ )\n\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\npbar(: tqd _ = Non_ )\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\nget_config_params_for_test(test_name)\nReturns the config for a given test, if it exists. The config attribute is a dictionary where the keys are the test names and the values are dictionaries of config values for that test.\nThe key in the config must match the name of the test, i.e. for a test called “time_series_univariate_inspection_raw” we could pass a config like this:\n{\n“time_series_univariate_inspection_raw”: {\n\n    “columns”: [“col1”, “col2”]\n\n}\n}\n\n\nrun(send=True)\nRuns the test plan\n\n\nlog_results()\nLogs the results of the test plan to ValidMind\nThis method will be called after the test plan has been run and all results have been collected. This method will log the results to ValidMind.\n\n\nsummarize()\nSummarizes the results of the test plan\nThis method will be called after the test plan has been run and all results have been logged to ValidMind. It will summarize the results of the test plan by creating an html table with the results of each test. This html table will be displayed in an VS Code, Jupyter or other notebook environment.\n\n\n\nclass validmind.vm_models.TestPlanDatasetResult(dataset: Dataset | None = None)\nBases: TestPlanResult\nResult wrapper for datasets that run as part of a test plan\n\ndataset(: Datase _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanMetricResult(figures: List[Figure] | None = None, metric: MetricResult | None = None)\nBases: TestPlanResult\nResult wrapper for metrics that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\nmetric(: MetricResult | Non _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanModelResult(model: Model | None = None)\nBases: TestPlanResult\nResult wrapper for models that run as part of a test plan\n\nmodel(: Mode _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanTestResult(figures: List[Figure] | None = None, test_results: TestResults | None = None)\nBases: TestPlanResult\nResult wrapper for test results produced by the tests that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestResult(*, test_name: str | None = None, column: str | None = None, passed: bool | None = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: str | Non )\n\n\ncolumn(: str | Non )\n\n\npassed(: bool | Non )\n\n\nvalues(: dic )\n\n\n\nclass validmind.vm_models.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.vm_models.ThresholdTest(test_context: TestContext, params: dict | None = None, test_results: TestResults | None = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/model_validation_tests_sklearn.html",
    "href": "validmind/model_validation_tests_sklearn.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions models trained with sklearn or that provide a sklearn-like API\n\n\n\nBases: Metric\nAccuracy Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nCharacteristic Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculates PSI for each of the dataset features\n\n\n\n\n\n\n\nBases: Metric\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nF1 Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPermutation Feature Importance\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPrecision Recall Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPrecision Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nRecall Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nROC AUC Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nROC Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: TestContextUtils\nSHAP Global Importance. Custom metric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBases: Metric\nPopulation Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the accuracy score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the F1 score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the ROC AUC score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the training set metrics are better than the test set metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "validmind/readme.html",
    "href": "validmind/readme.html",
    "title": "ValidMind",
    "section": "",
    "text": "pip install validmind\n\n\npip install validmind[r-support]\n\n\n\n\n\n\n\n\nEnsure you have poetry installed: https://python-poetry.org/\nAfter cloning this repo run:\n\npoetry shell\npoetry install\n\n\n\nIf you want to use the R support that is provided by the ValidMind Developer Framework, you must have R installed on your machine. You can download R from https://cran.r-project.org/. If you are on a Mac, you can install R using Homebrew:\nbrew install r\nOnce you have R installed, you can install the r-support extra to install the necessary dependencies for R by running:\npoetry install --extras r-support\n\n\n\nMake sure you bump the package version before merging a PR with the following command:\nmake version tag=patch\nThe value of tag corresponds to one of the options provided by Poetry: https://python-poetry.org/docs/cli/#version\n\n\n\n\nIf you want to integate the validmind package to your development environment, you must build the package wheel first, since we have not pushed the package to a public PyPI repository yet. Steps:\n\nRun make build to build a new Python package for the developer framework\nThis will create a new wheel file in the dist folder\nRun pip install <path-to-wheel> to install the newly built package in your environment\n\n\n\n\nAPI documentation can be generated in Markdown or HTML format. Our documentation pipeline uses Markdown documentation before generating the final HTML assets for the documentation site.\nFor local testing, HTML docs can be generated with Sphinx. Note that the output template is different since the documentation pipeline uses the source Markdown files for the final HTML output.\nMarkdown and HTML docs can be generated with the following commands:\n# Navigate to the docs folder\ncd docs/\n\n# Generate Markdown docs\nmake markdown\n\n# Generate HTML docs\nmake html\nThe resulting markdown and html under docs/_build folders will contain the generated documentation.\n\n\n\n\n\nIf you run into an error related to the ValidMind wheel, try:\npoetry add wheel\npoetry update wheel\npoetry install\nIf there are lightgbm errors partway through, run remove lightgbm, followed by poetry update wheel and poetry install."
  },
  {
    "objectID": "validmind/test_plans.html",
    "href": "validmind/test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "Test Plans entry point\n\n\nReturns a list of all available test plans\n\n\n\nReturns a list of all available tests\n\n\n\nReturns the test plan by name\n\n\n\nReturns a description of the test plan\n\n\n\nTest plan for sklearn classifier models\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan for sklearn classifier metrics\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models that includes both metrics and validation tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest plan for tabular datasets\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for generic tabular datasets"
  },
  {
    "objectID": "validmind/index.html",
    "href": "validmind/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Developer Framework\n\nValidMind Python Client\n\nInstallation\nContributing to ValidMind Developer Framework\nIntegrating the ValidMind Developer Framework to your development environment\nGenerating Docs\nKnown Issues\n\nPython Library API\nCore Library Tests\n\nData Validation Tests\n\nCore Library Tests\n\nModel Validation Tests for SKLearn-Compatible Models\n\nTest Plans\n\nlist_plans()\nlist_tests()\nget_by_name()\ndescribe_plan()\nTest Plans for SKLearn-Compatible Classifiers\nTest Plans for Tabular Datasets\n\nValidMind Models"
  },
  {
    "objectID": "validmind/data_validation_tests.html",
    "href": "validmind/data_validation_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions for any Pandas-compatible datasets\n\n\n\nBases: TestContextUtils\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadat is necessary to initialize dataset object that can be related to different metrics and test results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function\n\n\n\n\nBases: Metric\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the minority class does not represent more than a threshold of the total number of examples\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of duplicates is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique values in a column is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the Pearson correlation between two columns is less than a threshold\nInspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the skewness of a column is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique rows is greater than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of zeros is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Find all the information you need to use our platform for model risk management (MRM)\n\n\n\n\n    \n    \n    Column Cards\n    \n\n\n    \n        \n            \n                Model developers\n                Collect, manage, and automate your model documentation and testing with our developer framework.\n                Start\n            \n            \n                Model validators\n                Review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements.\n                Start\n            \n        \n    \n    \n\n\n\n\n\n\n    Three-Column Table Example\n    \n\n\n    \n        \n            \n                Join our community\n                Training\n                Support\n            \n            \n                Adopt open-source notebooks for your use case or become a contributor.\n                Learn more about effective model risk management with the ValidMind platform.\n                Need some help? Try our self-service documentation or email us at support@validmind.com.\n            \n            \n                Jupyter Notebooks\n                The three lines of defense\n                Troubleshooting documentation\n            \n            \n                Developer Framework"
  },
  {
    "objectID": "notebooks/passing_data.html",
    "href": "notebooks/passing_data.html",
    "title": "ValidMind",
    "section": "",
    "text": "In this notebook we will demonstrate how to make use of the test context to pass data between tests. Any object can be passed to the context, but we will use a simple dictionary to demonstrate the concept.\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  # Use your project ID\n  project = \"...\"\n)\n  \n\nTrue\n\n\n\n\nWe will build two simple metrics to demonstrate the concept. The first metric will create a sample dataframe with mock data, and the second will add one more column to it.\n\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MyFirstMetric(Metric):\n    type = \"dataset\"\n    key = \"my_first_metric\"\n\n    def run(self):\n        df = pd.DataFrame({\"column_a\": [1, 2, 3, 4, 5]})\n        # Store the dataframe in the test context\n        self.test_context.set_context_data(\"some_dataset\", df)\n\n        return self.cache_results(df.to_dict(\"records\"))\n\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MySecondMetric(Metric):\n    type = \"dataset\"\n    key = \"my_second_metric\"\n\n    def run(self):\n        # Get the dataframe from the test context. We can\n        # throw an error if it doesn't exist just to be sure\n        df = self.test_context.get_context_data(\"some_dataset\")\n        if df is None:\n            raise ValueError(\"'some_dataset' not found in test context\")\n        \n        new_df = df.copy()\n        new_df[\"column_b\"] = [5, 4, 3, 2, 1]\n\n        return self.cache_results(new_df.to_dict(\"records\"))\n\nNow let’s define a test plan that will run the two metrics in sequence. We need to make sure that the first metric is run before the second one.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MyFirstMetric, MySecondMetric]\n\nmy_custom_test_plan = MyCustomTestPlan()\nmy_custom_test_plan.run()\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        my_first_metric\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'column_a': 1}, {'column_a': 2}, {'column_a': 3}, {'column_a': 4}, {'column_a': 5}]\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        my_second_metric\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'column_a': 1, 'column_b': 5}, {'column_a': 2, 'column_b': 4}, {'column_a': 3, 'column_b': 3}, {'column_a': 4, 'column_b': 2}, {'column_a': 5, 'column_b': 1}]"
  },
  {
    "objectID": "notebooks/lending_club.html",
    "href": "notebooks/lending_club.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\ndf = pd.read_pickle(\"notebooks/datasets/_temp/df_loans_cleaned.pickle\")\n\ntargets = vm.DatasetTargets(\n    target_column=\"loan_status\",\n    class_labels={\n        \"Fully Paid\": \"Fully Paid\",\n        \"Charged Off\": \"Charged Off\",\n    }\n)\n\nvm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nTrue\n\n\n\nresults = vm.run_dataset_tests(df, target_column=\"loan_status\", dataset_type=\"training\", vm_dataset=vm_dataset, send=True)\n\nRunning data quality tests for \"training\" dataset...\n\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74.72it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\n\n\n\n\n\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest                 Passed      # Passed    # Errors    % Passed\n-------------------  --------  ----------  ----------  ----------\nclass_imbalance      True               1           0         100\nduplicates           False              0           1           0\ncardinality          False             14           7     66.6667\nmissing              False             25          53     32.0513\npearson_correlation  False              0          10           0\nskewness             False              3           6     33.3333\nzeros                False              1           3          25\n\n\n\n\ntrain_ds, val_ds = train_test_split(df, test_size=0.20)\n\nx_train = train_ds.drop(\"loan_status\", axis=1)\nx_val = val_ds.drop(\"loan_status\", axis=1)\ny_train = train_ds.loc[:, \"loan_status\"].astype(str)\ny_val = val_ds.loc[:, \"loan_status\"].astype(str)\n\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\nxgb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = xgb_model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nvm.log_model(xgb_model)"
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.staging.validmind.ai/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"cl1jyv16o000809lg98gi9tie\"\n)\n  \n\nTrue\n\n\n\n\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nWe can now initialize the TabularDataset test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n                                                                                                                           \n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        training\n                    \n                \n                \n                    Metric Value\n                    \n                        [[{'field': 'CreditScore', 'value': 1.0}, {'field': 'Geography', 'value': 0.010103440458197478}, {'field': 'Gender', 'value': 0.008251776778083898}, {'field': 'Age', 'value': -0.007269780957496768}, {'field': 'Tenure', 'value': -0.006914675142663373}, {'field': 'NumOfProducts', 'value': 0.005677094521946256}, {'field': 'HasCrCard', 'value': -0.009291152528707963}, {'field': 'IsActiveMember', 'value': 0.030554141043824444}, {'field': 'Exited', 'value': -0.025533166369817405}], [{'field': 'CreditScore', 'value': 0.010103440458197478}, {'field': 'Geography', 'value': 1.0}, {'field': 'Gender', 'value': 0.035023152881466464}, {'field': 'Age', 'value': 0.053602289473512775}, {'field': 'Tenure', 'value': 0.015510338111733172}, {'field': 'NumOfProducts', 'value': 0.011118424429054087}, {'field': 'HasCrCard', 'value': 0.021747611293409512}, {'field': 'IsActiveMember', 'value': 0.02017951122934769}, {'field': 'Exited', 'value': 0.1784101181767361}], [{'field': 'CreditScore', 'value': 0.008251776778083898}, {'field': 'G...\n                    \n                \n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n        \n        \n\n\n\n        \n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Surname', passed=False, values={'n_distinct': 2616, 'p_distinct': 0.327}), TestResult(test_name=None, column='Geography', passed=True, values={'n_distinct': 3, 'p_distinct': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389459}]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CustomerId', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Geography', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'skewness': -0.005920679739677088}), TestResult(test_name=None, column='CustomerId', passed=True, values={'skewness': 0.010032280260684402}), TestResult(test_name=None, column='CreditScore', passed=True, values={'skewness': -0.06195161237091896}), TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.0245221429799511}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.007692043774702702}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111804}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077728})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='CustomerId', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_unique': 2616, 'p_unique': 0.327}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_unique': 452, 'p_unique': 0.0565}), TestResult(test_name=None, column='Geography', passed=True, values={'n_unique': 3, 'p_unique': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                              Name                          Description                                      \n\n\nsklearn_classifier_metrics      SKLearnClassifierMetrics      Test plan for sklearn classifier metrics         \nsklearn_classifier_validation   SKLearnClassifierPerformance  Test plan for sklearn classifier models          \nsklearn_classifier              SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                  \ntabular_dataset                 TabularDataset                Test plan for generic tabular datasets           \ntabular_dataset_description     TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                  \ntabular_data_quality            TabularDataQuality            Test plan for data quality on tabular datasets   \nnormality_test_plan             NormalityTestPlan             Test plan to perform normality tests.            \nautocorrelation_test_plan       AutocorrelationTestPlan       Test plan to perform autocorrelation tests.      \nseasonality_test_plan           SesonalityTestPlan            Test plan to perform seasonality tests.          \nunit_root_test_plan             UnitRootTestPlan              Test plan to perform unit root tests.            \nstationarity_test_plan          StationarityTestPlan          Test plan to perform stationarity tests.         \ntimeseries_test_plan            TimeSeriesTestPlan            Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                  \ntimeseries_univariate_inspectionTimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                                         Name                                   Description                                                               \n\n\nCustom Test  dataset_metadata                           DatasetMetadata                        Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadat is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                           \nCustom Test  model_metadata                             ModelMetadata                          Custom class to collect the following metadata for a model:\n    - Model architecture\n    - Model hyperparameters\n    - Model task type                                                                           \nCustom Test  shap                                       SHAPGlobalImportance                   SHAP Global Importance. Custom metric                                     \nMetric       adf                                        ADFTest                                Augmented Dickey-Fuller unit root test for establishing the order of integration of\n    time series                                                                           \nMetric       accuracy                                   AccuracyScore                          Accuracy Score                                                            \nMetric       box_pierce                                 BoxPierce                              The Box-Pierce test is a statistical test used to determine\n    whether a given set of data has autocorrelations\n    that are different from zero.                                                                           \nMetric       csi                                        CharacteristicStabilityIndex           Characteristic Stability Index between two datasets                       \nMetric       confusion_matrix                           ConfusionMatrix                        Confusion Matrix                                                          \nMetric       dickey_fuller_gls                          DFGLSTest                              Dickey-Fuller GLS unit root test for\n    establishing the order of integration of time series                                                                           \nMetric       dataset_correlations                       DatasetCorrelations                    Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                           \nMetric       dataset_description                        DatasetDescription                     Collects a set of descriptive statistics for a dataset                    \nMetric       f1_score                                   F1Score                                F1 Score                                                                  \nMetric       jarque_bera                                JarqueBera                             The Jarque-Bera test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                           \nMetric       kpss                                       KPSSTest                               Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test for\n    establishing the order of integration of time series                                                                           \nMetric       kolmogorov_smirnov                         KolmogorovSmirnov                      The Kolmogorov-Smirnov metric is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                           \nMetric       ljung_box                                  LJungBox                               The Ljung-Box test is a statistical test used to determine\n    whether a given set of data has autocorrelations\n    that are different from zero.                                                                           \nMetric       lilliefors_test                            Lilliefors                             The Lilliefors test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                           \nMetric       pfi                                        PermutationFeatureImportance           Permutation Feature Importance                                            \nMetric       phillips_perron                            PhillipsPerronTest                     Phillips-Perron (PP) unit root test for\n    establishing the order of integration of time series                                                                           \nMetric       psi                                        PopulationStabilityIndex               Population Stability Index between two datasets                           \nMetric       pr_curve                                   PrecisionRecallCurve                   Precision Recall Curve                                                    \nMetric       precision                                  PrecisionScore                         Precision Score                                                           \nMetric       roc_auc                                    ROCAUCScore                            ROC AUC Score                                                             \nMetric       roc_curve                                  ROCCurve                               ROC Curve                                                                 \nMetric       recall                                     RecallScore                            Recall Score                                                              \nMetric       residuals_visual_inspection                ResidualsVisualInspection              Log plots for visual inspection of residuals                              \nMetric       runs_test                                  RunsTest                               The runs test is a statistical test used to determine whether a given set\n    of data has runs of positive and negative values that are longer than expected\n    under the null hypothesis of randomness.                                                                           \nMetric       seasonal_decompose                         SeasonalDecompose                      Calculates seasonal_decompose metric for each of the dataset features     \nMetric       seasonality_detection_with_acf             SeasonalityDetectionWithACF            Detects seasonality in a time series dataset using ACF and PACF.          \nMetric       shapiro_wilk                               ShapiroWilk                            The Shapiro-Wilk test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                           \nMetric       time_series_univariate_inspection_histogramTimeSeriesUnivariateInspectionHistogramGenerates a visual analysis of time series data by plotting the\n    histogram. The input dataset can have multiple time series if\n    necessary. In this case we produce a separate plot for each time series.                                                                           \nMetric       time_series_univariate_inspection_raw      TimeSeriesUnivariateInspectionRaw      Generates a visual analysis of time series data by plotting the\n    raw time series. The input dataset can have multiple time series\n    if necessary. In this case we produce a separate plot for each time series.                                                                           \nMetric       zivot_andrews                              ZivotAndrewsTest                       Zivot-Andrews unit root test for\n    establishing the order of integration of time series                                                                           \nThresholdTestaccuracy_score                             AccuracyTest                           Test that the accuracy score is above a threshold.                        \nThresholdTestclass_imbalance                            ClassImbalanceTest                     Test that the minority class does not represent more than a threshold\n    of the total number of examples                                                                           \nThresholdTestduplicates                                 DuplicatesTest                         Test that the number of duplicates is less than a threshold               \nThresholdTestf1_score                                   F1ScoreTest                            Test that the F1 score is above a threshold.                              \nThresholdTestcardinality                                HighCardinalityTest                    Test that the number of unique values in a column is less than a threshold\nThresholdTestpearson_correlation                        HighPearsonCorrelationTest             Test that the Pearson correlation between two columns is less than a threshold\n\n    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           \nThresholdTestmissing                                    MissingValuesTest                      Test that the number of missing values is less than a threshold           \nThresholdTestroc_auc_score                              ROCAUCScoreTest                        Test that the ROC AUC score is above a threshold.                         \nThresholdTestskewness                                   SkewnessTest                           Test that the skewness of a column is less than a threshold               \nThresholdTesttraining_test_degradation                  TrainingTestDegradationTest            Test that the training set metrics are better than the test set metrics.  \nThresholdTestunique                                     UniqueRowsTest                         Test that the number of unique rows is greater than a threshold           \nThresholdTestzeros                                      ZerosTest                              Test that the number of zeros is less than a threshold                    \n\n\n\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nWe can now run the SKLearnClassifier test plan:\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)"
  },
  {
    "objectID": "notebooks/library_intro_demos.html",
    "href": "notebooks/library_intro_demos.html",
    "title": "ValidMind",
    "section": "",
    "text": "The ValidMind Python client allows model developers and validators to automatically document different aspects of the model development lifecycle.\nFor modelers, the client provides the following high level features:\n\nLog qualitative data about the model’s conceptual soundness\nLog information about datasets and models\nLog training and evaluation metrics about datasets and models\nRun data quality checks\nRun model evaluation tests\n\nFor validators, the client also provides (TBD) the ability to effectively challenge the model’s performance according to its objective, use case and specific project’s requirements.\n\n\n\nThis notebook and the ValidMind client must be executed on an environment running Python >= 3.8.\n\n\n\n\nWhile we finish the process of making the library publicly accessible pip, it can be installed with the following command that will direct pip to the S3 bucket that contains the latest version of the client.\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv('./env')\n\nTrue\n\n\n\n\n\n\nBefore we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n\nNavigate to the dashboard and click on the “Create new Project” button\nProvide a name and description for the project\nSelect a model use case\nFor modeling objective, we only support automated documentation of Binary Clasification models at the moment\n\nAfter creating the project you will be provided with client library setup instructions. We have provided similar instructions below.\n\n\nEvery validation project in the ValidMind dashboard has an associated project identifier. In order to initialize the client, we need to provide the following arguments:\n\nproject: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for /projects/cl1jyvh2c000909lg1rk0a0zb the project identifier is cl1jyvh2c000909lg1rk0a0zb\napi_host: Location of the ValidMind API. This value is already set on this notebook.\napi_key: Account API key. This can be found in the settings page in the ValidMind dashboard\napi_secret: Account Secret key. Also found in the settings page in the ValidMind dashboard\n\n\n# Lookup your own project id\n# project='cla6walda00001wl6pdzagu9v'\nproject='clar3ppjg000f1gmikrfmkld6'\n\nWe can now initialize the client library with the vm.init function:\n\nimport validmind as vm\n\nvm.init(\n    project=project\n)\n\nTrue\n\n\n\n# Necessary imports for training our demo models\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nAs of version 0.8.x of the client library, the following logging and testing functions are available:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlog_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics\n\n\nrun_dataset_tests\nRuns dataset quality tests on the input dataset\n\n\nanalyze_dataset\nAnalyzes a dataset, computes summary statistics and runs data quality tests. This function combines log_dataset and run_dataset_tests\n\n\nlog_model\nLogs information about a model’s framework, architecture, target objective and training parameters\n\n\nlog_training_metrics\nExtracts and logs training metrics from a pre-trained model\n\n\nevaluate_model\nExtracts metadata and metrics from a train model instances and runs model evaluation tests according to the model objective, use case and specific validation requirements. This function combines log_model, log_training_metrics and an additional set of preconfigured model evaluation tests\n\n\n\nIn the example model training code in this notebook, we will demonstrate each of the documented client library functions.\n\n\nAnalyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n\nDescriptive statistics for numerical and categorical columns\nHistograms and value counts for summarizing distribution of values\nPearson correlation matrix for numerical columns\nCorelation plots for top 15 correlated features\n\nAdditionally, it will run a collection of data quality tests such as:\n\nClass imbalance test on target column\nDuplicate rows and duplicates based on primary key\nHigh cardinality test on categorical columns\nMissing values\nHighly correlated column pairs\nSkewness test\nZeros test (columns with too many zeros)\n\nArguments:\n\ndataset: Input dataset. Only Pandas DataFrames are supported at the moment\ndataset_type: Type of dataset, e.g. training, test, validation. Value needs to be set to training for now\ntargets: vm.DatasetTargets describing the label column and its values\nfeatures: Optional list of properties to specify for some features in the dataset\n\nReturns:\n\nresults: List of data quality test results\n\n\n\n\nLogs the following information about a model:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\nModel performance metrics from training, validation and test dataset\n\nAdditionally, this function runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n\nAccuracy score\nPrecision score\nRecall score\nF1 score\nROC AUC score\nROC AUC curve\nConfusion matrix\nPrecision Recall curve\nPermutation feature importance\nSHAP global importance\n\nArguments:\n\nmodel: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\ntrain_set: Training dataset tuple (x_train, y_train)\nval_set: Validation dataset tuple (x_val, y_val)\ntest_set: Test dataset tuple (x_test, y_test)\n\n\n\n\n\nWe’ll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n# Bank Customer Churn Dataset\nchurn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n\n# Health Insurance Cross-Sell Dataset\ninsurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")\n\n\nchurn_dataset2 = pd.read_csv(\"https://gist.githubusercontent.com/mehdi0501/5b9e64b51ed3bbddbe8f018fc7caf626/raw/ee9b21e5f5308299eb5f4d9dd251bc1b9c5ecc85/churn_test.csv\")\n\n\nchurn_dataset2.head()\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      Surname\n      CreditScore\n      Geography\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      0\n      1\n      15634602\n      Hargrave\n      619\n      France\n      Female\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n    \n    \n      1\n      2\n      15647311\n      Hill\n      608\n      Spain\n      Female\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n    \n    \n      2\n      3\n      15619304\n      Onio\n      502\n      France\n      Female\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n    \n    \n      3\n      4\n      15701354\n      Boni\n      699\n      France\n      Female\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n    \n    \n      4\n      5\n      15737888\n      Mitchell\n      850\n      Spain\n      Female\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n    \n  \n\n\n\n\n\nchurn_dataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        8000 non-null   int64  \n 1   CustomerId       8000 non-null   int64  \n 2   Surname          8000 non-null   object \n 3   CreditScore      8000 non-null   int64  \n 4   Geography        8000 non-null   object \n 5   Gender           8000 non-null   object \n 6   Age              8000 non-null   int64  \n 7   Tenure           8000 non-null   int64  \n 8   Balance          8000 non-null   float64\n 9   NumOfProducts    8000 non-null   int64  \n 10  HasCrCard        8000 non-null   int64  \n 11  IsActiveMember   8000 non-null   int64  \n 12  EstimatedSalary  8000 non-null   float64\n 13  Exited           8000 non-null   int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 875.1+ KB\n\n\n\nchurn_dataset.describe()\n\nWe can now go to Project Overview -> Documentation -> Model Overview and verify this content has been populated on the dashboard.\n\n\nAfter loading the dataset, we can log metadata and summary statistics, and run data quality checks for it using analyze_dataset. Note that the analyze_dataset function expects a targets definition. Additional information about columns can be provided with the features argument.\n\nchurn_targets = vm.DatasetTargets(\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nchurn_features = [\n    {\n        \"id\": \"RowNumber\",\n        \"type_options\": {\n            \"primary_key\": True,\n        }\n    }\n]\n\nanalyze_results = vm.analyze_dataset(\n    dataset=churn_dataset,\n    dataset_type=\"training\",\n    targets=churn_targets,\n    features=churn_features\n)\n\nAnalyzing dataset...\nPandas dataset detected.\nInferring dataset types...\nPreparing in-memory dataset copy...\nCalculating field statistics...\nCalculating feature correlations...\nGenerating correlation plots...\nSuccessfully logged dataset metadata and statistics.\nRunning data quality tests...\nRunning data quality tests for \"training\" dataset...\n\nPreparing dataset for tests...\nPreparing in-memory dataset copy...\n\n\n100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest             Passed      # Passed    # Errors    % Passed\n---------------  --------  ----------  ----------  ----------\nclass_imbalance  True               1           0         100\nduplicates       True               2           0         100\ncardinality      False              6           1     85.7143\nmissing          True              14           0         100\nskewness         False              6           1     85.7143\nzeros            False              0           2           0\n\n\n\nAfter running analyze_dataset, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\nDashboard -> Project Overview -> Documentation -> Data Description\n\n\n\nWe are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance.\n\ndef preprocess_churn_dataset(df):\n    # Drop columns with no correlation to target\n    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n    # Encode binary features\n    genders = {\"Male\": 0, \"Female\": 1}\n    df.replace({\"Gender\": genders}, inplace=True)\n\n    # Encode categorical features\n    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n    df.drop(\"Geography\", axis=1, inplace=True)\n\n    return df\n\n\npreprocessed_churn = preprocess_churn_dataset(churn_dataset)\n\n\ndef train_val_test_split_dataset(df):\n    train_df, test_df = train_test_split(df, test_size=0.20)\n\n    # This guarantees a 60/20/20 split\n    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n    # For training\n    x_train = train_ds.drop(\"Exited\", axis=1)\n    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n    x_val = val_ds.drop(\"Exited\", axis=1)\n    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n    # For testing\n    x_test = test_df.drop(\"Exited\", axis=1)\n    y_test = test_df.loc[:, \"Exited\"].astype(int)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)\n\n\ndef train_churn_dataset(x_train, y_train, x_val, y_val):\n    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n\n    xgb_model.set_params(\n        eval_metric=[\"error\", \"logloss\", \"auc\"],\n    )    \n\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=False,\n    )\n    return xgb_model\n\n\nxgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)\n\n\ndef model_accuracy(model, x, y):\n    y_pred = model.predict_proba(x)[:, -1]\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y, predictions)\n\n    print(f\"Accuracy: {accuracy}\")    \n\n\nmodel_accuracy(xgb_model, x_val, y_val)\n\n\n\n\nFinally, after training our model, we can log its model parameters, collect performance metrics and run model evaluation tests on it using evaluate_model:\n\neval_results = vm.evaluate_model(\n    xgb_model,\n    train_set=(x_train, y_train),\n    val_set=(x_val, y_val),\n    test_set=(x_test, y_test)\n)\n\nAfter running evaluate_model, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability"
  },
  {
    "objectID": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "href": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "title": "ValidMind",
    "section": "",
    "text": "Introduction\n\nExecutive Summary\nBeing able to make accurate and timely estimates of future claims is a fundamental task for actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on understanding future claims, with mortality being one of the central issues facing a life insurer.\nIn this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation.\n\n\nOverview of Mortality Case Study\n\n Case Study Data \nOur dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\nFor the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old.\nMore details on this dataset can be found in Section 2 of the data report https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf\n\n\n Case Study Model \nFor the case study in this paper, we used the statsmodel’s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\nThe  response variable used in this case study is the number of deaths. Policies exposed was used as a weight in the model. We also tried to fit the mortality rate, which is number of deaths/ policies exposed using Gaussian distribution with log link, that can be found in the Appendix\nThe features used in the mortality model are:\n\nAttained Age – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\nDuration – the number of years (starting with a value of one) the policyholder has had the policy.\nSmoking Status – if the policyholder is considered a smoker or not.\nPreferred Class – an underwriting structure used by insurers to classify and price policyholders. Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\nGender – A categorical feature in the model with two levels, male and female.\nGuaranteed Term Period – the length of the policy at issue during which the premium will remain constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\nFace_Amount_Band\nObservation Year\n\n\n\n\n\nSet Up\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport sklearn \nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport os\nimport xgboost as xgb\n\nFirst, let’s download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file.\n\n# directly curl from the SOA website and unzip\n! echo Working Directory = $(pwd)\n! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n! echo \"Done\"\n\nWorking Directory = /Users/andres/code/validmind-sdk/notebooks/insurance_mortality\nData folder already exists\nFile already exists\nDone\n\n\nSecond, sample 5% from the giant file. Another 10 minutes or so the first time you run it :)\n\n#sample 5% and save it out to a sample file\nif not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n    p = 0.05\n    random.seed(42)\n    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv', \n                        skiprows = lambda i: i>0 and random.random() >p)\n    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)\n\n\n\nEDA\n\n# load sample file \nsample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n                               'Face_Amount_Band', 'Preferred_Class', \n                               'Number_Of_Deaths','Policies_Exposed', \n                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator', \n                               'Expected_Death_QX2015VBT_by_Policy',\n                               'Issue_Age', 'Issue_Year'])\n\n# target variable\nsample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n\nsample_df.head()\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Gender\n      Smoker_Status\n      Insurance_Plan\n      Issue_Age\n      Duration\n      Attained_Age\n      Face_Amount_Band\n      Issue_Year\n      Preferred_Class\n      SOA_Anticipated_Level_Term_Period\n      SOA_Guaranteed_Level_Term_Period\n      SOA_Post_level_Term_Indicator\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      0\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      4.882191\n      0.001074\n      0.0\n    \n    \n      1\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      500000-999999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      25.795943\n      0.006449\n      0.0\n    \n    \n      2\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      1.117809\n      0.000134\n      0.0\n    \n    \n      3\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      250000-499999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      70.098636\n      0.009814\n      0.0\n    \n    \n      4\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      4\n      3\n      50000-99999\n      2006\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      493.523281\n      0.034547\n      0.0\n    \n  \n\n\n\n\n\n# filter pipeline\nsample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n               & (sample_df.Smoker_Status != 'Unknown') \n               & (sample_df.Insurance_Plan == ' Term')\n               & (-sample_df.Preferred_Class.isna())\n               & (sample_df.Attained_Age >= 18)\n               & (sample_df.Issue_Year >= 1980)\n               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n               & (sample_df.mort < 1)]\n\nprint(f'Count: {sample_df.shape[0]}')\nprint()\n\n# describe data\nsample_df.describe()\n\nCount: 307233\n\n\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Issue_Age\n      Duration\n      Attained_Age\n      Issue_Year\n      Preferred_Class\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      count\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      3.072330e+05\n      307233.000000\n    \n    \n      mean\n      2014.084001\n      42.248505\n      7.951434\n      49.199939\n      2006.640537\n      2.035013\n      0.018514\n      12.504679\n      1.932158e-02\n      0.001627\n    \n    \n      std\n      1.413654\n      12.777574\n      4.793230\n      13.340539\n      4.888334\n      0.962332\n      0.147063\n      29.112019\n      5.412559e-02\n      0.023061\n    \n    \n      min\n      2012.000000\n      18.000000\n      1.000000\n      18.000000\n      1984.000000\n      1.000000\n      0.000000\n      0.002732\n      1.918000e-07\n      0.000000\n    \n    \n      25%\n      2013.000000\n      32.000000\n      4.000000\n      39.000000\n      2003.000000\n      1.000000\n      0.000000\n      0.838356\n      7.766577e-04\n      0.000000\n    \n    \n      50%\n      2014.000000\n      42.000000\n      7.000000\n      49.000000\n      2007.000000\n      2.000000\n      0.000000\n      2.612022\n      3.316641e-03\n      0.000000\n    \n    \n      75%\n      2015.000000\n      52.000000\n      12.000000\n      59.000000\n      2011.000000\n      3.000000\n      0.000000\n      10.680379\n      1.470165e-02\n      0.000000\n    \n    \n      max\n      2016.000000\n      84.000000\n      30.000000\n      91.000000\n      2016.000000\n      4.000000\n      6.000000\n      655.938021\n      2.827005e+00\n      0.981233\n    \n  \n\n\n\n\n\n# Encode categorical variables\ncat_vars = ['Observation_Year', \n     'Gender', \n     'Smoker_Status',\n     'Face_Amount_Band', \n     'Preferred_Class',\n     'SOA_Anticipated_Level_Term_Period']\n\nonehot = preprocessing.OneHotEncoder()\nresults = onehot.fit_transform(sample_df[cat_vars]).toarray()\ncat_vars_encoded = list(onehot.get_feature_names_out())\nsample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n\n\n# categorical variables\nface_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\nterm_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\nfig, ax = plt.subplots(4,2, figsize = (20,30))\nax = ax.flatten()\nfor i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n       'Face_Amount_Band', 'Preferred_Class',\n       'SOA_Guaranteed_Level_Term_Period']):\n    if column == 'Face_Amount_Band':\n        order = face_amount_order\n    elif column == 'SOA_Guaranteed_Level_Term_Period':\n        order = term_period_order\n    else:\n        order = None\n    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\nplt.show()\n\n\n\n\n\n# age and duration variables\nfig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n\nsns.histplot(x = sample_df['Duration'], ax = ax[1])\nplt.show()\n\n\n\n\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n# log mort by Attained Age\n\ndef stratify(field):\n    fig, ax = plt.subplots(figsize = (7,3))\n    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n    plt.show()\n\nstratify('Smoker_Status')\nstratify('Preferred_Class')\nstratify('Gender')\nstratify('Observation_Year')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\nTrain/test split\nFirst we split the data into 80% for training and 20% for testing.\nIn this context because we don’t really need to do hyperparameter tuning so it’s not necessary to create a validation set.\n\n# create training (80%), validation (5%) and test set (15%)\nrandom_seed = 0\ntrain_df = sample_df.sample(frac = 0.8, random_state = random_seed)\ntest_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n\n# add constant variable\ntrain_df['Const'] = 1\ntest_df['Const'] = 1\n \nprint(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')\n\nTrain size: 245786, test size: 61447\n\n\n\ntrain_df.to_csv('train_df.csv', index = False)\ntest_df.to_csv('test_df.csv', index = False)\n\n\n\nGLM modeling 101\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, \\(μ\\), of the distribution depends on the independent variables, X, through\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\nwhere:\n\n\\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on \\(X\\)\n\\(Xβ\\) is the linear predictor, a linear combination of unknown parameters \\(β\\)\n\\(g\\) is the link function.\n\n\n\nModel 1: Poisson distribution with log link on count\n Target Variable  = [Number_Of_Deaths]\n Input Variables  = [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\nAs the  target variable is a count measure, we will fit GLM with Poisson distribution and log link.\nThe target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to https://en.wikipedia.org/wiki/Poisson_regression\n\nmodel1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n                                       + Attained_Age + Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres1 = model1.fit()\nres1.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:   Number_Of_Deaths   No. Observations:     245786   \n\n\n  Model:                  GLM         Df Residuals:       3076911.54 \n\n\n  Model Family:         Poisson       Df Model:                 26   \n\n\n  Link Function:          log         Scale:                 1.0000  \n\n\n  Method:                IRLS         Log-Likelihood:     -7.1471e+05\n\n\n  Date:            Mon, 05 Dec 2022   Deviance:           9.8740e+05 \n\n\n  Time:                22:28:25       Pearson chi2:        3.17e+06  \n\n\n  No. Iterations:         24          Pseudo R-squ. (CS):   0.6540   \n\n\n  Covariance Type:     nonrobust                                     \n\n\n\n\n                                                               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                                                    -9.2794     0.158   -58.838  0.000    -9.589    -8.970\n\n\n  C(Observation_Year)[T.2013]                                  -0.0545     0.007    -8.190  0.000    -0.067    -0.041\n\n\n  C(Observation_Year)[T.2014]                                  -0.0051     0.006    -0.789  0.430    -0.018     0.008\n\n\n  C(Observation_Year)[T.2015]                                  -0.1405     0.007   -20.705  0.000    -0.154    -0.127\n\n\n  C(Observation_Year)[T.2016]                                  -0.0813     0.007   -12.377  0.000    -0.094    -0.068\n\n\n  C(Gender)[T.Male]                                             0.3527     0.005    74.784  0.000     0.343     0.362\n\n\n  C(Smoker_Status)[T.Smoker]                                    1.0350     0.015    67.166  0.000     1.005     1.065\n\n\n  C(Face_Amount_Band)[T.   10000-24999]                        -0.7187     0.118    -6.104  0.000    -0.949    -0.488\n\n\n  C(Face_Amount_Band)[T.   25000-49999]                        -0.7632     0.117    -6.500  0.000    -0.993    -0.533\n\n\n  C(Face_Amount_Band)[T.   50000-99999]                        -0.9776     0.117    -8.372  0.000    -1.206    -0.749\n\n\n  C(Face_Amount_Band)[T.  100000-249999]                       -1.6819     0.116   -14.452  0.000    -1.910    -1.454\n\n\n  C(Face_Amount_Band)[T.  250000-499999]                       -2.0061     0.116   -17.222  0.000    -2.234    -1.778\n\n\n  C(Face_Amount_Band)[T.  500000-999999]                       -2.0428     0.117   -17.521  0.000    -2.271    -1.814\n\n\n  C(Face_Amount_Band)[T. 1000000-2499999]                      -2.0690     0.117   -17.721  0.000    -2.298    -1.840\n\n\n  C(Face_Amount_Band)[T. 2500000-4999999]                      -2.0173     0.138   -14.656  0.000    -2.287    -1.747\n\n\n  C(Face_Amount_Band)[T. 5000000-9999999]                      -2.0177     0.229    -8.795  0.000    -2.467    -1.568\n\n\n  C(Face_Amount_Band)[T.10000000+]                            -23.7738  1.48e+04    -0.002  0.999 -2.89e+04  2.89e+04\n\n\n  C(Preferred_Class)[T.2.0]                                     0.4593     0.005    94.004  0.000     0.450     0.469\n\n\n  C(Preferred_Class)[T.3.0]                                     0.4168     0.007    60.272  0.000     0.403     0.430\n\n\n  C(Preferred_Class)[T.4.0]                                     0.5337     0.011    48.013  0.000     0.512     0.555\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.10 yr anticipated]    -0.1692     0.105    -1.607  0.108    -0.376     0.037\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.15 yr anticipated]    -0.2569     0.105    -2.438  0.015    -0.463    -0.050\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.20 yr anticipated]    -0.4042     0.105    -3.844  0.000    -0.610    -0.198\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.25 yr anticipated]     0.0217     0.106     0.205  0.838    -0.186     0.229\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.30 yr anticipated]    -0.2437     0.105    -2.314  0.021    -0.450    -0.037\n\n\n  Attained_Age                                                  0.0739     0.000   254.173  0.000     0.073     0.075\n\n\n  Duration                                                      0.0497     0.001    92.131  0.000     0.049     0.051\n\n\n\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nres1.save('res1.pkl')\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nloaded = sm.load('res1.pkl')\n\n\nfitted = loaded.model.fit()\n\n\nfitted.predict(train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nfitted.params[\"Intercept\"]\n\n-9.279412567322963\n\n\nFirst, we show the lift chart that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we’re only look at the high-level average for each decile.\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat1'] = res1.predict(exog = train_df)\ntrain_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\ntest_df['mort_hat1'] = res1.predict(exog = test_df)\ntest_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n\n# groupby and aggregate by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\ntemp\n\n# lift chart \nfig, ax = plt.subplots(figsize = (7,3))\ntemp.plot(ax = ax)\nplt.title('Actual vs predicted mortality rate by deciles')\nplt.show() \n\nSecond, we can plot the partial dependency chart between the log mortality rate and key covariates like Attained Age or Duration to see more granular comparisons between actual vs predicted.\nWe can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well.\n\ndef pdp(df, agg_field, title, predict_col = 'death_hat1'):\n    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n    plt.legend(['actual','predicted']) \n    plt.xlabel(agg_field)\n    plt.ylabel('log_mort')\n    plt.title(title)\n    plt.show()\n    \npdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\npdp(train_df, 'Duration', 'How well does the model fit the train set')\n\n\npdp(test_df, 'Attained_Age', 'How well does the model fit the test set')\npdp(test_df, 'Duration', 'How well does the model fit the test set')\n\nThird, we look at Prediction Error by taking the difference between the Number Of Deaths (actual) and Predicted Number of Deaths and then normalized by Policies Exposed. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\nagg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\nagg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Testing error')\nplt.show()\n\n\n\n\nValidation\n\n1. Goodness of Fit\n\n Pseudo R-squared \nIn linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\nFor GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n\n\\(R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}\\)\n\n\nres1.pseudo_rsquared()\n\n\n\n Deviance \nThe (total) deviance for a model M with estimates \\({\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}\\), based on a dataset y, may be constructed by its likelihood as:\n\n\\({\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}\\)\n\nHere \\(\\hat \\theta_0\\) denotes the fitted values of the parameters in the model M, while \\(\\hat \\theta_s\\) denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y.\nIn large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\nHere we divided the deviance by the residual degree of freedom and observed a ratio much smaller than 1\n\nres1.deviance/res1.df_resid\n\n\n\n Pearson Statistic and dispersion \nSimilar to deviance test, the Pearson Statistic is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit.\nAdditionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. Overdispersion means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X.\nHere we divided the pearson statistic by the residual degree of freedom and observed a value very close to 1\n\nres1.pearson_chi2/res1.df_resid\n\n\n\n\n2. Feature importance\n\nConfidence intervals and p-values \nConfidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available.\n\nres1.summary()\n\nFrom the summary, we can see that all of the features other than SOA_Anticipated_Level_Term_Period are significant as all p-values are < 5%.\nDirectionally, the coeficients for the main features like Gender, Smoking Status, Attained_Age or Duration are all aligned with our intuition and the EDA charts that we created previously:\n\nMortality rate for Male is higher than Female\nMortality rate for Smoker is higher than non-Smoker\nMortality rate is higher as age is higher\nMortality rate is higher as duration is longer\n\n\n\n\n3. Main Effects\nWe want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given.\n\ndef pdp2(df, x, hue, predict_col = 'death_hat1'):\n    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (6,3))\n    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n    plt.xlabel(x)\n    plt.ylabel('log_mort')\n    plt.title(f'Log mortality by {x} and {hue}')\n    plt.show()\n    \npdp2(train_df, 'Attained_Age', 'Gender')\npdp2(train_df, 'Duration', 'Gender')\npdp2(train_df, 'Attained_Age', 'Smoker_Status')\npdp2(train_df, 'Duration', 'Smoker_Status')\npdp2(train_df, 'Attained_Age', 'Preferred_Class')\npdp2(train_df, 'Duration', 'Preferred_Class')\n\nWe can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\nAdditionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it’s almost constant regardless of ages.\n\n\n4. Interaction Effects\nOne of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features.\nHere we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions.\n\n Model 2: Poisson distribution with log link on Death Count with interactions \n\nmodel2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat2'] = res2.predict(exog = train_df)\ntrain_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\ntest_df['mort_hat2'] = res2.predict(exog = test_df)\ntest_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n\nres2.summary()\n\n\n\n Compared to the vanilla model \nFirst, pearson and deviance are reasonable\n\nprint(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n\nprint(f'deviance/df = {res2.deviance/res2.df_resid}')\n\nCompared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features.\n\nprint(f'AIC for Model 1 - No interaction: {res1.aic}')\nprint(f'AIC for Model 2 - With interactions: {res2.aic}')\n\nSide note on definition of AIC:  A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value.\n\n\n\n5. Correlated Features\nFor GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\nThis is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let’s show it again below. We don’t see severe correlation between any two features that requires dropping one from the feature set.\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\nplt.show()\n\n\n\n\nConclusion\nIn this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model. - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. - We checked for any necessary inclusion of interactions and handling of correlated features.\nApparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics.\n\n\nAppendix\n\nModel 1 not using formula\nThis is the explicit setup where we don’t lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis.\n\n# Target Variable\nY = ['Number_Of_Deaths']\n\n# Predictors (aka Input Variables)\nX = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const'] \n\n# Our choice for Link function is the Gaussian distribution for the nature of death frequency\nmodel = sm.GLM(endog = train_df[Y], \n               exog = train_df[X], \n               family=sm.families.Poisson(sm.families.links.log()),\n               freq_weights = train_df['Policies_Exposed'],\n               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres = model.fit()\nres.summary()\n\n\n\nModel 3: Gaussian distribution with log link on mortality rate\nThis is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. Pseudo R-squared is far worse than Model 1\n\nmodel2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration', \n                 data = train_df,\n                 family=sm.families.Gaussian(link = sm.families.links.log()),\n                 freq_weights = train_df['Policies_Exposed'])\nres2 = model2.fit()\nres2.summary()\n\n\n\nModel 4: XGBoost\nIn this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.\nNote that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time.\n\nX = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\nY = ['mort']#['Number_Of_Deaths']\n\nX_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\nfor x in X_cat:\n    train_df[x] = train_df[x].astype(\"category\")\n    test_df[x] = test_df[x].astype('category')\n\n\n# create model instance\nbst = xgb.XGBRegressor(n_estimators=50, \n                   max_depth=4, \n                   learning_rate=0.5, \n                   objective='count:poisson', \n                   enable_categorical = True, \n                   tree_method = 'approx', \n                   booster = 'gbtree', \n                   verbosity = 1)\n\n# fit model\nbst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n\n# make predictions\npreds = bst.predict(test_df[X])\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat4'] = bst.predict(train_df[X])\ntrain_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\ntest_df['mort_hat4'] = bst.predict(test_df[X])\ntest_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']\n\nLift chart does not show too much of a difference from Model 1\n\n# lift chart by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n\n# groupby and aggregate\nfig, ax = plt.subplots(figsize = (7,3))\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\ntemp.plot(ax = ax)\nplt.title('Actual vs Predicted by deciles')\nplt.show()\n\nPlotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better.\n\n# partial dependence plots\npdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\npdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\npdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\npdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')\n\nLooking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates.\n\npdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\npdp2(train_df, 'Duration', 'Gender','death_hat4')\npdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\npdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')\n\n\n\nCompare Model 1, Model 2 and Model 4\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\ntrain_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\ntrain_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n\nagg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\n# plt.ylim(0,1)\n# plt.xlim(30,85)\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\ntest_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\ntest_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n\nagg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\nplt.ylabel('Error')\n# plt.ylim(0,1)\n# plt.xlim(30,85)\na = plt.title('Testing Error')\nplt.show()\n\n\nres1.save('mortality_model.pickle')"
  },
  {
    "objectID": "notebooks/lending_club_regression.html",
    "href": "notebooks/lending_club_regression.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_rows = None\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n# vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/loan_data_2007_2014_preprocessed.csv\")\n\n# targets = vm.DatasetTargets(\n#     target_column=\"loan_status\",\n#     class_labels={\n#         \"Fully Paid\": \"Fully Paid\",\n#         \"Charged Off\": \"Charged Off\",\n#     }\n# )\n\n# vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nColumns (21,49) have mixed types.Specify dtype option on import or set low_memory=False.\n\n\n\nloan_data_defaults = df[df['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n\n\nloan_data_defaults.shape\n\n(43236, 209)\n\n\n\nloan_data_defaults['mths_since_last_delinq'].fillna(0, inplace=True)\nloan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nloan_data_defaults['CCF'].describe()\n\ncount    43236.000000\nmean         0.735952\nstd          0.200742\nmin          0.000438\n25%          0.632088\n50%          0.789908\n75%          0.888543\nmax          1.000000\nName: CCF, dtype: float64\n\n\n\nplt.hist(loan_data_defaults['CCF'], bins = 100)\n\n(array([   3.,   17.,   16.,   44.,   16.,   13.,   71.,   26.,    7.,\n          63.,   67.,   17.,   60.,   90.,   23.,   55.,   82.,   42.,\n          47.,  123.,   82.,   70.,  122.,   86.,   89.,  110.,  117.,\n         111.,  122.,  120.,  135.,  141.,  154.,  146.,  160.,  175.,\n         152.,  187.,  202.,  174.,  204.,  208.,  210.,  211.,  241.,\n         264.,  281.,  224.,  308.,  267.,  287.,  296.,  340.,  274.,\n         365.,  370.,  392.,  364.,  393.,  419.,  411.,  429.,  445.,\n         497.,  481.,  478.,  569.,  568.,  599.,  618.,  727.,  691.,\n         626.,  805.,  804.,  776.,  881.,  851.,  916.,  934.,  925.,\n        1078.,  933., 1218., 1041., 1082., 1336., 1040., 1374., 1073.,\n        1406., 1287.,  952., 1414.,  795., 1320.,  578.,  949.,  343.,\n         531.]),\n array([4.3800000e-04, 1.0433620e-02, 2.0429240e-02, 3.0424860e-02,\n        4.0420480e-02, 5.0416100e-02, 6.0411720e-02, 7.0407340e-02,\n        8.0402960e-02, 9.0398580e-02, 1.0039420e-01, 1.1038982e-01,\n        1.2038544e-01, 1.3038106e-01, 1.4037668e-01, 1.5037230e-01,\n        1.6036792e-01, 1.7036354e-01, 1.8035916e-01, 1.9035478e-01,\n        2.0035040e-01, 2.1034602e-01, 2.2034164e-01, 2.3033726e-01,\n        2.4033288e-01, 2.5032850e-01, 2.6032412e-01, 2.7031974e-01,\n        2.8031536e-01, 2.9031098e-01, 3.0030660e-01, 3.1030222e-01,\n        3.2029784e-01, 3.3029346e-01, 3.4028908e-01, 3.5028470e-01,\n        3.6028032e-01, 3.7027594e-01, 3.8027156e-01, 3.9026718e-01,\n        4.0026280e-01, 4.1025842e-01, 4.2025404e-01, 4.3024966e-01,\n        4.4024528e-01, 4.5024090e-01, 4.6023652e-01, 4.7023214e-01,\n        4.8022776e-01, 4.9022338e-01, 5.0021900e-01, 5.1021462e-01,\n        5.2021024e-01, 5.3020586e-01, 5.4020148e-01, 5.5019710e-01,\n        5.6019272e-01, 5.7018834e-01, 5.8018396e-01, 5.9017958e-01,\n        6.0017520e-01, 6.1017082e-01, 6.2016644e-01, 6.3016206e-01,\n        6.4015768e-01, 6.5015330e-01, 6.6014892e-01, 6.7014454e-01,\n        6.8014016e-01, 6.9013578e-01, 7.0013140e-01, 7.1012702e-01,\n        7.2012264e-01, 7.3011826e-01, 7.4011388e-01, 7.5010950e-01,\n        7.6010512e-01, 7.7010074e-01, 7.8009636e-01, 7.9009198e-01,\n        8.0008760e-01, 8.1008322e-01, 8.2007884e-01, 8.3007446e-01,\n        8.4007008e-01, 8.5006570e-01, 8.6006132e-01, 8.7005694e-01,\n        8.8005256e-01, 8.9004818e-01, 9.0004380e-01, 9.1003942e-01,\n        9.2003504e-01, 9.3003066e-01, 9.4002628e-01, 9.5002190e-01,\n        9.6001752e-01, 9.7001314e-01, 9.8000876e-01, 9.9000438e-01,\n        1.0000000e+00]),\n <BarContainer object of 100 artists>)\n\n\n\n\n\n\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n\n\nfeatures_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\n\n\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \n\n\nead_inputs_train = ead_inputs_train[features_all]\n\n\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport numpy as np\nimport scipy.stats as stat\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1, positive=False):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ / se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self\n\n\nreg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n\n\nLinearRegression()\n\n\n\nfeature_name = ead_inputs_train.columns.values\n\n\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_ead.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table\n\n\n\n\n\n  \n    \n      \n      Feature name\n      Coefficients\n      p_values\n    \n  \n  \n    \n      0\n      Intercept\n      1.109746e+00\n      NaN\n    \n    \n      1\n      grade:A\n      -3.030033e-01\n      0.000000e+00\n    \n    \n      2\n      grade:B\n      -2.364277e-01\n      0.000000e+00\n    \n    \n      3\n      grade:C\n      -1.720232e-01\n      0.000000e+00\n    \n    \n      4\n      grade:D\n      -1.198470e-01\n      0.000000e+00\n    \n    \n      5\n      grade:E\n      -6.768713e-02\n      0.000000e+00\n    \n    \n      6\n      grade:F\n      -2.045907e-02\n      4.428795e-03\n    \n    \n      7\n      home_ownership:MORTGAGE\n      -6.343341e-03\n      2.632464e-03\n    \n    \n      8\n      home_ownership:NONE\n      -5.539064e-03\n      9.318931e-01\n    \n    \n      9\n      home_ownership:OTHER\n      -2.426052e-03\n      9.335820e-01\n    \n    \n      10\n      home_ownership:OWN\n      -1.619582e-03\n      6.366112e-01\n    \n    \n      11\n      verification_status:Not Verified\n      5.339510e-05\n      9.828295e-01\n    \n    \n      12\n      verification_status:Source Verified\n      8.967822e-03\n      7.828941e-05\n    \n    \n      13\n      purpose:car\n      7.904787e-04\n      9.330252e-01\n    \n    \n      14\n      purpose:debt_consolidation\n      1.264922e-02\n      5.898438e-07\n    \n    \n      15\n      purpose:educational\n      9.643587e-02\n      1.801025e-06\n    \n    \n      16\n      purpose:home_improvement\n      1.923044e-02\n      4.873543e-05\n    \n    \n      17\n      purpose:house\n      1.607120e-02\n      1.653651e-01\n    \n    \n      18\n      purpose:major_purchase\n      2.984917e-02\n      2.197793e-05\n    \n    \n      19\n      purpose:medical\n      3.962479e-02\n      5.238263e-06\n    \n    \n      20\n      purpose:moving\n      4.577630e-02\n      2.987383e-06\n    \n    \n      21\n      purpose:other\n      3.706744e-02\n      0.000000e+00\n    \n    \n      22\n      purpose:renewable_energy\n      7.212969e-02\n      8.889877e-03\n    \n    \n      23\n      purpose:small_business\n      5.128674e-02\n      0.000000e+00\n    \n    \n      24\n      purpose:vacation\n      1.874863e-02\n      1.152702e-01\n    \n    \n      25\n      purpose:wedding\n      4.350522e-02\n      2.032121e-04\n    \n    \n      26\n      initial_list_status:w\n      1.318126e-02\n      6.115181e-09\n    \n    \n      27\n      term_int\n      4.551882e-03\n      0.000000e+00\n    \n    \n      28\n      emp_length_int\n      -1.591478e-03\n      4.404626e-10\n    \n    \n      29\n      mths_since_issue_d\n      -4.305274e-03\n      0.000000e+00\n    \n    \n      30\n      mths_since_earliest_cr_line\n      -3.634030e-05\n      2.742071e-03\n    \n    \n      31\n      funded_amnt\n      2.212126e-06\n      7.225181e-03\n    \n    \n      32\n      int_rate\n      -1.172652e-02\n      0.000000e+00\n    \n    \n      33\n      installment\n      -6.865607e-05\n      7.429261e-03\n    \n    \n      34\n      annual_inc\n      5.021817e-09\n      8.574696e-01\n    \n    \n      35\n      dti\n      2.832769e-04\n      3.632507e-02\n    \n    \n      36\n      delinq_2yrs\n      4.833234e-04\n      6.946456e-01\n    \n    \n      37\n      inq_last_6mths\n      1.131678e-02\n      0.000000e+00\n    \n    \n      38\n      mths_since_last_delinq\n      -1.965980e-04\n      3.220434e-06\n    \n    \n      39\n      mths_since_last_record\n      -5.085639e-05\n      3.291896e-01\n    \n    \n      40\n      open_acc\n      -2.142130e-03\n      4.218847e-15\n    \n    \n      41\n      pub_rec\n      6.782062e-03\n      4.252750e-02\n    \n    \n      42\n      total_acc\n      4.518110e-04\n      1.902931e-04\n    \n    \n      43\n      acc_now_delinq\n      9.974801e-03\n      5.012787e-01\n    \n    \n      44\n      total_rev_hi_lim\n      2.166527e-07\n      8.196014e-05\n    \n  \n\n\n\n\n\nead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\n\n\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\ny_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\nead_targets_test_temp = ead_targets_test\n\n\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.000000\n      0.530654\n    \n    \n      0\n      0.530654\n      1.000000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.736013\n    \n    \n      std\n      0.105194\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.161088\n    \n  \n\n\n\n\n\ny_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735992\n    \n    \n      std\n      0.105127\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.000000\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead)\n\n0.0291749760949319\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead)\n\n0.2822776667644732\n\n\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.set_params(\n    booster='gblinear',\n    eval_metric=mean_squared_error,\n)\n\nXGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, predictor=None, random_state=None,\n             reg_alpha=None, reg_lambda=None, ...)\n\n\n\nxgb_model.fit(ead_inputs_train, ead_targets_train, eval_set=[(ead_inputs_train, ead_targets_train), (ead_inputs_test, ead_targets_test)],)\n\n[0] validation_0-rmse:0.18787   validation_0-mean_squared_error:0.03529 validation_1-rmse:0.18834   validation_1-mean_squared_error:0.03547\n[1] validation_0-rmse:0.18491   validation_0-mean_squared_error:0.03419 validation_1-rmse:0.18507   validation_1-mean_squared_error:0.03425\n[2] validation_0-rmse:0.18372   validation_0-mean_squared_error:0.03375 validation_1-rmse:0.18379   validation_1-mean_squared_error:0.03378\n[3] validation_0-rmse:0.18299   validation_0-mean_squared_error:0.03348 validation_1-rmse:0.18312   validation_1-mean_squared_error:0.03353\n[4] validation_0-rmse:0.18238   validation_0-mean_squared_error:0.03326 validation_1-rmse:0.18246   validation_1-mean_squared_error:0.03329\n[5] validation_0-rmse:0.18177   validation_0-mean_squared_error:0.03304 validation_1-rmse:0.18202   validation_1-mean_squared_error:0.03313\n[6] validation_0-rmse:0.18120   validation_0-mean_squared_error:0.03283 validation_1-rmse:0.18152   validation_1-mean_squared_error:0.03295\n[7] validation_0-rmse:0.18075   validation_0-mean_squared_error:0.03267 validation_1-rmse:0.18116   validation_1-mean_squared_error:0.03282\n[8] validation_0-rmse:0.18032   validation_0-mean_squared_error:0.03251 validation_1-rmse:0.18073   validation_1-mean_squared_error:0.03266\n[9] validation_0-rmse:0.17989   validation_0-mean_squared_error:0.03236 validation_1-rmse:0.18038   validation_1-mean_squared_error:0.03254\n[10]    validation_0-rmse:0.17948   validation_0-mean_squared_error:0.03221 validation_1-rmse:0.17992   validation_1-mean_squared_error:0.03237\n[11]    validation_0-rmse:0.17909   validation_0-mean_squared_error:0.03207 validation_1-rmse:0.17954   validation_1-mean_squared_error:0.03224\n[12]    validation_0-rmse:0.17873   validation_0-mean_squared_error:0.03195 validation_1-rmse:0.17920   validation_1-mean_squared_error:0.03211\n[13]    validation_0-rmse:0.17841   validation_0-mean_squared_error:0.03183 validation_1-rmse:0.17896   validation_1-mean_squared_error:0.03202\n[14]    validation_0-rmse:0.17809   validation_0-mean_squared_error:0.03172 validation_1-rmse:0.17858   validation_1-mean_squared_error:0.03189\n[15]    validation_0-rmse:0.17779   validation_0-mean_squared_error:0.03161 validation_1-rmse:0.17825   validation_1-mean_squared_error:0.03177\n[16]    validation_0-rmse:0.17751   validation_0-mean_squared_error:0.03151 validation_1-rmse:0.17789   validation_1-mean_squared_error:0.03164\n[17]    validation_0-rmse:0.17717   validation_0-mean_squared_error:0.03139 validation_1-rmse:0.17761   validation_1-mean_squared_error:0.03155\n[18]    validation_0-rmse:0.17689   validation_0-mean_squared_error:0.03129 validation_1-rmse:0.17730   validation_1-mean_squared_error:0.03144\n[19]    validation_0-rmse:0.17664   validation_0-mean_squared_error:0.03120 validation_1-rmse:0.17706   validation_1-mean_squared_error:0.03135\n[20]    validation_0-rmse:0.17641   validation_0-mean_squared_error:0.03112 validation_1-rmse:0.17678   validation_1-mean_squared_error:0.03125\n[21]    validation_0-rmse:0.17619   validation_0-mean_squared_error:0.03104 validation_1-rmse:0.17656   validation_1-mean_squared_error:0.03117\n[22]    validation_0-rmse:0.17598   validation_0-mean_squared_error:0.03097 validation_1-rmse:0.17634   validation_1-mean_squared_error:0.03110\n[23]    validation_0-rmse:0.17580   validation_0-mean_squared_error:0.03090 validation_1-rmse:0.17614   validation_1-mean_squared_error:0.03102\n[24]    validation_0-rmse:0.17560   validation_0-mean_squared_error:0.03083 validation_1-rmse:0.17592   validation_1-mean_squared_error:0.03095\n[25]    validation_0-rmse:0.17542   validation_0-mean_squared_error:0.03077 validation_1-rmse:0.17574   validation_1-mean_squared_error:0.03088\n[26]    validation_0-rmse:0.17525   validation_0-mean_squared_error:0.03071 validation_1-rmse:0.17553   validation_1-mean_squared_error:0.03081\n[27]    validation_0-rmse:0.17507   validation_0-mean_squared_error:0.03065 validation_1-rmse:0.17533   validation_1-mean_squared_error:0.03074\n[28]    validation_0-rmse:0.17491   validation_0-mean_squared_error:0.03060 validation_1-rmse:0.17515   validation_1-mean_squared_error:0.03068\n[29]    validation_0-rmse:0.17481   validation_0-mean_squared_error:0.03056 validation_1-rmse:0.17505   validation_1-mean_squared_error:0.03064\n[30]    validation_0-rmse:0.17469   validation_0-mean_squared_error:0.03052 validation_1-rmse:0.17487   validation_1-mean_squared_error:0.03058\n[31]    validation_0-rmse:0.17455   validation_0-mean_squared_error:0.03047 validation_1-rmse:0.17470   validation_1-mean_squared_error:0.03052\n[32]    validation_0-rmse:0.17443   validation_0-mean_squared_error:0.03043 validation_1-rmse:0.17451   validation_1-mean_squared_error:0.03045\n[33]    validation_0-rmse:0.17431   validation_0-mean_squared_error:0.03038 validation_1-rmse:0.17443   validation_1-mean_squared_error:0.03043\n[34]    validation_0-rmse:0.17420   validation_0-mean_squared_error:0.03035 validation_1-rmse:0.17426   validation_1-mean_squared_error:0.03037\n[35]    validation_0-rmse:0.17412   validation_0-mean_squared_error:0.03032 validation_1-rmse:0.17418   validation_1-mean_squared_error:0.03034\n[36]    validation_0-rmse:0.17403   validation_0-mean_squared_error:0.03029 validation_1-rmse:0.17399   validation_1-mean_squared_error:0.03027\n[37]    validation_0-rmse:0.17393   validation_0-mean_squared_error:0.03025 validation_1-rmse:0.17388   validation_1-mean_squared_error:0.03024\n[38]    validation_0-rmse:0.17386   validation_0-mean_squared_error:0.03023 validation_1-rmse:0.17376   validation_1-mean_squared_error:0.03019\n[39]    validation_0-rmse:0.17377   validation_0-mean_squared_error:0.03020 validation_1-rmse:0.17370   validation_1-mean_squared_error:0.03017\n[40]    validation_0-rmse:0.17370   validation_0-mean_squared_error:0.03017 validation_1-rmse:0.17363   validation_1-mean_squared_error:0.03015\n[41]    validation_0-rmse:0.17363   validation_0-mean_squared_error:0.03015 validation_1-rmse:0.17358   validation_1-mean_squared_error:0.03013\n[42]    validation_0-rmse:0.17357   validation_0-mean_squared_error:0.03012 validation_1-rmse:0.17350   validation_1-mean_squared_error:0.03010\n[43]    validation_0-rmse:0.17350   validation_0-mean_squared_error:0.03010 validation_1-rmse:0.17346   validation_1-mean_squared_error:0.03009\n[44]    validation_0-rmse:0.17345   validation_0-mean_squared_error:0.03009 validation_1-rmse:0.17340   validation_1-mean_squared_error:0.03007\n[45]    validation_0-rmse:0.17339   validation_0-mean_squared_error:0.03007 validation_1-rmse:0.17334   validation_1-mean_squared_error:0.03005\n[46]    validation_0-rmse:0.17335   validation_0-mean_squared_error:0.03005 validation_1-rmse:0.17327   validation_1-mean_squared_error:0.03002\n[47]    validation_0-rmse:0.17329   validation_0-mean_squared_error:0.03003 validation_1-rmse:0.17320   validation_1-mean_squared_error:0.03000\n[48]    validation_0-rmse:0.17325   validation_0-mean_squared_error:0.03001 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[49]    validation_0-rmse:0.17321   validation_0-mean_squared_error:0.03000 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[50]    validation_0-rmse:0.17316   validation_0-mean_squared_error:0.02998 validation_1-rmse:0.17304   validation_1-mean_squared_error:0.02994\n[51]    validation_0-rmse:0.17312   validation_0-mean_squared_error:0.02997 validation_1-rmse:0.17302   validation_1-mean_squared_error:0.02994\n[52]    validation_0-rmse:0.17309   validation_0-mean_squared_error:0.02996 validation_1-rmse:0.17299   validation_1-mean_squared_error:0.02993\n[53]    validation_0-rmse:0.17305   validation_0-mean_squared_error:0.02995 validation_1-rmse:0.17293   validation_1-mean_squared_error:0.02991\n[54]    validation_0-rmse:0.17301   validation_0-mean_squared_error:0.02993 validation_1-rmse:0.17290   validation_1-mean_squared_error:0.02989\n[55]    validation_0-rmse:0.17298   validation_0-mean_squared_error:0.02992 validation_1-rmse:0.17285   validation_1-mean_squared_error:0.02988\n[56]    validation_0-rmse:0.17296   validation_0-mean_squared_error:0.02991 validation_1-rmse:0.17278   validation_1-mean_squared_error:0.02985\n[57]    validation_0-rmse:0.17292   validation_0-mean_squared_error:0.02990 validation_1-rmse:0.17276   validation_1-mean_squared_error:0.02985\n[58]    validation_0-rmse:0.17289   validation_0-mean_squared_error:0.02989 validation_1-rmse:0.17273   validation_1-mean_squared_error:0.02984\n[59]    validation_0-rmse:0.17287   validation_0-mean_squared_error:0.02988 validation_1-rmse:0.17269   validation_1-mean_squared_error:0.02982\n[60]    validation_0-rmse:0.17284   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17267   validation_1-mean_squared_error:0.02982\n[61]    validation_0-rmse:0.17282   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17264   validation_1-mean_squared_error:0.02981\n[62]    validation_0-rmse:0.17280   validation_0-mean_squared_error:0.02986 validation_1-rmse:0.17262   validation_1-mean_squared_error:0.02980\n[63]    validation_0-rmse:0.17278   validation_0-mean_squared_error:0.02985 validation_1-rmse:0.17257   validation_1-mean_squared_error:0.02978\n[64]    validation_0-rmse:0.17275   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[65]    validation_0-rmse:0.17274   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[66]    validation_0-rmse:0.17272   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17254   validation_1-mean_squared_error:0.02977\n[67]    validation_0-rmse:0.17270   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17249   validation_1-mean_squared_error:0.02975\n[68]    validation_0-rmse:0.17268   validation_0-mean_squared_error:0.02982 validation_1-rmse:0.17248   validation_1-mean_squared_error:0.02975\n[69]    validation_0-rmse:0.17267   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17246   validation_1-mean_squared_error:0.02974\n[70]    validation_0-rmse:0.17265   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17244   validation_1-mean_squared_error:0.02974\n[71]    validation_0-rmse:0.17264   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17240   validation_1-mean_squared_error:0.02972\n[72]    validation_0-rmse:0.17262   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17239   validation_1-mean_squared_error:0.02972\n[73]    validation_0-rmse:0.17261   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17237   validation_1-mean_squared_error:0.02971\n[74]    validation_0-rmse:0.17259   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17236   validation_1-mean_squared_error:0.02971\n[75]    validation_0-rmse:0.17258   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17234   validation_1-mean_squared_error:0.02970\n[76]    validation_0-rmse:0.17257   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17232   validation_1-mean_squared_error:0.02969\n[77]    validation_0-rmse:0.17256   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17230   validation_1-mean_squared_error:0.02969\n[78]    validation_0-rmse:0.17255   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17228   validation_1-mean_squared_error:0.02968\n[79]    validation_0-rmse:0.17254   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17226   validation_1-mean_squared_error:0.02967\n[80]    validation_0-rmse:0.17253   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17224   validation_1-mean_squared_error:0.02967\n[81]    validation_0-rmse:0.17252   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17223   validation_1-mean_squared_error:0.02966\n[82]    validation_0-rmse:0.17251   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17222   validation_1-mean_squared_error:0.02966\n[83]    validation_0-rmse:0.17250   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[84]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[85]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[86]    validation_0-rmse:0.17248   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[87]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17218   validation_1-mean_squared_error:0.02965\n[88]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[89]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[90]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[91]    validation_0-rmse:0.17245   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[92]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17215   validation_1-mean_squared_error:0.02964\n[93]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17214   validation_1-mean_squared_error:0.02963\n[94]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17213   validation_1-mean_squared_error:0.02963\n[95]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[96]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17212   validation_1-mean_squared_error:0.02962\n[97]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[98]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17210   validation_1-mean_squared_error:0.02962\n[99]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02972 validation_1-rmse:0.17209   validation_1-mean_squared_error:0.02962\n\n\nXGBRegressor(base_score=0.5, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=-1, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.5, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=0,\n             num_parallel_tree=None, predictor=None, random_state=0,\n             reg_alpha=0, reg_lambda=0, ...)\n\n\n\ny_hat_test_ead_xgb = xgb_model.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead_xgb)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.00000\n      0.52144\n    \n    \n      0\n      0.52144\n      1.00000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead_xgb)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead_xgb).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735745\n    \n    \n      std\n      0.101577\n    \n    \n      min\n      0.408254\n    \n    \n      25%\n      0.664853\n    \n    \n      50%\n      0.728506\n    \n    \n      75%\n      0.811329\n    \n    \n      max\n      1.310113\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead_xgb)\n\n0.029612655435575855\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead_xgb)\n\n0.2715104861315287"
  },
  {
    "objectID": "notebooks/send_custom_results.html",
    "href": "notebooks/send_custom_results.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  # Use your project ID\n  project = \"...\"\n)\n  \n\nTrue\n\n\n\n\nIt is possible to send custom metrics to ValidMind without implementing a Metric class. The only requirement is to construct an instance of a MetricResult that can be sent to the ValidMind API.\n\nfrom validmind.vm_models import MetricResult\n\naccuracy_metric = MetricResult(\n    type=\"evaluation\",\n    scope=\"test_dataset\",    \n    key=\"my_custom_accuracy\",\n    value=0.666\n)\n\n\nvm.log_metrics([accuracy_metric])\n\nTrue\n\n\n\n\n\nIt is possible to send custom test results to ValidMind without implementing a ThresholdTest class. The only requirement is to construct an instance of a TestResults that can be sent to the ValidMind API.\n\nfrom validmind.vm_models import TestResult, TestResults\n\ncustom_params = {\n    \"min_percent_threshold\": 0.5\n}\n\ncustom_test_result = TestResults(\n    category=\"model_performance\",\n    test_name=\"accuracy_threshold\",\n    params=custom_params,\n    passed=False,\n    results=[\n        TestResult(\n            passed=False,\n            values={\n                \"score\": 0.15,\n                \"threshold\": custom_params[\"min_percent_threshold\"],\n            },\n        )\n    ],\n)\n\n\nvm.log_test_results([custom_test_result])\n\nTrue\n\n\n\n\n\nIt is possible to implement custom metrics or threshold test classes. The are only two requirements for getting this to work:\n\nWe need to build a TestPlan that can execute the custom metric or threshold test (or add it to an existing TestPlan, TBD).\nWe need to implement a run method on the custom metric or threshold test class.\n\n\n\nThe following example shows how to implement a custom metric that calculates the mean of a list of numbers.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MeanMetric(Metric):\n    type = \"dataset\"\n    key = \"mean_of_values\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        return self.cache_results(mean)\n\n\n\n\nIt is possible to run a custom metric without running an entire test plan. This is useful for testing the metric before integrating it into a test plan.\nThe key idea is to create a TestContext object and pass it to the metric initializer. When a test plan is executed, the TestContext is created by the TestPlan class and passed down to every associated metric and threshold test. However, when we want to test a metric in isolation, we need to create the TestContext ourselves.\nIn this example we don’t need to pass any arguments to the TestContext initializer, but it is possible to pass any arguments as required by required_context.\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))\n\n\nWe can also inspect the results of the metric by accessing the result variable:\n\nmean_metric.result\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))\n\n\n\nmean_metric.result.metric.value\n\n3.0\n\n\n\nmean_metric.result.show()\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n\n\n\n\n\nThe following example shows how to implement a custom threshold test that fails if the mean of a list of numbers is greater than 5.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import ThresholdTest\n\n@dataclass\nclass MeanThresholdTest(ThresholdTest):\n    category = \"data_quality\"\n    name = \"mean_threshold\"\n    default_params = {\"mean_threshold\": 5}    \n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)        \n\n        passed = mean <= self.params[\"mean_threshold\"]\n        results = [\n            TestResult(\n                passed=passed,\n                values={\n                    \"mean\": mean,\n                    \"values\": values,\n                },\n            )\n        ]\n\n        return self.cache_results(results, passed=passed)\n\n\n\n\nSimilarly to custom metrics, it is also possible to run a custom threshold test without running an entire test plan:\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_threshold_test = MeanThresholdTest(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_threshold_test.run()\n\nTestPlanTestResult(figures=None, test_results=TestResults(category='data_quality', test_name='mean_threshold', params={'mean_threshold': 5, 'values': [1, 2, 3, 4, 5]}, passed=True, results=[TestResult(test_name=None, column=None, passed=True, values={'mean': 3.0, 'values': [1, 2, 3, 4, 5]})]))\n\n\n\ntest_results = mean_threshold_test.test_results.test_results\ntest_results.passed\n\nTrue\n\n\n\nfor result in test_results.results:\n    print(f\"passed: {result.passed}, values: {result.values}\")\n\npassed: True, values: {'mean': 3.0, 'values': [1, 2, 3, 4, 5]}\n\n\n\n\n\nThe following example shows how to implement a custom test plan that executes the custom metric and threshold test.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MeanMetric, MeanThresholdTest]\n\nmy_custom_test_plan = MyCustomTestPlan(config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\nmy_custom_test_plan.run()\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nIt is possible to register a custom test plan with ValidMind. This allows us to run the test plan using the same ValidMind Python API and combine it with other test plans as needed.\n\nvm.test_plans.register_test_plan(\"my_custom_test_plan\", MyCustomTestPlan)\n\nRegistered test plan: my_custom_test_plan\n\n\n\nvm.run_test_plan(\"my_custom_test_plan\", config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\nIt is possible to send figures with metrics and test results. The following example shows how to send a figure with a metric result.\n\n\nLet’s say we want to add a figure to our custom metric above. We can do this by adding a figures attribute to the cache_results method call. Let’s modify our custom metric code to do that.\n\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom validmind.vm_models import Figure, Metric\n\n@dataclass\nclass MeanMetricWithFigure(Metric):\n    type = \"dataset\"\n    key = \"mean_of_values_with_figure\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(key=self.key, figure=fig, metadata={})\n\n        return self.cache_results(mean, figures=[figure])\n\n\n\n\nSimilarly, we can add a figure to our custom threshold test by adding a figures attribute to the cache_results method call.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Figure, ThresholdTest\n\n@dataclass\nclass MeanThresholdTestWithFigure(ThresholdTest):\n    category = \"data_quality\"\n    name = \"mean_threshold_with_figure\"\n    default_params = {\"mean_threshold\": 5}    \n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)        \n\n        passed = mean <= self.params[\"mean_threshold\"]\n        results = [\n            TestResult(\n                passed=passed,\n                values={\n                    \"mean\": mean,\n                    \"values\": values,\n                },\n            )\n        ]\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(key=self.name, figure=fig, metadata={})        \n\n        return self.cache_results(results, passed=passed, figures=[figure])\n\nWe can now run a new test plan that includes our two new custom metrics and threshold tests.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlanWithFigures(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan_with_figures\"\n    required_context = []\n    tests = [MeanMetricWithFigure, MeanThresholdTestWithFigure]\n\nmy_custom_test_plan_with_figures = MyCustomTestPlanWithFigures(config={\n    \"mean_of_values_with_figure\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold_with_figure\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\nmy_custom_test_plan_with_figures.run()\n\n                                                                                                                                \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values_with_figure\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold With Figure\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold_with_figure\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/explore_x_train_lc.html",
    "href": "notebooks/explore_x_train_lc.html",
    "title": "ValidMind",
    "section": "",
    "text": "Explore x to train LC\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# PD Model\nvm.init(project=\"cl1jyvh2c000909lg1rk0a0zb\")\n\nTrue\n\n\n\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport scipy\n\n\ndef jeffreys_test(p: float, n: int = 0, d: int = 0) -> float:\n    \"\"\"\n    Perform a test that the test probability, p, is consistent with the observed number of \n    successes, d, from a number of trials, n.\n\n    This uses the Jeffrey's posterior probability, which is the Beta distribution with shape\n    parameters a = d + 1/2 and b = n - d + 1/2. The result is the one sided p-value representing the \n    probability that the test probability, p, is greater than the true probability.\n\n    :param p: the test probability to be compared to the number of successes given n trials\n    :param n: the number of trials\n    :param d: the number of successes [optional, default = 0]\n\n    :return p-value: one sided p-value of the test\n    \"\"\"\n    return scipy.stats.beta.cdf(p, d + 0.5, n - d + 0.5)\n\n\ndef update_result(s, d, n, dr, p, pval, out = 'Yet to decide'):\n    return ({'Segment': s,\n            'Defaults': d,\n            'Observations': n,\n            'Default Rate': dr,\n            'Calibrated PD': p,\n            'P-value': pval, \n            'Outcome': out})\n\n\ndef calculate_and_return(df = pd.DataFrame, cal_pd = {}, pool = None, obs = 'observed', threshold = 0.9):\n    \"\"\"\n    Take the input dataframe, analyse & clean, seprate poolwise.\n    Calculate the jeffreys statistic\n    \"\"\"\n    \n    result = pd.DataFrame(columns = ['Segment', 'Defaults', 'Observations', 'Default Rate', 'Calibrated PD', 'P-value', 'Outcome'])\n    \n    n = len(df[obs])\n    d = np.sum(df[obs])\n    dr = np.round(d/n,2)\n    p = cal_pd['Model']\n    pval = np.round(jeffreys_test(p, n, d),4)\n    if pval>=threshold:\n        out = 'Satisfactory'\n    else:\n        out = 'Not Satisfactory'\n    \n    result = result.append(update_result('Model', d, n, dr, p, pval, out), ignore_index = True)\n    \n    if pool != None:\n        samples = df.groupby(pool)\n        \n        for sample in samples:\n            n = len(sample[1][obs])\n            d = np.sum(sample[1][obs])\n            dr = np.round(d/n,2)\n            p = cal_pd[sample[0]]\n            pval = np.round(jeffreys_test(p, n, d),4)\n            \n            if pval>=threshold:\n                out = 'Satisfactory'\n            else:\n                out = 'Not Satisfactory'\n            \n            result = result.append(update_result(sample[0], d, n, dr, p, pval, out), ignore_index = True)\n            \n    return result\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/x_train_lc.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      28000.0\n      7.12\n      10\n      125000.0\n      15.97\n      0.0\n      26\n      725.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      11200.0\n      10.99\n      2\n      80600.0\n      15.93\n      0.0\n      15\n      670.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      2\n      14000.0\n      15.10\n      6\n      83000.0\n      18.17\n      0.0\n      13\n      660.0\n      1.0\n      76.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      12725.0\n      12.12\n      6\n      71300.0\n      29.70\n      0.0\n      13\n      675.0\n      2.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      7200.0\n      15.31\n      1\n      25000.0\n      32.98\n      0.0\n      8\n      700.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ndf[\"acc_now_delinq\"].value_counts()\n\n0.0    59802\n1.0      187\n2.0        7\n3.0        3\n5.0        1\nName: acc_now_delinq, dtype: int64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 60.0 MB\n\n\n\ntest_df = pd.read_csv(\"./notebooks/datasets/_temp/x_test_lc.csv\")\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      15500.0\n      8.90\n      10\n      100000.0\n      0.74\n      0.0\n      14\n      715.0\n      3.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      10800.0\n      11.67\n      10\n      68000.0\n      15.44\n      1.0\n      20\n      670.0\n      1.0\n      8.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      2\n      15850.0\n      15.10\n      2\n      36000.0\n      26.50\n      0.0\n      31\n      720.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      16000.0\n      15.31\n      2\n      80000.0\n      24.54\n      1.0\n      12\n      705.0\n      0.0\n      21.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      4\n      14000.0\n      12.12\n      10\n      90000.0\n      14.07\n      0.0\n      14\n      695.0\n      1.0\n      44.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ntest_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 20.0 MB\n\n\n\nmodel = joblib.load(\"./notebooks/datasets/_temp/lc_model.pickle\")\n\n\nsegments = [\n    {\n        \"name\": \"Grade\",\n        \"segments\": [\n            {\"name\": \"Grade A\", \"query\": \"grade_A == 1\"},\n            {\"name\": \"Grade B\", \"query\": \"grade_B == 1\"},\n            {\"name\": \"Grade C\", \"query\": \"grade_C == 1\"},\n            {\"name\": \"Grade D\", \"query\": \"grade_D == 1\"},\n            {\"name\": \"Grade E\", \"query\": \"grade_E == 1\"},\n            {\"name\": \"Grade F\", \"query\": \"grade_F == 1\"},\n            {\"name\": \"Grade G\", \"query\": \"grade_G == 1\"},\n        ]\n    },\n    {\n        \"name\": \"Delinquency\",\n        \"segments\": [\n            {\"name\": \"Delinquency: None\", \"query\": \"acc_now_delinq == 0\"},\n            {\"name\": \"Delinquency: 1 Account\", \"query\": \"acc_now_delinq == 1\"},\n            {\"name\": \"Delinquency: 2 Accounts\", \"query\": \"acc_now_delinq == 2\"},\n        ]\n    }\n]\n\n\ndef get_calibrated_pds(df, model, segments):\n    model_preds = model.predict_proba(df)[:, 1]\n    model_class_preds = (model_preds > 0.5).astype(int)\n\n    pds = {\"Model\": model_class_preds.sum() / len(model_class_preds)}\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            total_pds = class_pred.sum()\n            segment_pd = total_pds / len(class_pred)\n\n            pds[segment[\"name\"]] = segment_pd\n    return pds\n\n\ncalibrated_pds = get_calibrated_pds(df, model, segments)\ncalibrated_pds\n\n{'Model': 0.027933333333333334,\n 'Grade A': 0.0022715539494062983,\n 'Grade B': 0.007202947160059383,\n 'Grade C': 0.014655226404459197,\n 'Grade D': 0.043563336766220394,\n 'Grade E': 0.10736266241167085,\n 'Grade F': 0.1781818181818182,\n 'Grade G': 0.24396135265700483,\n 'Delinquency: None': 0.027791712651750778,\n 'Delinquency: 1 Account': 0.06951871657754011,\n 'Delinquency: 2 Accounts': 0.14285714285714285}\n\n\n\ndef process_observations(df, model, segments):\n    test_input = pd.DataFrame(columns = ['Segment', 'Observed'])\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            # Concat to test_input by adding all rows of class_pred and segment as a single value\n            test_input = pd.concat([test_input, pd.DataFrame({'Segment': segment[\"name\"], 'Observed': class_pred})], ignore_index=True)\n\n    return test_input\n\n\nobservations = process_observations(test_df, model, segments)\n\n\nresults = calculate_and_return(\n    observations,\n    cal_pd=calibrated_pds,\n    pool = 'Segment',\n    obs=\n    'Observed',\n    threshold = 0.85\n)\nresults\n\n\n\n\n\n  \n    \n      \n      Segment\n      Defaults\n      Observations\n      Default Rate\n      Calibrated PD\n      P-value\n      Outcome\n    \n  \n  \n    \n      0\n      Model\n      708\n      39999\n      0.02\n      0.027933\n      1.0000\n      Satisfactory\n    \n    \n      1\n      Delinquency: 1 Account\n      3\n      54\n      0.06\n      0.069519\n      0.6307\n      Not Satisfactory\n    \n    \n      2\n      Delinquency: 2 Accounts\n      0\n      6\n      0.00\n      0.142857\n      0.8352\n      Not Satisfactory\n    \n    \n      3\n      Delinquency: None\n      351\n      19939\n      0.02\n      0.027792\n      1.0000\n      Satisfactory\n    \n    \n      4\n      Grade A\n      2\n      3341\n      0.00\n      0.002272\n      0.9904\n      Satisfactory\n    \n    \n      5\n      Grade B\n      13\n      6023\n      0.00\n      0.007203\n      1.0000\n      Satisfactory\n    \n    \n      6\n      Grade C\n      24\n      5318\n      0.00\n      0.014655\n      1.0000\n      Satisfactory\n    \n    \n      7\n      Grade D\n      84\n      3133\n      0.03\n      0.043563\n      1.0000\n      Satisfactory\n    \n    \n      8\n      Grade E\n      120\n      1509\n      0.08\n      0.107363\n      0.9999\n      Satisfactory\n    \n    \n      9\n      Grade F\n      82\n      537\n      0.15\n      0.178182\n      0.9408\n      Satisfactory\n    \n    \n      10\n      Grade G\n      29\n      139\n      0.21\n      0.243961\n      0.8338\n      Not Satisfactory\n    \n  \n\n\n\n\n\n\nSend results to ValidMind\n\n# Test passed only if all values for 'Outcome' are 'Satisfactory'\npassed = results['Outcome'].all() == 'Satisfactory'\npassed\n\nFalse\n\n\n\n# Build a vm.TestResult object for each row in the results dataframe\ntest_results = []\nfor index, row in results.iterrows():\n    test_results.append(vm.TestResult(\n        passed=row['Outcome'] == 'Satisfactory',\n        values={\n            'segment': row['Segment'],\n            'defaults': row['Defaults'],\n            'observations': row['Observations'],\n            'default_rate': row['Default Rate'],\n            'calibrated_pd': row['Calibrated PD'],\n            'p_value': row['P-value']\n        }\n    ))\n\n\njeffreys_params = {\n    \"threshold\": 0.85\n}\n\njeffreys_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"jeffreys_test\",\n    params=jeffreys_params,\n    passed=passed,\n    results=test_results,\n)\n\n\nvm.log_test_results([\n    jeffreys_test_result\n])\n\nSuccessfully logged test results for test: jeffreys_test\n\n\nTrue"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-demo.html",
    "href": "notebooks/r_demo/r-ecm-demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "We want to be able to load R models using Python and describe, test and evaluate them using the ValidMind framework like we do with Python models. This notebook demonstrates how we can load R models either from an RDS file or by building the model in R directly in the notebook with the rpy2 package. Either way, we can then use the ValidMind framework to run a TestPlan designed for the model (in this case, a simple ECM model).\n\n# lets import the required libraries\nimport os\nimport tempfile\n\nimport pandas as pd\nimport rpy2.robjects as robjects\nfrom IPython.display import display_png\nfrom PIL import Image as PILImage\nfrom rpy2.robjects.packages import importr\n\n# import the R packages\ntidyverse = importr(\"tidyverse\")\nbroom = importr(\"broom\")\ngraphics = importr(\"graphics\")\ngrdevices = importr(\"grDevices\")\n\n\n# Load the RDS model we created earlier (in r-ecm-model notebook)\n# alternatively, the model could be recreated in rpy2 from scratch\nr_model = robjects.r[\"readRDS\"](\"r-ecm-model.rds\")\n\n\n# lets run summary on the model\n# in pure R, this would be: `summary(model)`\n# for this, however we want to get a string representation of the summary\n# so we can use it in python\nsummary = robjects.r[\"summary\"](r_model)\nsummary_str = str(summary)\nprint(summary_str)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\n\n\n# now lets something similar to run tidy, augment, and glance\n# in pure R, this would be: `tidy(model)`, `augment(model)`, `glance(model)`\n# however, we want to end up with a pandas dataframe containing the data in the Tibble created by these functions\ntidy = robjects.r[\"tidy\"](r_model)\ntidy_df = pd.DataFrame(robjects.conversion.rpy2py(tidy))\n\naugment = robjects.r[\"augment\"](r_model)\naugment_df = pd.DataFrame(robjects.conversion.rpy2py(augment))\n\nglance = robjects.r[\"glance\"](r_model)\nglance_df = pd.DataFrame(robjects.conversion.rpy2py(glance))\n\n# lets display the dataframes\ndisplay(tidy_df)\ndisplay(augment_df)\ndisplay(glance_df)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n    \n  \n  \n    \n      0\n      (Intercept)\n      deltaCorpProfits\n      deltaFedFundsRate\n      deltaUnempRate\n      CorpProfitsLag1\n      FedFundsRateLag1\n      UnempRateLag1\n      yLag1\n    \n    \n      1\n      -0.416021\n      0.011985\n      -0.123162\n      -1.484146\n      0.002708\n      0.065564\n      -0.053275\n      -0.033703\n    \n    \n      2\n      0.823671\n      0.002033\n      0.154749\n      0.43898\n      0.000826\n      0.049471\n      0.104092\n      0.019268\n    \n    \n      3\n      -0.505082\n      5.894868\n      -0.795883\n      -3.380892\n      3.27874\n      1.325304\n      -0.51181\n      -1.74917\n    \n    \n      4\n      0.614155\n      0.0\n      0.42721\n      0.000896\n      0.001265\n      0.186849\n      0.609448\n      0.082066\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      168\n      169\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n    \n  \n  \n    \n      0\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      ...\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n      178\n      179\n    \n    \n      1\n      -0.023333\n      0.0\n      0.126667\n      0.04\n      0.013333\n      0.063333\n      -0.05\n      -0.11\n      0.046667\n      -0.066667\n      ...\n      3.668281\n      4.628594\n      3.689168\n      2.55649\n      3.484343\n      1.330312\n      3.180822\n      2.360396\n      -3.238249\n      0.48375\n    \n    \n      2\n      4.012\n      2.183\n      4.194\n      1.068\n      3.195\n      6.352\n      11.655\n      3.034\n      1.692\n      4.836\n      ...\n      27.453\n      44.735\n      -79.877\n      86.058\n      49.698\n      15.633\n      -122.484\n      64.793\n      -58.055\n      -191.198\n    \n    \n      3\n      0.91\n      -0.723333\n      -1.21\n      0.76\n      0.44\n      0.403333\n      1.393333\n      1.28\n      2.743333\n      -0.563333\n      ...\n      -0.033333\n      0.003333\n      -0.013333\n      0.02\n      -0.003333\n      0.01\n      0.01\n      0.013333\n      0.013333\n      0.023333\n    \n    \n      4\n      0.133333\n      -0.1\n      -0.166667\n      -0.066667\n      -0.133333\n      -0.2\n      -0.433333\n      0.0\n      -0.133333\n      -0.033333\n      ...\n      -0.3\n      -0.3\n      -0.266667\n      -0.433333\n      -0.133333\n      -0.4\n      -0.133333\n      -0.166667\n      -0.3\n      -0.1\n    \n    \n      5\n      59.168\n      63.18\n      65.363\n      69.557\n      70.625\n      73.82\n      80.172\n      91.827\n      94.861\n      96.553\n      ...\n      1658.148\n      1685.601\n      1730.336\n      1650.459\n      1736.517\n      1786.215\n      1801.848\n      1679.364\n      1744.157\n      1686.102\n    \n    \n      6\n      4.563333\n      5.473333\n      4.75\n      3.54\n      4.3\n      4.74\n      5.143333\n      6.536667\n      7.816667\n      10.56\n      ...\n      0.116667\n      0.083333\n      0.086667\n      0.073333\n      0.093333\n      0.09\n      0.1\n      0.11\n      0.123333\n      0.136667\n    \n    \n      7\n      5.9\n      6.033333\n      5.933333\n      5.766667\n      5.7\n      5.566667\n      5.366667\n      4.933333\n      4.933333\n      4.8\n      ...\n      7.533333\n      7.233333\n      6.933333\n      6.666667\n      6.233333\n      6.1\n      5.7\n      5.566667\n      5.4\n      5.1\n    \n    \n      8\n      1.136667\n      1.113333\n      1.113333\n      1.24\n      1.28\n      1.293333\n      1.356667\n      1.306667\n      1.196667\n      1.243333\n      ...\n      68.969531\n      72.637812\n      77.266406\n      80.955574\n      83.512063\n      86.996406\n      88.326719\n      91.507541\n      93.867937\n      90.629688\n    \n    \n      9\n      -0.571137\n      0.018616\n      0.165415\n      -0.326461\n      -0.10769\n      0.07776\n      0.417852\n      -0.166964\n      -0.069546\n      0.416952\n      ...\n      2.133892\n      2.301005\n      0.741401\n      2.646158\n      1.939253\n      1.94912\n      -0.082572\n      1.779979\n      0.611129\n      -1.313894\n    \n    \n      10\n      0.547803\n      -0.018616\n      -0.038748\n      0.366461\n      0.121023\n      -0.014427\n      -0.467852\n      0.056964\n      0.116212\n      -0.483619\n      ...\n      1.534389\n      2.327588\n      2.947767\n      -0.089668\n      1.54509\n      -0.618807\n      3.263394\n      0.580416\n      -3.849378\n      1.797644\n    \n    \n      11\n      0.031876\n      0.026309\n      0.043736\n      0.032399\n      0.025897\n      0.024937\n      0.033407\n      0.029378\n      0.063377\n      0.031915\n      ...\n      0.035826\n      0.039586\n      0.050582\n      0.068121\n      0.053507\n      0.061846\n      0.075844\n      0.088754\n      0.087292\n      0.120518\n    \n    \n      12\n      1.715996\n      1.71653\n      1.716528\n      1.716291\n      1.716505\n      1.71653\n      1.71614\n      1.716525\n      1.716506\n      1.716114\n      ...\n      1.712317\n      1.70678\n      1.700683\n      1.716516\n      1.712178\n      1.715827\n      1.696552\n      1.715893\n      1.688317\n      1.710186\n    \n    \n      13\n      0.000436\n      0.0\n      0.000003\n      0.000198\n      0.000017\n      0.0\n      0.000334\n      0.000004\n      0.000042\n      0.00034\n      ...\n      0.003872\n      0.009922\n      0.020808\n      0.000027\n      0.006085\n      0.001148\n      0.040359\n      0.001537\n      0.066262\n      0.021487\n    \n    \n      14\n      0.325303\n      -0.011023\n      -0.023152\n      0.217675\n      0.071646\n      -0.008537\n      -0.278046\n      0.033784\n      0.070162\n      -0.287194\n      ...\n      0.913035\n      1.387735\n      1.76764\n      -0.054274\n      0.92795\n      -0.373291\n      1.983474\n      0.355264\n      -2.354259\n      1.120004\n    \n  \n\n15 rows × 178 columns\n\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      2.919125e-01\n    \n    \n      1\n      2.627560e-01\n    \n    \n      2\n      1.711475e+00\n    \n    \n      3\n      1.001190e+01\n    \n    \n      4\n      1.885342e-10\n    \n    \n      5\n      7.000000e+00\n    \n    \n      6\n      -3.441276e+02\n    \n    \n      7\n      7.062553e+02\n    \n    \n      8\n      7.348913e+02\n    \n    \n      9\n      4.979547e+02\n    \n    \n      10\n      1.700000e+02\n    \n    \n      11\n      1.780000e+02\n    \n  \n\n\n\n\n\n# finally, lets plot the model and somehow get the plots into python\n# in pure R, this would be: `plot(model)`\n# for this, however we want to get a png image of the plots\n\n# first of all, lets get a temporary file path that we can use to save the image\ntemp_file = tempfile.NamedTemporaryFile(suffix=\".png\")\n\n# now lets save the image to the temporary file using grDevices package\ngrdevices.png(temp_file.name, width=1200, height=800)\ngraphics.par(mfrow=robjects.IntVector([2, 2]))\nrobjects.r[\"plot\"](r_model) # creates 4 plots that will be combined into one image\ngrdevices.dev_off()\n\n# now we split the image into the 4 plots\nimage = PILImage.open(temp_file.name)\nwidth, height = image.size\nplot_width = width / 2\nplot_height = height / 2\nplots = [\n    image.crop((0, 0, plot_width, plot_height)),\n    image.crop((plot_width, 0, width, plot_height)),\n    image.crop((0, plot_height, plot_width, height)),\n    image.crop((plot_width, plot_height, width, height))\n]\n\n# display the plots\nfor plot in plots:\n    display_png(plot)\n\n# and finally, lets delete the temporary file\nos.remove(temp_file.name)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html",
    "href": "notebooks/r_demo/r-ecm-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "Install R with Homebrew on macOS:\nbrew install r"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "href": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "title": "ValidMind",
    "section": "Step 1: Load the Required Libraries",
    "text": "Step 1: Load the Required Libraries\nWe will start by loading the necessary libraries that we will use in this notebook. Here, we will use the ecm package to build the ECM model.\n\nlibrary(ecm)\nlibrary(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "href": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "title": "ValidMind",
    "section": "Step 2: Load the Data",
    "text": "Step 2: Load the Data\nNext, we will load the data that we will use to build the ECM model. For this example, we will use ecm to predict Wilshire 5000 index based on corporate profits, Federal Reserve funds rate, and unemployment rate\n\n# Load the data\ndata(Wilshire)\n# Use 2015-12-01 and earlier data to build models\ntrn <- Wilshire[Wilshire$date<='2015-12-01',]"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 3: Build the ECM Model",
    "text": "Step 3: Build the ECM Model\nNow, we will build the ECM model using the ecm package.\n\n# Assume all predictors are needed in the equilibrium and transient terms of ecm.\nxeq <- xtr <- trn[c('CorpProfits', 'FedFundsRate', 'UnempRate')]\nmodel <- ecm(trn$Wilshire5000, xeq, xtr, includeIntercept=TRUE)\n\nsummary(model)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\nsummary(model)$coefficients\n\n\n\nA matrix: 8 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits 0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate-1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1 0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1 0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1-0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1-0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\ntidy(model)\n\n\n\nA tibble: 8 × 5\n\n    termestimatestd.errorstatisticp.value\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    (Intercept)      -0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits  0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate   -1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1   0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1  0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1    -0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1            -0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\naugment(model)\n\n\n\nA tibble: 178 × 15\n\n    .rownamesdydeltaCorpProfitsdeltaFedFundsRatedeltaUnempRateCorpProfitsLag1FedFundsRateLag1UnempRateLag1yLag1.fitted.resid.hat.sigma.cooksd.std.resid\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    2 -0.023333333  4.012 0.91000000 0.13333333 59.168 4.5633335.9000001.1366667-0.57113659 0.5478032600.031876201.7159964.355351e-04 0.325303481\n    3  0.000000000  2.183-0.72333333-0.10000000 63.180 5.4733336.0333331.1133333 0.01861637-0.0186163730.026308501.7165304.104050e-07-0.011023359\n    4  0.126666667  4.194-1.21000000-0.16666667 65.363 4.7500005.9333331.1133333 0.16541470-0.0387480300.043736251.7165283.064464e-06-0.023152097\n    5  0.040000000  1.068 0.76000000-0.06666667 69.557 3.5400005.7666671.2400000-0.32646091 0.3664609080.032399111.7162911.983193e-04 0.217675258\n    6  0.013333333  3.195 0.44000000-0.13333333 70.625 4.3000005.7000001.2800000-0.10768956 0.1210228920.025897371.7165051.705888e-05 0.071646457\n    7  0.063333333  6.352 0.40333333-0.20000000 73.820 4.7400005.5666671.2933333 0.07776005-0.0144267170.024936681.7165302.329577e-07-0.008536516\n    8 -0.050000000 11.655 1.39333333-0.43333333 80.172 5.1433335.3666671.3566667 0.41785218-0.4678521810.033407001.7161403.339924e-04-0.278045827\n    9 -0.110000000  3.034 1.28000000 0.00000000 91.827 6.5366674.9333331.3066667-0.16696419 0.0569641920.029377801.7165254.318091e-06 0.033783635\n    10 0.046666667  1.692 2.74333333-0.13333333 94.861 7.8166674.9333331.1966667-0.06954559 0.1162122590.063376581.7165064.163627e-05 0.070161519\n    11-0.066666667  4.836-0.56333333-0.03333333 96.55310.5600004.8000001.2433333 0.41695185-0.4836185150.031915461.7161143.398987e-04-0.287194305\n    12-0.046666667  6.127-0.67333333 0.36666667101.389 9.9966674.7666671.1766667-0.16750282 0.1208361570.028032821.7165051.848956e-05 0.071614448\n    13-0.110000000  2.535 1.92666667 0.06666667107.516 9.3233335.1333331.1300000-0.13104919 0.0210491890.043999891.7165309.102812e-07 0.012578705\n    14-0.186666667  5.197 0.84000000 0.43333333110.05111.2500005.2000001.0200000-0.37615628 0.1894896130.039931241.7164666.638164e-05 0.112996186\n    15 0.003333333-13.096-2.74333333 0.96666667115.24812.0900005.6333330.8333333-0.89326786 0.8966011900.077122691.7150293.106433e-03 0.545326840\n    16 0.140000000-12.925-3.04333333 1.66666667102.152 9.3466676.6000000.8366667-2.16010566 2.3001056600.149084931.7057814.648641e-02 1.456915037\n    17 0.143333333  4.811-0.88333333 0.60000000 89.227 6.3033338.2666670.9766667-0.95851326 1.1018465940.045981031.7143362.617436e-03 0.659131663\n    18-0.043333333 16.609 0.74000000-0.40000000 94.038 5.4200008.8666671.1200000 0.38541881-0.4287521470.039334411.7162013.343566e-04-0.255593460\n    19 0.040000000  7.948-0.74666667-0.16666667110.647 6.1600008.4666671.0766667 0.23467281-0.1946728100.029288701.7164635.026893e-05-0.115448901\n    20 0.160000000  8.331-0.58666667-0.56666667118.595 5.4133338.3000001.1166667 0.79331290-0.6333128990.047063801.7158058.870848e-04-0.379066991\n    21 0.023333333  3.825 0.37000000-0.16666667126.926 4.8266677.7333331.2766667 0.03671404-0.0133807080.022620991.7165301.809310e-07-0.007908191\n    22 0.036666667  2.177 0.08666667 0.16666667130.751 5.1966677.5666671.3000000-0.40014906 0.4368157300.023399561.7161941.997742e-04 0.258267261\n    23 0.026666667  0.171-0.41000000 0.03333333132.928 5.2833337.7333331.3366667-0.16367336 0.1903400260.021229221.7164673.426111e-05 0.112413682\n    24-0.010000000 10.848-0.21333333-0.26666667133.099 4.8733337.7666671.3633333 0.35622331-0.3662233060.024874771.7162941.497267e-04-0.216693223\n    25 0.006666667  8.840 0.49666667-0.36666667143.947 4.6600007.5000001.3533333 0.42305580-0.4163891310.024524301.7162241.906916e-04-0.246331926\n    26 0.006666667  6.448 0.66333333-0.23333333152.787 5.1566677.1333331.3600000 0.25178222-0.2451155550.019335271.7164255.154897e-05-0.144623924\n    27-0.013333333  1.004 0.69333333-0.23333333159.235 5.8200006.9000001.3666667 0.25599483-0.2693281660.017280481.7164035.538973e-05-0.158743710\n    28-0.040000000  5.396 0.24333333-0.33333333160.239 6.5133336.6666671.3533333 0.57352801-0.6135280130.018415451.7158703.070187e-04-0.361826262\n    29 0.153333333 18.499 0.52666667-0.33333333165.635 6.7566676.3333331.3133333 0.74534680-0.5920134700.018631561.7159152.893460e-04-0.349176568\n    30 0.126666667  6.325 0.81666667 0.03333333184.134 7.2833336.0000001.4666667 0.11674323 0.0099234380.017495501.7165307.616416e-08 0.005849577\n    31-0.116666667 11.520 1.48333333-0.13333333190.459 8.1000006.0333331.5933333 0.40888139-0.5255480600.025206401.7160423.126651e-04-0.311018089\n    ⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n    150 -3.93984375 -40.796-0.146666667 0.666666671229.6192.086666675.33333351.19484-0.41969615-3.520147600.049995301.6939002.929332e-02-2.11021907\n    151-13.21515625-517.379-1.433333333 0.866666671188.8231.940000006.00000047.25500-6.29287534-6.922280910.450834881.5589003.056885e+00-5.45792159\n    152 -3.41295850 372.812-0.323333333 1.40000000 671.4440.506666676.86666734.03984 2.35246941-5.765427910.316976911.6304949.638047e-01-4.07608658\n    153  3.81120999  76.362-0.003333333 1.033333331044.2560.183333338.26666730.62689 0.33287856 3.478331430.119783231.6926747.982295e-02 2.16623560\n    154  4.38612351 152.571-0.023333333 0.333333331120.6180.180000009.30000034.43810 2.31067811 2.075445400.070595941.7085241.502320e-02 1.25787816\n    155  3.78281250 101.178-0.036666667 0.300000001273.1890.156666679.63333338.82422 1.99182565 1.790986850.063379231.7106189.889477e-03 1.08128468\n    156  1.98280482  75.820 0.013333333-0.100000001374.3670.120000009.93333342.60703 2.40347852-0.420673710.052818351.7162094.446089e-04-0.25255634\n    157  0.99476711 -13.432 0.060000000-0.200000001450.1870.133333339.83333344.58984 1.62109998-0.626332870.054187501.7158161.014071e-03-0.37629827\n    158 -1.36429067  62.316-0.006666667-0.166666671436.7550.193333339.63333345.58460 2.43239779-3.796688470.048396961.6902203.287650e-02-2.27408451\n    159  5.05359375  -4.611 0.000000000 0.033333331499.0710.186666679.46666744.22031 1.55575748 3.497836270.056815211.6940253.334564e-02 2.10441132\n    160  4.37222278-144.215-0.030000000-0.466666671494.4600.186666679.50000049.27391 0.44374298 3.928479800.090026541.6870467.160301e-02 2.40624425\n    161  0.91371224  72.900-0.063333333 0.033333331350.2450.156666679.03333353.64613 1.79302659-0.879314350.043377471.7151371.564013e-03-0.52529517\n    162 -3.86734127   7.241-0.010000000-0.066666671423.1450.093333339.06666754.55984 1.30859223-5.175933500.039141481.6677824.846909e-02-3.08523686\n    163 -0.01948413  76.687-0.010000000-0.366666671430.3860.083333339.00000050.69250 2.73901212-2.758496250.042936741.7027721.522165e-02-1.64752351\n    164  5.57569380 220.314 0.030000000-0.366666671507.0730.073333338.63333350.67302 4.68268067 0.893013130.098256501.7150064.112255e-03 0.54947240\n    165  0.03144905 -72.595 0.050000000-0.066666671727.3870.103333338.26666756.24871 1.15447874-1.123029690.061144801.7142143.733480e-03-0.67720706\n    166  2.23761905  31.842-0.010000000-0.166666671654.7920.153333338.20000056.28016 2.37120363-0.133584590.041488471.7164993.438860e-05-0.07972366\n    167  1.14496416 -22.685 0.016666667-0.233333331686.6340.143333338.03333358.51778 1.83236442-0.687400260.042991361.7156799.465378e-04-0.41056442\n    168  4.96225806  15.537-0.016666667-0.066666671663.9490.160000007.80000059.66274 1.96072774 3.001530320.035880771.7003481.484071e-02 1.78610468\n    169  4.34453125 -21.338-0.026666667-0.200000001679.4860.143333337.73333364.62500 1.59517181 2.749359440.035211341.7029731.220254e-02 1.63547900\n    170  3.66828125  27.453-0.033333333-0.300000001658.1480.116666677.53333368.96953 2.13389190 1.534389350.035826041.7123173.871937e-03 0.91303500\n    171  4.62859375  44.735 0.003333333-0.300000001685.6010.083333337.23333372.63781 2.30100541 2.327588340.039586121.7067809.922189e-03 1.38773488\n    172  3.68916752 -79.877-0.013333333-0.266666671730.3360.086666676.93333377.26641 0.74140097 2.947766550.050581811.7006832.080820e-02 1.76764015\n    173  2.55648972  86.058 0.020000000-0.433333331650.4590.073333336.66666780.95557 2.64615821-0.089668480.068120881.7165162.691596e-05-0.05427372\n    174  3.48434276  49.698-0.003333333-0.133333331736.5170.093333336.23333383.51206 1.93925279 1.545089970.053506951.7121786.084879e-03 0.92795006\n    175  1.33031250  15.633 0.010000000-0.400000001786.2150.090000006.10000086.99641 1.94911984-0.618807340.061846411.7158271.148277e-03-0.37329144\n    176  3.18082223-122.484 0.010000000-0.133333331801.8480.100000005.70000088.32672-0.08257207 3.263394300.075844431.6965524.035912e-02 1.98347420\n    177  2.36039552  64.793 0.013333333-0.166666671679.3640.110000005.56666791.50754 1.77997920 0.580416330.088753711.7158931.536609e-03 0.35526406\n    178 -3.23824901 -58.055 0.013333333-0.300000001744.1570.123333335.40000093.86794 0.61112886-3.849377870.087292381.6883176.626180e-02-2.35425908\n    179  0.48375000-191.198 0.023333333-0.100000001686.1020.136666675.10000090.62969-1.31389361 1.797643610.120518221.7101862.148698e-02 1.12000437\n\n\n\n\n\nglance(model)\n\n\n\nA tibble: 1 × 12\n\n    r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobs\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><int><int>\n\n\n    0.29191250.2627561.71147510.01191.885342e-107-344.1276706.2553734.8913497.9547170178\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(model)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 4: Save the ECM Model",
    "text": "Step 4: Save the ECM Model\nFinally, we will save the ECM model to a file so that we can use it later.\n\n# save the model to an RDS file\nsaveRDS(model, 'r-ecm-model.rds')"
  },
  {
    "objectID": "notebooks/r_demo/rpy2.html",
    "href": "notebooks/r_demo/rpy2.html",
    "title": "ValidMind",
    "section": "",
    "text": "from rpy2.robjects.packages import importr\n\n\nbase = importr('base')\n\n\nfrom rpy2.robjects.packages import importr\n\ntidyr = importr('tidyr')\nggplot2 = importr('ggplot2')\npurrr = importr('purrr')\nprintr = importr('printr')\npROC = importr('pROC') \nROCR = importr('ROCR') \ncaret = importr('caret')\ncar = importr('car')\nrpart = importr('rpart')\nrpart_plot = importr('rpart.plot')\n\n\nfrom rpy2.robjects import r\n\ndata = r['read.csv']('./datasets/bank_customer_churn.csv', stringsAsFactors = True)\n\n\nr['str'](data)\n\n'data.frame':   8000 obs. of  14 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : Factor w/ 2616 levels \"Abazu\",\"Abbie\",..: 1002 1060 1832 258 1634 485 156 1793 1032 970 ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : Factor w/ 3 levels \"France\",\"Germany\",..: 1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n<rpy2.rinterface_lib.sexp.NULLType object at 0x103d3c740> [RTYPES.NILSXP]\n\n\n\nr('''\n    knitr::kable(sapply(data, function(x) sum(is.na(x))), col.names = c(\"Missing Value Count\"))\n''')\n\n\n\n        StrVector with 10 elements.\n        \n        \n          \n          \n            \n            '|       ...\n            \n          \n            \n            '|:------...\n            \n          \n            \n            '|...    ...\n            \n          \n            \n            ...\n            \n          \n            \n            '|envir  ...\n            \n          \n            \n            '|overwri...\n            \n          \n            \n            '|       ...\n            \n          \n          \n        \n        \n        \n\n\n\nr(\"\"\"\n    # plot box plot\n    data[, names(data) %in% c('Age', 'Balance', 'CreditScore', 'EstimatedSalary')] %>%\n    gather() %>%\n    ggplot(aes(value)) +\n        facet_wrap(~ key, scales = \"free\") +\n        geom_boxplot() +\n        theme(axis.text.x = element_text(size = 7, angle=90), axis.text.y = element_text(size = 7))\n\"\"\")\n\nR[write to console]: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable\n\n\n\nRRuntimeError: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable"
  },
  {
    "objectID": "notebooks/r_demo/r-customer-churn-model.html",
    "href": "notebooks/r_demo/r-customer-churn-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebook demonstrates the process of creating a customer churn model using R. We will use the XGBoost and logistic regression algorithms to create two separate models and compare their performance.\nThe dataset used in this notebook is located at ../datasets/bank_customer_churn.csv.\n\n\n\n\nbrew install r\nYou additionally might need the following packages to run this notebook if you run into errors:\nbrew install harfbuzz fribidi libtiff libomp\n\n\n\ninstall.packages(\"xgboost\")\ninstall.packages(\"caret\")\ninstall.packages(\"pROC\")\n\n\n\n\nFirst, we load the required libraries.\n\n# load the required libraries\nlibrary(xgboost)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC)\n\n\n\n\nNow, we import the dataset and preprocess it by removing irrelevant columns, converting categorical variables, and one-hot encoding certain columns.\n\n# import the dataset\ndf <- read.csv(\"../datasets/bank_customer_churn.csv\", header = TRUE)\n\n# remove irrelevant columns\ndf <- df %>% select(-c(RowNumber, CustomerId, Surname, CreditScore))\n\n# Convert the 'Gender' column to 0 or 1 (assuming \"Female\" should be 0 and \"Male\" should be 1)\ndf$Gender <- ifelse(df$Gender == \"Female\", 0, 1)\n\n# one-hot encode categorical columns with caret\ndf <- dummyVars(\" ~ .\", data = df) %>% predict(df)\n\n# remove GeographySpain since it causes multicollinearity\ndf <- subset(df, select = -GeographySpain)\n\nsummary(df)\n\n GeographyFrance  GeographyGermany     Gender            Age       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :18.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:32.00  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :37.00  \n Mean   :0.5012   Mean   :0.2511   Mean   :0.5495   Mean   :38.95  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:44.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :92.00  \n     Tenure          Balance       NumOfProducts     HasCrCard     \n Min.   : 0.000   Min.   :     0   Min.   :1.000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:     0   1st Qu.:1.000   1st Qu.:0.0000  \n Median : 5.000   Median : 97264   Median :1.000   Median :1.0000  \n Mean   : 5.034   Mean   : 76434   Mean   :1.532   Mean   :0.7026  \n 3rd Qu.: 8.000   3rd Qu.:128045   3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :250898   Max.   :4.000   Max.   :1.0000  \n IsActiveMember   EstimatedSalary         Exited     \n Min.   :0.0000   Min.   :    11.58   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.: 50857.10   1st Qu.:0.000  \n Median :1.0000   Median : 99504.89   Median :0.000  \n Mean   :0.5199   Mean   : 99790.19   Mean   :0.202  \n 3rd Qu.:1.0000   3rd Qu.:149216.32   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :199992.48   Max.   :1.000  \n\n\n\n\n\nNext, we split the data into training and testing sets using a 70/30 ratio.\n\n# split data into training and testing sets\nset.seed(123)\ntrain_index <- sample(1:nrow(df), size = round(0.7*nrow(df)), replace = FALSE)\ndf_train <- df[train_index, ]\ndf_test <- df[-train_index, ]\n\n\n\n\nWe save the train and test datasets as CSV files.\n\n# save the train and test datasets as csv files\nwrite.csv(df_train, file = \"r_churn_train.csv\", row.names = FALSE)\nwrite.csv(df_test, file = \"r_churn_test.csv\", row.names = FALSE)\n\n\n\n\nWe convert the data into DMatrix format, which is required by the XGBoost library.\n\n# convert data into DMatrix format\ndtrain <- xgb.DMatrix(data = df_train[,-c(11)], label = df_train[,\"Exited\"])\ndtest <- xgb.DMatrix(data = df_test[,-c(11)], label = df_test[,\"Exited\"])\n\n\n\n\nWe set up the XGBoost parameters to be used during the training process.\n\n# set up XGBoost parameters\nparams <- list(\n  objective = \"binary:logistic\",\n  eval_metric = \"auc\",\n  max_depth = 3,\n  eta = 0.1,\n  gamma = 0.5,\n  subsample = 0.8,\n  colsample_bytree = 0.8,\n  min_child_weight = 1,\n  nthread = 4\n)\n\n\n\n\nWe train the XGBoost model using the parameters and data prepared earlier.\n\n# train the XGBoost model\nmodel <- xgb.train(\n  params = params,\n  data = dtrain,\n  nrounds = 100,\n  watchlist = list(train = dtrain, test = dtest),\n  early_stopping_rounds = 10\n)\n\n[1] train-auc:0.795105  test-auc:0.793822 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 10 rounds.\n\n[2] train-auc:0.820697  test-auc:0.808123 \n[3] train-auc:0.823965  test-auc:0.811294 \n[4] train-auc:0.837212  test-auc:0.823692 \n[5] train-auc:0.839206  test-auc:0.827146 \n[6] train-auc:0.843781  test-auc:0.832219 \n[7] train-auc:0.853531  test-auc:0.836494 \n[8] train-auc:0.857080  test-auc:0.838679 \n[9] train-auc:0.857191  test-auc:0.839732 \n[10]    train-auc:0.856166  test-auc:0.840575 \n[11]    train-auc:0.857386  test-auc:0.841168 \n[12]    train-auc:0.857084  test-auc:0.841385 \n[13]    train-auc:0.856794  test-auc:0.842336 \n[14]    train-auc:0.857827  test-auc:0.841208 \n[15]    train-auc:0.858503  test-auc:0.842312 \n[16]    train-auc:0.860074  test-auc:0.843007 \n[17]    train-auc:0.858916  test-auc:0.843317 \n[18]    train-auc:0.858676  test-auc:0.843113 \n[19]    train-auc:0.858821  test-auc:0.843309 \n[20]    train-auc:0.859816  test-auc:0.845118 \n[21]    train-auc:0.860960  test-auc:0.845355 \n[22]    train-auc:0.860677  test-auc:0.845800 \n[23]    train-auc:0.862110  test-auc:0.848165 \n[24]    train-auc:0.863008  test-auc:0.847703 \n[25]    train-auc:0.863292  test-auc:0.848459 \n[26]    train-auc:0.864104  test-auc:0.849124 \n[27]    train-auc:0.863918  test-auc:0.848931 \n[28]    train-auc:0.864840  test-auc:0.849988 \n[29]    train-auc:0.867052  test-auc:0.850861 \n[30]    train-auc:0.867307  test-auc:0.851330 \n[31]    train-auc:0.867691  test-auc:0.851321 \n[32]    train-auc:0.868384  test-auc:0.852436 \n[33]    train-auc:0.870037  test-auc:0.854785 \n[34]    train-auc:0.870912  test-auc:0.855056 \n[35]    train-auc:0.871229  test-auc:0.855566 \n[36]    train-auc:0.871868  test-auc:0.855823 \n[37]    train-auc:0.872831  test-auc:0.857390 \n[38]    train-auc:0.873618  test-auc:0.856942 \n[39]    train-auc:0.874207  test-auc:0.858250 \n[40]    train-auc:0.874417  test-auc:0.858442 \n[41]    train-auc:0.874423  test-auc:0.858399 \n[42]    train-auc:0.875082  test-auc:0.858565 \n[43]    train-auc:0.876418  test-auc:0.858693 \n[44]    train-auc:0.876752  test-auc:0.858042 \n[45]    train-auc:0.877068  test-auc:0.857847 \n[46]    train-auc:0.877774  test-auc:0.857960 \n[47]    train-auc:0.879001  test-auc:0.858793 \n[48]    train-auc:0.879490  test-auc:0.858593 \n[49]    train-auc:0.880067  test-auc:0.859837 \n[50]    train-auc:0.880651  test-auc:0.860296 \n[51]    train-auc:0.880768  test-auc:0.860439 \n[52]    train-auc:0.881091  test-auc:0.861333 \n[53]    train-auc:0.881637  test-auc:0.861322 \n[54]    train-auc:0.881942  test-auc:0.861473 \n[55]    train-auc:0.882016  test-auc:0.861432 \n[56]    train-auc:0.882721  test-auc:0.861169 \n[57]    train-auc:0.882949  test-auc:0.861621 \n[58]    train-auc:0.883306  test-auc:0.861810 \n[59]    train-auc:0.883506  test-auc:0.861506 \n[60]    train-auc:0.883632  test-auc:0.861516 \n[61]    train-auc:0.883968  test-auc:0.861404 \n[62]    train-auc:0.884255  test-auc:0.861261 \n[63]    train-auc:0.884719  test-auc:0.861073 \n[64]    train-auc:0.885060  test-auc:0.861043 \n[65]    train-auc:0.885252  test-auc:0.861357 \n[66]    train-auc:0.885285  test-auc:0.861367 \n[67]    train-auc:0.885594  test-auc:0.860972 \n[68]    train-auc:0.886119  test-auc:0.860755 \nStopping. Best iteration:\n[58]    train-auc:0.883306  test-auc:0.861810\n\n\n\n\n\n\nWe display a summary of the trained XGBoost model.\n\nsummary(model)\n\n                Length Class              Mode       \nhandle              1  xgb.Booster.handle externalptr\nraw             81624  -none-             raw        \nbest_iteration      1  -none-             numeric    \nbest_ntreelimit     1  -none-             numeric    \nbest_score          1  -none-             numeric    \nbest_msg            1  -none-             character  \nniter               1  -none-             numeric    \nevaluation_log      3  data.table         list       \ncall                6  -none-             call       \nparams             10  -none-             list       \ncallbacks           3  -none-             list       \nfeature_names      10  -none-             character  \nnfeatures           1  -none-             numeric    \n\n\n\n\n\nWe make predictions on the test data and calculate the accuracy of the model.\n\n# predict on test data\ntest_preds <- predict(model, dtest)\n\n# Convert predicted probabilities to binary predictions\ntest_preds_binary <- ifelse(test_preds > 0.5, 1, 0)\n\n# Calculate accuracy on test set\naccuracy <- sum(test_preds_binary == df_test[,\"Exited\"])/nrow(df_test)\naccuracy\n\n0.860416666666667\n\n\n\n\n\nWe calculate the confusion matrix, precision, recall, F1 score, and ROC AUC for the model.\n\n# Calculate the confusion matrix\ncm <- confusionMatrix(as.factor(test_preds_binary), as.factor(df_test[,\"Exited\"]))\n\n# Calculate precision, recall, and F1 score\nprecision <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[2, 1])\nrecall <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[1, 2])\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\ncat(\"Precision:\", precision, \"\\n\")\ncat(\"Recall:\", recall, \"\\n\")\ncat(\"F1 Score:\", f1_score, \"\\n\")\n\n# Calculate ROC AUC\nroc_obj <- roc(df_test[,\"Exited\"], test_preds)\nroc_auc <- auc(roc_obj)\ncat(\"ROC AUC:\", roc_auc, \"\\n\")\n\nPrecision: 0.7687075 \nRecall: 0.4584178 \nF1 Score: 0.5743329 \n\n\nSetting levels: control = 0, case = 1\n\nSetting direction: controls < cases\n\n\n\nROC AUC: 0.86181 \n\n\n\n\n\nWe save the trained XGBoost model as a JSON file.\n\n# save the model (notice the .json extension, we could also save it as .bin)\n# this ensures compatibility with the ValidMind sdk\nxgb.save(model, \"r_xgb_churn_model.json\")\n\nTRUE\n\n\n\n\n\nAs a comparison, we train a simple logistic regression model using the training data.\n\n# now lets train a simple logistic regression model\nlg_reg_model <- glm(Exited ~ ., data = as.data.frame(df_train), family = \"binomial\")\n\n\n\n\nWe display a summary of the trained logistic regression model.\n\nsummary(lg_reg_model)\n\n\nCall:\nglm(formula = Exited ~ ., family = \"binomial\", data = as.data.frame(df_train))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3284  -0.6470  -0.4563  -0.2781   2.8954  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      -3.732e+00  2.305e-01 -16.187  < 2e-16 ***\nGeographyFrance  -1.113e-01  9.474e-02  -1.174 0.240252    \nGeographyGermany  7.394e-01  1.054e-01   7.018 2.25e-12 ***\nGender           -4.974e-01  7.319e-02  -6.796 1.07e-11 ***\nAge               7.142e-02  3.433e-03  20.803  < 2e-16 ***\nTenure           -1.170e-02  1.263e-02  -0.926 0.354301    \nBalance           2.525e-06  6.995e-07   3.610 0.000306 ***\nNumOfProducts    -1.300e-01  6.475e-02  -2.008 0.044643 *  \nHasCrCard        -1.468e-02  8.025e-02  -0.183 0.854836    \nIsActiveMember   -9.979e-01  7.682e-02 -12.989  < 2e-16 ***\nEstimatedSalary   2.248e-07  6.337e-07   0.355 0.722854    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5612.8  on 5599  degrees of freedom\nResidual deviance: 4760.6  on 5589  degrees of freedom\nAIC: 4782.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\ncoef(lg_reg_model)\n\n(Intercept)-3.73163074444561GeographyFrance-0.111254692545161GeographyGermany0.73938562185064Gender-0.497380970370031Age0.0714194308317766Tenure-0.0116956480620419Balance2.52544411990314e-06NumOfProducts-0.130015233022276HasCrCard-0.0146825015464598IsActiveMember-0.997861450907317EstimatedSalary2.24751413745725e-07\n\n\n\n\n\nWe make predictions on the test data and calculate the accuracy of the logistic regression model.\n\n# Make predictions on test set\ntest_preds <- predict(lg_reg_model, newdata = as.data.frame(df_test), type = \"response\")\n\n# Convert predicted probabilities to binary predictions\ntest_preds_binary <- ifelse(test_preds > 0.5, 1, 0)\n\n# Calculate accuracy on test set\naccuracy <- sum(test_preds_binary == df_test[,\"Exited\"])/nrow(df_test)\naccuracy\n\n0.805416666666667\n\n\n\n\n\nWe calculate the confusion matrix, precision, recall, F1 score, and ROC AUC for the logistic regression model.\n\n# Calculate the confusion matrix\ncm <- confusionMatrix(as.factor(test_preds_binary), as.factor(df_test[,\"Exited\"]))\n\n# Calculate precision, recall, and F1 score\nprecision <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[2, 1])\nrecall <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[1, 2])\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\ncat(\"Precision:\", precision, \"\\n\")\ncat(\"Recall:\", recall, \"\\n\")\ncat(\"F1 Score:\", f1_score, \"\\n\")\n\n# Calculate ROC AUC\nroc_obj <- roc(df_test[,\"Exited\"], test_preds)\nroc_auc <- auc(roc_obj)\ncat(\"ROC AUC:\", roc_auc, \"\\n\")\n\nPrecision: 0.5890411 \nRecall: 0.1744422 \nF1 Score: 0.2691706 \n\n\nSetting levels: control = 0, case = 1\n\nSetting direction: controls < cases\n\n\n\nROC AUC: 0.7616043 \n\n\n\n\n\nWe save the trained logistic regression model as an RDS file.\n\n# save the model\nsaveRDS(lg_reg_model, \"r_log_reg_churn_model.rds\")"
  },
  {
    "objectID": "notebooks/r_demo/r-python.html",
    "href": "notebooks/r_demo/r-python.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nfrom pypmml import Model\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = Model.fromFile('./prune_dt.pmml')\n\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel.predict({\n    \"CreditScore\": 0.64,\n    \"Geography\": 0,\n    \"Gender\": 0,\n    \"Age\": 0.51936320,\n    \"Tenure\": 2,\n    \"Balance\": 0.9118043,\n    \"NumOfProducts\": 1,\n    \"HasCrCard\": 1,\n    \"IsActiveMember\": 1,\n    \"EstimatedSalary\": 0.506734893\n})\n\n\nmodel.inputNames"
  },
  {
    "objectID": "notebooks/time_series/time_series_demo.html",
    "href": "notebooks/time_series/time_series_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "Time Series Test Plan Demo\n\nimport pandas as pd\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clg4hlb8d0046jn8hwqnes4ak\"\n)\n  \n\nTrue\n\n\n\ndf = pd.read_csv(\"../datasets/lending_club_loan_rates.csv\", sep='\\t')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n  \n  \n    \n      0\n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      1\n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2\n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      3\n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      4\n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n  \n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 137 entries, 0 to 136\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Unnamed: 0         137 non-null    object \n 1   loan_rate_A        137 non-null    float64\n 2   loan_rate_B        137 non-null    float64\n 3   loan_rate_C        137 non-null    float64\n 4   loan_rate_D        137 non-null    float64\n 5   FEDFUNDS           137 non-null    float64\n 6   diff1_loan_rate_A  137 non-null    float64\n 7   diff1_loan_rate_B  137 non-null    float64\n 8   diff1_loan_rate_C  137 non-null    float64\n 9   diff1_loan_rate_D  137 non-null    float64\n 10  diff1_FEDFUNDS     137 non-null    float64\n 11  diff2_FEDFUNDS     137 non-null    float64\ndtypes: float64(11), object(1)\nmemory usage: 13.0+ KB\n\n\n\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2018-08-01\n      7.218997\n      11.161286\n      15.142618\n      19.857603\n      1.91\n      0.052118\n      0.045181\n      0.056796\n      0.088167\n      0.00\n      -0.09\n    \n    \n      2018-09-01\n      7.201281\n      11.191918\n      15.139769\n      19.748459\n      1.95\n      -0.017716\n      0.030632\n      -0.002849\n      -0.109144\n      0.04\n      0.04\n    \n    \n      2018-10-01\n      7.228498\n      11.208418\n      15.129105\n      19.792163\n      2.19\n      0.027218\n      0.016500\n      -0.010665\n      0.043704\n      0.24\n      0.20\n    \n    \n      2018-11-01\n      7.536897\n      11.390483\n      15.126869\n      19.632697\n      2.20\n      0.308399\n      0.182066\n      -0.002235\n      -0.159466\n      0.01\n      -0.23\n    \n    \n      2018-12-01\n      7.715209\n      11.459631\n      15.107476\n      19.558346\n      2.27\n      0.178312\n      0.069148\n      -0.019393\n      -0.074350\n      0.07\n      0.06\n    \n  \n\n137 rows × 11 columns\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                              Name                          Description                                      \n\n\nsklearn_classifier_metrics      SKLearnClassifierMetrics      Test plan for sklearn classifier metrics         \nsklearn_classifier_validation   SKLearnClassifierPerformance  Test plan for sklearn classifier models          \nsklearn_classifier              SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                  \ntabular_dataset                 TabularDataset                Test plan for generic tabular datasets           \ntabular_dataset_description     TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                  \ntabular_data_quality            TabularDataQuality            Test plan for data quality on tabular datasets   \nnormality_test_plan             NormalityTestPlan             Test plan to perform normality tests.            \nautocorrelation_test_plan       AutocorrelationTestPlan       Test plan to perform autocorrelation tests.      \nseasonality_test_plan           SesonalityTestPlan            Test plan to perform seasonality tests.          \nstationarity_test_plan          StationarityTestPlan          Test plan to perform stationarity tests.         \ntimeseries_test_plan            TimeSeriesTestPlan            Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                  \ntimeseries_univariate_inspectionTimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.\n\n\n\n\n\nloan_rate_columns = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\ndiff1_loan_rate_columns = [\"diff1_loan_rate_A\", \"diff1_loan_rate_B\", \"diff1_loan_rate_C\", \"diff1_loan_rate_D\"]\n\ntest_plan_config = {\n    \"time_series_univariate_inspection_raw\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    },\n    \"time_series_univariate_inspection_histogram\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    }\n}\n\nvm.run_test_plan(\"timeseries_univariate_inspection\", config=test_plan_config, dataset=vm_dataset)\n\n                                                                                                                                 \n\n\nResults for time_series_univariate_inspection Test Plan:\n            Logged the following plots\n            to the ValidMind platform:\n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/explore_metrics.html",
    "href": "notebooks/time_series/explore_metrics.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Data libraries \nimport pandas as pd\n\n# ML libraries\nfrom numpy import argmax\nimport scipy.stats as stats\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\nfrom arch.unitroot import PhillipsPerron\nfrom arch.unitroot import ZivotAndrews\nfrom arch.unitroot import DFGLS\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport xgboost as xgb\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# ValidMind libraries \nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nimport validmind as vm\nfrom validmind.vm_models.test_context import TestContext\n\n\n# Quick hack to load local library code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\ndf = pd.read_csv(\"/Users/juanvalidmind/Dev/github/validmind/validmind-python/notebooks/datasets/lending_club_loan_rates.csv\", sep='\\t')\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n  \n\n\n\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"cl1jyvh2c000909lg1rk0a0zb\"\n)\n\nTrue\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"loan_rate_A\",    \n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                              Name                          Description                                      \n\n\nsklearn_classifier_metrics      SKLearnClassifierMetrics      Test plan for sklearn classifier metrics         \nsklearn_classifier_validation   SKLearnClassifierPerformance  Test plan for sklearn classifier models          \nsklearn_classifier              SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                  \ntabular_dataset                 TabularDataset                Test plan for generic tabular datasets           \ntabular_dataset_description     TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                  \ntabular_data_quality            TabularDataQuality            Test plan for data quality on tabular datasets   \nnormality_test_plan             NormalityTestPlan             Test plan to perform normality tests.            \nautocorrelation_test_plan       AutocorrelationTestPlan       Test plan to perform autocorrelation tests.      \nseasonality_test_plan           SesonalityTestPlan            Test plan to perform seasonality tests.          \nunit_root_test_plan             UnitRootTestPlan              Test plan to perform unit root tests.            \nstationarity_test_plan          StationarityTestPlan          Test plan to perform stationarity tests.         \ntimeseries_test_plan            TimeSeriesTestPlan            Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                  \ntimeseries_univariate_inspectionTimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.\n\n\n\n\nCreate Train and Test Datasets\n\ntest_size = 30\ntrain_ds = df[:-test_size]\ntest_ds = df[-test_size:]\n\n\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"loan_rate_A\")\nvm_test_ds = vm.init_dataset(dataset=test_ds, type=\"generic\", target_column=\"loan_rate_A\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nOriginal\n\nadftest = adfuller(df['loan_rate_A'])\nadftest\n\n(-1.917289312690944,\n 0.32397189281015515,\n 1,\n 135,\n {'1%': -3.479742586699182,\n  '5%': -2.88319822181578,\n  '10%': -2.578319684499314},\n -71.08908853191068)\n\n\nValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import ADFTest\ntest_context = TestContext(train_ds=vm_train_ds)\nmetric = ADFTest(test_context=test_context)\nmetric.run()\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='evaluation', scope='test', key='adf', value={'stat': -2.6036355880044475, 'pvalue': 0.09224819101584802, 'usedlag': 11, 'nobs': 95, 'icbest': -291.89983267237756}, value_formatter='key_values'))\n\n\n\n\n\nOriginal\n\nkpsstest = kpss(df['loan_rate_A'])\nkpsstest\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n\n\n(1.012356679488042,\n 0.01,\n 6,\n {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739})\n\n\nValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import KPSSTest\ntest_context = TestContext(train_ds=vm_train_ds)\nmetric = KPSSTest(test_context=test_context)\nmetric.run()\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='evaluation', scope='test', key='kpss', value={'stat': 0.06862873540051123, 'pvalue': 0.1, 'usedlag': 8}, value_formatter='key_values'))\n\n\n\n\n\nOriginal\n\npp = PhillipsPerron(df['loan_rate_A'])\npp\n\n\n\nPhillips-Perron Test (Z-tau)\n\n  Test Statistic    -2.027\n\n\n  P-value            0.275\n\n\n  Lags                  13\n\nTrend: ConstantCritical Values: -3.48 (1%), -2.88 (5%), -2.58 (10%)Null Hypothesis: The process contains a unit root.Alternative Hypothesis: The process is weakly stationary.\n\n\n\npp.nobs\n\n136\n\n\nValidMind\n\n\n\nOriginal\n\nza = ZivotAndrews(df['loan_rate_A'])\nza\n\n\n\nZivot-Andrews Results\n\n  Test Statistic    -3.499\n\n\n  P-value            0.680\n\n\n  Lags                   1\n\nTrend: ConstantCritical Values: -5.28 (1%), -4.81 (5%), -4.57 (10%)Null Hypothesis: The process contains a unit root with a single structural break.Alternative Hypothesis: The process is trend and break stationary.\n\n\n\nza.nobs\n\n137\n\n\nValidMind\n\n\n\nOriginal\n\ndfgls = DFGLS(df['loan_rate_A'])\ndfgls\n\n\n\nDickey-Fuller GLS Results\n\n  Test Statistic    -1.798\n\n\n  P-value            0.071\n\n\n  Lags                   1\n\nTrend: ConstantCritical Values: -2.71 (1%), -2.09 (5%), -1.78 (10%)Null Hypothesis: The process contains a unit root.Alternative Hypothesis: The process is weakly stationary.\n\n\n\ndfgls.nobs\n\n135\n\n\nValidMind\n\n\n\nOff ValidMind\n\nsd = seasonal_decompose(df['loan_rate_A'])\nsd\n\n<statsmodels.tsa.seasonal.DecomposeResult at 0x28a928550>\n\n\n\nsd.trend\n\nDate\n2007-08-01   NaN\n2007-09-01   NaN\n2007-10-01   NaN\n2007-11-01   NaN\n2007-12-01   NaN\n              ..\n2018-08-01   NaN\n2018-09-01   NaN\n2018-10-01   NaN\n2018-11-01   NaN\n2018-12-01   NaN\nName: trend, Length: 137, dtype: float64\n\n\n\nfig, ax = plt.subplots()\n\n\n\n\nIn ValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalDecomposeMetricWithFigure\ntest_context = TestContext(train_ds=vm_train_ds)\nsd_metric = SeasonalDecomposeMetricWithFigure(test_context=test_context)\nsd_metric.run()\n\nTestPlanMetricResult(figures=[Figure(key='seasonal_decomposition_with_figure', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)], metric=MetricResult(type='evaluation', scope='', key='seasonal_decomposition_with_figure', value={'loan_rate_A': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.050284773390468294, 'resid': nan, 'observed': 7.7666666666666675}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.06087962072801919, 'resid': nan, 'observed': 7.841428571428572}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.01749661199350142, 'resid': nan, 'observed': 7.83}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.047258378330469496, 'resid': nan, 'observed': 7.779090909090908}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.0850517814632488, 'resid': nan, 'observed': 7.695833333333333}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.06564185816692857, 'resid': nan, 'observed': 7.961333333333333}, {'date': '2008-02-01', 'trend': 8.005048767959094, 'seasonal': 0.008943337297934315, 'resid': 0.1163412280763044, 'observed': 8.130333333333333}, {'date': '2008-03-01', 'trend': 8.036669799705125, 'seasonal': -0.002099404440702747, 'resid': 0.09171531902129201, 'observed': 8.126285714285714}, {'date': '2008-04-01', 'trend': 8.079229323514648, 'seasonal': 0.020168623084867585, 'resid': -0.007314613266182244, 'observed': 8.092083333333333}, {'date': '2008-05-01', 'trend': 8.144335744871071, 'seasonal': -0.01070385076597299, 'resid': 0.017796677323472867, 'observed': 8.151428571428571}, {'date': '2008-06-01', 'trend': 8.239462007497334, 'seasonal': -0.028819584167133785, 'resid': -0.005142423330199462, 'observed': 8.2055}, {'date': '2008-07-01', 'trend': 8.343548073071103, 'seasonal': 0.0027433998162858285, 'resid': -0.12585669027869376, 'observed': 8.220434782608695}, {'date': '2008-08-01', 'trend': 8.417007001892738, 'seasonal': -0.050284773390468294, 'resid': -0.0797222285022692, 'observed': 8.287}, {'date': '2008-09-01', 'trend': 8.47796981223055, 'seasonal': -0.06087962072801919, 'resid': -0.3370901915025301, 'observed': 8.08}, {'date': '2008-10-01', 'trend': 8.531636932259564, 'seasonal': 0.01749661199350142, 'resid': 0.06372359860407714, 'observed': 8.612857142857143}, {'date': '2008-11-01', 'trend': 8.589412987821856, 'seasonal': -0.047258378330469496, 'resid': 0.016633269296492413, 'observed': 8.558787878787879}, {'date': '2008-12-01', 'trend': 8.652802325527823, 'seasonal': 0.0850517814632488, 'resid': 0.4613125596755969, 'observed': 9.199166666666668}, {'date': '2009-01-01', 'trend': 8.714159305191414, 'seasonal': 0.06564185816692857, 'resid': 0.17626441041215057, 'observed': 8.956065573770493}, {'date': '2009-02-01', 'trend': 8.752934265042391, 'seasonal': 0.008943337297934315, 'resid': 0.13673778227506048, 'observed': 8.898615384615386}, {'date': '2009-03-01', 'trend': 8.775809794329618, 'seasonal': -0.002099404440702747, 'resid': 0.04740072122219648, 'observed': 8.821111111111112}, {'date': '2009-04-01', 'trend': 8.783591261550063, 'seasonal': 0.020168623084867585, 'resid': -0.11849106743062912, 'observed': 8.685268817204301}, {'date': '2009-05-01', 'trend': 8.768108091755868, 'seasonal': -0.01070385076597299, 'resid': 0.18746418006273688, 'observed': 8.944868421052632}, {'date': '2009-06-01', 'trend': 8.727065734609644, 'seasonal': -0.028819584167133785, 'resid': 0.23515810487663832, 'observed': 8.933404255319148}, {'date': '2009-07-01', 'trend': 8.660325582011376, 'seasonal': 0.0027433998162858285, 'resid': 0.30202905738802394, 'observed': 8.965098039215686}, {'date': '2009-08-01', 'trend': 8.568932625411962, 'seasonal': -0.050284773390468294, 'resid': -0.04571207220498089, 'observed': 8.472935779816513}, {'date': '2009-09-01', 'trend': 8.454561132788863, 'seasonal': -0.06087962072801919, 'resid': 0.04939541101607955, 'observed': 8.443076923076923}, {'date': '2009-10-01', 'trend': 8.34828280363049, 'seasonal': 0.01749661199350142, 'resid': 0.0707560174468764, 'observed': 8.436535433070867}, {'date': '2009-11-01', 'trend': 8.23694393241163, 'seasonal': -0.047258378330469496, 'resid': 0.1738279594323525, 'observed': 8.363513513513514}, {'date': '2009-12-01', 'trend': 8.116444096407664, 'seasonal': 0.0850517814632488, 'resid': 0.20792858256074262, 'observed': 8.409424460431655}, {'date': '2010-01-01', 'trend': 7.995096023438102, 'seasonal': 0.06564185816692857, 'resid': 0.0833062360420266, 'observed': 8.144044117647057}, {'date': '2010-02-01', 'trend': 7.889837683206626, 'seasonal': 0.008943337297934315, 'resid': -0.3815751381516201, 'observed': 7.517205882352941}, {'date': '2010-03-01', 'trend': 7.807926812466593, 'seasonal': -0.002099404440702747, 'resid': -0.34822261760672807, 'observed': 7.457604790419162}, {'date': '2010-04-01', 'trend': 7.704196284836861, 'seasonal': 0.020168623084867585, 'resid': -0.22626966982649002, 'observed': 7.498095238095239}, {'date': '2010-05-01', 'trend': 7.556635938363027, 'seasonal': -0.01070385076597299, 'resid': -0.08602299668796279, 'observed': 7.4599090909090915}, {'date': '2010-06-01', 'trend': 7.386339108881637, 'seasonal': -0.028819584167133785, 'resid': 0.1688479966530175, 'observed': 7.52636752136752}, {'date': '2010-07-01', 'trend': 7.234176921822315, 'seasonal': 0.0027433998162858285, 'resid': 0.22286070025920932, 'observed': 7.45978102189781}, {'date': '2010-08-01', 'trend': 7.139619393217407, 'seasonal': -0.050284773390468294, 'resid': 0.3627180117520081, 'observed': 7.452052631578947}, {'date': '2010-09-01', 'trend': 7.084157646540273, 'seasonal': -0.06087962072801919, 'resid': 0.47482114774146483, 'observed': 7.498099173553719}, {'date': '2010-10-01', 'trend': 7.0310647217648095, 'seasonal': 0.01749661199350142, 'resid': -0.1565808142777916, 'observed': 6.891980519480519}, {'date': '2010-11-01', 'trend': 6.991663604498002, 'seasonal': -0.047258378330469496, 'resid': -0.5777851144356879, 'observed': 6.366620111731844}, {'date': '2010-12-01', 'trend': 6.958236325031823, 'seasonal': 0.0850517814632488, 'resid': -0.7240941518351232, 'observed': 6.319193954659949}, {'date': '2011-01-01', 'trend': 6.9214056863686135, 'seasonal': 0.06564185816692857, 'resid': -0.40466541054050575, 'observed': 6.582382133995036}, {'date': '2011-02-01', 'trend': 6.889921185335679, 'seasonal': 0.008943337297934315, 'resid': -0.08937734314643411, 'observed': 6.80948717948718}, {'date': '2011-03-01', 'trend': 6.856889534975067, 'seasonal': -0.002099404440702747, 'resid': -0.020548557500655016, 'observed': 6.834241573033709}, {'date': '2011-04-01', 'trend': 6.858796195374667, 'seasonal': 0.020168623084867585, 'resid': -0.03173655758996877, 'observed': 6.847228260869565}, {'date': '2011-05-01', 'trend': 6.921615774690598, 'seasonal': -0.01070385076597299, 'resid': 0.25423732980671804, 'observed': 7.165149253731343}, {'date': '2011-06-01', 'trend': 7.018217147459357, 'seasonal': -0.028819584167133785, 'resid': 0.029475088064770363, 'observed': 7.018872651356993}, {'date': '2011-07-01', 'trend': 7.11041588830481, 'seasonal': 0.0027433998162858285, 'resid': -0.029818724129772076, 'observed': 7.083340563991324}, {'date': '2011-08-01', 'trend': 7.1830792783975985, 'seasonal': -0.050284773390468294, 'resid': -0.0599294403121207, 'observed': 7.0728650646950095}, {'date': '2011-09-01', 'trend': 7.247497405208671, 'seasonal': -0.06087962072801919, 'resid': -0.10209065269770515, 'observed': 7.084527131782947}, {'date': '2011-10-01', 'trend': 7.309379082976638, 'seasonal': 0.01749661199350142, 'resid': 0.024436715871515715, 'observed': 7.351312410841655}, {'date': '2011-11-01', 'trend': 7.353215232472669, 'seasonal': -0.047258378330469496, 'resid': 0.10900126981090005, 'observed': 7.4149581239531}, {'date': '2011-12-01', 'trend': 7.387931350546878, 'seasonal': 0.0850517814632488, 'resid': 0.11630575687876285, 'observed': 7.589288888888889}, {'date': '2012-01-01', 'trend': 7.433640607004464, 'seasonal': 0.06564185816692857, 'resid': 0.025774514885588032, 'observed': 7.525056980056981}, {'date': '2012-02-01', 'trend': 7.478542797820363, 'seasonal': 0.008943337297934315, 'resid': 0.12324756053387684, 'observed': 7.610733695652174}, {'date': '2012-03-01', 'trend': 7.5208057544537, 'seasonal': -0.002099404440702747, 'resid': 0.06032375032145173, 'observed': 7.579030100334449}, {'date': '2012-04-01', 'trend': 7.555975713482307, 'seasonal': 0.020168623084867585, 'resid': 0.011455663432825416, 'observed': 7.5876}, {'date': '2012-05-01', 'trend': 7.57810750100383, 'seasonal': -0.01070385076597299, 'resid': -0.09055854773216213, 'observed': 7.476845102505695}, {'date': '2012-06-01', 'trend': 7.596137974654016, 'seasonal': -0.028819584167133785, 'resid': -0.02695475412324547, 'observed': 7.540363636363637}, {'date': '2012-07-01', 'trend': 7.6153264121992175, 'seasonal': 0.0027433998162858285, 'resid': 0.04080192195124229, 'observed': 7.658871733966746}, {'date': '2012-08-01', 'trend': 7.628451463506009, 'seasonal': -0.050284773390468294, 'resid': -0.0031802158143688156, 'observed': 7.574986474301172}, {'date': '2012-09-01', 'trend': 7.638002595789809, 'seasonal': -0.06087962072801919, 'resid': 0.019593706315086022, 'observed': 7.596716681376876}, {'date': '2012-10-01', 'trend': 7.658826993137152, 'seasonal': 0.01749661199350142, 'resid': 0.006878272803619381, 'observed': 7.683201877934273}, {'date': '2012-11-01', 'trend': 7.677789303574363, 'seasonal': -0.047258378330469496, 'resid': -0.01629936786684364, 'observed': 7.61423155737705}, {'date': '2012-12-01', 'trend': 7.68098745426593, 'seasonal': 0.0850517814632488, 'resid': 0.0567075873402256, 'observed': 7.822746823069404}, {'date': '2013-01-01', 'trend': 7.6779520344633525, 'seasonal': 0.06564185816692857, 'resid': 0.008527654331044757, 'observed': 7.752121546961326}, {'date': '2013-02-01', 'trend': 7.677057001432264, 'seasonal': 0.008943337297934315, 'resid': 0.012670021380605325, 'observed': 7.698670360110803}, {'date': '2013-03-01', 'trend': 7.682469578470097, 'seasonal': -0.002099404440702747, 'resid': 0.039950436657629505, 'observed': 7.720320610687024}, {'date': '2013-04-01', 'trend': 7.691712575890414, 'seasonal': 0.020168623084867585, 'resid': 0.2342138270083863, 'observed': 7.946095025983668}, {'date': '2013-05-01', 'trend': 7.706156831040658, 'seasonal': -0.01070385076597299, 'resid': -0.12200745325962684, 'observed': 7.573445527015058}, {'date': '2013-06-01', 'trend': 7.718205987083048, 'seasonal': -0.028819584167133785, 'resid': -0.16886757446403147, 'observed': 7.520518828451883}, {'date': '2013-07-01', 'trend': 7.72646306080784, 'seasonal': 0.0027433998162858285, 'resid': -0.12333999400747178, 'observed': 7.605866466616654}, {'date': '2013-08-01', 'trend': 7.738107999623397, 'seasonal': -0.050284773390468294, 'resid': -0.08131227732781923, 'observed': 7.606510948905109}, {'date': '2013-09-01', 'trend': 7.7509664574560295, 'seasonal': -0.06087962072801919, 'resid': 0.005007218952953606, 'observed': 7.695094055680964}, {'date': '2013-10-01', 'trend': 7.755204227116213, 'seasonal': 0.01749661199350142, 'resid': 0.03395560260807626, 'observed': 7.806656441717791}, {'date': '2013-11-01', 'trend': 7.750869193258667, 'seasonal': -0.047258378330469496, 'resid': 0.13382830227119422, 'observed': 7.837439117199391}, {'date': '2013-12-01', 'trend': 7.741988403538943, 'seasonal': 0.0850517814632488, 'resid': 0.06167882326227134, 'observed': 7.888719008264463}, {'date': '2014-01-01', 'trend': 7.724369241349729, 'seasonal': 0.06564185816692857, 'resid': 0.09430803164457958, 'observed': 7.884319131161237}, {'date': '2014-02-01', 'trend': 7.7012496300205235, 'seasonal': 0.008943337297934315, 'resid': 0.1357583401657619, 'observed': 7.84595130748422}, {'date': '2014-03-01', 'trend': 7.667279037619656, 'seasonal': -0.002099404440702747, 'resid': 0.21646301811787813, 'observed': 7.881642651296831}, {'date': '2014-04-01', 'trend': 7.6252937267207255, 'seasonal': 0.020168623084867585, 'resid': 0.24101710741265112, 'observed': 7.886479457218244}, {'date': '2014-05-01', 'trend': 7.584095565289032, 'seasonal': -0.01070385076597299, 'resid': -0.044371431323670635, 'observed': 7.529020283199388}, {'date': '2014-06-01', 'trend': 7.536200793048272, 'seasonal': -0.028819584167133785, 'resid': -0.15557608988697594, 'observed': 7.351805118994163}, {'date': '2014-07-01', 'trend': 7.483981914831136, 'seasonal': 0.0027433998162858285, 'resid': -0.13500503111416087, 'observed': 7.351720283533261}, {'date': '2014-08-01', 'trend': 7.429040668396031, 'seasonal': -0.050284773390468294, 'resid': -0.07296943491799038, 'observed': 7.305786460087572}, {'date': '2014-09-01', 'trend': 7.36455133491674, 'seasonal': -0.06087962072801919, 'resid': -0.12314738731106348, 'observed': 7.1805243268776575}, {'date': '2014-10-01', 'trend': 7.294261771259082, 'seasonal': 0.01749661199350142, 'resid': 0.0018203256941895195, 'observed': 7.313578708946773}, {'date': '2014-11-01', 'trend': 7.234071423791347, 'seasonal': -0.047258378330469496, 'resid': 0.15494793014887884, 'observed': 7.341760975609756}, {'date': '2014-12-01', 'trend': 7.191857825197275, 'seasonal': 0.0850517814632488, 'resid': -0.04198699058463887, 'observed': 7.234922616075885}, {'date': '2015-01-01', 'trend': 7.15300977164523, 'seasonal': 0.06564185816692857, 'resid': 0.06621081632638982, 'observed': 7.284862446138549}, {'date': '2015-02-01', 'trend': 7.114847188355677, 'seasonal': 0.008943337297934315, 'resid': 0.0030275524107579964, 'observed': 7.126818078064369}, {'date': '2015-03-01', 'trend': 7.08538334834315, 'seasonal': -0.002099404440702747, 'resid': -0.030252066688752053, 'observed': 7.053031877213695}, {'date': '2015-04-01', 'trend': 7.054016766629, 'seasonal': 0.020168623084867585, 'resid': -0.04604468619627991, 'observed': 7.028140703517588}, {'date': '2015-05-01', 'trend': 7.013202303085584, 'seasonal': -0.01070385076597299, 'resid': -0.05970775464519265, 'observed': 6.942790697674418}, {'date': '2015-06-01', 'trend': 6.9738382666741785, 'seasonal': -0.028819584167133785, 'resid': -0.02011034424566108, 'observed': 6.924908338261384}, {'date': '2015-07-01', 'trend': 6.938361615724768, 'seasonal': 0.0027433998162858285, 'resid': -0.09484123652406432, 'observed': 6.846263779016989}, {'date': '2015-08-01', 'trend': 6.908688078173169, 'seasonal': -0.050284773390468294, 'resid': 0.03693766087185387, 'observed': 6.895340965654555}, {'date': '2015-09-01', 'trend': 6.87437653070801, 'seasonal': -0.06087962072801919, 'resid': 0.07034075103002743, 'observed': 6.883837661010018}, {'date': '2015-10-01', 'trend': 6.828130516887571, 'seasonal': 0.01749661199350142, 'resid': 0.011840284793749958, 'observed': 6.857467413674822}, {'date': '2015-11-01', 'trend': 6.8029077692831725, 'seasonal': -0.047258378330469496, 'resid': 0.06267575488702634, 'observed': 6.818325145839729}, {'date': '2015-12-01', 'trend': 6.809591954262458, 'seasonal': 0.0850517814632488, 'resid': -0.08102216375352447, 'observed': 6.813621571972182}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.06564185816692857, 'resid': nan, 'observed': 6.854723867456379}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.008943337297934315, 'resid': nan, 'observed': 6.844791755508173}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.002099404440702747, 'resid': nan, 'observed': 6.51158106060606}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.020168623084867585, 'resid': nan, 'observed': 6.459687188434696}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.01070385076597299, 'resid': nan, 'observed': 6.905898270251756}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.028819584167133785, 'resid': nan, 'observed': 7.12222120518688}], 'loan_rate_B': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.004683903074718656, 'resid': nan, 'observed': 9.497692307692308}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.02878733626807229, 'resid': nan, 'observed': 9.276666666666666}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.07292104494645359, 'resid': nan, 'observed': 9.433333333333334}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.03883917943439751, 'resid': nan, 'observed': 9.467777777777778}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.020002635551053742, 'resid': nan, 'observed': 9.3875}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.057087759887233934, 'resid': nan, 'observed': 9.693125}, {'date': '2008-02-01', 'trend': 9.815614077323866, 'seasonal': -0.0026761200794264053, 'resid': 0.22279237983421207, 'observed': 10.035730337078652}, {'date': '2008-03-01', 'trend': 9.903222857432645, 'seasonal': -0.00016982951529819455, 'resid': 0.15404374627620265, 'observed': 10.05709677419355}, {'date': '2008-04-01', 'trend': 10.006583968543756, 'seasonal': 3.19587595681934e-05, 'resid': 0.15461214287211555, 'observed': 10.16122807017544}, {'date': '2008-05-01', 'trend': 10.121889524099311, 'seasonal': -0.021465549087508966, 'resid': -0.022507308345133377, 'observed': 10.077916666666669}, {'date': '2008-06-01', 'trend': 10.267257038529324, 'seasonal': 0.0048551821613346344, 'resid': -0.20393040250884076, 'observed': 10.068181818181818}, {'date': '2008-07-01', 'trend': 10.43272381322504, 'seasonal': -0.07584606528025915, 'resid': -0.19872959979663263, 'observed': 10.158148148148149}, {'date': '2008-08-01', 'trend': 10.595628603740092, 'seasonal': -0.004683903074718656, 'resid': -0.14730833702901014, 'observed': 10.443636363636363}, {'date': '2008-09-01', 'trend': 10.760741481659695, 'seasonal': 0.02878733626807229, 'resid': -0.3561954845944334, 'observed': 10.433333333333334}, {'date': '2008-10-01', 'trend': 10.92033892958057, 'seasonal': 0.07292104494645359, 'resid': -0.2359266411936919, 'observed': 10.757333333333332}, {'date': '2008-11-01', 'trend': 11.06172533831073, 'seasonal': -0.03883917943439751, 'resid': -0.11177504776522074, 'observed': 10.911111111111111}, {'date': '2008-12-01', 'trend': 11.192890459921573, 'seasonal': -0.020002635551053742, 'resid': 0.26009918861649195, 'observed': 11.432987012987011}, {'date': '2009-01-01', 'trend': 11.319548541808823, 'seasonal': 0.057087759887233934, 'resid': 0.2422042780140867, 'observed': 11.618840579710144}, {'date': '2009-02-01', 'trend': 11.435830739857689, 'seasonal': -0.0026761200794264053, 'resid': 0.5865751099514681, 'observed': 12.01972972972973}, {'date': '2009-03-01', 'trend': 11.551304435676638, 'seasonal': -0.00016982951529819455, 'resid': 0.4846718454515647, 'observed': 12.035806451612904}, {'date': '2009-04-01', 'trend': 11.656503680463528, 'seasonal': 3.19587595681934e-05, 'resid': 0.35632150363404713, 'observed': 12.012857142857143}, {'date': '2009-05-01', 'trend': 11.74012123279847, 'seasonal': -0.021465549087508966, 'resid': -0.09909428020218936, 'observed': 11.619561403508772}, {'date': '2009-06-01', 'trend': 11.79583062627412, 'seasonal': 0.0048551821613346344, 'resid': -0.1261858084354538, 'observed': 11.6745}, {'date': '2009-07-01', 'trend': 11.813709500580932, 'seasonal': -0.07584606528025915, 'resid': -0.14623950367674132, 'observed': 11.591623931623932}, {'date': '2009-08-01', 'trend': 11.7577233646958, 'seasonal': -0.004683903074718656, 'resid': 0.047893871712256, 'observed': 11.800933333333337}, {'date': '2009-09-01', 'trend': 11.645087849532663, 'seasonal': 0.02878733626807229, 'resid': 0.17352987749040427, 'observed': 11.84740506329114}, {'date': '2009-10-01', 'trend': 11.531122201859514, 'seasonal': 0.07292104494645359, 'resid': 0.2640002314549018, 'observed': 11.86804347826087}, {'date': '2009-11-01', 'trend': 11.436971193428466, 'seasonal': -0.03883917943439751, 'resid': 0.40909020822815567, 'observed': 11.807222222222224}, {'date': '2009-12-01', 'trend': 11.380949115633296, 'seasonal': -0.020002635551053742, 'resid': 0.5129548652092385, 'observed': 11.87390134529148}, {'date': '2010-01-01', 'trend': 11.346807882383448, 'seasonal': 0.057087759887233934, 'resid': 0.20312358849855025, 'observed': 11.607019230769232}, {'date': '2010-02-01', 'trend': 11.308341582352016, 'seasonal': -0.0026761200794264053, 'resid': -0.6177816448452021, 'observed': 10.687883817427387}, {'date': '2010-03-01', 'trend': 11.25711953661246, 'seasonal': -0.00016982951529819455, 'resid': -0.5925497070971618, 'observed': 10.6644}, {'date': '2010-04-01', 'trend': 11.175070289518446, 'seasonal': 3.19587595681934e-05, 'resid': -0.5260141979635501, 'observed': 10.649088050314464}, {'date': '2010-05-01', 'trend': 11.034948084615989, 'seasonal': -0.021465549087508966, 'resid': -0.28977624182218864, 'observed': 10.723706293706291}, {'date': '2010-06-01', 'trend': 10.85903782999656, 'seasonal': 0.0048551821613346344, 'resid': 0.36193223056055446, 'observed': 11.225825242718448}, {'date': '2010-07-01', 'trend': 10.708866411920479, 'seasonal': -0.07584606528025915, 'resid': 0.5878887442688713, 'observed': 11.22090909090909}, {'date': '2010-08-01', 'trend': 10.639141951735194, 'seasonal': -0.004683903074718656, 'resid': 0.6139989246332928, 'observed': 11.248456973293768}, {'date': '2010-09-01', 'trend': 10.622033529095082, 'seasonal': 0.02878733626807229, 'resid': 0.5197314602182407, 'observed': 11.170552325581395}, {'date': '2010-10-01', 'trend': 10.605833809461833, 'seasonal': 0.07292104494645359, 'resid': -0.10304056869400115, 'observed': 10.575714285714286}, {'date': '2010-11-01', 'trend': 10.613230598722458, 'seasonal': -0.03883917943439751, 'resid': -0.8377729221782316, 'observed': 9.736618497109829}, {'date': '2010-12-01', 'trend': 10.624008511489265, 'seasonal': -0.020002635551053742, 'resid': -0.8813469164006392, 'observed': 9.722658959537572}, {'date': '2011-01-01', 'trend': 10.61271994083742, 'seasonal': 0.057087759887233934, 'resid': -0.5156601180274538, 'observed': 10.1541475826972}, {'date': '2011-02-01', 'trend': 10.599581112470494, 'seasonal': -0.0026761200794264053, 'resid': -0.1295365713384361, 'observed': 10.467368421052631}, {'date': '2011-03-01', 'trend': 10.596316101806579, 'seasonal': -0.00016982951529819455, 'resid': -0.12183301927923353, 'observed': 10.474313253012047}, {'date': '2011-04-01', 'trend': 10.642422990347459, 'seasonal': 3.19587595681934e-05, 'resid': -0.19207342300261188, 'observed': 10.450381526104415}, {'date': '2011-05-01', 'trend': 10.760849905635485, 'seasonal': -0.021465549087508966, 'resid': 0.3605514036233296, 'observed': 11.099935760171306}, {'date': '2011-06-01', 'trend': 10.91443738038156, 'seasonal': 0.0048551821613346344, 'resid': 0.18897312011393141, 'observed': 11.108265682656826}, {'date': '2011-07-01', 'trend': 11.0539560569123, 'seasonal': -0.07584606528025915, 'resid': 0.08943296369441832, 'observed': 11.06754295532646}, {'date': '2011-08-01', 'trend': 11.162766577022664, 'seasonal': -0.004683903074718656, 'resid': -0.07159144587776857, 'observed': 11.086491228070177}, {'date': '2011-09-01', 'trend': 11.268679036144619, 'seasonal': 0.02878733626807229, 'resid': -0.043308557541674626, 'observed': 11.254157814871016}, {'date': '2011-10-01', 'trend': 11.388977923898196, 'seasonal': 0.07292104494645359, 'resid': 0.13677515256110262, 'observed': 11.598674121405752}, {'date': '2011-11-01', 'trend': 11.488625115138078, 'seasonal': -0.03883917943439751, 'resid': 0.10611869262731599, 'observed': 11.555904628330996}, {'date': '2011-12-01', 'trend': 11.570384807610292, 'seasonal': -0.020002635551053742, 'resid': 0.039090050162983274, 'observed': 11.589472222222222}, {'date': '2012-01-01', 'trend': 11.66228664035388, 'seasonal': 0.057087759887233934, 'resid': -0.08359184349081586, 'observed': 11.635782556750298}, {'date': '2012-02-01', 'trend': 11.76319166689917, 'seasonal': -0.0026761200794264053, 'resid': -0.16332961717150074, 'observed': 11.597185929648242}, {'date': '2012-03-01', 'trend': 11.860822521307425, 'seasonal': -0.00016982951529819455, 'resid': 0.025742071551277383, 'observed': 11.886394763343404}, {'date': '2012-04-01', 'trend': 11.934068361018747, 'seasonal': 3.19587595681934e-05, 'resid': -0.008626997919452104, 'observed': 11.925473321858863}, {'date': '2012-05-01', 'trend': 11.99482340535943, 'seasonal': -0.021465549087508966, 'resid': 0.04301869790214507, 'observed': 12.016376554174066}, {'date': '2012-06-01', 'trend': 12.048930270550104, 'seasonal': 0.0048551821613346344, 'resid': 0.10027205527578197, 'observed': 12.15405750798722}, {'date': '2012-07-01', 'trend': 12.091371298420814, 'seasonal': -0.07584606528025915, 'resid': 0.21186988270164997, 'observed': 12.227395115842205}, {'date': '2012-08-01', 'trend': 12.13362143172545, 'seasonal': -0.004683903074718656, 'resid': 0.21942217599061886, 'observed': 12.34835970464135}, {'date': '2012-09-01', 'trend': 12.161887323854964, 'seasonal': 0.02878733626807229, 'resid': 0.1447551839749598, 'observed': 12.335429844097996}, {'date': '2012-10-01', 'trend': 12.17420943046534, 'seasonal': 0.07292104494645359, 'resid': 0.028171769838638658, 'observed': 12.275302245250431}, {'date': '2012-11-01', 'trend': 12.181736211600096, 'seasonal': -0.03883917943439751, 'resid': 0.19450053649706522, 'observed': 12.337397568662764}, {'date': '2012-12-01', 'trend': 12.175118735274879, 'seasonal': -0.020002635551053742, 'resid': -0.04857205325722414, 'observed': 12.1065440464666}, {'date': '2013-01-01', 'trend': 12.130128219115964, 'seasonal': 0.057087759887233934, 'resid': -0.04992057760023518, 'observed': 12.137295401402962}, {'date': '2013-02-01', 'trend': 12.05328151565516, 'seasonal': -0.0026761200794264053, 'resid': 0.059070888731092334, 'observed': 12.109676284306826}, {'date': '2013-03-01', 'trend': 11.977871938153276, 'seasonal': -0.00016982951529819455, 'resid': 0.07458371115522532, 'observed': 12.052285819793203}, {'date': '2013-04-01', 'trend': 11.921326746462128, 'seasonal': 3.19587595681934e-05, 'resid': 0.1339541188363741, 'observed': 12.05531282405807}, {'date': '2013-05-01', 'trend': 11.881069792553062, 'seasonal': -0.021465549087508966, 'resid': 0.20757555574345343, 'observed': 12.067179799209006}, {'date': '2013-06-01', 'trend': 11.854568105629241, 'seasonal': 0.0048551821613346344, 'resid': 0.08501154335650916, 'observed': 11.944434831147085}, {'date': '2013-07-01', 'trend': 11.836843225112068, 'seasonal': -0.07584606528025915, 'resid': -0.40375175496345206, 'observed': 11.357245404868356}, {'date': '2013-08-01', 'trend': 11.814459446444463, 'seasonal': -0.004683903074718656, 'resid': -0.43558701081386386, 'observed': 11.37418853255588}, {'date': '2013-09-01', 'trend': 11.789602866611254, 'seasonal': 0.02878733626807229, 'resid': -0.318619046741066, 'observed': 11.49977115613826}, {'date': '2013-10-01', 'trend': 11.76503003465179, 'seasonal': 0.07292104494645359, 'resid': -0.08407474697564411, 'observed': 11.7538763326226}, {'date': '2013-11-01', 'trend': 11.719249496700483, 'seasonal': -0.03883917943439751, 'resid': 0.2122462702069169, 'observed': 11.892656587473002}, {'date': '2013-12-01', 'trend': 11.646817938065396, 'seasonal': -0.020002635551053742, 'resid': 0.2884292389703734, 'observed': 11.915244541484716}, {'date': '2014-01-01', 'trend': 11.592579216683331, 'seasonal': 0.057087759887233934, 'resid': 0.25353079740203666, 'observed': 11.903197773972602}, {'date': '2014-02-01', 'trend': 11.558608636178356, 'seasonal': -0.0026761200794264053, 'resid': 0.25063070761575534, 'observed': 11.806563223714685}, {'date': '2014-03-01', 'trend': 11.516112499599217, 'seasonal': -0.00016982951529819455, 'resid': 0.2428982943044916, 'observed': 11.75884096438841}, {'date': '2014-04-01', 'trend': 11.460639561456194, 'seasonal': 3.19587595681934e-05, 'resid': 0.2983381922199614, 'observed': 11.759009712435724}, {'date': '2014-05-01', 'trend': 11.375487965050318, 'seasonal': -0.021465549087508966, 'resid': -0.0892724159628101, 'observed': 11.26475}, {'date': '2014-06-01', 'trend': 11.261071150944403, 'seasonal': 0.0048551821613346344, 'resid': -0.2574191099917732, 'observed': 11.008507223113964}, {'date': '2014-07-01', 'trend': 11.142598337443784, 'seasonal': -0.07584606528025915, 'resid': -0.0753085724316206, 'observed': 10.991443699731905}, {'date': '2014-08-01', 'trend': 11.019433954700187, 'seasonal': -0.004683903074718656, 'resid': -0.09005374605251894, 'observed': 10.92469630557295}, {'date': '2014-09-01', 'trend': 10.88433692264195, 'seasonal': 0.02878733626807229, 'resid': 0.016231846311807734, 'observed': 10.92935610522183}, {'date': '2014-10-01', 'trend': 10.744093252808314, 'seasonal': 0.07292104494645359, 'resid': 0.17592657035175308, 'observed': 10.99294086810652}, {'date': '2014-11-01', 'trend': 10.623740180200944, 'seasonal': -0.03883917943439751, 'resid': 0.025052737481477398, 'observed': 10.609953738248024}, {'date': '2014-12-01', 'trend': 10.532876719632691, 'seasonal': -0.020002635551053742, 'resid': -0.06093023191390368, 'observed': 10.451943852167734}, {'date': '2015-01-01', 'trend': 10.449153446358014, 'seasonal': 0.057087759887233934, 'resid': 0.016909733029543995, 'observed': 10.523150939274792}, {'date': '2015-02-01', 'trend': 10.365652693622092, 'seasonal': -0.0026761200794264053, 'resid': -0.1323117009765743, 'observed': 10.230664872566091}, {'date': '2015-03-01', 'trend': 10.28402776621621, 'seasonal': -0.00016982951529819455, 'resid': -0.1914473905615518, 'observed': 10.09241054613936}, {'date': '2015-04-01', 'trend': 10.19996188279399, 'seasonal': 3.19587595681934e-05, 'resid': -0.14040178687606952, 'observed': 10.05959205467749}, {'date': '2015-05-01', 'trend': 10.131545323404618, 'seasonal': -0.021465549087508966, 'resid': -0.03438585913579207, 'observed': 10.075693915181317}, {'date': '2015-06-01', 'trend': 10.084529148703163, 'seasonal': 0.0048551821613346344, 'resid': -0.07254407656989471, 'observed': 10.016840254294603}, {'date': '2015-07-01', 'trend': 10.038650783234946, 'seasonal': -0.07584606528025915, 'resid': 0.010947392004321585, 'observed': 9.973752109959008}, {'date': '2015-08-01', 'trend': 10.003718613520608, 'seasonal': -0.004683903074718656, 'resid': -0.06066488076219157, 'observed': 9.938369829683698}, {'date': '2015-09-01', 'trend': 9.98791271441926, 'seasonal': 0.02878733626807229, 'resid': -0.06001572731742389, 'observed': 9.956684323369908}, {'date': '2015-10-01', 'trend': 9.98083261842099, 'seasonal': 0.07292104494645359, 'resid': -0.1057222155422445, 'observed': 9.9480314478252}, {'date': '2015-11-01', 'trend': 9.973055835915497, 'seasonal': -0.03883917943439751, 'resid': 0.07864907670333576, 'observed': 10.012865733184436}, {'date': '2015-12-01', 'trend': 9.974260889533937, 'seasonal': -0.020002635551053742, 'resid': -0.033614589586505546, 'observed': 9.920643664396378}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.057087759887233934, 'resid': nan, 'observed': 9.953370355808923}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.0026761200794264053, 'resid': nan, 'observed': 9.962073382887873}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.00016982951529819455, 'resid': nan, 'observed': 9.981660457385225}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 3.19587595681934e-05, 'resid': nan, 'observed': 10.000419839473144}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.021465549087508966, 'resid': nan, 'observed': 9.948223350253809}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.0048551821613346344, 'resid': nan, 'observed': 10.17323210606468}], 'loan_rate_C': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.015376141878352585, 'resid': nan, 'observed': 10.9475}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.04035308860321538, 'resid': nan, 'observed': 10.829166666666666}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.08058524115091258, 'resid': nan, 'observed': 10.825925925925926}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05290731070035628, 'resid': nan, 'observed': 10.967037037037038}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.02602059045270812, 'resid': nan, 'observed': 10.805}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.03449672569012794, 'resid': nan, 'observed': 11.288055555555555}, {'date': '2008-02-01', 'trend': 11.327619414408064, 'seasonal': 0.022574335467601885, 'resid': 0.17544261376069673, 'observed': 11.525636363636362}, {'date': '2008-03-01', 'trend': 11.40659510885251, 'seasonal': 0.010641559838160935, 'resid': 0.1864414922288681, 'observed': 11.60367816091954}, {'date': '2008-04-01', 'trend': 11.501532490144058, 'seasonal': -0.006495259466358668, 'resid': 0.1185565193223005, 'observed': 11.61359375}, {'date': '2008-05-01', 'trend': 11.613468328225728, 'seasonal': -0.02756787427454523, 'resid': 0.04891436086363189, 'observed': 11.634814814814815}, {'date': '2008-06-01', 'trend': 11.752703752291348, 'seasonal': -0.024120179891552147, 'resid': -0.07946592534097095, 'observed': 11.649117647058825}, {'date': '2008-07-01', 'trend': 11.912461763337886, 'seasonal': -0.03616359408614569, 'resid': -0.09501611796969031, 'observed': 11.78128205128205}, {'date': '2008-08-01', 'trend': 12.059014162327783, 'seasonal': -0.015376141878352585, 'resid': -0.17488802044942972, 'observed': 11.86875}, {'date': '2008-09-01', 'trend': 12.193218649730547, 'seasonal': 0.04035308860321538, 'resid': -0.43023840500042565, 'observed': 11.803333333333336}, {'date': '2008-10-01', 'trend': 12.322309884211464, 'seasonal': 0.08058524115091258, 'resid': -0.27263871510596693, 'observed': 12.13025641025641}, {'date': '2008-11-01', 'trend': 12.448988871382019, 'seasonal': -0.05290731070035628, 'resid': -0.0469148940149938, 'observed': 12.349166666666669}, {'date': '2008-12-01', 'trend': 12.57335810715044, 'seasonal': -0.02602059045270812, 'resid': 0.21718303124747265, 'observed': 12.764520547945205}, {'date': '2009-01-01', 'trend': 12.688007401986994, 'seasonal': 0.03449672569012794, 'resid': 0.4402231450501505, 'observed': 13.162727272727272}, {'date': '2009-02-01', 'trend': 12.803605690204769, 'seasonal': 0.022574335467601885, 'resid': 0.34204219654984847, 'observed': 13.16822222222222}, {'date': '2009-03-01', 'trend': 12.93876053448086, 'seasonal': 0.010641559838160935, 'resid': 0.23259790568097963, 'observed': 13.182}, {'date': '2009-04-01', 'trend': 13.068278952698869, 'seasonal': -0.006495259466358668, 'resid': 0.07167784522903088, 'observed': 13.13346153846154}, {'date': '2009-05-01', 'trend': 13.172533700281956, 'seasonal': -0.02756787427454523, 'resid': 0.010276892439188185, 'observed': 13.1552427184466}, {'date': '2009-06-01', 'trend': 13.251917774111583, 'seasonal': -0.024120179891552147, 'resid': -0.11424619235087095, 'observed': 13.11355140186916}, {'date': '2009-07-01', 'trend': 13.296720793031207, 'seasonal': -0.03616359408614569, 'resid': -0.19212582639604223, 'observed': 13.06843137254902}, {'date': '2009-08-01', 'trend': 13.317664880122448, 'seasonal': -0.015376141878352585, 'resid': 0.053670857715500855, 'observed': 13.355959595959597}, {'date': '2009-09-01', 'trend': 13.331744536272288, 'seasonal': 0.04035308860321538, 'resid': 0.18774237512449657, 'observed': 13.55984}, {'date': '2009-10-01', 'trend': 13.350328926794504, 'seasonal': 0.08058524115091258, 'resid': 0.05127761287650147, 'observed': 13.482191780821918}, {'date': '2009-11-01', 'trend': 13.373796709193448, 'seasonal': -0.05290731070035628, 'resid': 0.17845583960214642, 'observed': 13.499345238095238}, {'date': '2009-12-01', 'trend': 13.411937154210145, 'seasonal': -0.02602059045270812, 'resid': 0.13364318467023942, 'observed': 13.519559748427676}, {'date': '2010-01-01', 'trend': 13.47182703139438, 'seasonal': 0.03449672569012794, 'resid': -0.023363230768720024, 'observed': 13.482960526315788}, {'date': '2010-02-01', 'trend': 13.524905311746926, 'seasonal': 0.022574335467601885, 'resid': -0.1968325883909979, 'observed': 13.35064705882353}, {'date': '2010-03-01', 'trend': 13.554786033403438, 'seasonal': 0.010641559838160935, 'resid': -0.22794068224683492, 'observed': 13.337486910994764}, {'date': '2010-04-01', 'trend': 13.565108944517233, 'seasonal': -0.006495259466358668, 'resid': -0.13461368505087465, 'observed': 13.424}, {'date': '2010-05-01', 'trend': 13.542554509996647, 'seasonal': -0.02756787427454523, 'resid': -0.08705560123934074, 'observed': 13.42793103448276}, {'date': '2010-06-01', 'trend': 13.496382208505665, 'seasonal': -0.024120179891552147, 'resid': 0.28397173761965344, 'observed': 13.756233766233766}, {'date': '2010-07-01', 'trend': 13.45590861730361, 'seasonal': -0.03616359408614569, 'resid': 0.44336103738859645, 'observed': 13.86310606060606}, {'date': '2010-08-01', 'trend': 13.436416752050274, 'seasonal': -0.015376141878352585, 'resid': 0.4141230261917138, 'observed': 13.835163636363635}, {'date': '2010-09-01', 'trend': 13.43336842161868, 'seasonal': 0.04035308860321538, 'resid': 0.3240517691303332, 'observed': 13.797773279352228}, {'date': '2010-10-01', 'trend': 13.424416386656207, 'seasonal': 0.08058524115091258, 'resid': -0.01299325960628371, 'observed': 13.492008368200835}, {'date': '2010-11-01', 'trend': 13.432512703846879, 'seasonal': -0.05290731070035628, 'resid': -0.4313831709243, 'observed': 12.948222222222222}, {'date': '2010-12-01', 'trend': 13.453768023204553, 'seasonal': -0.02602059045270812, 'resid': -0.4651999042347351, 'observed': 12.96254752851711}, {'date': '2011-01-01', 'trend': 13.459765699171086, 'seasonal': 0.03449672569012794, 'resid': -0.42565586748416406, 'observed': 13.06860655737705}, {'date': '2011-02-01', 'trend': 13.460094510618998, 'seasonal': 0.022574335467601885, 'resid': -0.18547258440435596, 'observed': 13.297196261682243}, {'date': '2011-03-01', 'trend': 13.474198015666648, 'seasonal': 0.010641559838160935, 'resid': -0.16706179772703109, 'observed': 13.317777777777778}, {'date': '2011-04-01', 'trend': 13.527026690304142, 'seasonal': -0.006495259466358668, 'resid': -0.2916711367201349, 'observed': 13.228860294117649}, {'date': '2011-05-01', 'trend': 13.626640663310605, 'seasonal': -0.02756787427454523, 'resid': 0.21830956390511572, 'observed': 13.817382352941175}, {'date': '2011-06-01', 'trend': 13.752231546995366, 'seasonal': -0.024120179891552147, 'resid': 0.14879874525573725, 'observed': 13.876910112359552}, {'date': '2011-07-01', 'trend': 13.87357241867144, 'seasonal': -0.03616359408614569, 'resid': 0.048965113091762456, 'observed': 13.886373937677057}, {'date': '2011-08-01', 'trend': 13.98461653130225, 'seasonal': -0.015376141878352585, 'resid': -0.1494531553813426, 'observed': 13.819787234042554}, {'date': '2011-09-01', 'trend': 14.100343020217863, 'seasonal': 0.04035308860321538, 'resid': 0.010937693995820695, 'observed': 14.1516338028169}, {'date': '2011-10-01', 'trend': 14.23031804536025, 'seasonal': 0.08058524115091258, 'resid': 0.09513274952487473, 'observed': 14.406036036036037}, {'date': '2011-11-01', 'trend': 14.339700465210372, 'seasonal': -0.05290731070035628, 'resid': 0.13813675203204037, 'observed': 14.424929906542056}, {'date': '2011-12-01', 'trend': 14.423699396916051, 'seasonal': -0.02602059045270812, 'resid': 0.1023422461682363, 'observed': 14.50002105263158}, {'date': '2012-01-01', 'trend': 14.525801405280475, 'seasonal': 0.03449672569012794, 'resid': -0.11698417748223075, 'observed': 14.443313953488373}, {'date': '2012-02-01', 'trend': 14.653494230019042, 'seasonal': 0.022574335467601885, 'resid': -0.08852099677628465, 'observed': 14.58754756871036}, {'date': '2012-03-01', 'trend': 14.776156584948811, 'seasonal': 0.010641559838160935, 'resid': 0.018064059937437444, 'observed': 14.80486220472441}, {'date': '2012-04-01', 'trend': 14.874257215713214, 'seasonal': -0.006495259466358668, 'resid': -0.006585485658620364, 'observed': 14.861176470588235}, {'date': '2012-05-01', 'trend': 14.960668914357903, 'seasonal': -0.02756787427454523, 'resid': -0.1228567872097933, 'observed': 14.810244252873565}, {'date': '2012-06-01', 'trend': 15.055737285304788, 'seasonal': -0.024120179891552147, 'resid': -0.13159453204980495, 'observed': 14.900022573363431}, {'date': '2012-07-01', 'trend': 15.163703517893621, 'seasonal': -0.03616359408614569, 'resid': 0.18616975361187818, 'observed': 15.313709677419354}, {'date': '2012-08-01', 'trend': 15.269229382631286, 'seasonal': -0.015376141878352585, 'resid': 0.20322604727295804, 'observed': 15.457079288025891}, {'date': '2012-09-01', 'trend': 15.360602120013258, 'seasonal': 0.04035308860321538, 'resid': 0.057283058531540164, 'observed': 15.458238267148014}, {'date': '2012-10-01', 'trend': 15.441987157155793, 'seasonal': 0.08058524115091258, 'resid': -0.06872568825608992, 'observed': 15.453846710050616}, {'date': '2012-11-01', 'trend': 15.522336244532635, 'seasonal': -0.05290731070035628, 'resid': -0.018428933832278373, 'observed': 15.451}, {'date': '2012-12-01', 'trend': 15.600145184438405, 'seasonal': -0.02602059045270812, 'resid': 0.1814672679131936, 'observed': 15.75559186189889}, {'date': '2013-01-01', 'trend': 15.634210920934038, 'seasonal': 0.03449672569012794, 'resid': 0.11022507972889475, 'observed': 15.77893272635306}, {'date': '2013-02-01', 'trend': 15.620066570421177, 'seasonal': 0.022574335467601885, 'resid': 0.14190864366076897, 'observed': 15.784549549549547}, {'date': '2013-03-01', 'trend': 15.605915442823965, 'seasonal': 0.010641559838160935, 'resid': 0.18424891839050375, 'observed': 15.80080592105263}, {'date': '2013-04-01', 'trend': 15.611799860495061, 'seasonal': -0.006495259466358668, 'resid': 0.2131690446521172, 'observed': 15.81847364568082}, {'date': '2013-05-01', 'trend': 15.625247369996853, 'seasonal': -0.02756787427454523, 'resid': 0.18364567910286636, 'observed': 15.781325174825174}, {'date': '2013-06-01', 'trend': 15.604135339634787, 'seasonal': -0.024120179891552147, 'resid': 0.21634104940709334, 'observed': 15.796356209150328}, {'date': '2013-07-01', 'trend': 15.547031858857277, 'seasonal': -0.03616359408614569, 'resid': -0.2759145472435113, 'observed': 15.23495371752762}, {'date': '2013-08-01', 'trend': 15.47551431977326, 'seasonal': -0.015376141878352585, 'resid': -0.263767342285952, 'observed': 15.196370835608956}, {'date': '2013-09-01', 'trend': 15.388764348476775, 'seasonal': 0.04035308860321538, 'resid': -0.04979777984810272, 'observed': 15.379319657231887}, {'date': '2013-10-01', 'trend': 15.294595030065839, 'seasonal': 0.08058524115091258, 'resid': 0.298811072856343, 'observed': 15.673991344073094}, {'date': '2013-11-01', 'trend': 15.181142980694998, 'seasonal': -0.05290731070035628, 'resid': 0.42535992402581335, 'observed': 15.553595594020456}, {'date': '2013-12-01', 'trend': 15.037494206306443, 'seasonal': -0.02602059045270812, 'resid': 0.13483392333511915, 'observed': 15.146307539188854}, {'date': '2014-01-01', 'trend': 14.907041932227768, 'seasonal': 0.03449672569012794, 'resid': 0.07619485248493643, 'observed': 15.017733510402833}, {'date': '2014-02-01', 'trend': 14.801318577692147, 'seasonal': 0.022574335467601885, 'resid': 0.00543491432361955, 'observed': 14.829327827483368}, {'date': '2014-03-01', 'trend': 14.68834291186608, 'seasonal': 0.010641559838160935, 'resid': -0.024956139701048135, 'observed': 14.674028332003193}, {'date': '2014-04-01', 'trend': 14.55425093073973, 'seasonal': -0.006495259466358668, 'resid': 0.13743192159438591, 'observed': 14.685187592867758}, {'date': '2014-05-01', 'trend': 14.399014204506061, 'seasonal': -0.02756787427454523, 'resid': -0.17968428749337514, 'observed': 14.19176204273814}, {'date': '2014-06-01', 'trend': 14.251438055411992, 'seasonal': -0.024120179891552147, 'resid': -0.28896911960845106, 'observed': 13.938348755911989}, {'date': '2014-07-01', 'trend': 14.12541654679463, 'seasonal': -0.03616359408614569, 'resid': -0.12714635983071618, 'observed': 13.962106592877769}, {'date': '2014-08-01', 'trend': 14.00546444533696, 'seasonal': -0.015376141878352585, 'resid': -0.05823085205471999, 'observed': 13.931857451403888}, {'date': '2014-09-01', 'trend': 13.888750175213392, 'seasonal': 0.04035308860321538, 'resid': 0.0033137977947689454, 'observed': 13.932417061611376}, {'date': '2014-10-01', 'trend': 13.776028499486884, 'seasonal': 0.08058524115091258, 'resid': 0.04607265202341461, 'observed': 13.902686392661211}, {'date': '2014-11-01', 'trend': 13.683189812670596, 'seasonal': -0.05290731070035628, 'resid': -0.031063386145972684, 'observed': 13.599219115824267}, {'date': '2014-12-01', 'trend': 13.620293788031292, 'seasonal': -0.02602059045270812, 'resid': -0.03541675845120716, 'observed': 13.558856439127377}, {'date': '2015-01-01', 'trend': 13.567226974222327, 'seasonal': 0.03449672569012794, 'resid': -0.021055296264795055, 'observed': 13.58066840364766}, {'date': '2015-02-01', 'trend': 13.513730927922582, 'seasonal': 0.022574335467601885, 'resid': -0.14876276413578457, 'observed': 13.3875424992544}, {'date': '2015-03-01', 'trend': 13.460183939403779, 'seasonal': 0.010641559838160935, 'resid': -0.15615432197536414, 'observed': 13.314671177266575}, {'date': '2015-04-01', 'trend': 13.408445378415205, 'seasonal': -0.006495259466358668, 'resid': -0.062725588780694, 'observed': 13.339224530168153}, {'date': '2015-05-01', 'trend': 13.3634748819022, 'seasonal': -0.02756787427454523, 'resid': -0.026310385780782358, 'observed': 13.309596621846872}, {'date': '2015-06-01', 'trend': 13.324726093696405, 'seasonal': -0.024120179891552147, 'resid': 0.010403671655124475, 'observed': 13.311009585459978}, {'date': '2015-07-01', 'trend': 13.295059444065489, 'seasonal': -0.03616359408614569, 'resid': 0.05694638193523351, 'observed': 13.315842231914576}, {'date': '2015-08-01', 'trend': 13.28903396947279, 'seasonal': -0.015376141878352585, 'resid': 0.020558873578782184, 'observed': 13.29421670117322}, {'date': '2015-09-01', 'trend': 13.302630073928466, 'seasonal': 0.04035308860321538, 'resid': -0.0580530751409206, 'observed': 13.28493008739076}, {'date': '2015-10-01', 'trend': 13.319559651730444, 'seasonal': 0.08058524115091258, 'resid': -0.09169698972528272, 'observed': 13.308447903156074}, {'date': '2015-11-01', 'trend': 13.335995695872539, 'seasonal': -0.05290731070035628, 'resid': -0.16892269615494468, 'observed': 13.114165689017238}, {'date': '2015-12-01', 'trend': 13.36357309550888, 'seasonal': -0.02602059045270812, 'resid': -0.2236135560608082, 'observed': 13.113938948995363}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.03449672569012794, 'resid': nan, 'observed': 13.313586302637669}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.022574335467601885, 'resid': nan, 'observed': 13.51001321003963}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.010641559838160935, 'resid': nan, 'observed': 13.518506973417573}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.006495259466358668, 'resid': nan, 'observed': 13.54169860126461}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.02756787427454523, 'resid': nan, 'observed': 13.501587610160705}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.024120179891552147, 'resid': nan, 'observed': 13.780876188418324}], 'loan_rate_D': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.04677017162484018, 'resid': nan, 'observed': 12.267}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.050974187840284596, 'resid': nan, 'observed': 12.436666666666667}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.08472596878988434, 'resid': nan, 'observed': 12.737368421052633}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05303848060048236, 'resid': nan, 'observed': 12.609444444444444}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.11016855981499699, 'resid': nan, 'observed': 12.47888888888889}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.09055776109670126, 'resid': nan, 'observed': 13.0082}, {'date': '2008-02-01', 'trend': 12.946022013302766, 'seasonal': 0.014301916284665678, 'resid': 0.27560199633849425, 'observed': 13.235925925925926}, {'date': '2008-03-01', 'trend': 13.039515068858323, 'seasonal': 0.05402694293008556, 'resid': 0.05243359796768923, 'observed': 13.145975609756098}, {'date': '2008-04-01', 'trend': 13.122614914059561, 'seasonal': 0.02189789187409405, 'resid': 0.113748063631563, 'observed': 13.258260869565218}, {'date': '2008-05-01', 'trend': 13.21303629450572, 'seasonal': 0.00613243158641516, 'resid': -0.245168726092135, 'observed': 12.974}, {'date': '2008-06-01', 'trend': 13.345142221285036, 'seasonal': -0.030307932806885735, 'resid': -0.024417621811482743, 'observed': 13.290416666666667}, {'date': '2008-07-01', 'trend': 13.493951933332248, 'seasonal': 0.005243223388796764, 'resid': -0.18599515672104458, 'observed': 13.3132}, {'date': '2008-08-01', 'trend': 13.620062329512132, 'seasonal': 0.04677017162484018, 'resid': -0.20599916780364066, 'observed': 13.460833333333332}, {'date': '2008-09-01', 'trend': 13.739128813596224, 'seasonal': 0.050974187840284596, 'resid': -0.30343633476984244, 'observed': 13.486666666666666}, {'date': '2008-10-01', 'trend': 13.860800265690038, 'seasonal': 0.08472596878988434, 'resid': -0.26376152859756974, 'observed': 13.681764705882353}, {'date': '2008-11-01', 'trend': 13.987063226033499, 'seasonal': -0.05303848060048236, 'resid': -0.09886345511043668, 'observed': 13.83516129032258}, {'date': '2008-12-01', 'trend': 14.103115309366833, 'seasonal': -0.11016855981499699, 'resid': 0.43076753616244823, 'observed': 14.423714285714285}, {'date': '2009-01-01', 'trend': 14.210347506336527, 'seasonal': -0.09055776109670126, 'resid': 0.5150179470678666, 'observed': 14.634807692307692}, {'date': '2009-02-01', 'trend': 14.336290452098496, 'seasonal': 0.014301916284665678, 'resid': 0.28537537355232334, 'observed': 14.635967741935485}, {'date': '2009-03-01', 'trend': 14.475571637398703, 'seasonal': 0.05402694293008556, 'resid': 0.0739308314359172, 'observed': 14.603529411764706}, {'date': '2009-04-01', 'trend': 14.613243023661541, 'seasonal': 0.02189789187409405, 'resid': 0.08568100227258282, 'observed': 14.720821917808218}, {'date': '2009-05-01', 'trend': 14.738657673096528, 'seasonal': 0.00613243158641516, 'resid': -0.2030401046829425, 'observed': 14.54175}, {'date': '2009-06-01', 'trend': 14.82944765925895, 'seasonal': -0.030307932806885735, 'resid': -0.29122305978539564, 'observed': 14.507916666666668}, {'date': '2009-07-01', 'trend': 14.885117054715643, 'seasonal': 0.005243223388796764, 'resid': -0.22108755083171103, 'observed': 14.669272727272729}, {'date': '2009-08-01', 'trend': 14.9323876132182, 'seasonal': 0.04677017162484018, 'resid': 0.14823351950478605, 'observed': 15.127391304347826}, {'date': '2009-09-01', 'trend': 14.981868158243856, 'seasonal': 0.050974187840284596, 'resid': 0.13001479677300157, 'observed': 15.162857142857142}, {'date': '2009-10-01', 'trend': 15.024424507165973, 'seasonal': 0.08472596878988434, 'resid': 0.20053702404414345, 'observed': 15.3096875}, {'date': '2009-11-01', 'trend': 15.073536093923963, 'seasonal': -0.05303848060048236, 'resid': 0.19669246932114728, 'observed': 15.217190082644628}, {'date': '2009-12-01', 'trend': 15.147354517126578, 'seasonal': -0.11016855981499699, 'resid': 0.1834592039787434, 'observed': 15.220645161290324}, {'date': '2010-01-01', 'trend': 15.227469522958767, 'seasonal': -0.09055776109670126, 'resid': 0.03703054583024161, 'observed': 15.173942307692307}, {'date': '2010-02-01', 'trend': 15.281231545226769, 'seasonal': 0.014301916284665678, 'resid': -0.06420693089919113, 'observed': 15.231326530612243}, {'date': '2010-03-01', 'trend': 15.315594738321419, 'seasonal': 0.05402694293008556, 'resid': -0.17391797754780133, 'observed': 15.195703703703703}, {'date': '2010-04-01', 'trend': 15.333071365101393, 'seasonal': 0.02189789187409405, 'resid': -0.2049692569754867, 'observed': 15.15}, {'date': '2010-05-01', 'trend': 15.317257753325439, 'seasonal': 0.00613243158641516, 'resid': -0.032140184911854414, 'observed': 15.29125}, {'date': '2010-06-01', 'trend': 15.284795966826582, 'seasonal': -0.030307932806885735, 'resid': 0.2755707895097156, 'observed': 15.530058823529412}, {'date': '2010-07-01', 'trend': 15.264769830143157, 'seasonal': 0.005243223388796764, 'resid': 0.2998776568505589, 'observed': 15.569890710382513}, {'date': '2010-08-01', 'trend': 15.268900778305587, 'seasonal': 0.04677017162484018, 'resid': 0.20139090573967602, 'observed': 15.517061855670104}, {'date': '2010-09-01', 'trend': 15.281649843601524, 'seasonal': 0.050974187840284596, 'resid': 0.2652791943646423, 'observed': 15.597903225806451}, {'date': '2010-10-01', 'trend': 15.293769553052561, 'seasonal': 0.08472596878988434, 'resid': -0.08441506207232927, 'observed': 15.294080459770116}, {'date': '2010-11-01', 'trend': 15.349641612544133, 'seasonal': -0.05303848060048236, 'resid': -0.44333269169207823, 'observed': 14.853270440251572}, {'date': '2010-12-01', 'trend': 15.440148556758828, 'seasonal': -0.11016855981499699, 'resid': -0.5244980692329876, 'observed': 14.805481927710844}, {'date': '2011-01-01', 'trend': 15.519144828167612, 'seasonal': -0.09055776109670126, 'resid': -0.3201088062013446, 'observed': 15.108478260869566}, {'date': '2011-02-01', 'trend': 15.597932287226568, 'seasonal': 0.014301916284665678, 'resid': -0.21630087017790176, 'observed': 15.395933333333332}, {'date': '2011-03-01', 'trend': 15.683697841260013, 'seasonal': 0.05402694293008556, 'resid': -0.4006503161049927, 'observed': 15.337074468085106}, {'date': '2011-04-01', 'trend': 15.810629568251665, 'seasonal': 0.02189789187409405, 'resid': -0.5330251976823192, 'observed': 15.29950226244344}, {'date': '2011-05-01', 'trend': 15.991605993913954, 'seasonal': 0.00613243158641516, 'resid': 0.4849387398539594, 'observed': 16.48267716535433}, {'date': '2011-06-01', 'trend': 16.191505172815155, 'seasonal': -0.030307932806885735, 'resid': 0.3496010793194603, 'observed': 16.51079831932773}, {'date': '2011-07-01', 'trend': 16.374712577701146, 'seasonal': 0.005243223388796764, 'resid': 0.10510592730511684, 'observed': 16.48506172839506}, {'date': '2011-08-01', 'trend': 16.541513156944013, 'seasonal': 0.04677017162484018, 'resid': -0.09549347349638734, 'observed': 16.492789855072466}, {'date': '2011-09-01', 'trend': 16.74736510541923, 'seasonal': 0.050974187840284596, 'resid': -0.11779077005276303, 'observed': 16.680548523206753}, {'date': '2011-10-01', 'trend': 16.988299020223373, 'seasonal': 0.08472596878988434, 'resid': 0.18477162115623194, 'observed': 17.25779661016949}, {'date': '2011-11-01', 'trend': 17.1806862665515, 'seasonal': -0.05303848060048236, 'resid': 0.10534071979610485, 'observed': 17.232988505747123}, {'date': '2011-12-01', 'trend': 17.32296555978484, 'seasonal': -0.11016855981499699, 'resid': 0.01054715587431257, 'observed': 17.223344155844156}, {'date': '2012-01-01', 'trend': 17.47914706744071, 'seasonal': -0.09055776109670126, 'resid': -0.30099555634400893, 'observed': 17.08759375}, {'date': '2012-02-01', 'trend': 17.65589571162486, 'seasonal': 0.014301916284665678, 'resid': -0.2501658818777842, 'observed': 17.420031746031743}, {'date': '2012-03-01', 'trend': 17.82761029986977, 'seasonal': 0.05402694293008556, 'resid': 0.37178557599208595, 'observed': 18.253422818791943}, {'date': '2012-04-01', 'trend': 17.968315594668944, 'seasonal': 0.02189789187409405, 'resid': 0.1753543804929724, 'observed': 18.16556786703601}, {'date': '2012-05-01', 'trend': 18.08628632218981, 'seasonal': 0.00613243158641516, 'resid': 0.14148671886059142, 'observed': 18.233905472636817}, {'date': '2012-06-01', 'trend': 18.202423731325155, 'seasonal': -0.030307932806885735, 'resid': 0.0021572511271205352, 'observed': 18.17427304964539}, {'date': '2012-07-01', 'trend': 18.320598459912215, 'seasonal': 0.005243223388796764, 'resid': 0.24410149851717358, 'observed': 18.569943181818186}, {'date': '2012-08-01', 'trend': 18.42990775735452, 'seasonal': 0.04677017162484018, 'resid': 0.17319793308960643, 'observed': 18.649875862068967}, {'date': '2012-09-01', 'trend': 18.494241138814044, 'seasonal': 0.050974187840284596, 'resid': 0.0993973074338729, 'observed': 18.6446126340882}, {'date': '2012-10-01', 'trend': 18.528943565510193, 'seasonal': 0.08472596878988434, 'resid': 0.056990040168004935, 'observed': 18.670659574468083}, {'date': '2012-11-01', 'trend': 18.56571075355779, 'seasonal': -0.05303848060048236, 'resid': 0.13875072899200927, 'observed': 18.65142300194932}, {'date': '2012-12-01', 'trend': 18.60424801768697, 'seasonal': -0.11016855981499699, 'resid': 0.09812802101825394, 'observed': 18.592207478890227}, {'date': '2013-01-01', 'trend': 18.629536628909282, 'seasonal': -0.09055776109670126, 'resid': 0.01594504523090072, 'observed': 18.55492391304348}, {'date': '2013-02-01', 'trend': 18.63721728737489, 'seasonal': 0.014301916284665678, 'resid': -0.0753944820559945, 'observed': 18.576124721603563}, {'date': '2013-03-01', 'trend': 18.645549887500888, 'seasonal': 0.05402694293008556, 'resid': -0.05824583218228971, 'observed': 18.641330998248684}, {'date': '2013-04-01', 'trend': 18.660288795601627, 'seasonal': 0.02189789187409405, 'resid': -0.07166875918886748, 'observed': 18.610517928286853}, {'date': '2013-05-01', 'trend': 18.679619934627297, 'seasonal': 0.00613243158641516, 'resid': -0.01438444168541117, 'observed': 18.6713679245283}, {'date': '2013-06-01', 'trend': 18.68159073892394, 'seasonal': -0.030307932806885735, 'resid': 0.0104221307371343, 'observed': 18.66170493685419}, {'date': '2013-07-01', 'trend': 18.656660551740597, 'seasonal': 0.005243223388796764, 'resid': 0.027534188815457356, 'observed': 18.68943796394485}, {'date': '2013-08-01', 'trend': 18.616553704147748, 'seasonal': 0.04677017162484018, 'resid': 0.051393007344301106, 'observed': 18.71471688311689}, {'date': '2013-09-01', 'trend': 18.563483001311955, 'seasonal': 0.050974187840284596, 'resid': 0.1652968269120169, 'observed': 18.779754016064256}, {'date': '2013-10-01', 'trend': 18.507362836112627, 'seasonal': 0.08472596878988434, 'resid': 0.29716318200725855, 'observed': 18.88925198690977}, {'date': '2013-11-01', 'trend': 18.41784152773823, 'seasonal': -0.05303848060048236, 'resid': 0.5319748789859714, 'observed': 18.89677792612372}, {'date': '2013-12-01', 'trend': 18.275788119034477, 'seasonal': -0.11016855981499699, 'resid': 0.2285322986157389, 'observed': 18.39415185783522}, {'date': '2014-01-01', 'trend': 18.117404473515993, 'seasonal': -0.09055776109670126, 'resid': 0.12780832927896363, 'observed': 18.154655041698256}, {'date': '2014-02-01', 'trend': 17.95915940844471, 'seasonal': 0.014301916284665678, 'resid': 0.04036792599108799, 'observed': 18.013829250720462}, {'date': '2014-03-01', 'trend': 17.798431684504315, 'seasonal': 0.05402694293008556, 'resid': 0.07747097363834579, 'observed': 17.929929601072747}, {'date': '2014-04-01', 'trend': 17.627098506088636, 'seasonal': 0.02189789187409405, 'resid': 0.32603896271619315, 'observed': 17.975035360678923}, {'date': '2014-05-01', 'trend': 17.438528742234112, 'seasonal': 0.00613243158641516, 'resid': -0.28632208266985126, 'observed': 17.158339091150676}, {'date': '2014-06-01', 'trend': 17.258728659263024, 'seasonal': -0.030307932806885735, 'resid': -0.46296876511446683, 'observed': 16.76545196134167}, {'date': '2014-07-01', 'trend': 17.108719410371155, 'seasonal': 0.005243223388796764, 'resid': -0.3294791867461425, 'observed': 16.78448344701381}, {'date': '2014-08-01', 'trend': 16.98323278819809, 'seasonal': 0.04677017162484018, 'resid': -0.2082131214857493, 'observed': 16.821789838337182}, {'date': '2014-09-01', 'trend': 16.877995723697357, 'seasonal': 0.050974187840284596, 'resid': -0.1137542252631317, 'observed': 16.81521568627451}, {'date': '2014-10-01', 'trend': 16.778774884200576, 'seasonal': 0.08472596878988434, 'resid': -0.12170681826720817, 'observed': 16.741794034723252}, {'date': '2014-11-01', 'trend': 16.71401294490989, 'seasonal': -0.05303848060048236, 'resid': -0.14241291850788204, 'observed': 16.518561545801525}, {'date': '2014-12-01', 'trend': 16.69983988983558, 'seasonal': -0.11016855981499699, 'resid': -0.13250508316919682, 'observed': 16.457166246851386}, {'date': '2015-01-01', 'trend': 16.699303658669496, 'seasonal': -0.09055776109670126, 'resid': -0.11732721829564548, 'observed': 16.49141867927715}, {'date': '2015-02-01', 'trend': 16.69508156806944, 'seasonal': 0.014301916284665678, 'resid': -0.04399680336592135, 'observed': 16.665386680988185}, {'date': '2015-03-01', 'trend': 16.69018220555109, 'seasonal': 0.05402694293008556, 'resid': 0.008473474306158219, 'observed': 16.752682622787333}, {'date': '2015-04-01', 'trend': 16.688963166928986, 'seasonal': 0.02189789187409405, 'resid': 0.06012113223847462, 'observed': 16.770982191041554}, {'date': '2015-05-01', 'trend': 16.69606287739243, 'seasonal': 0.00613243158641516, 'resid': 0.10591040883275618, 'observed': 16.8081057178116}, {'date': '2015-06-01', 'trend': 16.713701422181142, 'seasonal': -0.030307932806885735, 'resid': 0.09213852352302712, 'observed': 16.775532012897283}, {'date': '2015-07-01', 'trend': 16.74506767376765, 'seasonal': 0.005243223388796764, 'resid': 0.011222950315704111, 'observed': 16.76153384747215}, {'date': '2015-08-01', 'trend': 16.809868367240238, 'seasonal': 0.04677017162484018, 'resid': -0.1132292753874796, 'observed': 16.7434092634776}, {'date': '2015-09-01', 'trend': 16.898763840746042, 'seasonal': 0.050974187840284596, 'resid': -0.17372646789268384, 'observed': 16.776011560693643}, {'date': '2015-10-01', 'trend': 16.985313395517174, 'seasonal': 0.08472596878988434, 'resid': -0.3182981309334191, 'observed': 16.75174123337364}, {'date': '2015-11-01', 'trend': 17.06891528315394, 'seasonal': -0.05303848060048236, 'resid': -0.33686940427972323, 'observed': 16.679007398273736}, {'date': '2015-12-01', 'trend': 17.17336476486541, 'seasonal': -0.11016855981499699, 'resid': -0.3431507357422, 'observed': 16.720045469308214}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.09055776109670126, 'resid': nan, 'observed': 16.981329494896624}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.014301916284665678, 'resid': nan, 'observed': 17.7306925087108}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.05402694293008556, 'resid': nan, 'observed': 17.82086815920398}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.02189789187409405, 'resid': nan, 'observed': 17.77998596913209}, {'date': '2016-05-01', 'trend': nan, 'seasonal': 0.00613243158641516, 'resid': nan, 'observed': 17.805547243003602}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.030307932806885735, 'resid': nan, 'observed': 18.284878048780485}], 'FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.0908829365079365, 'resid': nan, 'observed': 5.02}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.0898933531746032, 'resid': nan, 'observed': 4.94}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.0028100198412698354, 'resid': nan, 'observed': 4.76}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05135664682539679, 'resid': nan, 'observed': 4.49}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.045783730158730156, 'resid': nan, 'observed': 4.24}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.043670634920634936, 'resid': nan, 'observed': 3.94}, {'date': '2008-02-01', 'trend': 3.311666666666667, 'seasonal': -0.04234623015873019, 'resid': -0.2893204365079368, 'observed': 2.98}, {'date': '2008-03-01', 'trend': 3.0554166666666664, 'seasonal': -0.03817956349206348, 'resid': -0.4072371031746031, 'observed': 2.61}, {'date': '2008-04-01', 'trend': 2.7670833333333333, 'seasonal': -0.029012896825396865, 'resid': -0.45807043650793666, 'observed': 2.28}, {'date': '2008-05-01', 'trend': 2.438333333333333, 'seasonal': -0.019585813492063467, 'resid': -0.4387475198412696, 'observed': 1.98}, {'date': '2008-06-01', 'trend': 2.0975, 'seasonal': 0.027757936507936475, 'resid': -0.12525793650793662, 'observed': 2.0}, {'date': '2008-07-01', 'trend': 1.7695833333333333, 'seasonal': 0.05859126984126982, 'resid': 0.18182539682539667, 'observed': 2.01}, {'date': '2008-08-01', 'trend': 1.4966666666666666, 'seasonal': 0.0908829365079365, 'resid': 0.4124503968253969, 'observed': 2.0}, {'date': '2008-09-01', 'trend': 1.2804166666666665, 'seasonal': 0.0898933531746032, 'resid': 0.4396899801587303, 'observed': 1.81}, {'date': '2008-10-01', 'trend': 1.0904166666666666, 'seasonal': 0.0028100198412698354, 'resid': -0.12322668650793644, 'observed': 0.97}, {'date': '2008-11-01', 'trend': 0.9266666666666664, 'seasonal': -0.05135664682539679, 'resid': -0.4853100198412696, 'observed': 0.39}, {'date': '2008-12-01', 'trend': 0.7770833333333333, 'seasonal': -0.045783730158730156, 'resid': -0.5712996031746032, 'observed': 0.16}, {'date': '2009-01-01', 'trend': 0.6254166666666667, 'seasonal': -0.043670634920634936, 'resid': -0.43174603174603177, 'observed': 0.15}, {'date': '2009-02-01', 'trend': 0.4716666666666666, 'seasonal': -0.04234623015873019, 'resid': -0.2093204365079364, 'observed': 0.22}, {'date': '2009-03-01', 'trend': 0.3258333333333333, 'seasonal': -0.03817956349206348, 'resid': -0.10765376984126984, 'observed': 0.18}, {'date': '2009-04-01', 'trend': 0.22125, 'seasonal': -0.029012896825396865, 'resid': -0.04223710317460314, 'observed': 0.15}, {'date': '2009-05-01', 'trend': 0.1745833333333333, 'seasonal': -0.019585813492063467, 'resid': 0.025002480158730148, 'observed': 0.18}, {'date': '2009-06-01', 'trend': 0.16166666666666665, 'seasonal': 0.027757936507936475, 'resid': 0.020575396825396865, 'observed': 0.21}, {'date': '2009-07-01', 'trend': 0.1583333333333333, 'seasonal': 0.05859126984126982, 'resid': -0.056924603174603114, 'observed': 0.16}, {'date': '2009-08-01', 'trend': 0.15291666666666665, 'seasonal': 0.0908829365079365, 'resid': -0.08379960317460314, 'observed': 0.16}, {'date': '2009-09-01', 'trend': 0.14833333333333332, 'seasonal': 0.0898933531746032, 'resid': -0.08822668650793652, 'observed': 0.15}, {'date': '2009-10-01', 'trend': 0.14958333333333332, 'seasonal': 0.0028100198412698354, 'resid': -0.03239335317460316, 'observed': 0.12}, {'date': '2009-11-01', 'trend': 0.1525, 'seasonal': -0.05135664682539679, 'resid': 0.01885664682539679, 'observed': 0.12}, {'date': '2009-12-01', 'trend': 0.15208333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.01370039682539683, 'observed': 0.12}, {'date': '2010-01-01', 'trend': 0.15166666666666667, 'seasonal': -0.043670634920634936, 'resid': 0.002003968253968265, 'observed': 0.11}, {'date': '2010-02-01', 'trend': 0.15374999999999997, 'seasonal': -0.04234623015873019, 'resid': 0.01859623015873022, 'observed': 0.13}, {'date': '2010-03-01', 'trend': 0.15666666666666665, 'seasonal': -0.03817956349206348, 'resid': 0.041512896825396835, 'observed': 0.16}, {'date': '2010-04-01', 'trend': 0.16124999999999998, 'seasonal': -0.029012896825396865, 'resid': 0.0677628968253969, 'observed': 0.2}, {'date': '2010-05-01', 'trend': 0.1670833333333333, 'seasonal': -0.019585813492063467, 'resid': 0.05250248015873017, 'observed': 0.2}, {'date': '2010-06-01', 'trend': 0.17250000000000001, 'seasonal': 0.027757936507936475, 'resid': -0.020257936507936496, 'observed': 0.18}, {'date': '2010-07-01', 'trend': 0.1775, 'seasonal': 0.05859126984126982, 'resid': -0.05609126984126982, 'observed': 0.18}, {'date': '2010-08-01', 'trend': 0.18124999999999997, 'seasonal': 0.0908829365079365, 'resid': -0.08213293650793646, 'observed': 0.19}, {'date': '2010-09-01', 'trend': 0.18166666666666667, 'seasonal': 0.0898933531746032, 'resid': -0.08156001984126987, 'observed': 0.19}, {'date': '2010-10-01', 'trend': 0.17666666666666667, 'seasonal': 0.0028100198412698354, 'resid': 0.0105233134920635, 'observed': 0.19}, {'date': '2010-11-01', 'trend': 0.16791666666666666, 'seasonal': -0.05135664682539679, 'resid': 0.07343998015873013, 'observed': 0.19}, {'date': '2010-12-01', 'trend': 0.15958333333333333, 'seasonal': -0.045783730158730156, 'resid': 0.06620039682539683, 'observed': 0.18}, {'date': '2011-01-01', 'trend': 0.15125000000000002, 'seasonal': -0.043670634920634936, 'resid': 0.062420634920634925, 'observed': 0.17}, {'date': '2011-02-01', 'trend': 0.14291666666666666, 'seasonal': -0.04234623015873019, 'resid': 0.05942956349206353, 'observed': 0.16}, {'date': '2011-03-01', 'trend': 0.1345833333333333, 'seasonal': -0.03817956349206348, 'resid': 0.04359623015873019, 'observed': 0.14}, {'date': '2011-04-01', 'trend': 0.12499999999999999, 'seasonal': -0.029012896825396865, 'resid': 0.004012896825396885, 'observed': 0.1}, {'date': '2011-05-01', 'trend': 0.11541666666666667, 'seasonal': -0.019585813492063467, 'resid': -0.005830853174603204, 'observed': 0.09}, {'date': '2011-06-01', 'trend': 0.10625, 'seasonal': 0.027757936507936475, 'resid': -0.044007936507936475, 'observed': 0.09}, {'date': '2011-07-01', 'trend': 0.09791666666666668, 'seasonal': 0.05859126984126982, 'resid': -0.08650793650793649, 'observed': 0.07}, {'date': '2011-08-01', 'trend': 0.09166666666666666, 'seasonal': 0.0908829365079365, 'resid': -0.08254960317460315, 'observed': 0.1}, {'date': '2011-09-01', 'trend': 0.08875, 'seasonal': 0.0898933531746032, 'resid': -0.0986433531746032, 'observed': 0.08}, {'date': '2011-10-01', 'trend': 0.09, 'seasonal': 0.0028100198412698354, 'resid': -0.022810019841269825, 'observed': 0.07}, {'date': '2011-11-01', 'trend': 0.09458333333333334, 'seasonal': -0.05135664682539679, 'resid': 0.036773313492063454, 'observed': 0.08}, {'date': '2011-12-01', 'trend': 0.10041666666666667, 'seasonal': -0.045783730158730156, 'resid': 0.015367063492063494, 'observed': 0.07}, {'date': '2012-01-01', 'trend': 0.10708333333333334, 'seasonal': -0.043670634920634936, 'resid': 0.016587301587301602, 'observed': 0.08}, {'date': '2012-02-01', 'trend': 0.11208333333333333, 'seasonal': -0.04234623015873019, 'resid': 0.030262896825396866, 'observed': 0.1}, {'date': '2012-03-01', 'trend': 0.11583333333333333, 'seasonal': -0.03817956349206348, 'resid': 0.052346230158730155, 'observed': 0.13}, {'date': '2012-04-01', 'trend': 0.12208333333333334, 'seasonal': -0.029012896825396865, 'resid': 0.04692956349206354, 'observed': 0.14}, {'date': '2012-05-01', 'trend': 0.12916666666666665, 'seasonal': -0.019585813492063467, 'resid': 0.05041914682539682, 'observed': 0.16}, {'date': '2012-06-01', 'trend': 0.13624999999999998, 'seasonal': 0.027757936507936475, 'resid': -0.0040079365079364535, 'observed': 0.16}, {'date': '2012-07-01', 'trend': 0.1425, 'seasonal': 0.05859126984126982, 'resid': -0.041091269841269804, 'observed': 0.16}, {'date': '2012-08-01', 'trend': 0.14708333333333334, 'seasonal': 0.0908829365079365, 'resid': -0.10796626984126984, 'observed': 0.13}, {'date': '2012-09-01', 'trend': 0.14958333333333332, 'seasonal': 0.0898933531746032, 'resid': -0.09947668650793651, 'observed': 0.14}, {'date': '2012-10-01', 'trend': 0.15041666666666667, 'seasonal': 0.0028100198412698354, 'resid': 0.006773313492063497, 'observed': 0.16}, {'date': '2012-11-01', 'trend': 0.14875, 'seasonal': -0.05135664682539679, 'resid': 0.0626066468253968, 'observed': 0.16}, {'date': '2012-12-01', 'trend': 0.14375000000000002, 'seasonal': -0.045783730158730156, 'resid': 0.06203373015873014, 'observed': 0.16}, {'date': '2013-01-01', 'trend': 0.13791666666666666, 'seasonal': -0.043670634920634936, 'resid': 0.04575396825396829, 'observed': 0.14}, {'date': '2013-02-01', 'trend': 0.13291666666666666, 'seasonal': -0.04234623015873019, 'resid': 0.05942956349206353, 'observed': 0.15}, {'date': '2013-03-01', 'trend': 0.12833333333333333, 'seasonal': -0.03817956349206348, 'resid': 0.049846230158730166, 'observed': 0.14}, {'date': '2013-04-01', 'trend': 0.12291666666666667, 'seasonal': -0.029012896825396865, 'resid': 0.056096230158730186, 'observed': 0.15}, {'date': '2013-05-01', 'trend': 0.11666666666666667, 'seasonal': -0.019585813492063467, 'resid': 0.012919146825396799, 'observed': 0.11}, {'date': '2013-06-01', 'trend': 0.11041666666666666, 'seasonal': 0.027757936507936475, 'resid': -0.04817460317460314, 'observed': 0.09}, {'date': '2013-07-01', 'trend': 0.10458333333333333, 'seasonal': 0.05859126984126982, 'resid': -0.07317460317460316, 'observed': 0.09}, {'date': '2013-08-01', 'trend': 0.09833333333333334, 'seasonal': 0.0908829365079365, 'resid': -0.10921626984126984, 'observed': 0.08}, {'date': '2013-09-01', 'trend': 0.0925, 'seasonal': 0.0898933531746032, 'resid': -0.1023933531746032, 'observed': 0.08}, {'date': '2013-10-01', 'trend': 0.08750000000000001, 'seasonal': 0.0028100198412698354, 'resid': -0.0003100198412698471, 'observed': 0.09}, {'date': '2013-11-01', 'trend': 0.08416666666666667, 'seasonal': -0.05135664682539679, 'resid': 0.047189980158730126, 'observed': 0.08}, {'date': '2013-12-01', 'trend': 0.08374999999999999, 'seasonal': -0.045783730158730156, 'resid': 0.05203373015873016, 'observed': 0.09}, {'date': '2014-01-01', 'trend': 0.08416666666666667, 'seasonal': -0.043670634920634936, 'resid': 0.029503968253968275, 'observed': 0.07}, {'date': '2014-02-01', 'trend': 0.08458333333333333, 'seasonal': -0.04234623015873019, 'resid': 0.027762896825396864, 'observed': 0.07}, {'date': '2014-03-01', 'trend': 0.08541666666666667, 'seasonal': -0.03817956349206348, 'resid': 0.03276289682539681, 'observed': 0.08}, {'date': '2014-04-01', 'trend': 0.08583333333333333, 'seasonal': -0.029012896825396865, 'resid': 0.03317956349206353, 'observed': 0.09}, {'date': '2014-05-01', 'trend': 0.08625, 'seasonal': -0.019585813492063467, 'resid': 0.02333581349206347, 'observed': 0.09}, {'date': '2014-06-01', 'trend': 0.08791666666666667, 'seasonal': 0.027757936507936475, 'resid': -0.01567460317460314, 'observed': 0.1}, {'date': '2014-07-01', 'trend': 0.09083333333333332, 'seasonal': 0.05859126984126982, 'resid': -0.059424603174603144, 'observed': 0.09}, {'date': '2014-08-01', 'trend': 0.09416666666666666, 'seasonal': 0.0908829365079365, 'resid': -0.09504960317460316, 'observed': 0.09}, {'date': '2014-09-01', 'trend': 0.09708333333333333, 'seasonal': 0.0898933531746032, 'resid': -0.09697668650793653, 'observed': 0.09}, {'date': '2014-10-01', 'trend': 0.09958333333333334, 'seasonal': 0.0028100198412698354, 'resid': -0.012393353174603182, 'observed': 0.09}, {'date': '2014-11-01', 'trend': 0.10208333333333333, 'seasonal': -0.05135664682539679, 'resid': 0.03927331349206346, 'observed': 0.09}, {'date': '2014-12-01', 'trend': 0.10458333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.06120039682539683, 'observed': 0.12}, {'date': '2015-01-01', 'trend': 0.10749999999999998, 'seasonal': -0.043670634920634936, 'resid': 0.04617063492063495, 'observed': 0.11}, {'date': '2015-02-01', 'trend': 0.11124999999999999, 'seasonal': -0.04234623015873019, 'resid': 0.0410962301587302, 'observed': 0.11}, {'date': '2015-03-01', 'trend': 0.11541666666666667, 'seasonal': -0.03817956349206348, 'resid': 0.03276289682539681, 'observed': 0.11}, {'date': '2015-04-01', 'trend': 0.11875, 'seasonal': -0.029012896825396865, 'resid': 0.030262896825396866, 'observed': 0.12}, {'date': '2015-05-01', 'trend': 0.12125, 'seasonal': -0.019585813492063467, 'resid': 0.018335813492063466, 'observed': 0.12}, {'date': '2015-06-01', 'trend': 0.1275, 'seasonal': 0.027757936507936475, 'resid': -0.025257936507936472, 'observed': 0.13}, {'date': '2015-07-01', 'trend': 0.1420833333333333, 'seasonal': 0.05859126984126982, 'resid': -0.07067460317460313, 'observed': 0.13}, {'date': '2015-08-01', 'trend': 0.16291666666666668, 'seasonal': 0.0908829365079365, 'resid': -0.11379960317460316, 'observed': 0.14}, {'date': '2015-09-01', 'trend': 0.1845833333333333, 'seasonal': 0.0898933531746032, 'resid': -0.13447668650793648, 'observed': 0.14}, {'date': '2015-10-01', 'trend': 0.20541666666666664, 'seasonal': 0.0028100198412698354, 'resid': -0.08822668650793647, 'observed': 0.12}, {'date': '2015-11-01', 'trend': 0.22624999999999998, 'seasonal': -0.05135664682539679, 'resid': -0.05489335317460319, 'observed': 0.12}, {'date': '2015-12-01', 'trend': 0.24708333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.038700396825396825, 'observed': 0.24}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.043670634920634936, 'resid': nan, 'observed': 0.34}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.04234623015873019, 'resid': nan, 'observed': 0.38}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.03817956349206348, 'resid': nan, 'observed': 0.36}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.029012896825396865, 'resid': nan, 'observed': 0.37}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.019585813492063467, 'resid': nan, 'observed': 0.37}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.027757936507936475, 'resid': nan, 'observed': 0.38}], 'diff1_loan_rate_A': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.052743362293617736, 'resid': nan, 'observed': 0.0600000000000013}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.010310036424414365, 'resid': nan, 'observed': 0.0747619047619041}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.07866104363465767, 'resid': nan, 'observed': -0.0114285714285715}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.06447017941083495, 'resid': nan, 'observed': -0.0509090909090916}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.13259497070685478, 'resid': nan, 'observed': -0.0832575757575755}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.031714528696978224, 'resid': nan, 'observed': 0.2655000000000003}, {'date': '2008-02-01', 'trend': 0.04308756038647344, 'seasonal': -0.04724202459970091, 'resid': 0.17315446421322706, 'observed': 0.1689999999999996}, {'date': '2008-03-01', 'trend': 0.031621031746031744, 'seasonal': -0.010757930825500861, 'resid': -0.02491071996814928, 'observed': -0.0040476190476184}, {'date': '2008-04-01', 'trend': 0.04255952380952382, 'seasonal': 0.02255283843870718, 'resid': -0.0993147432006122, 'observed': -0.0342023809523812}, {'date': '2008-05-01', 'trend': 0.06510642135642139, 'seasonal': -0.030587662937704367, 'resid': 0.024826479676521074, 'observed': 0.0593452380952381}, {'date': '2008-06-01', 'trend': 0.09512626262626267, 'seasonal': -0.017830922488024405, 'resid': -0.023223911566808864, 'observed': 0.0540714285714294}, {'date': '2008-07-01', 'trend': 0.10408606557377058, 'seasonal': 0.03184779489655618, 'resid': -0.12099907786163225, 'observed': 0.0149347826086945}, {'date': '2008-08-01', 'trend': 0.07345892882163388, 'seasonal': -0.052743362293617736, 'resid': 0.04584965086328945, 'observed': 0.0665652173913056}, {'date': '2008-09-01', 'trend': 0.06096281033781046, 'seasonal': -0.010310036424414365, 'resid': -0.2576527739133968, 'observed': -0.2070000000000007}, {'date': '2008-10-01', 'trend': 0.053667120029015226, 'seasonal': 0.07866104363465767, 'resid': 0.40052897919346997, 'observed': 0.5328571428571429}, {'date': '2008-11-01', 'trend': 0.057776055562292906, 'seasonal': -0.06447017941083495, 'resid': -0.047375140220722256, 'observed': -0.0540692640692643}, {'date': '2008-12-01', 'trend': 0.06338933770596704, 'seasonal': 0.13259497070685478, 'resid': 0.44439447946596633, 'observed': 0.6403787878787881}, {'date': '2009-01-01', 'trend': 0.06135697966358909, 'seasonal': -0.031714528696978224, 'resid': -0.2727435438627845, 'observed': -0.2431010928961736}, {'date': '2009-02-01', 'trend': 0.038774959850979306, 'seasonal': -0.04724202459970091, 'resid': -0.04898312440638529, 'observed': -0.0574501891551069}, {'date': '2009-03-01', 'trend': 0.022875529287226484, 'seasonal': -0.010757930825500861, 'resid': -0.08962187196600012, 'observed': -0.0775042735042745}, {'date': '2009-04-01', 'trend': 0.007781467220443654, 'seasonal': 0.02255283843870718, 'resid': -0.16617659956596104, 'observed': -0.1358422939068102}, {'date': '2009-05-01', 'trend': -0.015483169794193343, 'seasonal': -0.030587662937704367, 'resid': 0.3056704365802287, 'observed': 0.259599603848331}, {'date': '2009-06-01', 'trend': -0.04104235714622401, 'seasonal': -0.017830922488024405, 'resid': 0.04740911390076451, 'observed': -0.0114641657334839}, {'date': '2009-07-01', 'trend': -0.06674015259826861, 'seasonal': 0.03184779489655618, 'resid': 0.06658614159825002, 'observed': 0.0316937838965376}, {'date': '2009-08-01', 'trend': -0.09139295659941168, 'seasonal': -0.052743362293617736, 'resid': -0.3480259405061433, 'observed': -0.4921622593991728}, {'date': '2009-09-01', 'trend': -0.11437149262309977, 'seasonal': -0.010310036424414365, 'resid': 0.09482267230792384, 'observed': -0.0298588567395903}, {'date': '2009-10-01', 'trend': -0.1062783291583755, 'seasonal': 0.07866104363465767, 'resid': 0.021075795517662227, 'observed': -0.0065414900060556}, {'date': '2009-11-01', 'trend': -0.11133887121885848, 'seasonal': -0.06447017941083495, 'resid': 0.10278713107233983, 'observed': -0.0730219195573536}, {'date': '2009-12-01', 'trend': -0.12049983600396537, 'seasonal': 0.13259497070685478, 'resid': 0.0338158122152519, 'observed': 0.0459109469181413}, {'date': '2010-01-01', 'trend': -0.12134807296956264, 'seasonal': -0.031714528696978224, 'resid': -0.11231774111805665, 'observed': -0.2653803427845975}, {'date': '2010-02-01', 'trend': -0.10525834023147676, 'seasonal': -0.04724202459970091, 'resid': -0.47433787046293924, 'observed': -0.6268382352941169}, {'date': '2010-03-01', 'trend': -0.0819108707400321, 'seasonal': -0.010757930825500861, 'resid': 0.033067709631754365, 'observed': -0.0596010919337786}, {'date': '2010-04-01', 'trend': -0.10373052762973133, 'seasonal': 0.02255283843870718, 'resid': 0.12166813686710085, 'observed': 0.0404904476760767}, {'date': '2010-05-01', 'trend': -0.1475603464738341, 'seasonal': -0.030587662937704367, 'resid': 0.13996186222539117, 'observed': -0.0381861471861473}, {'date': '2010-06-01', 'trend': -0.17029682948139066, 'seasonal': -0.017830922488024405, 'resid': 0.25458618242784387, 'observed': 0.0664584304584288}, {'date': '2010-07-01', 'trend': -0.15216218705932197, 'seasonal': 0.03184779489655618, 'resid': 0.053727892693055786, 'observed': -0.06658649946971}, {'date': '2010-08-01', 'trend': -0.09455752860490758, 'seasonal': -0.052743362293617736, 'resid': 0.13957250057966172, 'observed': -0.0077283903188636}, {'date': '2010-09-01', 'trend': -0.05546174667713392, 'seasonal': -0.010310036424414365, 'resid': 0.11181832507632047, 'observed': 0.0460465419747722}, {'date': '2010-10-01', 'trend': -0.0530929247754636, 'seasonal': 0.07866104363465767, 'resid': -0.6316867729323936, 'observed': -0.6061186540731995}, {'date': '2010-11-01', 'trend': -0.039401117266809255, 'seasonal': -0.06447017941083495, 'resid': -0.42148911107103154, 'observed': -0.5253604077486758}, {'date': '2010-12-01', 'trend': -0.03342727946617814, 'seasonal': 0.13259497070685478, 'resid': -0.14659384831257133, 'observed': -0.0474261570718947}, {'date': '2011-01-01', 'trend': -0.03683063866320891, 'seasonal': -0.031714528696978224, 'resid': 0.3317333466952747, 'observed': 0.2631881793350876}, {'date': '2011-02-01', 'trend': -0.031484501032934334, 'seasonal': -0.04724202459970091, 'resid': 0.30583157112477843, 'observed': 0.2271050454921432}, {'date': '2011-03-01', 'trend': -0.03303165036061288, 'seasonal': -0.010757930825500861, 'resid': 0.06854397473264304, 'observed': 0.0247543935465293}, {'date': '2011-04-01', 'trend': 0.0019066603995984734, 'seasonal': 0.02255283843870718, 'resid': -0.011472811002449254, 'observed': 0.0129866878358564}, {'date': '2011-05-01', 'trend': 0.06281957931593297, 'seasonal': -0.030587662937704367, 'resid': 0.285689076483549, 'observed': 0.3179209928617776}, {'date': '2011-06-01', 'trend': 0.09660137276875817, 'seasonal': -0.017830922488024405, 'resid': -0.2250470526550836, 'observed': -0.1462766023743498}, {'date': '2011-07-01', 'trend': 0.09219874084545351, 'seasonal': 0.03184779489655618, 'resid': -0.0595786231076794, 'observed': 0.0644679126343303}, {'date': '2011-08-01', 'trend': 0.07266339009278908, 'seasonal': -0.052743362293617736, 'resid': -0.03039552709548534, 'observed': -0.010475499296314}, {'date': '2011-09-01', 'trend': 0.06441812681107223, 'seasonal': -0.010310036424414365, 'resid': -0.042446023298720666, 'observed': 0.0116620670879372}, {'date': '2011-10-01', 'trend': 0.06188167776796559, 'seasonal': 0.07866104363465767, 'resid': 0.1262425576560849, 'observed': 0.2667852790587082}, {'date': '2011-11-01', 'trend': 0.04383614949603275, 'seasonal': -0.06447017941083495, 'resid': 0.0842797430262469, 'observed': 0.0636457131114447}, {'date': '2011-12-01', 'trend': 0.03471611807420816, 'seasonal': 0.13259497070685478, 'resid': 0.007019676154726762, 'observed': 0.1743307649357897}, {'date': '2012-01-01', 'trend': 0.04570925645758612, 'seasonal': -0.031714528696978224, 'resid': -0.07822663659251661, 'observed': -0.0642319088319087}, {'date': '2012-02-01', 'trend': 0.044902190815899364, 'seasonal': -0.04724202459970091, 'resid': 0.08801654937899464, 'observed': 0.0856767155951931}, {'date': '2012-03-01', 'trend': 0.042262956633337116, 'seasonal': -0.010757930825500861, 'resid': -0.06320862112556136, 'observed': -0.0317035953177251}, {'date': '2012-04-01', 'trend': 0.03516995902860612, 'seasonal': 0.02255283843870718, 'resid': -0.0491528978017619, 'observed': 0.0085698996655514}, {'date': '2012-05-01', 'trend': 0.02213178752152366, 'seasonal': -0.030587662937704367, 'resid': -0.1022990220781248, 'observed': -0.1107548974943055}, {'date': '2012-06-01', 'trend': 0.018030473650186018, 'seasonal': -0.017830922488024405, 'resid': 0.06331898269578129, 'observed': 0.0635185338579429}, {'date': '2012-07-01', 'trend': 0.019188437545202486, 'seasonal': 0.03184779489655618, 'resid': 0.06747186516135023, 'observed': 0.1185080976031089}, {'date': '2012-08-01', 'trend': 0.0131250513067906, 'seasonal': -0.052743362293617736, 'resid': -0.04426694867874766, 'observed': -0.0838852596655748}, {'date': '2012-09-01', 'trend': 0.009551132283800178, 'seasonal': -0.010310036424414365, 'resid': 0.022489111216318088, 'observed': 0.0217302070757039}, {'date': '2012-10-01', 'trend': 0.020824397347343444, 'seasonal': 0.07866104363465767, 'resid': -0.013000244424603513, 'observed': 0.0864851965573976}, {'date': '2012-11-01', 'trend': 0.01896231043720962, 'seasonal': -0.06447017941083495, 'resid': -0.023462451583598262, 'observed': -0.0689703205572236}, {'date': '2012-12-01', 'trend': 0.0031981506915670166, 'seasonal': 0.13259497070685478, 'resid': 0.07272214429393273, 'observed': 0.2085152656923545}, {'date': '2013-01-01', 'trend': -0.003035419802576943, 'seasonal': -0.031714528696978224, 'resid': -0.035875327608523126, 'observed': -0.0706252761080783}, {'date': '2013-02-01', 'trend': -0.0008950330310897714, 'seasonal': -0.04724202459970091, 'resid': -0.005314129219731817, 'observed': -0.0534511868505225}, {'date': '2013-03-01', 'trend': 0.005412577037834394, 'seasonal': -0.010757930825500861, 'resid': 0.026995604363886767, 'observed': 0.0216502505762203}, {'date': '2013-04-01', 'trend': 0.009242997420316925, 'seasonal': 0.02255283843870718, 'resid': 0.1939785794376204, 'observed': 0.2257744152966445}, {'date': '2013-05-01', 'trend': 0.014444255150244148, 'seasonal': -0.030587662937704367, 'resid': -0.3565060911811502, 'observed': -0.3726494989686104}, {'date': '2013-06-01', 'trend': 0.012049156042391663, 'seasonal': -0.017830922488024405, 'resid': -0.04714493211754186, 'observed': -0.0529266985631746}, {'date': '2013-07-01', 'trend': 0.008257073724790395, 'seasonal': 0.03184779489655618, 'resid': 0.04524276954342472, 'observed': 0.0853476381647713}, {'date': '2013-08-01', 'trend': 0.011644938815555297, 'seasonal': -0.052743362293617736, 'resid': 0.041742905766517235, 'observed': 0.0006444822884548}, {'date': '2013-09-01', 'trend': 0.012858457832634297, 'seasonal': -0.010310036424414365, 'resid': 0.08603468536763466, 'observed': 0.0885831067758546}, {'date': '2013-10-01', 'trend': 0.004237769660182612, 'seasonal': 0.07866104363465767, 'resid': 0.02866357274198722, 'observed': 0.1115623860368275}, {'date': '2013-11-01', 'trend': -0.004335033857545581, 'seasonal': -0.06447017941083495, 'resid': 0.09958788874998034, 'observed': 0.0307826754815998}, {'date': '2013-12-01', 'trend': -0.008880789719724598, 'seasonal': 0.13259497070685478, 'resid': -0.07243428992205868, 'observed': 0.0512798910650715}, {'date': '2014-01-01', 'trend': -0.017619162189213094, 'seasonal': -0.031714528696978224, 'resid': 0.044933813782965515, 'observed': -0.0043998771032258}, {'date': '2014-02-01', 'trend': -0.02311961132920546, 'seasonal': -0.04724202459970091, 'resid': 0.03199381225188907, 'observed': -0.0383678236770173}, {'date': '2014-03-01', 'trend': -0.03397059240086849, 'seasonal': -0.010757930825500861, 'resid': 0.08041986703898045, 'observed': 0.0356913438126111}, {'date': '2014-04-01', 'trend': -0.041985310898930206, 'seasonal': 0.02255283843870718, 'resid': 0.024269278381636228, 'observed': 0.0048368059214132}, {'date': '2014-05-01', 'trend': -0.041198161431693886, 'seasonal': -0.030587662937704367, 'resid': -0.2856733496494574, 'observed': -0.3574591740188557}, {'date': '2014-06-01', 'trend': -0.04789477224075883, 'seasonal': -0.017830922488024405, 'resid': -0.11148946947644248, 'observed': -0.1772151642052257}, {'date': '2014-07-01', 'trend': -0.052218878217136055, 'seasonal': 0.03184779489655618, 'resid': 0.02028624785967833, 'observed': -8.483546090154448e-05}, {'date': '2014-08-01', 'trend': -0.05494124643510578, 'seasonal': -0.052743362293617736, 'resid': 0.06175078528303441, 'observed': -0.0459338234456891}, {'date': '2014-09-01', 'trend': -0.06448933347929107, 'seasonal': -0.010310036424414365, 'resid': -0.05046276330620895, 'observed': -0.1252621332099144}, {'date': '2014-10-01', 'trend': -0.07028956365765798, 'seasonal': 0.07866104363465767, 'resid': 0.12468290209211572, 'observed': 0.1330543820691154}, {'date': '2014-11-01', 'trend': -0.06019034746773441, 'seasonal': -0.06447017941083495, 'resid': 0.15284279354155278, 'observed': 0.0281822666629834}, {'date': '2014-12-01', 'trend': -0.04221359859407286, 'seasonal': 0.13259497070685478, 'resid': -0.1972197316466527, 'observed': -0.1068383595338708}, {'date': '2015-01-01', 'trend': -0.038848053552043765, 'seasonal': -0.031714528696978224, 'resid': 0.12050241231168518, 'observed': 0.0499398300626632}, {'date': '2015-02-01', 'trend': -0.0381625832895537, 'seasonal': -0.04724202459970091, 'resid': -0.0726397601849248, 'observed': -0.1580443680741794}, {'date': '2015-03-01', 'trend': -0.029463840012527352, 'seasonal': -0.010757930825500861, 'resid': -0.033564430012645786, 'observed': -0.073786200850674}, {'date': '2015-04-01', 'trend': -0.03136658171414957, 'seasonal': 0.02255283843870718, 'resid': -0.01607743042066501, 'observed': -0.0248911736961074}, {'date': '2015-05-01', 'trend': -0.04081446354341571, 'seasonal': -0.030587662937704367, 'resid': -0.013947879362049422, 'observed': -0.0853500058431695}, {'date': '2015-06-01', 'trend': -0.03936403641140541, 'seasonal': -0.017830922488024405, 'resid': 0.03931259948639521, 'observed': -0.0178823594130346}, {'date': '2015-07-01', 'trend': -0.03547665094941137, 'seasonal': 0.03184779489655618, 'resid': -0.0750157031915393, 'observed': -0.0786445592443945}, {'date': '2015-08-01', 'trend': -0.02967353755159856, 'seasonal': -0.052743362293617736, 'resid': 0.13149408648278169, 'observed': 0.0490771866375654}, {'date': '2015-09-01', 'trend': -0.034311547465159596, 'seasonal': -0.010310036424414365, 'resid': 0.03311827924503746, 'observed': -0.0115033046445365}, {'date': '2015-10-01', 'trend': -0.04624601382043859, 'seasonal': 0.07866104363465767, 'resid': -0.05878527714941489, 'observed': -0.0263702473351958}, {'date': '2015-11-01', 'trend': -0.02522274760439807, 'seasonal': -0.06447017941083495, 'resid': 0.050550659180140324, 'observed': -0.0391422678350927}, {'date': '2015-12-01', 'trend': 0.006684184979284752, 'seasonal': 0.13259497070685478, 'resid': -0.14398272955368682, 'observed': -0.0047035738675473}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.031714528696978224, 'resid': nan, 'observed': 0.0411022954841966}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.04724202459970091, 'resid': nan, 'observed': -0.0099321119482054}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.010757930825500861, 'resid': nan, 'observed': -0.3332106949021129}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.02255283843870718, 'resid': nan, 'observed': -0.0518938721713642}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.030587662937704367, 'resid': nan, 'observed': 0.4462110818170597}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.017830922488024405, 'resid': nan, 'observed': 0.2163229349351239}], 'diff1_loan_rate_B': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.07046143882383622, 'resid': nan, 'observed': 0.1343589743589746}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.032770515961085024, 'resid': nan, 'observed': -0.2210256410256423}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.0434329852966769, 'resid': nan, 'observed': 0.156666666666668}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.11246094776255657, 'resid': nan, 'observed': 0.0344444444444445}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.018135820501638387, 'resid': nan, 'observed': -0.0802777777777787}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.07022848869063908, 'resid': nan, 'observed': 0.3056250000000009}, {'date': '2008-02-01', 'trend': 0.07253161961495286, 'seasonal': -0.045894739401959414, 'resid': 0.31596845686565844, 'observed': 0.3426053370786519}, {'date': '2008-03-01', 'trend': 0.08760878010878013, 'seasonal': 0.0018055671824235378, 'resid': -0.06804791017630608, 'observed': 0.0213664371148976}, {'date': '2008-04-01', 'trend': 0.10336111111111115, 'seasonal': -0.0004989351068384237, 'resid': 0.0012691199776175741, 'observed': 0.1041312959818903}, {'date': '2008-05-01', 'trend': 0.11530555555555554, 'seasonal': -0.022198231228783477, 'resid': -0.17641872783554527, 'observed': -0.0833114035087732}, {'date': '2008-06-01', 'trend': 0.14536751443001444, 'seasonal': 0.02562000786713773, 'resid': -0.18072237078200068, 'observed': -0.0097348484848485}, {'date': '2008-07-01', 'trend': 0.16546677469571494, 'seasonal': -0.081401970823299, 'resid': 0.005901526093912465, 'observed': 0.0899663299663284}, {'date': '2008-08-01', 'trend': 0.16290479051505097, 'seasonal': 0.07046143882383622, 'resid': 0.05212198614932932, 'observed': 0.2854882154882165}, {'date': '2008-09-01', 'trend': 0.16511287791960133, 'seasonal': 0.032770515961085024, 'resid': -0.20818642418371616, 'observed': -0.0103030303030298}, {'date': '2008-10-01', 'trend': 0.15959744792087738, 'seasonal': 0.0434329852966769, 'resid': 0.12096956678244551, 'observed': 0.3239999999999998}, {'date': '2008-11-01', 'trend': 0.14138640873015867, 'seasonal': -0.11246094776255657, 'resid': 0.1248523168101759, 'observed': 0.153777777777778}, {'date': '2008-12-01', 'trend': 0.13116512161084526, 'seasonal': 0.018135820501638387, 'resid': 0.372574959763418, 'observed': 0.5218759018759016}, {'date': '2009-01-01', 'trend': 0.12665808188724859, 'seasonal': 0.07022848869063908, 'resid': -0.011033003854755474, 'observed': 0.1858535667231322}, {'date': '2009-02-01', 'trend': 0.11628219804886486, 'seasonal': -0.045894739401959414, 'resid': 0.33050169137268, 'observed': 0.4008891500195854}, {'date': '2009-03-01', 'trend': 0.11547369581894905, 'seasonal': 0.0018055671824235378, 'resid': -0.101202541118199, 'observed': 0.0160767218831736}, {'date': '2009-04-01', 'trend': 0.10519924478688923, 'seasonal': -0.0004989351068384237, 'resid': -0.1276496184358118, 'observed': -0.022949308755761}, {'date': '2009-05-01', 'trend': 0.08361755233494368, 'seasonal': -0.022198231228783477, 'resid': -0.45471506045453125, 'observed': -0.3932957393483711}, {'date': '2009-06-01', 'trend': 0.05570939347564919, 'seasonal': 0.02562000786713773, 'resid': -0.02639080485155902, 'observed': 0.0549385964912279}, {'date': '2009-07-01', 'trend': 0.017878874306814792, 'seasonal': -0.081401970823299, 'resid': -0.01935297185958379, 'observed': -0.082876068376068}, {'date': '2009-08-01', 'trend': -0.05598613588513567, 'seasonal': 0.07046143882383622, 'resid': 0.19483409877070224, 'observed': 0.2093094017094028}, {'date': '2009-09-01', 'trend': -0.11263551516313529, 'seasonal': 0.032770515961085024, 'resid': 0.12633672915985508, 'observed': 0.0464717299578048}, {'date': '2009-10-01', 'trend': -0.11396564767314926, 'seasonal': 0.0434329852966769, 'resid': 0.09117107734620196, 'observed': 0.0206384149697296}, {'date': '2009-11-01', 'trend': -0.09415100843104823, 'seasonal': -0.11246094776255657, 'resid': 0.1457907001549597, 'observed': -0.0608212560386451}, {'date': '2009-12-01', 'trend': -0.05602207779516801, 'seasonal': 0.018135820501638387, 'resid': 0.10456538036278631, 'observed': 0.0666791230692567}, {'date': '2010-01-01', 'trend': -0.03414123324984977, 'seasonal': 0.07022848869063908, 'resid': -0.3029693699630377, 'observed': -0.2668821145222484}, {'date': '2010-02-01', 'trend': -0.038466300031433656, 'seasonal': -0.045894739401959414, 'resid': -0.8347743739084519, 'observed': -0.919135413341845}, {'date': '2010-03-01', 'trend': -0.051222045739554606, 'seasonal': 0.0018055671824235378, 'resid': 0.025932661129744267, 'observed': -0.0234838174273868}, {'date': '2010-04-01', 'trend': -0.08204924709401365, 'seasonal': -0.0004989351068384237, 'resid': 0.06723623251531607, 'observed': -0.015311949685536}, {'date': '2010-05-01', 'trend': -0.14012220490245753, 'seasonal': -0.022198231228783477, 'resid': 0.2369386795230695, 'observed': 0.0746182433918285}, {'date': '2010-06-01', 'trend': -0.1759102546194294, 'seasonal': 0.02562000786713773, 'resid': 0.6524091957644453, 'observed': 0.5021189490121536}, {'date': '2010-07-01', 'trend': -0.15017141807608086, 'seasonal': -0.081401970823299, 'resid': 0.22665723709002406, 'observed': -0.0049161518093558}, {'date': '2010-08-01', 'trend': -0.0697244601852828, 'seasonal': 0.07046143882383622, 'resid': 0.02681090374612409, 'observed': 0.0275478823846775}, {'date': '2010-09-01', 'trend': -0.017108422640112805, 'seasonal': 0.032770515961085024, 'resid': -0.093566741033345, 'observed': -0.0779046477123728}, {'date': '2010-10-01', 'trend': -0.01619971963324996, 'seasonal': 0.0434329852966769, 'resid': -0.6220713055305368, 'observed': -0.5948380398671098}, {'date': '2010-11-01', 'trend': 0.007396789260623559, 'seasonal': -0.11246094776255657, 'resid': -0.7340316301025256, 'observed': -0.8390957886044585}, {'date': '2010-12-01', 'trend': 0.010777912766808017, 'seasonal': 0.018135820501638387, 'resid': -0.042873270840701305, 'observed': -0.0139595375722549}, {'date': '2011-01-01', 'trend': -0.011288570651843756, 'seasonal': 0.07022848869063908, 'resid': 0.37254870512083327, 'observed': 0.4314886231596286}, {'date': '2011-02-01', 'trend': -0.013138828366925894, 'seasonal': -0.045894739401959414, 'resid': 0.372254406124316, 'observed': 0.3132208383554307}, {'date': '2011-03-01', 'trend': -0.0032650106639154704, 'seasonal': 0.0018055671824235378, 'resid': 0.008404275440909534, 'observed': 0.0069448319594176}, {'date': '2011-04-01', 'trend': 0.046106888540878604, 'seasonal': -0.0004989351068384237, 'resid': -0.06953968034167218, 'observed': -0.023931726907632}, {'date': '2011-05-01', 'trend': 0.11842691528802644, 'seasonal': -0.022198231228783477, 'resid': 0.5533255500076458, 'observed': 0.6495542340668887}, {'date': '2011-06-01', 'trend': 0.15358747474607576, 'seasonal': 0.02562000786713773, 'resid': -0.17087756012769287, 'observed': 0.0083299224855206}, {'date': '2011-07-01', 'trend': 0.13951867653073943, 'seasonal': -0.081401970823299, 'resid': -0.09883943303780582, 'observed': -0.0407227273303654}, {'date': '2011-08-01', 'trend': 0.10881052011036284, 'seasonal': 0.07046143882383622, 'resid': -0.16032368619048354, 'observed': 0.0189482727437155}, {'date': '2011-09-01', 'trend': 0.10591245912195688, 'seasonal': 0.032770515961085024, 'resid': 0.028983611717797612, 'observed': 0.1676665868008395}, {'date': '2011-10-01', 'trend': 0.12029888775357502, 'seasonal': 0.0434329852966769, 'resid': 0.18078443348448392, 'observed': 0.3445163065347358}, {'date': '2011-11-01', 'trend': 0.09964719123988357, 'seasonal': -0.11246094776255657, 'resid': -0.029955736552082904, 'observed': -0.0427694930747559}, {'date': '2011-12-01', 'trend': 0.08175969247221472, 'seasonal': 0.018135820501638387, 'resid': -0.0663279190826275, 'observed': 0.0335675938912256}, {'date': '2012-01-01', 'trend': 0.09190183274358896, 'seasonal': 0.07022848869063908, 'resid': -0.11581998690615164, 'observed': 0.0463103345280764}, {'date': '2012-02-01', 'trend': 0.1009050265452881, 'seasonal': -0.045894739401959414, 'resid': -0.0936069142453843, 'observed': -0.0385966271020556}, {'date': '2012-03-01', 'trend': 0.09763085440825638, 'seasonal': 0.0018055671824235378, 'resid': 0.18977241210448131, 'observed': 0.2892088336951612}, {'date': '2012-04-01', 'trend': 0.07324583971131912, 'seasonal': -0.0004989351068384237, 'resid': -0.0336683460890211, 'observed': 0.0390785585154596}, {'date': '2012-05-01', 'trend': 0.06075504434068528, 'seasonal': -0.022198231228783477, 'resid': 0.0523464192033007, 'observed': 0.0909032323152025}, {'date': '2012-06-01', 'trend': 0.054106865190672766, 'seasonal': 0.02562000786713773, 'resid': 0.057954080755343716, 'observed': 0.1376809538131542}, {'date': '2012-07-01', 'trend': 0.042441027870710114, 'seasonal': -0.081401970823299, 'resid': 0.11229855080757159, 'observed': 0.0733376078549827}, {'date': '2012-08-01', 'trend': 0.042250133304635284, 'seasonal': 0.07046143882383622, 'resid': 0.00825301667067549, 'observed': 0.120964588799147}, {'date': '2012-09-01', 'trend': 0.028265892129516017, 'seasonal': 0.032770515961085024, 'resid': -0.07396626863395464, 'observed': -0.0129298605433536}, {'date': '2012-10-01', 'trend': 0.012322106610375349, 'seasonal': 0.0434329852966769, 'resid': -0.11588269075461714, 'observed': -0.0601275988475649}, {'date': '2012-11-01', 'trend': 0.0075267811347561175, 'seasonal': -0.11246094776255657, 'resid': 0.16702949004013315, 'observed': 0.0620953234123327}, {'date': '2012-12-01', 'trend': -0.006617476325216561, 'seasonal': 0.018135820501638387, 'resid': -0.24237186637258523, 'observed': -0.2308535221961634}, {'date': '2013-01-01', 'trend': -0.044990516158915986, 'seasonal': 0.07022848869063908, 'resid': 0.0055133824046385005, 'observed': 0.0307513549363616}, {'date': '2013-02-01', 'trend': -0.07684670346080483, 'seasonal': -0.045894739401959414, 'resid': 0.09512232576662744, 'observed': -0.0276191170961368}, {'date': '2013-03-01', 'trend': -0.07540957750188355, 'seasonal': 0.0018055671824235378, 'resid': 0.016213545805839818, 'observed': -0.0573904645136202}, {'date': '2013-04-01', 'trend': -0.05654519169114858, 'seasonal': -0.0004989351068384237, 'resid': 0.0600711310628522, 'observed': 0.0030270042648652}, {'date': '2013-05-01', 'trend': -0.04025695390906634, 'seasonal': -0.022198231228783477, 'resid': 0.07432216028878522, 'observed': 0.0118669751509354}, {'date': '2013-06-01', 'trend': -0.026501686923818615, 'seasonal': 0.02562000786713773, 'resid': -0.12186328900524213, 'observed': -0.122744968061923}, {'date': '2013-07-01', 'trend': -0.017724880517176876, 'seasonal': -0.081401970823299, 'resid': -0.48806257493825056, 'observed': -0.5871894262787265}, {'date': '2013-08-01', 'trend': -0.022383778667604298, 'seasonal': 0.07046143882383622, 'resid': -0.031134532468707926, 'observed': 0.016943127687524}, {'date': '2013-09-01', 'trend': -0.024856579833205748, 'seasonal': 0.032770515961085024, 'resid': 0.11766868745450074, 'observed': 0.12558262358238}, {'date': '2013-10-01', 'trend': -0.024572831959464192, 'seasonal': 0.0434329852966769, 'resid': 0.2352450231471282, 'observed': 0.2541051764843409}, {'date': '2013-11-01', 'trend': -0.04578053795130625, 'seasonal': -0.11246094776255657, 'resid': 0.2970217405642637, 'observed': 0.1387802548504009}, {'date': '2013-12-01', 'trend': -0.07243155863508847, 'seasonal': 0.018135820501638387, 'resid': 0.07688369214516377, 'observed': 0.0225879540117137}, {'date': '2014-01-01', 'trend': -0.05423872138206551, 'seasonal': 0.07022848869063908, 'resid': -0.028036534820687575, 'observed': -0.012046767512114}, {'date': '2014-02-01', 'trend': -0.03397058050497438, 'seasonal': -0.045894739401959414, 'resid': -0.016769230350985306, 'observed': -0.0966345502579191}, {'date': '2014-03-01', 'trend': -0.0424961365791401, 'seasonal': 0.0018055671824235378, 'resid': -0.007031689929556136, 'observed': -0.0477222593262727}, {'date': '2014-04-01', 'trend': -0.05547293814302131, 'seasonal': -0.0004989351068384237, 'resid': 0.056140621297174835, 'observed': 0.0001687480473151}, {'date': '2014-05-01', 'trend': -0.08515159640587747, 'seasonal': -0.022198231228783477, 'resid': -0.38690988480106325, 'observed': -0.4942597124357242}, {'date': '2014-06-01', 'trend': -0.11441681410591502, 'seasonal': 0.02562000786713773, 'resid': -0.16744597064725938, 'observed': -0.2562427768860367}, {'date': '2014-07-01', 'trend': -0.11847281350061632, 'seasonal': -0.081401970823299, 'resid': 0.18281126094185382, 'observed': -0.0170635233820615}, {'date': '2014-08-01', 'trend': -0.12316438274360003, 'seasonal': 0.07046143882383622, 'resid': -0.014044450239189987, 'observed': -0.0667473941589538}, {'date': '2014-09-01', 'trend': -0.135097032058235, 'seasonal': 0.032770515961085024, 'resid': 0.1069863157460306, 'observed': 0.0046597996488806}, {'date': '2014-10-01', 'trend': -0.14024366983363687, 'seasonal': 0.0434329852966769, 'resid': 0.16039544742165096, 'observed': 0.063584762884691}, {'date': '2014-11-01', 'trend': -0.12035307260737171, 'seasonal': -0.11246094776255657, 'resid': -0.15017310948856893, 'observed': -0.3829871298584972}, {'date': '2014-12-01', 'trend': -0.09086346056825195, 'seasonal': 0.018135820501638387, 'resid': -0.08528224601367623, 'observed': -0.1580098860802898}, {'date': '2015-01-01', 'trend': -0.08372327327467737, 'seasonal': 0.07022848869063908, 'resid': 0.084701871691097, 'observed': 0.0712070871070587}, {'date': '2015-02-01', 'trend': -0.0835007527359228, 'seasonal': -0.045894739401959414, 'resid': -0.16309057457081877, 'observed': -0.292486066708701}, {'date': '2015-03-01', 'trend': -0.0816249274058822, 'seasonal': 0.0018055671824235378, 'resid': -0.05843496620327213, 'observed': -0.1382543264267308}, {'date': '2015-04-01', 'trend': -0.08406588342221845, 'seasonal': -0.0004989351068384237, 'resid': 0.051746327067185974, 'observed': -0.0328184914618709}, {'date': '2015-05-01', 'trend': -0.0684165593893712, 'seasonal': -0.022198231228783477, 'resid': 0.10671665112198009, 'observed': 0.0161018605038254}, {'date': '2015-06-01', 'trend': -0.04701617470145597, 'seasonal': 0.02562000786713773, 'resid': -0.03745749405239336, 'observed': -0.0588536608867116}, {'date': '2015-07-01', 'trend': -0.04587836546821761, 'seasonal': -0.081401970823299, 'resid': 0.08419219195591982, 'observed': -0.0430881443355968}, {'date': '2015-08-01', 'trend': -0.034932169714336984, 'seasonal': 0.07046143882383622, 'resid': -0.07091154938480815, 'observed': -0.0353822802753089}, {'date': '2015-09-01', 'trend': -0.01580589910134819, 'seasonal': 0.032770515961085024, 'resid': 0.0013498768264733688, 'observed': 0.0183144936862102}, {'date': '2015-10-01', 'trend': -0.007080095998270134, 'seasonal': 0.0434329852966769, 'resid': -0.04500576484311507, 'observed': -0.0086528755447083}, {'date': '2015-11-01', 'trend': -0.007776782505493978, 'seasonal': -0.11246094776255657, 'resid': 0.18507201562728653, 'observed': 0.064834285359236}, {'date': '2015-12-01', 'trend': 0.0012050536184403146, 'seasonal': 0.018135820501638387, 'resid': -0.1115629429081362, 'observed': -0.0922220687880575}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.07022848869063908, 'resid': nan, 'observed': 0.032726691412547}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.045894739401959414, 'resid': nan, 'observed': 0.0087030270789458}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.0018055671824235378, 'resid': nan, 'observed': 0.0195870744973536}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.0004989351068384237, 'resid': nan, 'observed': 0.0187593820879179}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.022198231228783477, 'resid': nan, 'observed': -0.0521964892193356}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.02562000786713773, 'resid': nan, 'observed': 0.2250087558108724}], 'diff1_loan_rate_C': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.023538262427489662, 'resid': nan, 'observed': 0.2074999999999995}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.05848004070126249, 'resid': nan, 'observed': -0.118333333333334}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.042982962767393634, 'resid': nan, 'observed': -0.0032407407407397}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.13074174163157376, 'resid': nan, 'observed': 0.1411111111111118}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.02963753046734331, 'resid': nan, 'observed': -0.1620370370370381}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.03051548559335375, 'resid': nan, 'observed': 0.4830555555555555}, {'date': '2008-02-01', 'trend': 0.08177216880341873, 'seasonal': -0.009428661869998278, 'resid': 0.16523730114738655, 'observed': 0.237580808080807}, {'date': '2008-03-01', 'trend': 0.07897569444444455, 'seasonal': -0.009181965409746102, 'resid': 0.008248068248478558, 'observed': 0.078041797283177}, {'date': '2008-04-01', 'trend': 0.09493738129154798, 'seasonal': -0.014386009084824822, 'resid': -0.07063578312626287, 'observed': 0.0099155890804603}, {'date': '2008-05-01', 'trend': 0.11193583808167135, 'seasonal': -0.018321804588489938, 'resid': -0.07239296867836602, 'observed': 0.0212210648148154}, {'date': '2008-06-01', 'trend': 0.13923542406561806, 'seasonal': 0.006198504602688547, 'resid': -0.1311310964242989, 'observed': 0.0143028322440077}, {'date': '2008-07-01', 'trend': 0.15975801104653842, 'seasonal': -0.009292603974898495, 'resid': -0.01830100284841311, 'observed': 0.1321644042232268}, {'date': '2008-08-01', 'trend': 0.14655239898989897, 'seasonal': 0.023538262427489662, 'resid': -0.08262271269943802, 'observed': 0.0874679487179506}, {'date': '2008-09-01', 'trend': 0.13420448740276325, 'seasonal': 0.05848004070126249, 'resid': -0.25810119477069143, 'observed': -0.0654166666666657}, {'date': '2008-10-01', 'trend': 0.12909123448091658, 'seasonal': 0.042982962767393634, 'resid': 0.15484887967476466, 'observed': 0.3269230769230749}, {'date': '2008-11-01', 'trend': 0.1266789871705552, 'seasonal': -0.13074174163157376, 'resid': 0.22297301087127616, 'observed': 0.2189102564102576}, {'date': '2008-12-01', 'trend': 0.12436923576842174, 'seasonal': 0.02963753046734331, 'resid': 0.26134711504277275, 'observed': 0.4153538812785378}, {'date': '2009-01-01', 'trend': 0.11464929483655441, 'seasonal': 0.03051548559335375, 'resid': 0.25304194435215915, 'observed': 0.3982067247820673}, {'date': '2009-02-01', 'trend': 0.11559828821777357, 'seasonal': -0.009428661869998278, 'resid': -0.1006746768528264, 'observed': 0.0054949494949489}, {'date': '2009-03-01', 'trend': 0.1351548442760942, 'seasonal': -0.009181965409746102, 'resid': -0.1121951010885707, 'observed': 0.0137777777777774}, {'date': '2009-04-01', 'trend': 0.1295184182180072, 'seasonal': -0.014386009084824822, 'resid': -0.16367087067164196, 'observed': -0.0485384615384596}, {'date': '2009-05-01', 'trend': 0.10425474758308659, 'seasonal': -0.018321804588489938, 'resid': -0.06415176300953446, 'observed': 0.0217811799850622}, {'date': '2009-06-01', 'trend': 0.07938407382962666, 'seasonal': 0.006198504602688547, 'resid': -0.12727389500975692, 'observed': -0.0416913165774417}, {'date': '2009-07-01', 'trend': 0.04480301891962437, 'seasonal': -0.009292603974898495, 'resid': -0.08063044426486578, 'observed': -0.0451200293201399}, {'date': '2009-08-01', 'trend': 0.020944087091242646, 'seasonal': 0.023538262427489662, 'resid': 0.24304587389184482, 'observed': 0.2875282234105771}, {'date': '2009-09-01', 'trend': 0.014079656149836417, 'seasonal': 0.05848004070126249, 'resid': 0.13132070718930378, 'observed': 0.2038804040404027}, {'date': '2009-10-01', 'trend': 0.018584390522217834, 'seasonal': 0.042982962767393634, 'resid': -0.13921557246769328, 'observed': -0.0776482191780818}, {'date': '2009-11-01', 'trend': 0.023467782398942504, 'seasonal': -0.13074174163157376, 'resid': 0.12442741650595125, 'observed': 0.01715345727332}, {'date': '2009-12-01', 'trend': 0.03814044501669851, 'seasonal': 0.02963753046734331, 'resid': -0.04756346515160492, 'observed': 0.0202145103324369}, {'date': '2010-01-01', 'trend': 0.05988987718423532, 'seasonal': 0.03051548559335375, 'resid': -0.12700458488947597, 'observed': -0.0365992221118869}, {'date': '2010-02-01', 'trend': 0.05307828035254506, 'seasonal': -0.009428661869998278, 'resid': -0.1759630859748049, 'observed': -0.1323134674922581}, {'date': '2010-03-01', 'trend': 0.0298807216565112, 'seasonal': -0.009181965409746102, 'resid': -0.0338589040755303, 'observed': -0.0131601478287652}, {'date': '2010-04-01', 'trend': 0.01032291111379784, 'seasonal': -0.014386009084824822, 'resid': 0.09057618697626399, 'observed': 0.086513089005237}, {'date': '2010-05-01', 'trend': -0.022554434520587318, 'seasonal': -0.018321804588489938, 'resid': 0.044807273591834854, 'observed': 0.0039310344827576}, {'date': '2010-06-01', 'trend': -0.04617230149098248, 'seasonal': 0.006198504602688547, 'resid': 0.36827652863930116, 'observed': 0.3283027317510072}, {'date': '2010-07-01', 'trend': -0.040473591202054245, 'seasonal': -0.009292603974898495, 'resid': 0.15663848954924736, 'observed': 0.1068722943722946}, {'date': '2010-08-01', 'trend': -0.019491865253334312, 'seasonal': 0.023538262427489662, 'resid': -0.03198882141657905, 'observed': -0.0279424242424237}, {'date': '2010-09-01', 'trend': -0.0030483304315947, 'seasonal': 0.05848004070126249, 'resid': -0.09282206728107689, 'observed': -0.0373903570114091}, {'date': '2010-10-01', 'trend': -0.008952034962472556, 'seasonal': 0.042982962767393634, 'resid': -0.3397958389563118, 'observed': -0.3057649111513907}, {'date': '2010-11-01', 'trend': 0.008096317190669238, 'seasonal': -0.13074174163157376, 'resid': -0.42114072153771037, 'observed': -0.5437861459786149}, {'date': '2010-12-01', 'trend': 0.021255319357675065, 'seasonal': 0.02963753046734331, 'resid': -0.03656754353013088, 'observed': 0.0143253062948875}, {'date': '2011-01-01', 'trend': 0.005997675966532471, 'seasonal': 0.03051548559335375, 'resid': 0.06954586730005408, 'observed': 0.1060590288599403}, {'date': '2011-02-01', 'trend': 0.000328811447912961, 'seasonal': -0.009428661869998278, 'resid': 0.2376895547272785, 'observed': 0.2285897043051932}, {'date': '2011-03-01', 'trend': 0.014103505047649581, 'seasonal': -0.009181965409746102, 'resid': 0.01565997645763072, 'observed': 0.0205815160955342}, {'date': '2011-04-01', 'trend': 0.05282867463749468, 'seasonal': -0.014386009084824822, 'resid': -0.12736014921280076, 'observed': -0.0889174836601309}, {'date': '2011-05-01', 'trend': 0.09961397300645969, 'seasonal': -0.018321804588489938, 'resid': 0.5072298904055588, 'observed': 0.5885220588235285}, {'date': '2011-06-01', 'trend': 0.1255908836847626, 'seasonal': 0.006198504602688547, 'resid': -0.07226162886907493, 'observed': 0.0595277594183762}, {'date': '2011-07-01', 'trend': 0.12134087167607463, 'seasonal': -0.009292603974898495, 'resid': -0.10258444238367274, 'observed': 0.0094638253175034}, {'date': '2011-08-01', 'trend': 0.1110441126308099, 'seasonal': 0.023538262427489662, 'resid': -0.20116907869280046, 'observed': -0.0665867036345009}, {'date': '2011-09-01', 'trend': 0.11572648891561448, 'seasonal': 0.05848004070126249, 'resid': 0.1576400391574702, 'observed': 0.3318465687743472}, {'date': '2011-10-01', 'trend': 0.12997502514238415, 'seasonal': 0.042982962767393634, 'resid': 0.08144424530935751, 'observed': 0.2544022332191353}, {'date': '2011-11-01', 'trend': 0.10938241985012398, 'seasonal': -0.13074174163157376, 'resid': 0.04025319228746928, 'observed': 0.0188938705060195}, {'date': '2011-12-01', 'trend': 0.08399893170567779, 'seasonal': 0.02963753046734331, 'resid': -0.03854531608349801, 'observed': 0.0750911460895231}, {'date': '2012-01-01', 'trend': 0.10210200836442407, 'seasonal': 0.03051548559335375, 'resid': -0.1893245931009846, 'observed': -0.0567070991432068}, {'date': '2012-02-01', 'trend': 0.12769282473856813, 'seasonal': -0.009428661869998278, 'resid': 0.025969452353417057, 'observed': 0.1442336152219869}, {'date': '2012-03-01', 'trend': 0.12266235492976871, 'seasonal': -0.009181965409746102, 'resid': 0.10383424649402768, 'observed': 0.2173146360140503}, {'date': '2012-04-01', 'trend': 0.0981006307644038, 'seasonal': -0.014386009084824822, 'resid': -0.02740035581575338, 'observed': 0.0563142658638256}, {'date': '2012-05-01', 'trend': 0.08641169864468848, 'seasonal': -0.018321804588489938, 'resid': -0.11902211177087095, 'observed': -0.0509322177146724}, {'date': '2012-06-01', 'trend': 0.09506837094688565, 'seasonal': 0.006198504602688547, 'resid': -0.011488555059705906, 'observed': 0.0897783204898683}, {'date': '2012-07-01', 'trend': 0.1079662325888333, 'seasonal': -0.009292603974898495, 'resid': 0.3150134754419876, 'observed': 0.4136871040559224}, {'date': '2012-08-01', 'trend': 0.10552586473766148, 'seasonal': 0.023538262427489662, 'resid': 0.01430548344138646, 'observed': 0.1433696106065376}, {'date': '2012-09-01', 'trend': 0.09137273738197532, 'seasonal': 0.05848004070126249, 'resid': -0.14869379896111562, 'observed': 0.0011589791221222}, {'date': '2012-10-01', 'trend': 0.0813850371425335, 'seasonal': 0.042982962767393634, 'resid': -0.12875955700732464, 'observed': -0.0043915570973975}, {'date': '2012-11-01', 'trend': 0.08034908737684145, 'seasonal': -0.13074174163157376, 'resid': 0.04754594420411681, 'observed': -0.0028467100506155}, {'date': '2012-12-01', 'trend': 0.07780893990577106, 'seasonal': 0.02963753046734331, 'resid': 0.19714539152577595, 'observed': 0.3045918618988903}, {'date': '2013-01-01', 'trend': 0.03406573649563173, 'seasonal': 0.03051548559335375, 'resid': -0.041240357634815775, 'observed': 0.0233408644541697}, {'date': '2013-02-01', 'trend': -0.014144350512861207, 'seasonal': -0.009428661869998278, 'resid': 0.029189835579346284, 'observed': 0.0056168231964868}, {'date': '2013-03-01', 'trend': -0.014151127597210836, 'seasonal': -0.009181965409746102, 'resid': 0.03958946451003964, 'observed': 0.0162563715030827}, {'date': '2013-04-01', 'trend': 0.005884417671098063, 'seasonal': -0.014386009084824822, 'resid': 0.02616931604191646, 'observed': 0.0176677246281897}, {'date': '2013-05-01', 'trend': 0.01344750950178895, 'seasonal': -0.018321804588489938, 'resid': -0.032274175768945215, 'observed': -0.0371484708556462}, {'date': '2013-06-01', 'trend': -0.021112030362065867, 'seasonal': 0.006198504602688547, 'resid': 0.029944560084530225, 'observed': 0.0150310343251529}, {'date': '2013-07-01', 'trend': -0.057103480777511054, 'seasonal': -0.009292603974898495, 'resid': -0.49500640687029673, 'observed': -0.5614024916227063}, {'date': '2013-08-01', 'trend': -0.07151753908401697, 'seasonal': 0.023538262427489662, 'resid': 0.009396394737863103, 'observed': -0.0385828819186642}, {'date': '2013-09-01', 'trend': -0.0867499712964841, 'seasonal': 0.05848004070126249, 'resid': 0.2112187522181545, 'observed': 0.1829488216229329}, {'date': '2013-10-01', 'trend': -0.09416931841093755, 'seasonal': 0.042982962767393634, 'resid': 0.3458580424847493, 'observed': 0.2946716868412053}, {'date': '2013-11-01', 'trend': -0.11345204937083736, 'seasonal': -0.13074174163157376, 'resid': 0.12379804094977402, 'observed': -0.1203957500526371}, {'date': '2013-12-01', 'trend': -0.1436487743885572, 'seasonal': 0.02963753046734331, 'resid': -0.2932768109103897, 'observed': -0.4072880548316036}, {'date': '2014-01-01', 'trend': -0.13045227407867463, 'seasonal': 0.03051548559335375, 'resid': -0.02863724030070021, 'observed': -0.1285740287860211}, {'date': '2014-02-01', 'trend': -0.10572335453562173, 'seasonal': -0.009428661869998278, 'resid': -0.0732536665138444, 'observed': -0.1884056829194644}, {'date': '2014-03-01', 'trend': -0.11297566582606595, 'seasonal': -0.009181965409746102, 'resid': -0.03314186424436505, 'observed': -0.1552994954801771}, {'date': '2014-04-01', 'trend': -0.1340919811263499, 'seasonal': -0.014386009084824822, 'resid': 0.15963725107574123, 'observed': 0.0111592608645665}, {'date': '2014-05-01', 'trend': -0.15523672623366971, 'seasonal': -0.018321804588489938, 'resid': -0.3198670193074588, 'observed': -0.4934255501296185}, {'date': '2014-06-01', 'trend': -0.14757614909406946, 'seasonal': 0.006198504602688547, 'resid': -0.11203564233476958, 'observed': -0.2534132868261505}, {'date': '2014-07-01', 'trend': -0.12602150861736044, 'seasonal': -0.009292603974898495, 'resid': 0.15907194955803716, 'observed': 0.0237578369657782}, {'date': '2014-08-01', 'trend': -0.11995210145767261, 'seasonal': 0.023538262427489662, 'resid': 0.06616469755630375, 'observed': -0.0302491414738792}, {'date': '2014-09-01', 'trend': -0.11671427012356601, 'seasonal': 0.05848004070126249, 'resid': 0.05879383962979062, 'observed': 0.0005596102074871}, {'date': '2014-10-01', 'trend': -0.11272167572650926, 'seasonal': 0.042982962767393634, 'resid': 0.04000804400895212, 'observed': -0.0297306689501635}, {'date': '2014-11-01', 'trend': -0.0928386868162864, 'seasonal': -0.13074174163157376, 'resid': -0.07988684838908375, 'observed': -0.3034672768369439}, {'date': '2014-12-01', 'trend': -0.06289602463930324, 'seasonal': 0.02963753046734331, 'resid': -0.007104182524930561, 'observed': -0.0403626766968905}, {'date': '2015-01-01', 'trend': -0.05306681380896677, 'seasonal': 0.03051548559335375, 'resid': 0.04436329273589483, 'observed': 0.0218119645202818}, {'date': '2015-02-01', 'trend': -0.05349604629974409, 'seasonal': -0.009428661869998278, 'resid': -0.13020119622351675, 'observed': -0.1931259043932591}, {'date': '2015-03-01', 'trend': -0.053546988518803344, 'seasonal': -0.009181965409746102, 'resid': -0.01014236805927456, 'observed': -0.072871321987824}, {'date': '2015-04-01', 'trend': -0.05173856098857298, 'seasonal': -0.014386009084824822, 'resid': 0.0906779229749733, 'observed': 0.0245533529015755}, {'date': '2015-05-01', 'trend': -0.04497049651300695, 'seasonal': -0.018321804588489938, 'resid': 0.03366439278021779, 'observed': -0.0296279083212791}, {'date': '2015-06-01', 'trend': -0.03874878820579345, 'seasonal': 0.006198504602688547, 'resid': 0.03396324721621081, 'observed': 0.0014129636131059}, {'date': '2015-07-01', 'trend': -0.029666649630916878, 'seasonal': -0.009292603974898495, 'resid': 0.04379190006041227, 'observed': 0.0048326464545969}, {'date': '2015-08-01', 'trend': -0.006025474592698351, 'seasonal': 0.023538262427489662, 'resid': -0.03913831857614461, 'observed': -0.0216255307413533}, {'date': '2015-09-01', 'trend': 0.013596104455676111, 'seasonal': 0.05848004070126249, 'resid': -0.0813627589393993, 'observed': -0.0092866137824607}, {'date': '2015-10-01', 'trend': 0.01692957780197725, 'seasonal': 0.042982962767393634, 'resid': -0.03639472480405788, 'observed': 0.023517815765313}, {'date': '2015-11-01', 'trend': 0.01643604414209548, 'seasonal': -0.13074174163157376, 'resid': -0.07997651664935754, 'observed': -0.1942822141388358}, {'date': '2015-12-01', 'trend': 0.02757739963634078, 'seasonal': 0.02963753046734331, 'resid': -0.05744167012555859, 'observed': -0.0002267400218745}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.03051548559335375, 'resid': nan, 'observed': 0.1996473536423035}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.009428661869998278, 'resid': nan, 'observed': 0.196426907401964}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.009181965409746102, 'resid': nan, 'observed': 0.00849376337794}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.014386009084824822, 'resid': nan, 'observed': 0.0231916278470389}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.018321804588489938, 'resid': nan, 'observed': -0.040110991103905}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.006198504602688547, 'resid': nan, 'observed': 0.2792885782576189}], 'diff1_loan_rate_D': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.04796336379262829, 'resid': nan, 'observed': -0.4674444444444443}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.01064043177203127, 'resid': nan, 'observed': 0.1696666666666679}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.040188196506185894, 'resid': nan, 'observed': 0.3007017543859636}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.13132803383378105, 'resid': nan, 'observed': -0.1279239766081872}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.050693663657927046, 'resid': nan, 'observed': -0.1305555555555546}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.022104325250881424, 'resid': nan, 'observed': 0.5293111111111095}, {'date': '2008-02-01', 'trend': 0.07385787037037043, 'seasonal': 0.08221064578468242, 'resid': 0.07165740977087458, 'observed': 0.2277259259259274}, {'date': '2008-03-01', 'trend': 0.09349305555555552, 'seasonal': 0.046161442202006006, 'resid': -0.22960481392738952, 'observed': -0.089950316169828}, {'date': '2008-04-01', 'trend': 0.08309984520123835, 'seasonal': -0.025692635499406227, 'resid': 0.054878050107287774, 'observed': 0.1122852598091199}, {'date': '2008-05-01', 'trend': 0.09042138044616072, 'seasonal': -0.009329044731091838, 'resid': -0.36535320528028664, 'observed': -0.2842608695652178}, {'date': '2008-06-01', 'trend': 0.13210592677931376, 'seasonal': -0.0300039488367143, 'resid': 0.21431468872406756, 'observed': 0.316416666666667}, {'date': '2008-07-01', 'trend': 0.14880971204721205, 'seasonal': 0.04198757175226802, 'resid': -0.16801395046614717, 'observed': 0.0227833333333329}, {'date': '2008-08-01', 'trend': 0.1261103961798854, 'seasonal': 0.04796336379262829, 'resid': -0.02644042663918049, 'observed': 0.1476333333333332}, {'date': '2008-09-01', 'trend': 0.11906648408409015, 'seasonal': 0.01064043177203127, 'resid': -0.10387358252278851, 'observed': 0.0258333333333329}, {'date': '2008-10-01', 'trend': 0.12167145209381695, 'seasonal': 0.040188196506185894, 'resid': 0.03323839061568354, 'observed': 0.1950980392156864}, {'date': '2008-11-01', 'trend': 0.1262629603434582, 'seasonal': -0.13132803383378105, 'resid': 0.15846165793054953, 'observed': 0.1533965844402267}, {'date': '2008-12-01', 'trend': 0.1160520833333332, 'seasonal': -0.050693663657927046, 'resid': 0.5231945757162989, 'observed': 0.588552995391705}, {'date': '2009-01-01', 'trend': 0.1072321969696969, 'seasonal': -0.022104325250881424, 'resid': 0.12596553487459233, 'observed': 0.2110934065934078}, {'date': '2009-02-01', 'trend': 0.12594294576196746, 'seasonal': 0.08221064578468242, 'resid': -0.20699354191885957, 'observed': 0.0011600496277903}, {'date': '2009-03-01', 'trend': 0.13928118530020703, 'seasonal': 0.046161442202006006, 'resid': -0.21788095767298993, 'observed': -0.0324383301707769}, {'date': '2009-04-01', 'trend': 0.13767138626283845, 'seasonal': -0.025692635499406227, 'resid': 0.005313755280079668, 'observed': 0.1172925060435119}, {'date': '2009-05-01', 'trend': 0.1254146494349873, 'seasonal': -0.009329044731091838, 'resid': -0.2951575225121146, 'observed': -0.1790719178082191}, {'date': '2009-06-01', 'trend': 0.09078998616242023, 'seasonal': -0.0300039488367143, 'resid': -0.09461937065903793, 'observed': -0.033833333333332}, {'date': '2009-07-01', 'trend': 0.055669395456693874, 'seasonal': 0.04198757175226802, 'resid': 0.06369909339709831, 'observed': 0.1613560606060602}, {'date': '2009-08-01', 'trend': 0.04727055850255738, 'seasonal': 0.04796336379262829, 'resid': 0.36288465477991383, 'observed': 0.4581185770750995}, {'date': '2009-09-01', 'trend': 0.04948054502565673, 'seasonal': 0.01064043177203127, 'resid': -0.024655138288372395, 'observed': 0.0354658385093156}, {'date': '2009-10-01', 'trend': 0.04255634892211584, 'seasonal': 0.040188196506185894, 'resid': 0.06408581171455706, 'observed': 0.1468303571428588}, {'date': '2009-11-01', 'trend': 0.049111586757990926, 'seasonal': -0.13132803383378105, 'resid': -0.01028097027958308, 'observed': -0.0924974173553732}, {'date': '2009-12-01', 'trend': 0.07381842320261445, 'seasonal': -0.050693663657927046, 'resid': -0.019669680898992403, 'observed': 0.003455078645695}, {'date': '2010-01-01', 'trend': 0.08011500583218881, 'seasonal': -0.022104325250881424, 'resid': -0.10471353417932248, 'observed': -0.0467028535980151}, {'date': '2010-02-01', 'trend': 0.05376202226800264, 'seasonal': 0.08221064578468242, 'resid': -0.07858844513274756, 'observed': 0.0573842229199375}, {'date': '2010-03-01', 'trend': 0.03436319309464945, 'seasonal': 0.046161442202006006, 'resid': -0.11614746220519535, 'observed': -0.0356228269085399}, {'date': '2010-04-01', 'trend': 0.017476626779975968, 'seasonal': -0.025692635499406227, 'resid': -0.037487694984276135, 'observed': -0.0457037037037064}, {'date': '2010-05-01', 'trend': -0.01581361177595589, 'seasonal': -0.009329044731091838, 'resid': 0.16639265650704893, 'observed': 0.1412500000000012}, {'date': '2010-06-01', 'trend': -0.03246178649885558, 'seasonal': -0.0300039488367143, 'resid': 0.30127455886498217, 'observed': 0.2388088235294123}, {'date': '2010-07-01', 'trend': -0.020026136683425844, 'seasonal': 0.04198757175226802, 'resid': 0.017870451784258425, 'observed': 0.0398318868531006}, {'date': '2010-08-01', 'trend': 0.004130948162431072, 'seasonal': 0.04796336379262829, 'resid': -0.10492316666746845, 'observed': -0.0528288547124091}, {'date': '2010-09-01', 'trend': 0.012749065295937015, 'seasonal': 0.01064043177203127, 'resid': 0.05745187306837942, 'observed': 0.0808413701363477}, {'date': '2010-10-01', 'trend': 0.012119709451035108, 'seasonal': 0.040188196506185894, 'resid': -0.3561306719935578, 'observed': -0.3038227660363368}, {'date': '2010-11-01', 'trend': 0.05587205949157374, 'seasonal': -0.13132803383378105, 'resid': -0.3653540451763349, 'observed': -0.4408100195185422}, {'date': '2010-12-01', 'trend': 0.09050694421469359, 'seasonal': -0.050693663657927046, 'resid': -0.08760179309749513, 'observed': -0.0477885125407286}, {'date': '2011-01-01', 'trend': 0.07899627140878604, 'seasonal': -0.022104325250881424, 'resid': 0.2461043870008176, 'observed': 0.3029963331587222}, {'date': '2011-02-01', 'trend': 0.07878745905895457, 'seasonal': 0.08221064578468242, 'resid': 0.12645696762012915, 'observed': 0.2874550724637661}, {'date': '2011-03-01', 'trend': 0.08576555403344432, 'seasonal': 0.046161442202006006, 'resid': -0.19078586148367616, 'observed': -0.0588588652482258}, {'date': '2011-04-01', 'trend': 0.12693172699165314, 'seasonal': -0.025692635499406227, 'resid': -0.13881129713391321, 'observed': -0.0375722056416663}, {'date': '2011-05-01', 'trend': 0.18097642566228855, 'seasonal': -0.009329044731091838, 'resid': 1.0115275219796918, 'observed': 1.1831749029108884}, {'date': '2011-06-01', 'trend': 0.19989917890120262, 'seasonal': -0.0300039488367143, 'resid': -0.14177407609108672, 'observed': 0.0281211539734016}, {'date': '2011-07-01', 'trend': 0.1832074048859894, 'seasonal': 0.04198757175226802, 'resid': -0.25093156757092755, 'observed': -0.0257365909326701}, {'date': '2011-08-01', 'trend': 0.1668005792428685, 'seasonal': 0.04796336379262829, 'resid': -0.20703581635809032, 'observed': 0.0077281266774065}, {'date': '2011-09-01', 'trend': 0.20585194847521862, 'seasonal': 0.01064043177203127, 'resid': -0.02873371211296368, 'observed': 0.1877586681342862}, {'date': '2011-10-01', 'trend': 0.24093391480414197, 'seasonal': 0.040188196506185894, 'resid': 0.2961259756524087, 'observed': 0.5772480869627366}, {'date': '2011-11-01', 'trend': 0.19238724632812745, 'seasonal': -0.13132803383378105, 'resid': -0.0858673169167124, 'observed': -0.024808104422366}, {'date': '2011-12-01', 'trend': 0.14227929323333952, 'seasonal': -0.050693663657927046, 'resid': -0.10122997947837928, 'observed': -0.0096443499029668}, {'date': '2012-01-01', 'trend': 0.15618150765586591, 'seasonal': -0.022104325250881424, 'resid': -0.2698275882491408, 'observed': -0.1357504058441563}, {'date': '2012-02-01', 'trend': 0.17674864418415093, 'seasonal': 0.08221064578468242, 'resid': 0.07347870606290956, 'observed': 0.3324379960317429}, {'date': '2012-03-01', 'trend': 0.17171458824491456, 'seasonal': 0.046161442202006006, 'resid': 0.6155150423132797, 'observed': 0.8333910727602003}, {'date': '2012-04-01', 'trend': 0.1407052947991684, 'seasonal': -0.025692635499406227, 'resid': -0.20286761105569498, 'observed': -0.0878549517559328}, {'date': '2012-05-01', 'trend': 0.11797072752086618, 'seasonal': -0.009329044731091838, 'resid': -0.040304077188967946, 'observed': 0.0683376056008064}, {'date': '2012-06-01', 'trend': 0.11613740913534441, 'seasonal': -0.0300039488367143, 'resid': -0.1457658832900574, 'observed': -0.0596324229914273}, {'date': '2012-07-01', 'trend': 0.11817472858706453, 'seasonal': 0.04198757175226802, 'resid': 0.23550783183346014, 'observed': 0.3956701321727927}, {'date': '2012-08-01', 'trend': 0.10930929744230405, 'seasonal': 0.04796336379262829, 'resid': -0.07733998098414745, 'observed': 0.0799326802507849}, {'date': '2012-09-01', 'trend': 0.06433338145952333, 'seasonal': 0.01064043177203127, 'resid': -0.0802370412123206, 'observed': -0.005263227980766}, {'date': '2012-10-01', 'trend': 0.034702426696149306, 'seasonal': 0.040188196506185894, 'resid': -0.0488436828224537, 'observed': 0.0260469403798815}, {'date': '2012-11-01', 'trend': 0.03676718804759696, 'seasonal': -0.13132803383378105, 'resid': 0.07532427326741989, 'observed': -0.0192365725187642}, {'date': '2012-12-01', 'trend': 0.03853726412917851, 'seasonal': -0.050693663657927046, 'resid': -0.047059123530342556, 'observed': -0.0592155230590911}, {'date': '2013-01-01', 'trend': 0.025288611222311335, 'seasonal': -0.022104325250881424, 'resid': -0.040467851818179315, 'observed': -0.0372835658467494}, {'date': '2013-02-01', 'trend': 0.007680658465607946, 'seasonal': 0.08221064578468242, 'resid': -0.06869049569020547, 'observed': 0.0212008085600849}, {'date': '2013-03-01', 'trend': 0.008332600125998891, 'seasonal': 0.046161442202006006, 'resid': 0.010712234317116001, 'observed': 0.0652062766451209}, {'date': '2013-04-01', 'trend': 0.014738908100739245, 'seasonal': -0.025692635499406227, 'resid': -0.01985934256316332, 'observed': -0.0308130699618303}, {'date': '2013-05-01', 'trend': 0.019331139025670276, 'seasonal': -0.009329044731091838, 'resid': 0.050847901946869264, 'observed': 0.0608499962414477}, {'date': '2013-06-01', 'trend': 0.001970804296641291, 'seasonal': -0.0300039488367143, 'resid': 0.01837015686596121, 'observed': -0.0096629876741118}, {'date': '2013-07-01', 'trend': -0.02493018718334297, 'seasonal': 0.04198757175226802, 'resid': 0.01067564252174015, 'observed': 0.0277330270906652}, {'date': '2013-08-01', 'trend': -0.04010684759284679, 'seasonal': 0.04796336379262829, 'resid': 0.017422402972249493, 'observed': 0.025278919172031}, {'date': '2013-09-01', 'trend': -0.053070702835793224, 'seasonal': 0.01064043177203127, 'resid': 0.10746740401113256, 'observed': 0.0650371329473706}, {'date': '2013-10-01', 'trend': -0.0561201651993278, 'seasonal': 0.040188196506185894, 'resid': 0.12542993953865528, 'observed': 0.1094979708455134}, {'date': '2013-11-01', 'trend': -0.08952130837439813, 'seasonal': -0.13132803383378105, 'resid': 0.2283752814221278, 'observed': 0.0075259392139486}, {'date': '2013-12-01', 'trend': -0.14205340870375593, 'seasonal': -0.050693663657927046, 'resid': -0.30987899592681656, 'observed': -0.5026260682884995}, {'date': '2014-01-01', 'trend': -0.1583836455184818, 'seasonal': -0.022104325250881424, 'resid': -0.05900884536759997, 'observed': -0.2394968161369632}, {'date': '2014-02-01', 'trend': -0.15824506507128125, 'seasonal': 0.08221064578468242, 'resid': -0.06479137169119416, 'observed': -0.140825790977793}, {'date': '2014-03-01', 'trend': -0.16072772394039378, 'seasonal': 0.046161442202006006, 'resid': 0.03066663209067208, 'observed': -0.0838996496477157}, {'date': '2014-04-01', 'trend': -0.1713331784156777, 'seasonal': -0.025692635499406227, 'resid': 0.2421315735212604, 'observed': 0.0451057596061765}, {'date': '2014-05-01', 'trend': -0.18856976385452964, 'seasonal': -0.009329044731091838, 'resid': -0.6187974609426257, 'observed': -0.8166962695282471}, {'date': '2014-06-01', 'trend': -0.1798000829710844, 'seasonal': -0.0300039488367143, 'resid': -0.1830830980012057, 'observed': -0.3928871298090044}, {'date': '2014-07-01', 'trend': -0.15000924889187242, 'seasonal': 0.04198757175226802, 'resid': 0.1270531628117415, 'observed': 0.0190314856721371}, {'date': '2014-08-01', 'trend': -0.12548662217305767, 'seasonal': 0.04796336379262829, 'resid': 0.11482964970380218, 'observed': 0.0373063913233728}, {'date': '2014-09-01', 'trend': -0.10523706450073718, 'seasonal': 0.01064043177203127, 'resid': 0.08802248066603362, 'observed': -0.0065741520626723}, {'date': '2014-10-01', 'trend': -0.09922083949678266, 'seasonal': 0.040188196506185894, 'resid': -0.014389008560660332, 'observed': -0.0734216515512571}, {'date': '2014-11-01', 'trend': -0.06476193929068524, 'seasonal': -0.13132803383378105, 'resid': -0.027142515797261096, 'observed': -0.2232324889217274}, {'date': '2014-12-01', 'trend': -0.014173055074311044, 'seasonal': -0.050693663657927046, 'resid': 0.003471419782099597, 'observed': -0.0613952989501385}, {'date': '2015-01-01', 'trend': -0.0005362311660852649, 'seasonal': -0.022104325250881424, 'resid': 0.05689298884272978, 'observed': 0.0342524324257631}, {'date': '2015-02-01', 'trend': -0.0042220906000517075, 'seasonal': 0.08221064578468242, 'resid': 0.09597944652640449, 'observed': 0.1739680017110352}, {'date': '2015-03-01', 'trend': -0.004899362518352068, 'seasonal': 0.046161442202006006, 'resid': 0.046033862115494165, 'observed': 0.0872959417991481}, {'date': '2015-04-01', 'trend': -0.001219038622103299, 'seasonal': -0.025692635499406227, 'resid': 0.04521124237573082, 'observed': 0.0182995682542213}, {'date': '2015-05-01', 'trend': 0.007099710463441603, 'seasonal': -0.009329044731091838, 'resid': 0.039352861037695835, 'observed': 0.0371235267700456}, {'date': '2015-06-01', 'trend': 0.017638544788709968, 'seasonal': -0.0300039488367143, 'resid': -0.020208300866312073, 'observed': -0.0325737049143164}, {'date': '2015-07-01', 'trend': 0.03136625158651259, 'seasonal': 0.04198757175226802, 'resid': -0.08735198876391281, 'observed': -0.0139981654251322}, {'date': '2015-08-01', 'trend': 0.06480069347258703, 'seasonal': 0.04796336379262829, 'resid': -0.13088864125976782, 'observed': -0.0181245839945525}, {'date': '2015-09-01', 'trend': 0.08889547350580251, 'seasonal': 0.01064043177203127, 'resid': -0.06693360806178938, 'observed': 0.0326022972160444}, {'date': '2015-10-01', 'trend': 0.08654955477113249, 'seasonal': 0.040188196506185894, 'resid': -0.1510080785973219, 'observed': -0.0242703273200035}, {'date': '2015-11-01', 'trend': 0.08360188763677234, 'seasonal': -0.13132803383378105, 'resid': -0.025007688902894593, 'observed': -0.0727338350999033}, {'date': '2015-12-01', 'trend': 0.1044494817114668, 'seasonal': -0.050693663657927046, 'resid': -0.01271774701906156, 'observed': 0.0410380710344782}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.022104325250881424, 'resid': nan, 'observed': 0.2612840255884094}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.08221064578468242, 'resid': nan, 'observed': 0.7493630138141754}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.046161442202006006, 'resid': nan, 'observed': 0.0901756504931796}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.025692635499406227, 'resid': nan, 'observed': -0.0408821900718905}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.009329044731091838, 'resid': nan, 'observed': 0.0255612738715136}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.0300039488367143, 'resid': nan, 'observed': 0.4793308057768826}], 'diff1_FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.03606088789682545, 'resid': nan, 'observed': -0.2400000000000002}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.00277963789682541, 'resid': nan, 'observed': -0.0799999999999991}, {'date': '2007-10-01', 'trend': nan, 'seasonal': -0.08331411210317463, 'resid': nan, 'observed': -0.1800000000000006}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05039744543650794, 'resid': nan, 'observed': -0.2699999999999996}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.009342137896825394, 'resid': nan, 'observed': -0.25}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.01609064980158735, 'resid': nan, 'observed': -0.3000000000000002}, {'date': '2008-02-01', 'trend': -0.2612499999999999, 'seasonal': -0.05034536210317461, 'resid': -0.6484046378968253, 'observed': -0.96}, {'date': '2008-03-01', 'trend': -0.2562499999999999, 'seasonal': 0.007935887896825382, 'resid': -0.12168588789682556, 'observed': -0.3700000000000001}, {'date': '2008-04-01', 'trend': -0.2883333333333333, 'seasonal': 0.012935887896825347, 'resid': -0.054602554563492084, 'observed': -0.33}, {'date': '2008-05-01', 'trend': -0.32875, 'seasonal': 0.013196304563492082, 'resid': 0.015553695436508082, 'observed': -0.2999999999999998}, {'date': '2008-06-01', 'trend': -0.3408333333333333, 'seasonal': 0.05111297123015873, 'resid': 0.3097203621031746, 'observed': 0.02}, {'date': '2008-07-01', 'trend': -0.32791666666666663, 'seasonal': 0.03460255456349204, 'resid': 0.30331411210317427, 'observed': 0.0099999999999997}, {'date': '2008-08-01', 'trend': -0.27291666666666664, 'seasonal': 0.03606088789682545, 'resid': 0.22685577876984153, 'observed': -0.0099999999999997}, {'date': '2008-09-01', 'trend': -0.21625, 'seasonal': 0.00277963789682541, 'resid': 0.023470362103174696, 'observed': -0.1899999999999999}, {'date': '2008-10-01', 'trend': -0.18999999999999997, 'seasonal': -0.08331411210317463, 'resid': -0.5666858878968255, 'observed': -0.8400000000000001}, {'date': '2008-11-01', 'trend': -0.16374999999999998, 'seasonal': -0.05039744543650794, 'resid': -0.3658525545634921, 'observed': -0.58}, {'date': '2008-12-01', 'trend': -0.14958333333333332, 'seasonal': 0.009342137896825394, 'resid': -0.08975880456349208, 'observed': -0.23}, {'date': '2009-01-01', 'trend': -0.15166666666666664, 'seasonal': 0.01609064980158735, 'resid': 0.1255760168650793, 'observed': -0.01}, {'date': '2009-02-01', 'trend': -0.15374999999999994, 'seasonal': -0.05034536210317461, 'resid': 0.27409536210317453, 'observed': 0.07}, {'date': '2009-03-01', 'trend': -0.14583333333333331, 'seasonal': 0.007935887896825382, 'resid': 0.09789744543650793, 'observed': -0.04}, {'date': '2009-04-01', 'trend': -0.1045833333333333, 'seasonal': 0.012935887896825347, 'resid': 0.061647445436507964, 'observed': -0.03}, {'date': '2009-05-01', 'trend': -0.04666666666666665, 'seasonal': 0.013196304563492082, 'resid': 0.06347036210317457, 'observed': 0.03}, {'date': '2009-06-01', 'trend': -0.012916666666666656, 'seasonal': 0.05111297123015873, 'resid': -0.008196304563492074, 'observed': 0.03}, {'date': '2009-07-01', 'trend': -0.00333333333333332, 'seasonal': 0.03460255456349204, 'resid': -0.08126922123015862, 'observed': -0.0499999999999999}, {'date': '2009-08-01', 'trend': -0.0054166666666666495, 'seasonal': 0.03606088789682545, 'resid': -0.030644221230158798, 'observed': 0.0}, {'date': '2009-09-01', 'trend': -0.004583333333333315, 'seasonal': 0.00277963789682541, 'resid': -0.008196304563492095, 'observed': -0.01}, {'date': '2009-10-01', 'trend': 0.0012500000000000163, 'seasonal': -0.08331411210317463, 'resid': 0.05206411210317462, 'observed': -0.03}, {'date': '2009-11-01', 'trend': 0.002916666666666683, 'seasonal': -0.05039744543650794, 'resid': 0.047480778769841255, 'observed': 0.0}, {'date': '2009-12-01', 'trend': -0.00041666666666665005, 'seasonal': 0.009342137896825394, 'resid': -0.008925471230158744, 'observed': 0.0}, {'date': '2010-01-01', 'trend': -0.00041666666666665374, 'seasonal': 0.01609064980158735, 'resid': -0.025673983134920596, 'observed': -0.0099999999999999}, {'date': '2010-02-01', 'trend': 0.0020833333333333415, 'seasonal': -0.05034536210317461, 'resid': 0.06826202876984128, 'observed': 0.02}, {'date': '2010-03-01', 'trend': 0.002916666666666675, 'seasonal': 0.007935887896825382, 'resid': 0.01914744543650794, 'observed': 0.03}, {'date': '2010-04-01', 'trend': 0.004583333333333341, 'seasonal': 0.012935887896825347, 'resid': 0.022480778769841312, 'observed': 0.04}, {'date': '2010-05-01', 'trend': 0.0058333333333333405, 'seasonal': 0.013196304563492082, 'resid': -0.01902963789682542, 'observed': 0.0}, {'date': '2010-06-01', 'trend': 0.005416666666666674, 'seasonal': 0.05111297123015873, 'resid': -0.0765296378968254, 'observed': -0.02}, {'date': '2010-07-01', 'trend': 0.005000000000000007, 'seasonal': 0.03460255456349204, 'resid': -0.03960255456349204, 'observed': 0.0}, {'date': '2010-08-01', 'trend': 0.0037500000000000077, 'seasonal': 0.03606088789682545, 'resid': -0.029810887896825455, 'observed': 0.01}, {'date': '2010-09-01', 'trend': 0.00041666666666667894, 'seasonal': 0.00277963789682541, 'resid': -0.003196304563492089, 'observed': 0.0}, {'date': '2010-10-01', 'trend': -0.004999999999999984, 'seasonal': -0.08331411210317463, 'resid': 0.08831411210317461, 'observed': 0.0}, {'date': '2010-11-01', 'trend': -0.008749999999999982, 'seasonal': -0.05039744543650794, 'resid': 0.05914744543650792, 'observed': 0.0}, {'date': '2010-12-01', 'trend': -0.008333333333333316, 'seasonal': 0.009342137896825394, 'resid': -0.011008804563492078, 'observed': -0.01}, {'date': '2011-01-01', 'trend': -0.008333333333333312, 'seasonal': 0.01609064980158735, 'resid': -0.017757316468253938, 'observed': -0.0099999999999999}, {'date': '2011-02-01', 'trend': -0.008333333333333309, 'seasonal': -0.05034536210317461, 'resid': 0.04867869543650792, 'observed': -0.01}, {'date': '2011-03-01', 'trend': -0.008333333333333307, 'seasonal': 0.007935887896825382, 'resid': -0.019602554563491977, 'observed': -0.0199999999999999}, {'date': '2011-04-01', 'trend': -0.009583333333333305, 'seasonal': 0.012935887896825347, 'resid': -0.043352554563492046, 'observed': -0.04}, {'date': '2011-05-01', 'trend': -0.009583333333333305, 'seasonal': 0.013196304563492082, 'resid': -0.013612971230158777, 'observed': -0.01}, {'date': '2011-06-01', 'trend': -0.009166666666666639, 'seasonal': 0.05111297123015873, 'resid': -0.04194630456349209, 'observed': 0.0}, {'date': '2011-07-01', 'trend': -0.008333333333333309, 'seasonal': 0.03460255456349204, 'resid': -0.04626922123015863, 'observed': -0.0199999999999999}, {'date': '2011-08-01', 'trend': -0.006249999999999984, 'seasonal': 0.03606088789682545, 'resid': 0.00018911210317453658, 'observed': 0.03}, {'date': '2011-09-01', 'trend': -0.002916666666666654, 'seasonal': 0.00277963789682541, 'resid': -0.019862971230158757, 'observed': -0.02}, {'date': '2011-10-01', 'trend': 0.001250000000000007, 'seasonal': -0.08331411210317463, 'resid': 0.07206411210317473, 'observed': -0.0099999999999999}, {'date': '2011-11-01', 'trend': 0.004583333333333336, 'seasonal': -0.05039744543650794, 'resid': 0.0558141121031745, 'observed': 0.0099999999999999}, {'date': '2011-12-01', 'trend': 0.005833333333333333, 'seasonal': 0.009342137896825394, 'resid': -0.025175471230158626, 'observed': -0.0099999999999999}, {'date': '2012-01-01', 'trend': 0.006666666666666661, 'seasonal': 0.01609064980158735, 'resid': -0.012757316468254112, 'observed': 0.0099999999999999}, {'date': '2012-02-01', 'trend': 0.004999999999999991, 'seasonal': -0.05034536210317461, 'resid': 0.06534536210317463, 'observed': 0.02}, {'date': '2012-03-01', 'trend': 0.003749999999999992, 'seasonal': 0.007935887896825382, 'resid': 0.018314112103174622, 'observed': 0.03}, {'date': '2012-04-01', 'trend': 0.006249999999999983, 'seasonal': 0.012935887896825347, 'resid': -0.00918588789682533, 'observed': 0.01}, {'date': '2012-05-01', 'trend': 0.007083333333333312, 'seasonal': 0.013196304563492082, 'resid': -0.00027963789682549506, 'observed': 0.0199999999999999}, {'date': '2012-06-01', 'trend': 0.007083333333333312, 'seasonal': 0.05111297123015873, 'resid': -0.05819630456349204, 'observed': 0.0}, {'date': '2012-07-01', 'trend': 0.006249999999999983, 'seasonal': 0.03460255456349204, 'resid': -0.04085255456349202, 'observed': 0.0}, {'date': '2012-08-01', 'trend': 0.004583333333333321, 'seasonal': 0.03606088789682545, 'resid': -0.07064422123015876, 'observed': -0.03}, {'date': '2012-09-01', 'trend': 0.0024999999999999875, 'seasonal': 0.00277963789682541, 'resid': 0.004720362103174603, 'observed': 0.01}, {'date': '2012-10-01', 'trend': 0.0008333333333333214, 'seasonal': -0.08331411210317463, 'resid': 0.10248077876984121, 'observed': 0.0199999999999999}, {'date': '2012-11-01', 'trend': -0.0016666666666666746, 'seasonal': -0.05039744543650794, 'resid': 0.05206411210317462, 'observed': 0.0}, {'date': '2012-12-01', 'trend': -0.005, 'seasonal': 0.009342137896825394, 'resid': -0.004342137896825394, 'observed': 0.0}, {'date': '2013-01-01', 'trend': -0.005833333333333333, 'seasonal': 0.01609064980158735, 'resid': -0.030257316468253918, 'observed': -0.0199999999999999}, {'date': '2013-02-01', 'trend': -0.004999999999999996, 'seasonal': -0.05034536210317461, 'resid': 0.06534536210317451, 'observed': 0.0099999999999999}, {'date': '2013-03-01', 'trend': -0.004583333333333325, 'seasonal': 0.007935887896825382, 'resid': -0.013352554563491957, 'observed': -0.0099999999999999}, {'date': '2013-04-01', 'trend': -0.005416666666666658, 'seasonal': 0.012935887896825347, 'resid': 0.002480778769841211, 'observed': 0.0099999999999999}, {'date': '2013-05-01', 'trend': -0.0062499999999999865, 'seasonal': 0.013196304563492082, 'resid': -0.046946304563492, 'observed': -0.0399999999999999}, {'date': '2013-06-01', 'trend': -0.0062499999999999865, 'seasonal': 0.05111297123015873, 'resid': -0.06486297123015874, 'observed': -0.02}, {'date': '2013-07-01', 'trend': -0.005833333333333324, 'seasonal': 0.03460255456349204, 'resid': -0.028769221230158716, 'observed': 0.0}, {'date': '2013-08-01', 'trend': -0.0062499999999999865, 'seasonal': 0.03606088789682545, 'resid': -0.03981088789682536, 'observed': -0.0099999999999999}, {'date': '2013-09-01', 'trend': -0.005833333333333324, 'seasonal': 0.00277963789682541, 'resid': 0.003053695436507914, 'observed': 0.0}, {'date': '2013-10-01', 'trend': -0.004999999999999999, 'seasonal': -0.08331411210317463, 'resid': 0.09831411210317453, 'observed': 0.0099999999999999}, {'date': '2013-11-01', 'trend': -0.0033333333333333366, 'seasonal': -0.05039744543650794, 'resid': 0.043730778769841376, 'observed': -0.0099999999999999}, {'date': '2013-12-01', 'trend': -0.0004166666666666752, 'seasonal': 0.009342137896825394, 'resid': 0.0010745287698411803, 'observed': 0.0099999999999999}, {'date': '2014-01-01', 'trend': 0.0004166666666666582, 'seasonal': 0.01609064980158735, 'resid': -0.036507316468253906, 'observed': -0.0199999999999999}, {'date': '2014-02-01', 'trend': 0.00041666666666665374, 'seasonal': -0.05034536210317461, 'resid': 0.04992869543650796, 'observed': 0.0}, {'date': '2014-03-01', 'trend': 0.0008333333333333161, 'seasonal': 0.007935887896825382, 'resid': 0.0012307787698412013, 'observed': 0.0099999999999999}, {'date': '2014-04-01', 'trend': 0.00041666666666665374, 'seasonal': 0.012935887896825347, 'resid': -0.003352554563492101, 'observed': 0.0099999999999999}, {'date': '2014-05-01', 'trend': 0.00041666666666665374, 'seasonal': 0.013196304563492082, 'resid': -0.013612971230158736, 'observed': 0.0}, {'date': '2014-06-01', 'trend': 0.001666666666666654, 'seasonal': 0.05111297123015873, 'resid': -0.04277963789682539, 'observed': 0.01}, {'date': '2014-07-01', 'trend': 0.002916666666666658, 'seasonal': 0.03460255456349204, 'resid': -0.0475192212301587, 'observed': -0.01}, {'date': '2014-08-01', 'trend': 0.0033333333333333244, 'seasonal': 0.03606088789682545, 'resid': -0.039394221230158774, 'observed': 0.0}, {'date': '2014-09-01', 'trend': 0.002916666666666662, 'seasonal': 0.00277963789682541, 'resid': -0.005696304563492072, 'observed': 0.0}, {'date': '2014-10-01', 'trend': 0.0024999999999999996, 'seasonal': -0.08331411210317463, 'resid': 0.08081411210317463, 'observed': 0.0}, {'date': '2014-11-01', 'trend': 0.0024999999999999996, 'seasonal': -0.05039744543650794, 'resid': 0.04789744543650794, 'observed': 0.0}, {'date': '2014-12-01', 'trend': 0.0024999999999999996, 'seasonal': 0.009342137896825394, 'resid': 0.018157862103174605, 'observed': 0.03}, {'date': '2015-01-01', 'trend': 0.0029166666666666664, 'seasonal': 0.01609064980158735, 'resid': -0.029007316468253917, 'observed': -0.0099999999999999}, {'date': '2015-02-01', 'trend': 0.00375, 'seasonal': -0.05034536210317461, 'resid': 0.04659536210317461, 'observed': 0.0}, {'date': '2015-03-01', 'trend': 0.004166666666666667, 'seasonal': 0.007935887896825382, 'resid': -0.01210255456349205, 'observed': 0.0}, {'date': '2015-04-01', 'trend': 0.003333333333333333, 'seasonal': 0.012935887896825347, 'resid': -0.00626922123015878, 'observed': 0.0099999999999999}, {'date': '2015-05-01', 'trend': 0.0025, 'seasonal': 0.013196304563492082, 'resid': -0.01569630456349208, 'observed': 0.0}, {'date': '2015-06-01', 'trend': 0.0062499999999999995, 'seasonal': 0.05111297123015873, 'resid': -0.04736297123015873, 'observed': 0.01}, {'date': '2015-07-01', 'trend': 0.014583333333333328, 'seasonal': 0.03460255456349204, 'resid': -0.04918588789682537, 'observed': 0.0}, {'date': '2015-08-01', 'trend': 0.020833333333333322, 'seasonal': 0.03606088789682545, 'resid': -0.04689422123015877, 'observed': 0.01}, {'date': '2015-09-01', 'trend': 0.021666666666666647, 'seasonal': 0.00277963789682541, 'resid': -0.024446304563492057, 'observed': 0.0}, {'date': '2015-10-01', 'trend': 0.02083333333333332, 'seasonal': -0.08331411210317463, 'resid': 0.04248077876984131, 'observed': -0.02}, {'date': '2015-11-01', 'trend': 0.020833333333333322, 'seasonal': -0.05039744543650794, 'resid': 0.029564112103174618, 'observed': 0.0}, {'date': '2015-12-01', 'trend': 0.020833333333333322, 'seasonal': 0.009342137896825394, 'resid': 0.08982452876984129, 'observed': 0.12}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.01609064980158735, 'resid': nan, 'observed': 0.1}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.05034536210317461, 'resid': nan, 'observed': 0.0399999999999999}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.007935887896825382, 'resid': nan, 'observed': -0.02}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.012935887896825347, 'resid': nan, 'observed': 0.01}, {'date': '2016-05-01', 'trend': nan, 'seasonal': 0.013196304563492082, 'resid': nan, 'observed': 0.0}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.05111297123015873, 'resid': nan, 'observed': 0.01}], 'diff2_FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -8.8045634920585e-05, 'resid': nan, 'observed': -0.25}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.034827628968254, 'resid': nan, 'observed': 0.160000000000001}, {'date': '2007-10-01', 'trend': nan, 'seasonal': -0.087640128968254, 'resid': nan, 'observed': -0.1000000000000014}, {'date': '2007-11-01', 'trend': nan, 'seasonal': 0.03137028769841272, 'resid': nan, 'observed': -0.0899999999999989}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.05819320436507936, 'resid': nan, 'observed': 0.0199999999999995}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.018572668650793697, 'resid': nan, 'observed': -0.0500000000000002}, {'date': '2008-02-01', 'trend': 0.009583333333333338, 'seasonal': -0.06279637896825398, 'resid': -0.6067869543650791, 'observed': -0.6599999999999997}, {'date': '2008-03-01', 'trend': 0.004999999999999993, 'seasonal': 0.056734871031745994, 'resid': 0.5282651289682538, 'observed': 0.5899999999999999}, {'date': '2008-04-01', 'trend': -0.03208333333333333, 'seasonal': 0.0034536210317460195, 'resid': 0.06862971230158732, 'observed': 0.04}, {'date': '2008-05-01', 'trend': -0.040416666666666656, 'seasonal': -0.001285962301587255, 'resid': 0.0717026289682541, 'observed': 0.0300000000000002}, {'date': '2008-06-01', 'trend': -0.012083333333333354, 'seasonal': 0.03637028769841267, 'resid': 0.29571304563492046, 'observed': 0.3199999999999998}, {'date': '2008-07-01', 'trend': 0.012916666666666684, 'seasonal': -0.018056795634920637, 'resid': -0.004859871031746246, 'observed': -0.0100000000000002}, {'date': '2008-08-01', 'trend': 0.055, 'seasonal': -8.8045634920585e-05, 'resid': -0.07491195436507891, 'observed': -0.0199999999999995}, {'date': '2008-09-01', 'trend': 0.056666666666666664, 'seasonal': -0.034827628968254, 'resid': -0.20183903769841277, 'observed': -0.1800000000000001}, {'date': '2008-10-01', 'trend': 0.026250000000000006, 'seasonal': -0.087640128968254, 'resid': -0.5886098710317461, 'observed': -0.6500000000000001}, {'date': '2008-11-01', 'trend': 0.02625, 'seasonal': 0.03137028769841272, 'resid': 0.2023797123015874, 'observed': 0.2600000000000001}, {'date': '2008-12-01', 'trend': 0.014166666666666671, 'seasonal': 0.05819320436507936, 'resid': 0.27764012896825396, 'observed': 0.35}, {'date': '2009-01-01', 'trend': -0.0020833333333333073, 'seasonal': 0.018572668650793697, 'resid': 0.2035106646825396, 'observed': 0.22}, {'date': '2009-02-01', 'trend': -0.0020833333333333194, 'seasonal': -0.06279637896825398, 'resid': 0.1448797123015873, 'observed': 0.08}, {'date': '2009-03-01', 'trend': 0.007916666666666655, 'seasonal': 0.056734871031745994, 'resid': -0.17465153769841266, 'observed': -0.11}, {'date': '2009-04-01', 'trend': 0.04125, 'seasonal': 0.0034536210317460195, 'resid': -0.03470362103174602, 'observed': 0.01}, {'date': '2009-05-01', 'trend': 0.05791666666666667, 'seasonal': -0.001285962301587255, 'resid': 0.003369295634920581, 'observed': 0.06}, {'date': '2009-06-01', 'trend': 0.03375, 'seasonal': 0.03637028769841267, 'resid': -0.07012028769841266, 'observed': 0.0}, {'date': '2009-07-01', 'trend': 0.009583333333333345, 'seasonal': -0.018056795634920637, 'resid': -0.0715265376984126, 'observed': -0.0799999999999999}, {'date': '2009-08-01', 'trend': -0.0020833333333333186, 'seasonal': -8.8045634920585e-05, 'resid': 0.0521713789682538, 'observed': 0.0499999999999999}, {'date': '2009-09-01', 'trend': 0.0008333333333333439, 'seasonal': -0.034827628968254, 'resid': 0.023994295634920653, 'observed': -0.01}, {'date': '2009-10-01', 'trend': 0.00583333333333334, 'seasonal': -0.087640128968254, 'resid': 0.061806795634920766, 'observed': -0.0199999999999999}, {'date': '2009-11-01', 'trend': 0.0016666666666666735, 'seasonal': 0.03137028769841272, 'resid': -0.0030369543650793952, 'observed': 0.03}, {'date': '2009-12-01', 'trend': -0.0033333333333333253, 'seasonal': 0.05819320436507936, 'resid': -0.054859871031746034, 'observed': 0.0}, {'date': '2010-01-01', 'trend': 3.2786273695961652e-18, 'seasonal': 0.018572668650793697, 'resid': -0.028572668650793602, 'observed': -0.0099999999999999}, {'date': '2010-02-01', 'trend': 0.002500000000000003, 'seasonal': -0.06279637896825398, 'resid': 0.09029637896825397, 'observed': 0.03}, {'date': '2010-03-01', 'trend': 0.000833333333333341, 'seasonal': 0.056734871031745994, 'resid': -0.047568204365079435, 'observed': 0.0099999999999999}, {'date': '2010-04-01', 'trend': 0.0016666666666666705, 'seasonal': 0.0034536210317460195, 'resid': 0.00487971230158731, 'observed': 0.01}, {'date': '2010-05-01', 'trend': 0.0012499999999999996, 'seasonal': -0.001285962301587255, 'resid': -0.039964037698412745, 'observed': -0.04}, {'date': '2010-06-01', 'trend': -0.00041666666666666664, 'seasonal': 0.03637028769841267, 'resid': -0.055953621031746004, 'observed': -0.02}, {'date': '2010-07-01', 'trend': -0.00041666666666666973, 'seasonal': -0.018056795634920637, 'resid': 0.03847346230158731, 'observed': 0.02}, {'date': '2010-08-01', 'trend': -0.0012500000000000074, 'seasonal': -8.8045634920585e-05, 'resid': 0.011338045634920593, 'observed': 0.01}, {'date': '2010-09-01', 'trend': -0.003333333333333333, 'seasonal': -0.034827628968254, 'resid': 0.02816096230158733, 'observed': -0.01}, {'date': '2010-10-01', 'trend': -0.005416666666666658, 'seasonal': -0.087640128968254, 'resid': 0.09305679563492066, 'observed': 0.0}, {'date': '2010-11-01', 'trend': -0.0037499999999999916, 'seasonal': 0.03137028769841272, 'resid': -0.027620287698412727, 'observed': 0.0}, {'date': '2010-12-01', 'trend': 0.00041666666666667445, 'seasonal': 0.05819320436507936, 'resid': -0.06860987103174604, 'observed': -0.01}, {'date': '2011-01-01', 'trend': 1.2189323624530364e-17, 'seasonal': 0.018572668650793697, 'resid': -0.018572668650793683, 'observed': 2.775557561562892e-17}, {'date': '2011-02-01', 'trend': 1.2171976389770596e-17, 'seasonal': -0.06279637896825398, 'resid': 0.06279637896825394, 'observed': -2.775557561562892e-17}, {'date': '2011-03-01', 'trend': 8.211024452956887e-18, 'seasonal': 0.056734871031745994, 'resid': -0.0667348710317459, 'observed': -0.0099999999999999}, {'date': '2011-04-01', 'trend': -0.0012499999999999924, 'seasonal': 0.0034536210317460195, 'resid': -0.02220362103174603, 'observed': -0.02}, {'date': '2011-05-01', 'trend': 3.4231876592608994e-18, 'seasonal': -0.001285962301587255, 'resid': 0.03128596230158725, 'observed': 0.03}, {'date': '2011-06-01', 'trend': 0.00041666666666667027, 'seasonal': 0.03637028769841267, 'resid': -0.02678695436507934, 'observed': 0.01}, {'date': '2011-07-01', 'trend': 0.0008333333333333352, 'seasonal': -0.018056795634920637, 'resid': -0.0027765376984125976, 'observed': -0.0199999999999999}, {'date': '2011-08-01', 'trend': 0.002083333333333332, 'seasonal': -8.8045634920585e-05, 'resid': 0.04800471230158715, 'observed': 0.0499999999999999}, {'date': '2011-09-01', 'trend': 0.0033333333333333244, 'seasonal': -0.034827628968254, 'resid': -0.018505704365079333, 'observed': -0.05}, {'date': '2011-10-01', 'trend': 0.004166666666666654, 'seasonal': -0.087640128968254, 'resid': 0.09347346230158735, 'observed': 0.01}, {'date': '2011-11-01', 'trend': 0.003333333333333321, 'seasonal': 0.03137028769841272, 'resid': -0.01470362103174614, 'observed': 0.0199999999999999}, {'date': '2011-12-01', 'trend': 0.0012499999999999872, 'seasonal': 0.05819320436507936, 'resid': -0.07944320436507925, 'observed': -0.0199999999999999}, {'date': '2012-01-01', 'trend': 0.0008333333333333207, 'seasonal': 0.018572668650793697, 'resid': 0.0005939980158728812, 'observed': 0.0199999999999999}, {'date': '2012-02-01', 'trend': -0.0016666666666666794, 'seasonal': -0.06279637896825398, 'resid': 0.07446304563492065, 'observed': 0.01}, {'date': '2012-03-01', 'trend': -0.001250000000000009, 'seasonal': 0.056734871031745994, 'resid': -0.04548487103174609, 'observed': 0.0099999999999999}, {'date': '2012-04-01', 'trend': 0.0024999999999999883, 'seasonal': 0.0034536210317460195, 'resid': -0.025953621031745908, 'observed': -0.0199999999999999}, {'date': '2012-05-01', 'trend': 0.0008333333333333248, 'seasonal': -0.001285962301587255, 'resid': 0.01045262896825383, 'observed': 0.0099999999999999}, {'date': '2012-06-01', 'trend': -4.336808689942018e-18, 'seasonal': 0.03637028769841267, 'resid': -0.05637028769841257, 'observed': -0.0199999999999999}, {'date': '2012-07-01', 'trend': -0.0008333333333333335, 'seasonal': -0.018056795634920637, 'resid': 0.01889012896825397, 'observed': 0.0}, {'date': '2012-08-01', 'trend': -0.0016666666666666629, 'seasonal': -8.8045634920585e-05, 'resid': -0.028245287698412752, 'observed': -0.03}, {'date': '2012-09-01', 'trend': -0.0020833333333333255, 'seasonal': -0.034827628968254, 'resid': 0.07691096230158732, 'observed': 0.04}, {'date': '2012-10-01', 'trend': -0.0016666666666666588, 'seasonal': -0.087640128968254, 'resid': 0.09930679563492056, 'observed': 0.0099999999999999}, {'date': '2012-11-01', 'trend': -0.002499999999999992, 'seasonal': 0.03137028769841272, 'resid': -0.04887028769841263, 'observed': -0.0199999999999999}, {'date': '2012-12-01', 'trend': -0.0033333333333333244, 'seasonal': 0.05819320436507936, 'resid': -0.054859871031746034, 'observed': 0.0}, {'date': '2013-01-01', 'trend': -0.0008333333333333337, 'seasonal': 0.018572668650793697, 'resid': -0.037739335317460265, 'observed': -0.0199999999999999}, {'date': '2013-02-01', 'trend': 0.0008333333333333378, 'seasonal': -0.06279637896825398, 'resid': 0.09196304563492054, 'observed': 0.0299999999999999}, {'date': '2013-03-01', 'trend': 0.00041666666666667114, 'seasonal': 0.056734871031745994, 'resid': -0.07715153769841257, 'observed': -0.0199999999999999}, {'date': '2013-04-01', 'trend': -0.0008333333333333318, 'seasonal': 0.0034536210317460195, 'resid': 0.017379712301587212, 'observed': 0.0199999999999999}, {'date': '2013-05-01', 'trend': -0.0008333333333333326, 'seasonal': -0.001285962301587255, 'resid': -0.04788070436507931, 'observed': -0.0499999999999999}, {'date': '2013-06-01', 'trend': -4.383067982634732e-18, 'seasonal': 0.03637028769841267, 'resid': -0.016370287698412766, 'observed': 0.0199999999999999}, {'date': '2013-07-01', 'trend': 0.00041666666666665916, 'seasonal': -0.018056795634920637, 'resid': 0.03764012896825398, 'observed': 0.02}, {'date': '2013-08-01', 'trend': -0.0004166666666666755, 'seasonal': -8.8045634920585e-05, 'resid': -0.009495287698412638, 'observed': -0.0099999999999999}, {'date': '2013-09-01', 'trend': 0.0004166666666666503, 'seasonal': -0.034827628968254, 'resid': 0.044410962301587247, 'observed': 0.0099999999999999}, {'date': '2013-10-01', 'trend': 0.0008333333333333127, 'seasonal': -0.087640128968254, 'resid': 0.09680679563492059, 'observed': 0.0099999999999999}, {'date': '2013-11-01', 'trend': 0.00166666666666665, 'seasonal': 0.03137028769841272, 'resid': -0.05303695436507927, 'observed': -0.0199999999999999}, {'date': '2013-12-01', 'trend': 0.0029166666666666542, 'seasonal': 0.05819320436507936, 'resid': -0.04110987103174611, 'observed': 0.0199999999999999}, {'date': '2014-01-01', 'trend': 0.0008333333333333255, 'seasonal': 0.018572668650793697, 'resid': -0.049406001984126924, 'observed': -0.0299999999999999}, {'date': '2014-02-01', 'trend': -1.2238474123016374e-17, 'seasonal': -0.06279637896825398, 'resid': 0.0827963789682539, 'observed': 0.0199999999999999}, {'date': '2014-03-01', 'trend': 0.0004166666666666546, 'seasonal': 0.056734871031745994, 'resid': -0.04715153769841275, 'observed': 0.0099999999999999}, {'date': '2014-04-01', 'trend': -0.0004166666666666711, 'seasonal': 0.0034536210317460195, 'resid': -0.0030369543650793484, 'observed': 0.0}, {'date': '2014-05-01', 'trend': -3.469446951953614e-18, 'seasonal': -0.001285962301587255, 'resid': -0.008714037698412641, 'observed': -0.0099999999999999}, {'date': '2014-06-01', 'trend': 0.0012499999999999963, 'seasonal': 0.03637028769841267, 'resid': -0.027620287698412665, 'observed': 0.01}, {'date': '2014-07-01', 'trend': 0.0012500000000000007, 'seasonal': -0.018056795634920637, 'resid': -0.003193204365079364, 'observed': -0.02}, {'date': '2014-08-01', 'trend': 0.00041666666666666767, 'seasonal': -8.8045634920585e-05, 'resid': 0.009671378968253918, 'observed': 0.01}, {'date': '2014-09-01', 'trend': -0.0004166666666666624, 'seasonal': -0.034827628968254, 'resid': 0.03524429563492066, 'observed': 0.0}, {'date': '2014-10-01', 'trend': -0.00041666666666666236, 'seasonal': -0.087640128968254, 'resid': 0.08805679563492066, 'observed': 0.0}, {'date': '2014-11-01', 'trend': -4.9150498486009767e-20, 'seasonal': 0.03137028769841272, 'resid': -0.03137028769841272, 'observed': 0.0}, {'date': '2014-12-01', 'trend': 7.719519468096792e-19, 'seasonal': 0.05819320436507936, 'resid': -0.028193204365079362, 'observed': 0.03}, {'date': '2015-01-01', 'trend': 0.00041666666666666686, 'seasonal': 0.018572668650793697, 'resid': -0.05898933531746026, 'observed': -0.0399999999999999}, {'date': '2015-02-01', 'trend': 0.0008333333333333334, 'seasonal': -0.06279637896825398, 'resid': 0.07196304563492054, 'observed': 0.0099999999999999}, {'date': '2015-03-01', 'trend': 0.00041666666666666686, 'seasonal': 0.056734871031745994, 'resid': -0.05715153769841266, 'observed': 0.0}, {'date': '2015-04-01', 'trend': -0.0008333333333333333, 'seasonal': 0.0034536210317460195, 'resid': 0.007379712301587214, 'observed': 0.0099999999999999}, {'date': '2015-05-01', 'trend': -0.0008333333333333328, 'seasonal': -0.001285962301587255, 'resid': -0.007880704365079311, 'observed': -0.0099999999999999}, {'date': '2015-06-01', 'trend': 0.003749999999999999, 'seasonal': 0.03637028769841267, 'resid': -0.030120287698412667, 'observed': 0.01}, {'date': '2015-07-01', 'trend': 0.008333333333333333, 'seasonal': -0.018056795634920637, 'resid': -0.00027653769841269604, 'observed': -0.01}, {'date': '2015-08-01', 'trend': 0.006250000000000003, 'seasonal': -8.8045634920585e-05, 'resid': 0.0038380456349205823, 'observed': 0.01}, {'date': '2015-09-01', 'trend': 0.0008333333333333408, 'seasonal': -0.034827628968254, 'resid': 0.023994295634920657, 'observed': -0.01}, {'date': '2015-10-01', 'trend': -0.0008333333333333208, 'seasonal': -0.087640128968254, 'resid': 0.06847346230158732, 'observed': -0.02}, {'date': '2015-11-01', 'trend': 1.2238474123016374e-17, 'seasonal': 0.03137028769841272, 'resid': -0.011370287698412734, 'observed': 0.02}, {'date': '2015-12-01', 'trend': 8.578207588705312e-18, 'seasonal': 0.05819320436507936, 'resid': 0.06180679563492062, 'observed': 0.12}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.018572668650793697, 'resid': nan, 'observed': -0.0199999999999999}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.06279637896825398, 'resid': nan, 'observed': -0.06}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.056734871031745994, 'resid': nan, 'observed': -0.06}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.0034536210317460195, 'resid': nan, 'observed': 0.03}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.001285962301587255, 'resid': nan, 'observed': -0.01}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.03637028769841267, 'resid': nan, 'observed': 0.01}]}, value_formatter=None))\n\n\n\n\n\nOff ValidMind\n\n# Seasonal decomposition\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nsd = seasonal_decompose(df['loan_rate_A'], model=\"additive\") \nresiduals = sd.resid\n\n\n # Create subplots\nfrom statsmodels.graphics.tsaplots import plot_acf\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\n# Plot 1: Residuals histogram\nsns.histplot(residuals, kde=True, ax=axes[0, 0])\naxes[0, 0].set_xlabel(\"Residuals\")\naxes[0, 0].set_title(\"Residuals Histogram\")\n\n# Plot 2: Residuals Q-Q plot\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title(\"Residuals Q-Q Plot\")\n\n# Plot 3: Residuals autocorrelation plot\nplot_acf(residuals, ax=axes[1, 0], lags=100)\naxes[1, 0].set_title(\"Residuals Autocorrelation\")\n\n# Plot 4: Residuals box plot\nsns.boxplot(y=residuals, ax=axes[1, 1])\naxes[1, 1].set_ylabel(\"Residuals\")\naxes[1, 1].set_title(\"Residuals Box Plot\")\n\n# Adjust the layout\nplt.tight_layout()\n\nWarning: converting a masked element to nan.\n\n\n\n\n\nIn ValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import ResidualsVisualInspection\ntest_context = TestContext(train_ds=vm_train_ds)\nrvi_test = ResidualsVisualInspection(test_context=test_context)\nrvi_test.run()\n\nTestPlanMetricResult(figures=[Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)], metric=None)\n\n\n\nrvi_test.result.figures\n\n[Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)]\n\n\n\nrvi_test.result.figures[0].figure\n\n\n\n\n\nrvi_test.result.figures[1].figure"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#data-quality-and-relevance",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#data-quality-and-relevance",
    "title": "ValidMind",
    "section": "4.1.2. Data Quality and Relevance",
    "text": "4.1.2. Data Quality and Relevance"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#data-process-adjustments-and-treatment",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#data-process-adjustments-and-treatment",
    "title": "ValidMind",
    "section": "4.1.3. Data Process, Adjustments and Treatment",
    "text": "4.1.3. Data Process, Adjustments and Treatment\n## A. Missing Values Analysis\nStep 1: Calculate the percentage of missing values in each column\n\nmissing_values = df.isnull().sum() / len(df) * 100\nmissing_values_df = pd.DataFrame({\"column_name\": missing_values.index, \"missing_percentage\": missing_values.values})\nmissing_values_df = missing_values_df.sort_values(\"missing_percentage\", ascending=False)\n\nStep 2: Display the missing values percentage in a table format\n\ndisplay(missing_values_df)\n\n\n\n\n\n  \n    \n      \n      column_name\n      missing_percentage\n    \n  \n  \n    \n      0\n      loan_rate_A\n      0.0\n    \n    \n      1\n      loan_rate_B\n      0.0\n    \n    \n      2\n      loan_rate_C\n      0.0\n    \n    \n      3\n      loan_rate_D\n      0.0\n    \n    \n      4\n      FEDFUNDS\n      0.0\n    \n    \n      5\n      diff1_loan_rate_A\n      0.0\n    \n    \n      6\n      diff1_loan_rate_B\n      0.0\n    \n    \n      7\n      diff1_loan_rate_C\n      0.0\n    \n    \n      8\n      diff1_loan_rate_D\n      0.0\n    \n    \n      9\n      diff1_FEDFUNDS\n      0.0\n    \n    \n      10\n      diff2_FEDFUNDS\n      0.0\n    \n  \n\n\n\n\nStep 3: Visualize the missing values\nCreate a bar plot to show the missing values percentage per column.\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"missing_percentage\", y=\"column_name\", data=missing_values_df, orient=\"h\")\nplt.title(\"Percentage of Missing Values per Column\")\nplt.xlabel(\"Percentage\")\nplt.ylabel(\"Column Name\")\nplt.show()\n\n\n\n\nCreate a matrix plot to show the pattern of missing values in the dataset.\n\nplt.figure(figsize=(10, 6))\nmsno.matrix(df)\nplt.title(\"Missing Values Matrix\")\nplt.show()\n\n<Figure size 1000x600 with 0 Axes>\n\n\n\n\n\nStep 4: Analyze the patterns of missing values\nBased on the bar plot and matrix plot, analyze if the missing values are random or if there are specific patterns in the data.\nStep 5: Handle missing values\nBased on the analysis, decide how to handle the missing values.\n## B. Outliers Analysis\nStep 1: Visualize the dataset using box plots\nVisualize the data using box plots to get an initial sense of the presence of outliers.\n\n# Box plot for loan rates\nloan_rates = df[[\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]]\nsns.set(rc={\"figure.figsize\": (10, 5)})\nsns.boxplot(data=loan_rates)\nplt.title(\"Box Plots of Loan Rates\")\nplt.xlabel(\"Loan Rates\")\nplt.ylabel(\"Values\")\nplt.show()\n\n# Box plot for FEDFUNDS and diff FEDFUNDS\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nsns.boxplot(data=df[\"FEDFUNDS\"], ax=axes[0])\naxes[0].set_title(\"Box Plot of FEDFUNDS\")\naxes[0].set_xlabel(\"FEDFUNDS\")\naxes[0].set_ylabel(\"Values\")\n\nsns.boxplot(data=df[\"diff1_FEDFUNDS\"], ax=axes[1])\naxes[1].set_title(\"Box Plot of Diff FEDFUNDS\")\naxes[1].set_xlabel(\"Diff FEDFUNDS\")\naxes[1].set_ylabel(\"Values\")\n\nplt.tight_layout()\nplt.show()\n\n# Box plot for diff loan rates\ndiff_loan_rates = df[[\"diff1_loan_rate_A\", \"diff1_loan_rate_B\", \"diff1_loan_rate_C\", \"diff1_loan_rate_D\"]]\nsns.set(rc={\"figure.figsize\": (10, 5)})\nsns.boxplot(data=diff_loan_rates)\nplt.title(\"Box Plots of Diff Loan Rates\")\nplt.xlabel(\"Diff Loan Rates\")\nplt.ylabel(\"Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 2: Calculate Z-scores\nCalculate the Z-scores for each column in your dataset to identify data points that are far from the mean.\n\nz_scores = np.abs(stats.zscore(df))\n\nStep 3: Set a threshold and identify outliers\nSet a threshold (e.g., 3) to identify data points with Z-scores higher than the threshold.\n\nthreshold = 3\noutliers = np.where(z_scores > threshold)\noutliers_df = pd.DataFrame({\"row\": outliers[0], \"column\": outliers[1]})\n\nStep 4: Analyze the outliers\nAnalyze the outliers by looking at their frequency, index, and corresponding column.\n\noutliers_count = outliers_df.groupby([\"row\", \"column\"]).size().reset_index(name=\"count\")\noutliers_info = outliers_count.merge(outliers_df, on=[\"row\", \"column\"], how=\"left\")\n\n# Add the column names and dates to the outliers_info DataFrame\noutliers_info[\"column_name\"] = outliers_info[\"column\"].apply(lambda x: df.columns[x])\noutliers_info[\"date\"] = outliers_info[\"row\"].apply(lambda x: df.index[x])\n\n# Reorder, sort by column_name, and display the DataFrame\noutliers_info = outliers_info[[\"date\", \"column_name\", \"count\"]].sort_values(by=\"column_name\")\ndisplay(outliers_info)\n\n\n\n\n\n  \n    \n      \n      date\n      column_name\n      count\n    \n  \n  \n    \n      0\n      2007-08-01\n      FEDFUNDS\n      1\n    \n    \n      1\n      2007-09-01\n      FEDFUNDS\n      1\n    \n    \n      2\n      2007-10-01\n      FEDFUNDS\n      1\n    \n    \n      3\n      2007-11-01\n      FEDFUNDS\n      1\n    \n    \n      4\n      2007-12-01\n      FEDFUNDS\n      1\n    \n    \n      5\n      2008-01-01\n      FEDFUNDS\n      1\n    \n    \n      10\n      2008-10-01\n      diff1_FEDFUNDS\n      1\n    \n    \n      12\n      2008-11-01\n      diff1_FEDFUNDS\n      1\n    \n    \n      6\n      2008-02-01\n      diff1_FEDFUNDS\n      1\n    \n    \n      16\n      2010-10-01\n      diff1_loan_rate_A\n      1\n    \n    \n      9\n      2008-10-01\n      diff1_loan_rate_A\n      1\n    \n    \n      13\n      2008-12-01\n      diff1_loan_rate_A\n      1\n    \n    \n      14\n      2010-02-01\n      diff1_loan_rate_A\n      1\n    \n    \n      17\n      2010-11-01\n      diff1_loan_rate_A\n      1\n    \n    \n      15\n      2010-02-01\n      diff1_loan_rate_B\n      1\n    \n    \n      18\n      2010-11-01\n      diff1_loan_rate_B\n      1\n    \n    \n      20\n      2011-05-01\n      diff1_loan_rate_B\n      1\n    \n    \n      24\n      2013-07-01\n      diff1_loan_rate_C\n      1\n    \n    \n      19\n      2010-11-01\n      diff1_loan_rate_C\n      1\n    \n    \n      21\n      2011-05-01\n      diff1_loan_rate_C\n      1\n    \n    \n      23\n      2012-03-01\n      diff1_loan_rate_D\n      1\n    \n    \n      22\n      2011-05-01\n      diff1_loan_rate_D\n      1\n    \n    \n      25\n      2014-05-01\n      diff1_loan_rate_D\n      1\n    \n    \n      11\n      2008-10-01\n      diff2_FEDFUNDS\n      1\n    \n    \n      7\n      2008-02-01\n      diff2_FEDFUNDS\n      1\n    \n    \n      8\n      2008-03-01\n      diff2_FEDFUNDS\n      1\n    \n  \n\n\n\n\n\n# Time series plot for loan rates with outliers marked\nloan_rates = df[[\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]]\nloan_rates_outliers = outliers_info[outliers_info[\"column_name\"].str.contains(\"loan_rate\")]\n\nfig, ax = plt.subplots(figsize=(10, 5))\nloan_rates.plot(ax=ax)\nloan_rates_outliers.apply(lambda x: ax.annotate('Outlier', xy=(x.date, df.loc[x.date][x.column_name]), \n                                                fontsize=8, color='red', rotation=45), axis=1)\nplt.title(\"Time Series of Loan Rates with Outliers Marked\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Values\")\nplt.show()\n\n# Time series plot for FEDFUNDS with outliers marked\nfed_funds_outliers = outliers_info[outliers_info[\"column_name\"] == \"FEDFUNDS\"]\n\nfig, ax = plt.subplots(figsize=(10, 5))\ndf[\"FEDFUNDS\"].plot(ax=ax)\nfed_funds_outliers.apply(lambda x: ax.annotate('Outlier', xy=(x.date, df.loc[x.date][\"FEDFUNDS\"]), \n                                               fontsize=8, color='red', rotation=45), axis=1)\nplt.title(\"Time Series of FEDFUNDS with Outliers Marked\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Values\")\nplt.show()\n\n# Time series plot for diff loan rates with outliers marked\ndiff_loan_rates = df[[\"diff1_loan_rate_A\", \"diff1_loan_rate_B\", \"diff1_loan_rate_C\", \"diff1_loan_rate_D\"]]\ndiff_loan_rates_outliers = outliers_info[outliers_info[\"column_name\"].str.contains(\"diff1_loan_rate\")]\n\nfig, ax = plt.subplots(figsize=(10, 5))\ndiff_loan_rates.plot(ax=ax)\ndiff_loan_rates_outliers.apply(lambda x: ax.annotate('Outlier', xy=(x.date, df.loc[x.date][x.column_name]), \n                                                     fontsize=8, color='red', rotation=45), axis=1)\nplt.title(\"Time Series of Diff Loan Rates with Outliers Marked\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Values\")\nplt.show()\n\n# Time series plot for diff FEDFUNDS with outliers marked\ndiff_fed_funds_outliers = outliers_info[outliers_info[\"column_name\"] == \"diff1_FEDFUNDS\"]\n\nfig, ax = plt.subplots(figsize=(10, 5))\ndf[\"diff1_FEDFUNDS\"].plot(ax=ax)\ndiff_fed_funds_outliers.apply(lambda x: ax.annotate('Outlier', xy=(x.date, df.loc[x.date][\"diff1_FEDFUNDS\"]), \n                                                    fontsize=8, color='red', rotation=45), axis=1)\nplt.title(\"Time Series of Diff FEDFUNDS with Outliers Marked\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Values\")\nplt.show()"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#c.-seasonality-analysis",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#c.-seasonality-analysis",
    "title": "ValidMind",
    "section": "C. Seasonality Analysis",
    "text": "C. Seasonality Analysis\nStep 1: Seasonal decomposition\nPerform seasonal decomposition on each time series.\n\nfreq = 12  # Assuming monthly data, adjust the frequency accordingly\n\nloan_rate_decompositions = {}\nfor col in loan_rates.columns:\n    loan_rate_decompositions[col] = seasonal_decompose(df[col], model='additive', period=freq)\n\nfed_funds_decomposition = seasonal_decompose(df[\"FEDFUNDS\"], model='additive', period=freq)\n\ndiff_loan_rate_decompositions = {}\nfor col in diff_loan_rates.columns:\n    diff_loan_rate_decompositions[col] = seasonal_decompose(df[col], model='additive', period=freq)\n\nStep 2: Visualize seasonal decomposition\nCreate plots for seasonal components.\n\ndef plot_decomposition_components(series_decompositions, title):\n    n = len(series_decompositions)\n    fig, axes = plt.subplots(nrows=n, ncols=4, figsize=(18, 3 * n))\n    \n    if n == 1:\n        axes = [axes]  # Make sure axes is a list even when there's only one subplot\n    \n    for row_axes, (col, decomposition) in zip(axes, series_decompositions.items()):\n        for ax, component, comp_title in zip(row_axes, [decomposition.observed, decomposition.trend, decomposition.seasonal, decomposition.resid],\n                                            [\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"]):\n            component.plot(ax=ax)\n            ax.set_title(f\"{comp_title} Component of {col}\")\n            ax.set_ylabel(\"Values\")\n    \n    plt.suptitle(title, y=1.02, fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\nplot_decomposition_components(loan_rate_decompositions, \"Decomposition of Loan Rates\")\nplot_decomposition_components({\"FEDFUNDS\": fed_funds_decomposition}, \"Decomposition of FEDFUNDS\")\nplot_decomposition_components(diff_loan_rate_decompositions, \"Decomposition of Diff Loan Rates and Diff FEDFUNDS\")\n\n\n\n\n\n\n\n\n\n\nCreate ACF plots.\n\n\nall_variables = [df['loan_rate_A'], df['loan_rate_B'], df['loan_rate_C'], df['loan_rate_D'], df['FEDFUNDS'], \n                 df['diff1_loan_rate_A'], df['diff1_loan_rate_B'], df['diff1_loan_rate_C'], df['diff1_loan_rate_D'], \n                 df['diff1_FEDFUNDS'], df['diff2_FEDFUNDS']]\n\n# Create a figure with subplots\nfig, ax = plt.subplots(11, 1, figsize=(15, 4*11))\n\n# Plot ACF for each variable\nfor i, variable in enumerate(all_variables):\n    plot_acf(variable, ax=ax[i], lags=40)\n    ax[i].set_title(\"ACF Plot for {}\".format(variable.name))\n\n# Adjust the layout of the subplots\nplt.tight_layout()\n\n\n\n\nStep 3: Residuals Analysis\n\n# Define a function to perform residual analysis\ndef residual_analysis(residuals, variable_name):\n    residuals = residuals.dropna()  # drop NaN values\n    fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n    ax = ax.flatten()\n    \n    # QQ plot\n    qqplot(residuals, line='s', ax=ax[0])\n    ax[0].set_title(f'QQ Plot of Residuals ({variable_name})')\n    \n    # Histogram with KDE\n    sns.distplot(residuals, ax=ax[1])\n    ax[1].set_title(f'Histogram with KDE of Residuals ({variable_name})')\n    \n    # Residual series dot plot\n    sns.lineplot(data=residuals, linewidth=0.5, color='red', ax=ax[2])\n    ax[2].set_title(f'Residual Series Dot Plot ({variable_name})')\n    \n    # ACF plot\n    plot_acf(residuals, ax=ax[3], lags=100)\n    ax[3].set_title(f'ACF Plot of Residuals ({variable_name})')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# Perform residual analysis for each variable\nfor variable_name in loan_rate_decompositions.keys():\n    residual_analysis(loan_rate_decompositions[variable_name].resid, variable_name)\n\n# Perform residual analysis for FEDFUNDS\nresidual_analysis(fed_funds_decomposition.resid, \"FEDFUNDS\")\n\n# Perform residual analysis for each difference variable\nfor variable_name in diff_loan_rate_decompositions.keys():\n    residual_analysis(diff_loan_rate_decompositions[variable_name].resid, variable_name)\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\nTest if Residuals are Normaly Distributed.\n\n# Remove Nan values before computing any test statistics.\nfor col in loan_rates.columns:\n    loan_rate_decompositions[col].trend.dropna(inplace=True)\n    loan_rate_decompositions[col].seasonal.dropna(inplace=True)\n    loan_rate_decompositions[col].resid.dropna(inplace=True)\n\nfed_funds_decomposition.trend.dropna(inplace=True)\nfed_funds_decomposition.seasonal.dropna(inplace=True)\nfed_funds_decomposition.resid.dropna(inplace=True)\n\nfor col in diff_loan_rates.columns:\n    diff_loan_rate_decompositions[col].trend.dropna(inplace=True)\n    diff_loan_rate_decompositions[col].seasonal.dropna(inplace=True)\n    diff_loan_rate_decompositions[col].resid.dropna(inplace=True)\n\n\ndef compute_normality_statistics(data):\n    ks_stat, ks_p = stats.kstest(data, 'norm')\n    sw_stat, sw_p = stats.shapiro(data)\n    lf_stat, lf_p = lilliefors(data)\n    jb_stat, jb_p = stats.jarque_bera(data)\n\n    return pd.DataFrame(\n        {\n            \"Test\": [\n                \"Kolmogorov-Smirnov\",\n                \"Shapiro-Wilk\",\n                \"Lilliefors\",\n                \"Jarque-Bera\",\n            ],\n            \"Statistic\": [ks_stat, sw_stat, lf_stat, jb_stat],\n            \"P-value\": [ks_p, sw_p, lf_p, jb_p],\n        }\n    )\n\n\n\n# Initialize an empty DataFrame to store the results\nnormality_statistics_results = pd.DataFrame()\n\n# Compute normality statistics for each variable\nfor variable_name in loan_rate_decompositions.keys():\n    data = loan_rate_decompositions[variable_name].resid\n    normality_stats = compute_normality_statistics(data)\n    normality_stats[\"Variable\"] = variable_name\n    normality_statistics_results = normality_statistics_results.append(normality_stats, ignore_index=True)\n\n# Compute normality statistics for FEDFUNDS\ndata = fed_funds_decomposition.resid\nnormality_stats = compute_normality_statistics(data)\nnormality_stats[\"Variable\"] = \"FEDFUNDS\"\nnormality_statistics_results = normality_statistics_results.append(normality_stats, ignore_index=True)\n\n# Compute normality statistics for each difference variable\nfor variable_name in diff_loan_rate_decompositions.keys():\n    data = diff_loan_rate_decompositions[variable_name].resid\n    normality_stats = compute_normality_statistics(data)\n    normality_stats[\"Variable\"] = variable_name\n    normality_statistics_results = normality_statistics_results.append(normality_stats, ignore_index=True)\n\n\ndef test_normality_statistics(normality_statistics_results, p_value_threshold=0.05):\n    # Add a Pass/Fail column based on the p-value threshold\n    normality_statistics_results['Pass/Fail'] = normality_statistics_results['P-value'].apply(lambda p: 'Pass' if p >= p_value_threshold else 'Fail')\n    \n    # Display the results as a table\n    display(normality_statistics_results)\n\ntest_normality_statistics(normality_statistics_results, p_value_threshold=0.05)\n\n\n\n\n\n  \n    \n      \n      Test\n      Statistic\n      P-value\n      Variable\n      Pass/Fail\n    \n  \n  \n    \n      0\n      Kolmogorov-Smirnov\n      0.361205\n      4.090343e-15\n      loan_rate_A\n      Fail\n    \n    \n      1\n      Shapiro-Wilk\n      0.964680\n      2.367732e-03\n      loan_rate_A\n      Fail\n    \n    \n      2\n      Lilliefors\n      0.090317\n      2.188144e-02\n      loan_rate_A\n      Fail\n    \n    \n      3\n      Jarque-Bera\n      22.683894\n      1.186465e-05\n      loan_rate_A\n      Fail\n    \n    \n      4\n      Kolmogorov-Smirnov\n      0.316534\n      1.237558e-11\n      loan_rate_B\n      Fail\n    \n    \n      5\n      Shapiro-Wilk\n      0.950467\n      1.692832e-04\n      loan_rate_B\n      Fail\n    \n    \n      6\n      Lilliefors\n      0.112750\n      1.031154e-03\n      loan_rate_B\n      Fail\n    \n    \n      7\n      Jarque-Bera\n      24.607237\n      4.535305e-06\n      loan_rate_B\n      Fail\n    \n    \n      8\n      Kolmogorov-Smirnov\n      0.343031\n      1.229136e-13\n      loan_rate_C\n      Fail\n    \n    \n      9\n      Shapiro-Wilk\n      0.992633\n      7.564428e-01\n      loan_rate_C\n      Pass\n    \n    \n      10\n      Lilliefors\n      0.047168\n      7.618809e-01\n      loan_rate_C\n      Pass\n    \n    \n      11\n      Jarque-Bera\n      0.206857\n      9.017404e-01\n      loan_rate_C\n      Pass\n    \n    \n      12\n      Kolmogorov-Smirnov\n      0.312985\n      2.225322e-11\n      loan_rate_D\n      Fail\n    \n    \n      13\n      Shapiro-Wilk\n      0.989167\n      4.321428e-01\n      loan_rate_D\n      Pass\n    \n    \n      14\n      Lilliefors\n      0.045615\n      8.004312e-01\n      loan_rate_D\n      Pass\n    \n    \n      15\n      Jarque-Bera\n      1.373671\n      5.031658e-01\n      loan_rate_D\n      Pass\n    \n    \n      16\n      Kolmogorov-Smirnov\n      0.430685\n      1.396740e-21\n      FEDFUNDS\n      Fail\n    \n    \n      17\n      Shapiro-Wilk\n      0.759088\n      4.823187e-13\n      FEDFUNDS\n      Fail\n    \n    \n      18\n      Lilliefors\n      0.191270\n      1.000000e-03\n      FEDFUNDS\n      Fail\n    \n    \n      19\n      Jarque-Bera\n      255.448548\n      3.388852e-56\n      FEDFUNDS\n      Fail\n    \n    \n      20\n      Kolmogorov-Smirnov\n      0.361380\n      3.954988e-15\n      diff1_loan_rate_A\n      Fail\n    \n    \n      21\n      Shapiro-Wilk\n      0.943480\n      5.220129e-05\n      diff1_loan_rate_A\n      Fail\n    \n    \n      22\n      Lilliefors\n      0.106437\n      2.682603e-03\n      diff1_loan_rate_A\n      Fail\n    \n    \n      23\n      Jarque-Bera\n      32.392802\n      9.246818e-08\n      diff1_loan_rate_A\n      Fail\n    \n    \n      24\n      Kolmogorov-Smirnov\n      0.359857\n      5.300637e-15\n      diff1_loan_rate_B\n      Fail\n    \n    \n      25\n      Shapiro-Wilk\n      0.893433\n      5.663049e-08\n      diff1_loan_rate_B\n      Fail\n    \n    \n      26\n      Lilliefors\n      0.137282\n      1.000000e-03\n      diff1_loan_rate_B\n      Fail\n    \n    \n      27\n      Jarque-Bera\n      122.026511\n      3.178922e-27\n      diff1_loan_rate_B\n      Fail\n    \n    \n      28\n      Kolmogorov-Smirnov\n      0.376455\n      2.017554e-16\n      diff1_loan_rate_C\n      Fail\n    \n    \n      29\n      Shapiro-Wilk\n      0.970360\n      7.531957e-03\n      diff1_loan_rate_C\n      Fail\n    \n    \n      30\n      Lilliefors\n      0.086871\n      3.264423e-02\n      diff1_loan_rate_C\n      Fail\n    \n    \n      31\n      Jarque-Bera\n      12.705219\n      1.742195e-03\n      diff1_loan_rate_C\n      Fail\n    \n    \n      32\n      Kolmogorov-Smirnov\n      0.331651\n      9.369166e-13\n      diff1_loan_rate_D\n      Fail\n    \n    \n      33\n      Shapiro-Wilk\n      0.926674\n      4.010225e-06\n      diff1_loan_rate_D\n      Fail\n    \n    \n      34\n      Lilliefors\n      0.110326\n      1.665161e-03\n      diff1_loan_rate_D\n      Fail\n    \n    \n      35\n      Jarque-Bera\n      104.263860\n      2.287655e-23\n      diff1_loan_rate_D\n      Fail\n    \n  \n\n\n\n\nTest if Residuals are Autocorrelated.\n\ndef compute_autocorrelation_statistics(data, lags=10):\n    lb_results = acorr_ljungbox(data, lags=lags, return_df=True)\n    lb_stat = lb_results['lb_stat'].iloc[-1]\n    lb_p_value = lb_results['lb_pvalue'].iloc[-1]\n\n    bp_results = acorr_ljungbox(data, lags=lags, boxpierce=True, return_df=True)\n    bp_stat = bp_results['lb_stat'].iloc[-1]\n    bp_p_value = bp_results['lb_pvalue'].iloc[-1]\n\n    runs_stat, runs_p_value = runstest_1samp(data)\n    \n    return pd.DataFrame(\n        {\n            \"Test\": [                \"Ljung-Box\",                \"Runs Test\",                \"Box-Pierce Test\",            ],\n            \"Statistic\": [                lb_stat,                runs_stat,                bp_stat,            ],\n            \"P-value\": [                lb_p_value,                runs_p_value,                bp_p_value,            ],\n        }\n    )\n\n\n\n# Initialize an empty DataFrame to store the results\nautocorrelation_statistics_results = pd.DataFrame()\n\n# Compute autocorrelation statistics for each variable\nfor variable_name in loan_rate_decompositions.keys():\n    data = loan_rate_decompositions[variable_name].resid.dropna()\n    autocorrelation_stats = compute_autocorrelation_statistics(data)\n    autocorrelation_stats[\"Variable\"] = variable_name\n    autocorrelation_statistics_results = autocorrelation_statistics_results.append(autocorrelation_stats, ignore_index=True)\n\n# Compute autocorrelation statistics for FEDFUNDS\ndata = fed_funds_decomposition.resid.dropna()\nautocorrelation_stats = compute_autocorrelation_statistics(data)\nautocorrelation_stats[\"Variable\"] = \"FEDFUNDS\"\nautocorrelation_statistics_results = autocorrelation_statistics_results.append(autocorrelation_stats, ignore_index=True)\n\n# Compute autocorrelation statistics for each difference variable\nfor variable_name in diff_loan_rate_decompositions.keys():\n    data = diff_loan_rate_decompositions[variable_name].resid.dropna()\n    autocorrelation_stats = compute_autocorrelation_statistics(data)\n    autocorrelation_stats[\"Variable\"] = variable_name\n    autocorrelation_statistics_results = autocorrelation_statistics_results.append(autocorrelation_stats, ignore_index=True)\n\nautocorrelation_statistics_results = autocorrelation_statistics_results.reset_index(drop=True)\n\n\n\n\ndef test_autocorrelation_statistics(autocorrelation_statistics_results, p_value_threshold=0.05):\n    \n    autocorrelation_statistics_results['Threshold'] = p_value_threshold\n    autocorrelation_statistics_results['P-value'] = pd.to_numeric(autocorrelation_statistics_results['P-value'], errors='coerce')\n    \n    autocorrelation_statistics_results['Pass/Fail'] = autocorrelation_statistics_results['P-value'].apply(\n        lambda p: 'Pass' if p >= p_value_threshold else 'Fail'\n    )\n    display(autocorrelation_statistics_results)\n\n\n\n    \n\n\ntest_autocorrelation_statistics(autocorrelation_statistics_results, p_value_threshold=0.01)\n\n\n\n\n\n  \n    \n      \n      Test\n      Statistic\n      P-value\n      Variable\n      Threshold\n      Pass/Fail\n    \n  \n  \n    \n      0\n      Ljung-Box\n      116.663720\n      2.401553e-20\n      loan_rate_A\n      0.01\n      Fail\n    \n    \n      1\n      Runs Test\n      -5.648902\n      1.614762e-08\n      loan_rate_A\n      0.01\n      Fail\n    \n    \n      2\n      Box-Pierce Test\n      116.663720\n      2.401553e-20\n      loan_rate_A\n      0.01\n      Fail\n    \n    \n      3\n      Ljung-Box\n      204.098406\n      2.253253e-38\n      loan_rate_B\n      0.01\n      Fail\n    \n    \n      4\n      Runs Test\n      -6.375888\n      1.819052e-10\n      loan_rate_B\n      0.01\n      Fail\n    \n    \n      5\n      Box-Pierce Test\n      204.098406\n      2.253253e-38\n      loan_rate_B\n      0.01\n      Fail\n    \n    \n      6\n      Ljung-Box\n      125.601717\n      3.678593e-22\n      loan_rate_C\n      0.01\n      Fail\n    \n    \n      7\n      Runs Test\n      -5.657398\n      1.536855e-08\n      loan_rate_C\n      0.01\n      Fail\n    \n    \n      8\n      Box-Pierce Test\n      125.601717\n      3.678593e-22\n      loan_rate_C\n      0.01\n      Fail\n    \n    \n      9\n      Ljung-Box\n      89.525260\n      6.651950e-15\n      loan_rate_D\n      0.01\n      Fail\n    \n    \n      10\n      Runs Test\n      -6.008704\n      1.870119e-09\n      loan_rate_D\n      0.01\n      Fail\n    \n    \n      11\n      Box-Pierce Test\n      89.525260\n      6.651950e-15\n      loan_rate_D\n      0.01\n      Fail\n    \n    \n      12\n      Ljung-Box\n      169.890971\n      2.922023e-31\n      FEDFUNDS\n      0.01\n      Fail\n    \n    \n      13\n      Runs Test\n      -6.518261\n      7.112729e-11\n      FEDFUNDS\n      0.01\n      Fail\n    \n    \n      14\n      Box-Pierce Test\n      169.890971\n      2.922023e-31\n      FEDFUNDS\n      0.01\n      Fail\n    \n    \n      15\n      Ljung-Box\n      31.658078\n      4.564856e-04\n      diff1_loan_rate_A\n      0.01\n      Fail\n    \n    \n      16\n      Runs Test\n      -0.934551\n      3.500195e-01\n      diff1_loan_rate_A\n      0.01\n      Pass\n    \n    \n      17\n      Box-Pierce Test\n      31.658078\n      4.564856e-04\n      diff1_loan_rate_A\n      0.01\n      Fail\n    \n    \n      18\n      Ljung-Box\n      94.434755\n      7.038946e-16\n      diff1_loan_rate_B\n      0.01\n      Fail\n    \n    \n      19\n      Runs Test\n      -0.627961\n      5.300294e-01\n      diff1_loan_rate_B\n      0.01\n      Pass\n    \n    \n      20\n      Box-Pierce Test\n      94.434755\n      7.038946e-16\n      diff1_loan_rate_B\n      0.01\n      Fail\n    \n    \n      21\n      Ljung-Box\n      54.151940\n      4.546097e-08\n      diff1_loan_rate_C\n      0.01\n      Fail\n    \n    \n      22\n      Runs Test\n      -3.261335\n      1.108888e-03\n      diff1_loan_rate_C\n      0.01\n      Fail\n    \n    \n      23\n      Box-Pierce Test\n      54.151940\n      4.546097e-08\n      diff1_loan_rate_C\n      0.01\n      Fail\n    \n    \n      24\n      Ljung-Box\n      32.703162\n      3.054971e-04\n      diff1_loan_rate_D\n      0.01\n      Fail\n    \n    \n      25\n      Runs Test\n      -1.632116\n      1.026551e-01\n      diff1_loan_rate_D\n      0.01\n      Pass\n    \n    \n      26\n      Box-Pierce Test\n      32.703162\n      3.054971e-04\n      diff1_loan_rate_D\n      0.01\n      Fail\n    \n  \n\n\n\n\nStep 3: Test for seasonality using the Augmented Dickey-Fuller (ADF) test\n\n\n\nStep 4: Analyze the seasonality test results\nStep 5: Interpret the results\nBased on the p-values from the ADF test, you can infer whether the time series are stationary or not. If the p-value is less than 0.05 (or your chosen significance level), you can reject the null hypothesis and conclude that the series is stationary. Otherwise, you fail to reject the null hypothesis and cannot conclude that the series is stationary.\nStep 6: Handle seasonality"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#variable-analysis",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#variable-analysis",
    "title": "ValidMind",
    "section": "4.2.4 Variable Analysis",
    "text": "4.2.4 Variable Analysis\n## A. Feature Analysis"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#a.1.-univariate-analysis",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#a.1.-univariate-analysis",
    "title": "ValidMind",
    "section": "A.1. Univariate Analysis",
    "text": "A.1. Univariate Analysis\n\nVisual Inspection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#a.2-multivariave-analysis",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#a.2-multivariave-analysis",
    "title": "ValidMind",
    "section": "A.2 Multivariave Analysis",
    "text": "A.2 Multivariave Analysis\n\nVisual Inspection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#b.-variable-selection",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#b.-variable-selection",
    "title": "ValidMind",
    "section": "B. Variable Selection",
    "text": "B. Variable Selection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_offvm.html#b.1-selection-process",
    "href": "notebooks/time_series/cre_rate_model_offvm.html#b.1-selection-process",
    "title": "ValidMind",
    "section": "B.1 Selection Process",
    "text": "B.1 Selection Process\n\nARIMA Analysis\nStep 1: Stationarity Analysis\nThe first step in any time series analysis is to visualize the data and check for stationarity. Stationarity is important because most time series models, including ARIMA, assume that the underlying data is stationary.\nUnit Root Tests.\n\ndef compute_unit_root_statistics(df):\n    def apply_tests(series):\n        # ADF Test\n        adf_result = adfuller(series)\n        adf_test_stat = adf_result[0]\n        adf_p_value = adf_result[1]\n\n        # KPSS Test\n        kpss_result = kpss(series, nlags='auto')\n        kpss_test_stat = kpss_result[0]\n        kpss_p_value = kpss_result[1]\n\n        # PP Test\n        pp = PhillipsPerron(series)\n        pp_test_stat = pp.stat\n        pp_p_value = pp.pvalue\n\n        # Zivot-Andrews Test\n        za = ZivotAndrews(series)\n        za_test_stat = za.stat\n        za_p_value = za.pvalue\n\n        return pd.Series({\n            'ADF': adf_test_stat, 'ADF p-value': adf_p_value,\n            'KPSS': kpss_test_stat, 'KPSS p-value': kpss_p_value,\n            'PP': pp_test_stat, 'PP p-value': pp_p_value,\n            'ZA': za_test_stat, 'ZA p-value': za_p_value,\n        })\n\n    return df.apply(apply_tests)\n\nunit_root_statistics = compute_unit_root_statistics(df)\n\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2018: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2018: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2018: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2018: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n  warnings.warn(\n/Users/andres/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n  warnings.warn(\n\n\n\nunit_root_statistics\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n  \n  \n    \n      ADF\n      -1.917289\n      -3.159930\n      -2.530667\n      -1.617159\n      -0.168543\n      -1.031383e+01\n      -8.632056e+00\n      -8.618071e+00\n      -4.103447e+00\n      -3.632288\n      -3.327780\n    \n    \n      ADF p-value\n      0.323972\n      0.022424\n      0.108190\n      0.474219\n      0.942169\n      3.134761e-18\n      5.720336e-14\n      6.211940e-14\n      9.558947e-04\n      0.005173\n      0.013681\n    \n    \n      KPSS\n      1.012357\n      0.263073\n      0.809961\n      1.564146\n      0.375746\n      8.592937e-02\n      1.514857e-01\n      3.335876e-01\n      1.887051e-01\n      1.153749\n      0.061353\n    \n    \n      KPSS p-value\n      0.010000\n      0.100000\n      0.010000\n      0.010000\n      0.087609\n      1.000000e-01\n      1.000000e-01\n      1.000000e-01\n      1.000000e-01\n      0.010000\n      0.100000\n    \n    \n      PP\n      -2.027462\n      -2.446084\n      -2.264992\n      -1.853613\n      -3.970772\n      -1.034179e+01\n      -8.657786e+00\n      -9.063228e+00\n      -9.725544e+00\n      -5.639326\n      -21.677880\n    \n    \n      PP p-value\n      0.274663\n      0.129150\n      0.183507\n      0.354187\n      0.001572\n      2.673442e-18\n      4.915171e-14\n      4.504048e-15\n      9.295675e-17\n      0.000001\n      0.000000\n    \n    \n      ZA\n      -3.498670\n      -5.246346\n      -4.681009\n      -4.566136\n      -4.263683\n      -1.068546e+01\n      -8.970778e+00\n      -9.255829e+00\n      -4.753945e+00\n      -5.147877\n      -5.228518\n    \n    \n      ZA p-value\n      0.680477\n      0.011857\n      0.074135\n      0.100013\n      0.208163\n      1.000000e-05\n      1.000000e-05\n      1.000000e-05\n      6.055853e-02\n      0.017935\n      0.012958\n    \n  \n\n\n\n\n\ndef test_unit_root_statistics(test_results, threshold=0.01):\n    formatted_results = []\n    for column in test_results.columns:\n        # for test_name in ['ADF', 'KPSS', 'PP', 'ZA', 'DFGLS']:\n        for test_name in ['ADF', 'KPSS', 'PP', 'ZA']:\n            test_stat = test_results.loc[test_name, column]\n            p_value = test_results.loc[f'{test_name} p-value', column]\n            pass_fail = 'Pass' if p_value > threshold else 'Fail'\n            formatted_results.append({\n                'Test': test_name,\n                'Statistic': test_stat,\n                'P-value': p_value,\n                'Variable': column,\n                'Threshold': threshold,\n                'Pass/Fail': pass_fail\n            })\n\n    return pd.DataFrame(formatted_results)\n\nformatted_results = test_unit_root_statistics(unit_root_statistics)\ndisplay(formatted_results)\n\n\n\n\n\n  \n    \n      \n      Test\n      Statistic\n      P-value\n      Variable\n      Threshold\n      Pass/Fail\n    \n  \n  \n    \n      0\n      ADF\n      -1.917289\n      3.239719e-01\n      loan_rate_A\n      0.01\n      Pass\n    \n    \n      1\n      KPSS\n      1.012357\n      1.000000e-02\n      loan_rate_A\n      0.01\n      Fail\n    \n    \n      2\n      PP\n      -2.027462\n      2.746634e-01\n      loan_rate_A\n      0.01\n      Pass\n    \n    \n      3\n      ZA\n      -3.498670\n      6.804771e-01\n      loan_rate_A\n      0.01\n      Pass\n    \n    \n      4\n      ADF\n      -3.159930\n      2.242441e-02\n      loan_rate_B\n      0.01\n      Pass\n    \n    \n      5\n      KPSS\n      0.263073\n      1.000000e-01\n      loan_rate_B\n      0.01\n      Pass\n    \n    \n      6\n      PP\n      -2.446084\n      1.291496e-01\n      loan_rate_B\n      0.01\n      Pass\n    \n    \n      7\n      ZA\n      -5.246346\n      1.185734e-02\n      loan_rate_B\n      0.01\n      Pass\n    \n    \n      8\n      ADF\n      -2.530667\n      1.081899e-01\n      loan_rate_C\n      0.01\n      Pass\n    \n    \n      9\n      KPSS\n      0.809961\n      1.000000e-02\n      loan_rate_C\n      0.01\n      Fail\n    \n    \n      10\n      PP\n      -2.264992\n      1.835073e-01\n      loan_rate_C\n      0.01\n      Pass\n    \n    \n      11\n      ZA\n      -4.681009\n      7.413460e-02\n      loan_rate_C\n      0.01\n      Pass\n    \n    \n      12\n      ADF\n      -1.617159\n      4.742193e-01\n      loan_rate_D\n      0.01\n      Pass\n    \n    \n      13\n      KPSS\n      1.564146\n      1.000000e-02\n      loan_rate_D\n      0.01\n      Fail\n    \n    \n      14\n      PP\n      -1.853613\n      3.541874e-01\n      loan_rate_D\n      0.01\n      Pass\n    \n    \n      15\n      ZA\n      -4.566136\n      1.000130e-01\n      loan_rate_D\n      0.01\n      Pass\n    \n    \n      16\n      ADF\n      -0.168543\n      9.421688e-01\n      FEDFUNDS\n      0.01\n      Pass\n    \n    \n      17\n      KPSS\n      0.375746\n      8.760949e-02\n      FEDFUNDS\n      0.01\n      Pass\n    \n    \n      18\n      PP\n      -3.970772\n      1.571534e-03\n      FEDFUNDS\n      0.01\n      Fail\n    \n    \n      19\n      ZA\n      -4.263683\n      2.081630e-01\n      FEDFUNDS\n      0.01\n      Pass\n    \n    \n      20\n      ADF\n      -10.313828\n      3.134761e-18\n      diff1_loan_rate_A\n      0.01\n      Fail\n    \n    \n      21\n      KPSS\n      0.085929\n      1.000000e-01\n      diff1_loan_rate_A\n      0.01\n      Pass\n    \n    \n      22\n      PP\n      -10.341787\n      2.673442e-18\n      diff1_loan_rate_A\n      0.01\n      Fail\n    \n    \n      23\n      ZA\n      -10.685459\n      1.000000e-05\n      diff1_loan_rate_A\n      0.01\n      Fail\n    \n    \n      24\n      ADF\n      -8.632056\n      5.720336e-14\n      diff1_loan_rate_B\n      0.01\n      Fail\n    \n    \n      25\n      KPSS\n      0.151486\n      1.000000e-01\n      diff1_loan_rate_B\n      0.01\n      Pass\n    \n    \n      26\n      PP\n      -8.657786\n      4.915171e-14\n      diff1_loan_rate_B\n      0.01\n      Fail\n    \n    \n      27\n      ZA\n      -8.970778\n      1.000000e-05\n      diff1_loan_rate_B\n      0.01\n      Fail\n    \n    \n      28\n      ADF\n      -8.618071\n      6.211940e-14\n      diff1_loan_rate_C\n      0.01\n      Fail\n    \n    \n      29\n      KPSS\n      0.333588\n      1.000000e-01\n      diff1_loan_rate_C\n      0.01\n      Pass\n    \n    \n      30\n      PP\n      -9.063228\n      4.504048e-15\n      diff1_loan_rate_C\n      0.01\n      Fail\n    \n    \n      31\n      ZA\n      -9.255829\n      1.000000e-05\n      diff1_loan_rate_C\n      0.01\n      Fail\n    \n    \n      32\n      ADF\n      -4.103447\n      9.558947e-04\n      diff1_loan_rate_D\n      0.01\n      Fail\n    \n    \n      33\n      KPSS\n      0.188705\n      1.000000e-01\n      diff1_loan_rate_D\n      0.01\n      Pass\n    \n    \n      34\n      PP\n      -9.725544\n      9.295675e-17\n      diff1_loan_rate_D\n      0.01\n      Fail\n    \n    \n      35\n      ZA\n      -4.753945\n      6.055853e-02\n      diff1_loan_rate_D\n      0.01\n      Pass\n    \n    \n      36\n      ADF\n      -3.632288\n      5.172913e-03\n      diff1_FEDFUNDS\n      0.01\n      Fail\n    \n    \n      37\n      KPSS\n      1.153749\n      1.000000e-02\n      diff1_FEDFUNDS\n      0.01\n      Fail\n    \n    \n      38\n      PP\n      -5.639326\n      1.045285e-06\n      diff1_FEDFUNDS\n      0.01\n      Fail\n    \n    \n      39\n      ZA\n      -5.147877\n      1.793470e-02\n      diff1_FEDFUNDS\n      0.01\n      Pass\n    \n    \n      40\n      ADF\n      -3.327780\n      1.368147e-02\n      diff2_FEDFUNDS\n      0.01\n      Pass\n    \n    \n      41\n      KPSS\n      0.061353\n      1.000000e-01\n      diff2_FEDFUNDS\n      0.01\n      Pass\n    \n    \n      42\n      PP\n      -21.677880\n      0.000000e+00\n      diff2_FEDFUNDS\n      0.01\n      Fail\n    \n    \n      43\n      ZA\n      -5.228518\n      1.295767e-02\n      diff2_FEDFUNDS\n      0.01\n      Pass\n    \n  \n\n\n\n\nStep 2: Identify the order of the AR model\nThe order of the autoregressive (AR) model can be determined by looking at the partial autocorrelation function (PACF) plot. The PACF shows the correlation between a time series and its lagged values after accounting for the correlation at all shorter lags.\n\ndef find_ar_order(column, max_lag):\n    pacf_values, pacf_confidence_intervals = pacf(column, nlags=max_lag, alpha=0.05, method=\"ywmle\")\n    \n    for lag, pacf_value, ci in zip(range(len(pacf_values)), pacf_values, pacf_confidence_intervals):\n        if pacf_value <= ci[1]:\n            return lag - 1\n\n    return None\n\nmax_lag = 10  # Maximum lag to consider\nar_orders = {}\n\nfor col in df.columns:\n    ar_order = find_ar_order(df[col], max_lag)\n    ar_orders[col] = ar_order\n\n# Create a DataFrame from the ar_orders dictionary\nar_orders_df = pd.DataFrame.from_dict(ar_orders, orient='index', columns=['AR_order'])\nar_orders_df.reset_index(inplace=True)\nar_orders_df.columns = ['Column', 'AR_order']\n\n# Display the DataFrame\ndisplay(ar_orders_df)\n\n\n\n\n\n  \n    \n      \n      Column\n      AR_order\n    \n  \n  \n    \n      0\n      loan_rate_A\n      -1\n    \n    \n      1\n      loan_rate_B\n      -1\n    \n    \n      2\n      loan_rate_C\n      -1\n    \n    \n      3\n      loan_rate_D\n      -1\n    \n    \n      4\n      FEDFUNDS\n      -1\n    \n    \n      5\n      diff1_loan_rate_A\n      -1\n    \n    \n      6\n      diff1_loan_rate_B\n      -1\n    \n    \n      7\n      diff1_loan_rate_C\n      -1\n    \n    \n      8\n      diff1_loan_rate_D\n      -1\n    \n    \n      9\n      diff1_FEDFUNDS\n      -1\n    \n    \n      10\n      diff2_FEDFUNDS\n      -1"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#data-quality-and-relevance",
    "href": "notebooks/time_series/cre_rate_model_demo.html#data-quality-and-relevance",
    "title": "ValidMind",
    "section": "4.1.2. Data Quality and Relevance",
    "text": "4.1.2. Data Quality and Relevance"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#data-process-adjustments-and-treatment",
    "href": "notebooks/time_series/cre_rate_model_demo.html#data-process-adjustments-and-treatment",
    "title": "ValidMind",
    "section": "4.1.3. Data Process, Adjustments and Treatment",
    "text": "4.1.3. Data Process, Adjustments and Treatment"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#a.-missing-values-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#a.-missing-values-analysis",
    "title": "ValidMind",
    "section": "A. Missing Values Analysis",
    "text": "A. Missing Values Analysis\nStep 1: Calculate the percentage of missing values in each column\nStep 2: Display the missing values percentage in a table format\nStep 3: Visualize the missing values\n## B. Outliers Analysis\nStep 1: Visualize the dataset using box plots\nVisualize the data using box plots to get an initial sense of the presence of outliers.\nStep 2: Calculate Z-scores\nStep 3: Set a threshold and identify outliers\nSet a threshold (e.g., 3) to identify data points with Z-scores higher than the threshold.\nStep 4: Analyze the outliers\nAnalyze the outliers by looking at their frequency, index, and corresponding column."
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#c.-seasonality-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#c.-seasonality-analysis",
    "title": "ValidMind",
    "section": "C. Seasonality Analysis",
    "text": "C. Seasonality Analysis\nStep 1: Seasonal decomposition\nPerform seasonal decomposition on each time series.\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalDecompose\ntest_context = TestContext(train_ds=vm_train_ds)\nsd_metric = SeasonalDecompose(test_context=test_context)\n\nStep 2: Visualize seasonal decomposition\nCreate plots for observed, trend, seasonal and residual components.\n\nsd_metric.run()\nsd_metric.result.show()\n\n\n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        seasonal_decompose\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': [{'Date': '2007-08-01', 'loan_rate_A': 7.7666666666666675, 'trend': nan, 'seasonal': -0.050284773390468364, 'resid': nan}, {'Date': '2007-09-01', 'loan_rate_A': 7.841428571428572, 'trend': nan, 'seasonal': -0.06087962072801926, 'resid': nan}, {'Date': '2007-10-01', 'loan_rate_A': 7.83, 'trend': nan, 'seasonal': 0.01749661199350169, 'resid': nan}, {'Date': '2007-11-01', 'loan_rate_A': 7.779090909090908, 'trend': nan, 'seasonal': -0.047258378330469565, 'resid': nan}, {'Date': '2007-12-01', 'loan_rate_A': 7.695833333333333, 'trend': nan, 'seasonal': 0.08505178146324885, 'resid': nan}, {'Date': '2008-01-01', 'loan_rate_A': 7.961333333333333, 'trend': nan, 'seasonal': 0.06564185816692848, 'resid': nan}, {'Date': '2008-02-01', 'loan_rate_A': 8.130333333333333, 'trend': 8.005048767959094, 'seasonal': 0.008943337297934253, 'resid': 0.11634122807630445}, {'Date': '2008-03-01', 'loan_rate_A': 8.126285714285714, 'trend': 8.036669799705125, 'seasonal': -0.002099404440702811, 'resid': 0.09171531902129207},...\n                    \n                \n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n\n\nSeasonality Detection using ACF and PACF.\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalityDetectionWithACF\ntest_context = TestContext(train_ds=vm_train_ds)\nacf_metric = SeasonalityDetectionWithACF(test_context=test_context)\nacf_metric.run()\nacf_metric.result.show()\n\nStep 3: Residuals Analysis\nResiduals series, histogram, Q-Q and ACF plots.\n\n# Comment: How do I pass the residuals of seasonal decomponsition done before using SeasonalDecomposeMetricWithFigure?\nfrom validmind.model_validation.statsmodels.metrics import ResidualsVisualInspection\ntest_context = TestContext(train_ds=vm_train_ds)\nrvi_metric = ResidualsVisualInspection(test_context=test_context)\nrvi_metric.run()\n\n\nrvi_metric.result.show()\n\nTest if Residuals are Normaly Distributed.\n\n# Comment: How do I pass the residuals of seasonal decomponsition done before using SeasonalDecomposeMetricWithFigure?\nvm.run_test_plan(\"normality_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nTest if Residuals are Autocorrelated.\n\n# Comment: How do I pass the residuals of seasonal decomponsition done before using SeasonalDecomposeMetricWithFigure?\nvm.run_test_plan(\"autocorrelation_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nStep 4: Test for seasonality using the Augmented Dickey-Fuller (ADF) test\nStep 5: Analyze the seasonality test results\nStep 6: Interpret the results\nStep 7: Handle seasonality"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#variable-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#variable-analysis",
    "title": "ValidMind",
    "section": "4.2.4 Variable Analysis",
    "text": "4.2.4 Variable Analysis\n## A. Feature Analysis"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#a.1.-univariate-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#a.1.-univariate-analysis",
    "title": "ValidMind",
    "section": "A.1. Univariate Analysis",
    "text": "A.1. Univariate Analysis\n\nVisual Inspection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#a.2-multivariave-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#a.2-multivariave-analysis",
    "title": "ValidMind",
    "section": "A.2 Multivariave Analysis",
    "text": "A.2 Multivariave Analysis\n\nVisual Inspection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#b.-variable-selection",
    "href": "notebooks/time_series/cre_rate_model_demo.html#b.-variable-selection",
    "title": "ValidMind",
    "section": "B. Variable Selection",
    "text": "B. Variable Selection\n\nARIMA Analysis\nStep 1: Identify the Integration order (Stationarity Analysis)\nUnit Root Tests.\n\nvm.run_test_plan(\"unit_root_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nStep 2: Identify the AR order\nStep 3: Identify the MA order\n\nvm.run_test_plan(\"normality_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRun SeasonalDecomposeMetricWithFigure Test\n\ntest_context = TestContext(train_ds=vm_train_ds)\nsd_metric = SeasonalDecomposeMetricWithFigure(test_context=test_context)\nsd_metric.run()\n\nRun ResidualsVisualInspection Test\n\ntest_context = TestContext(train_ds=vm_train_ds, test_ds=vm_test_ds)\nrvi_test = ResidualsVisualInspection(test_context=test_context)\nrvi_test.run()\n\n\nvm.run_test_plan(\"seasonality_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning Metric: seasonality_detection_with_acf:  50%|█████     | 1/2 [00:02<00:02,  2.68s/it]The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n\n\nResults for autocorrelation_test_plan Test Plan:\n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        ljung_box\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 99.79377801179028, 'pvalue': 1.691207273732272e-23}, 'loan_rate_B': {'stat': 98.47643762617452, 'pvalue': 3.289150569642567e-23}, 'loan_rate_C': {'stat': 102.72120544262356, 'pvalue': 3.8579272650362314e-24}, 'loan_rate_D': {'stat': 102.06119689975492, 'pvalue': 5.383276667018511e-24}, 'FEDFUNDS': {'stat': 92.49617760172814, 'pvalue': 6.745493792730931e-22}, 'diff1_loan_rate_A': {'stat': 0.4402266706066844, 'pvalue': 0.5070130714060204}, 'diff1_loan_rate_B': {'stat': 9.03053378524947, 'pvalue': 0.0026550694107563377}, 'diff1_loan_rate_C': {'stat': 8.105512733213732, 'pvalue': 0.004413083679995846}, 'diff1_loan_rate_D': {'stat': 2.8385839077615724, 'pvalue': 0.09202528239248291}, 'diff1_FEDFUNDS': {'stat': 47.24114990634128, 'pvalue': 6.276875252119377e-12}, 'diff2_FEDFUNDS': {'stat': 2.412848114121581, 'pvalue': 0.12034323868942863}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        box_pierce\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 99.79377801179028, 'pvalue': 1.691207273732272e-23}, 'loan_rate_B': {'stat': 98.47643762617452, 'pvalue': 3.289150569642567e-23}, 'loan_rate_C': {'stat': 102.72120544262356, 'pvalue': 3.8579272650362314e-24}, 'loan_rate_D': {'stat': 102.06119689975492, 'pvalue': 5.383276667018511e-24}, 'FEDFUNDS': {'stat': 92.49617760172814, 'pvalue': 6.745493792730931e-22}, 'diff1_loan_rate_A': {'stat': 0.4402266706066844, 'pvalue': 0.5070130714060204}, 'diff1_loan_rate_B': {'stat': 9.03053378524947, 'pvalue': 0.0026550694107563377}, 'diff1_loan_rate_C': {'stat': 8.105512733213732, 'pvalue': 0.004413083679995846}, 'diff1_loan_rate_D': {'stat': 2.8385839077615724, 'pvalue': 0.09202528239248291}, 'diff1_FEDFUNDS': {'stat': 47.24114990634128, 'pvalue': 6.276875252119377e-12}, 'diff2_FEDFUNDS': {'stat': 2.412848114121581, 'pvalue': 0.12034323868942863}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        runs_test\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -8.632956969312945, 'pvalue': 5.978593166876075e-18}, 'loan_rate_B': {'stat': -9.225267842093881, 'pvalue': 2.8285309180225434e-20}, 'loan_rate_C': {'stat': -9.408682037015677, 'pvalue': 5.0239675387170514e-21}, 'loan_rate_D': {'stat': -10.196439746918763, 'pvalue': 2.056741863262086e-24}, 'FEDFUNDS': {'stat': -10.094086373225906, 'pvalue': 5.8674887432668504e-24}, 'diff1_loan_rate_A': {'stat': 0.09804933849375877, 'pvalue': 0.9218931156254718}, 'diff1_loan_rate_B': {'stat': -0.6586877630844152, 'pvalue': 0.5100962925233736}, 'diff1_loan_rate_C': {'stat': -2.8948770301798645, 'pvalue': 0.0037930709179304486}, 'diff1_loan_rate_D': {'stat': -0.5392715515560418, 'pvalue': 0.589699495478653}, 'diff1_FEDFUNDS': {'stat': -8.465639721231678, 'pvalue': 2.5475051177682628e-17}, 'diff2_FEDFUNDS': {'stat': 1.8723472602592905, 'pvalue': 0.061158576378083265}}\n                    \n                \n            \n        \n        \n        \n        \n        \n\n\nResults for normality_test_plan Test Plan:\n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        jarque_bera\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 2.848172850453067, 'pvalue': 0.24072828607015606, 'skew': 0.3894929136110148, 'kurtosis': 2.821048639289756}, 'loan_rate_B': {'stat': 7.948577904082513, 'pvalue': 0.018792659227110216, 'skew': -0.1678688191978654, 'kurtosis': 1.7076614865010464}, 'loan_rate_C': {'stat': 1.4151119572602828, 'pvalue': 0.4928472559373931, 'skew': -0.2667218461199135, 'kurtosis': 2.818765023550964}, 'loan_rate_D': {'stat': 5.805990427261884, 'pvalue': 0.05485866032647532, 'skew': -0.3686251358879733, 'kurtosis': 2.1289430194058165}, 'FEDFUNDS': {'stat': 351.77897186101717, 'pvalue': 4.094179086914819e-77, 'skew': 2.807862376508006, 'kurtosis': 9.882392761343626}, 'diff1_loan_rate_A': {'stat': 49.69819494996339, 'pvalue': 1.615005799924691e-11, 'skew': -0.292775263452962, 'kurtosis': 6.287003081958673}, 'diff1_loan_rate_B': {'stat': 84.6129819949766, 'pvalue': 4.2317929537986954e-19, 'skew': -0.9659733072874014, 'kurtosis': 6.904637635216992}, 'diff1_loan_rate_C': {'stat': 13.59037152041562, 'pvalue': 0.00...\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        kolmogorov_smirnov\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 0.08132576894776711, 'pvalue': 0.08981725287412488}, 'loan_rate_B': {'stat': 0.12791454984124828, 'pvalue': 0.0009999999999998899}, 'loan_rate_C': {'stat': 0.13865442725902158, 'pvalue': 0.0009999999999998899}, 'loan_rate_D': {'stat': 0.12708648400842287, 'pvalue': 0.0009999999999998899}, 'FEDFUNDS': {'stat': 0.4185270469008754, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_A': {'stat': 0.17590867451812156, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_B': {'stat': 0.18838563953847634, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_C': {'stat': 0.1810182591944418, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_D': {'stat': 0.16004784597113098, 'pvalue': 0.0009999999999998899}, 'diff1_FEDFUNDS': {'stat': 0.3742488536123778, 'pvalue': 0.0009999999999998899}, 'diff2_FEDFUNDS': {'stat': 0.28686393808100397, 'pvalue': 0.0009999999999998899}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        shapiro_wilk\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 0.975836992263794, 'pvalue': 0.04807629808783531}, 'loan_rate_B': {'stat': 0.9356268644332886, 'pvalue': 6.0137466789456084e-05}, 'loan_rate_C': {'stat': 0.9475300312042236, 'pvalue': 0.0003490431990940124}, 'loan_rate_D': {'stat': 0.9415098428726196, 'pvalue': 0.00014052187907509506}, 'FEDFUNDS': {'stat': 0.4598355293273926, 'pvalue': 4.666416175806635e-18}, 'diff1_loan_rate_A': {'stat': 0.8952228426933289, 'pvalue': 4.106910580503609e-07}, 'diff1_loan_rate_B': {'stat': 0.8786153793334961, 'pvalue': 7.289008863153867e-08}, 'diff1_loan_rate_C': {'stat': 0.9339078068733215, 'pvalue': 4.726010956801474e-05}, 'diff1_loan_rate_D': {'stat': 0.8760568499565125, 'pvalue': 5.656733037540107e-08}, 'diff1_FEDFUNDS': {'stat': 0.4879304766654968, 'pvalue': 1.3033836108341916e-17}, 'diff2_FEDFUNDS': {'stat': 0.59507155418396, 'pvalue': 1.008704643589071e-15}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        lilliefors_test\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 0.08132576894776711, 'pvalue': 0.08981725287412488}, 'loan_rate_B': {'stat': 0.12791454984124828, 'pvalue': 0.0009999999999998899}, 'loan_rate_C': {'stat': 0.13865442725902158, 'pvalue': 0.0009999999999998899}, 'loan_rate_D': {'stat': 0.12708648400842287, 'pvalue': 0.0009999999999998899}, 'FEDFUNDS': {'stat': 0.4185270469008754, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_A': {'stat': 0.17590867451812156, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_B': {'stat': 0.18838563953847634, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_C': {'stat': 0.1810182591944418, 'pvalue': 0.0009999999999998899}, 'diff1_loan_rate_D': {'stat': 0.16004784597113098, 'pvalue': 0.0009999999999998899}, 'diff1_FEDFUNDS': {'stat': 0.3742488536123778, 'pvalue': 0.0009999999999998899}, 'diff2_FEDFUNDS': {'stat': 0.28686393808100397, 'pvalue': 0.0009999999999998899}}\n                    \n                \n            \n        \n        \n        \n        \n        \n\n\nResults for residuals_test_plan Test Plan:\n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n        \n        \n\n\nResults for seasonality_test_plan Test Plan:\n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        seasonal_decompose\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': [{'Date': '2007-08-01', 'loan_rate_A': 7.7666666666666675, 'trend': nan, 'seasonal': -0.050284773390468364, 'resid': nan}, {'Date': '2007-09-01', 'loan_rate_A': 7.841428571428572, 'trend': nan, 'seasonal': -0.06087962072801926, 'resid': nan}, {'Date': '2007-10-01', 'loan_rate_A': 7.83, 'trend': nan, 'seasonal': 0.01749661199350169, 'resid': nan}, {'Date': '2007-11-01', 'loan_rate_A': 7.779090909090908, 'trend': nan, 'seasonal': -0.047258378330469565, 'resid': nan}, {'Date': '2007-12-01', 'loan_rate_A': 7.695833333333333, 'trend': nan, 'seasonal': 0.08505178146324885, 'resid': nan}, {'Date': '2008-01-01', 'loan_rate_A': 7.961333333333333, 'trend': nan, 'seasonal': 0.06564185816692848, 'resid': nan}, {'Date': '2008-02-01', 'loan_rate_A': 8.130333333333333, 'trend': 8.005048767959094, 'seasonal': 0.008943337297934253, 'resid': 0.11634122807630445}, {'Date': '2008-03-01', 'loan_rate_A': 8.126285714285714, 'trend': 8.036669799705125, 'seasonal': -0.002099404440702811, 'resid': 0.09171531902129207},...\n                    \n                \n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        seasonality_detection_with_acf\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'acf_values': array([ 1.        ,  0.95235645,  0.89628237,  0.83365556,  0.78298876,\n        0.73759763,  0.68398844,  0.62657655,  0.56468994,  0.50331047,\n        0.42183716,  0.34023758,  0.25859584,  0.18191055,  0.11034449,\n        0.04315845, -0.0184955 , -0.0788836 , -0.12609771, -0.17004327,\n       -0.21018971, -0.24498837, -0.28174988, -0.31377348, -0.33500256,\n       -0.3343806 , -0.32206274, -0.30733837, -0.29602269, -0.28022519,\n       -0.26123446, -0.23725676, -0.22539932, -0.20575443, -0.18358424,\n       -0.16270934, -0.13889279, -0.11922772, -0.09506086, -0.06371048,\n       -0.03221894]), 'pacf_values': array([ 1.00000000e+00,  9.61340944e-01, -1.42725992e-01, -1.13803080e-01,\n        1.45601541e-01,  1.12916226e-02, -2.01201298e-01, -4.90737660e-02,\n       -4.91281155e-02, -7.22772152e-02, -4.00627990e-01, -1.93018777e-02,\n       -5.08939592e-03, -1.57441937e-01, -1.01953152e-01,  9.65840221e-02,\n        2.09312300e-02, -1.34464258e-01,  1.90609883e-01,  1.59998039e-01,\n     ...\n                    \n                \n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots"
  },
  {
    "objectID": "notebooks/intro-r.html",
    "href": "notebooks/intro-r.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "notebooks/intro-r.html#initializing-the-validmind-library",
    "href": "notebooks/intro-r.html#initializing-the-validmind-library",
    "title": "ValidMind",
    "section": "Initializing the ValidMind Library",
    "text": "Initializing the ValidMind Library\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\nvm.init(\n      api_host = \"http://localhost:3000/api/v1/tracking\",\n      api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n      api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n      project = \"clfmztk8t0000qvoo2wh30ex7\"\n)\n\nTrue\n\n\n\nUsing a demo dataset\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\nRunning a dataset evaluation test plan\nWe will now run a basic tabular dataset test plan that will compute and log the description, metrics and tests for the dataset. For now, ValidMind supports metrics and tests for a single dataset, so we first will combine the test and train sets into one to get complete datapoints.\n\n## load the train and test dataframes\n# assume that the train and test datasets have been saved to disk as csv files\n# see the r-customer-churn-model.ipynb notebook for details on how this can be done\ndf_train = pd.read_csv(\"r_demo/r_churn_train.csv\")\ndf_test = pd.read_csv(\"r_demo/r_churn_test.csv\")\n\nvm_train_ds = vm.init_dataset(dataset=df_train, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=df_test, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n# run dataset tests on df_train and df_test combined\nvm_dataset_combined = vm.init_dataset(\n    dataset=pd.concat([df_train, df_test]),\n    type=\"generic\",\n    target_column=\"Exited\",\n)\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset_combined)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                                                                                                 \n\n\n\nResults for tabular_dataset_description Test Plan:Logged the following dataset to the ValidMind platform:\n  \n    \n      \n      GeographyFrance\n      GeographyGermany\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.00000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      0.50125\n      0.251125\n      0.549500\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      0.50003\n      0.433687\n      0.497575\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      0.00000\n      0.000000\n      0.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      0.00000\n      0.000000\n      0.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      1.00000\n      0.000000\n      1.000000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      1.00000\n      1.000000\n      1.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      1.00000\n      1.000000\n      1.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        generic\n                    \n                \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for tabular_data_quality Test Plan:\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=False, values={'correlations': [{'column': 'GeographyGermany', 'correlation': -0.5805318438630581}]}), TestResult(test_name=None, column='GeographyGermany', passed=False, values={'correlations': [{'column': 'Balance', 'correlation': 0.4062612295669473}]}), TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389454}]})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.024522142979951}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.0076920437747027195}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111807}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077725})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\n\nRunning a model evaluation test plan for our LogisticRegression R model\nWe will now run a basic model evaluation test plan that is compatible with the R model we will be loading.\n\n## load the model\n# assume that the model has been saved to disk as a serialized R object (.rds)\n# see the r-customer-churn-model.ipynb notebook for details on how this can be done\n# model_type must be passed for R models:\n# Currently, LogisticRegression, LinearRegression (glm and lm in R) XGBClassifier and XGBRegressor are supported\nvm_model = vm.init_r_model(\"r_demo/r_log_reg_churn_model.rds\", model_type=\"LogisticRegression\")\n\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n                                                                                                                                                                                                                 \n\n\nResults for sklearn_classifier_metrics Test Plan:\n        Logged the following model to the ValidMind platform:\n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.8604166666666667\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.5743329097839899\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7687074829931972\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.45841784989858014\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7113798740840568\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'GeographyFrance': 2e-06, 'GeographyGermany': 5e-05, 'Gender': 7e-06, 'Age': 0.000446, 'Tenure': 0.000397, 'Balance': 0.001007, 'NumOfProducts': 0.000273, 'HasCrCard': 0.000234, 'IsActiveMember': 4.2e-05, 'EstimatedSalary': 0.000501}\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial   new  percent_new       psi\nbin                                                       \n1       2840          0.50714  1218     0.507500  0.000000\n2       1155          0.20625   474     0.197500  0.000379\n3        464          0.08286   218     0.090833  0.000733\n4        301          0.05375   126     0.052500  0.000029\n5        172          0.03071    72     0.030000  0.000017\n6        174          0.03107    73     0.030417  0.000014\n7        116          0.02071    57     0.023750  0.000415\n8        121          0.02161    54     0.022500  0.000036\n9        152          0.02714    67     0.027917  0.000022\n10       105          0.01875    41     0.017083  0.000155\n                \n            \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for sklearn_classifier_validation Test Plan:\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.72125, 'threshold': 0.7})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.14993646759847523, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.49822262593987565, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=True, values={'test_score': 0.72125, 'train_score': 0.7255357142857143, 'degradation': 0.005906965296578953}), TestResult(test_name='precision', column=None, passed=False, values={'test_score': 0.20068027210884354, 'train_score': 0.19011976047904192, 'degradation': -0.05554662810005893}), TestResult(test_name='recall', column=None, passed=False, values={'test_score': 0.11967545638945233, 'train_score': 0.11308993766696349, 'degradation': -0.05823257893980294}), TestResult(test_name='f1', column=None, passed=False, values={'test_score': 0.14993646759847523, 'train_score': 0.14182021217197097, 'degradation': -0.05722918688531157})]"
  },
  {
    "objectID": "notebooks/test_plan_summmary.html",
    "href": "notebooks/test_plan_summmary.html",
    "title": "ValidMind",
    "section": "",
    "text": "import validmind as vm\n\nvm.init(\n    api_host = \"http://localhost:3000/api/v1/tracking\",\n    api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n    api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n    project = \"cleytvf7i0000w5oo2a9ygkmi\"\n)\n\nTrue\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n                                                                                                                                                                                                                 \n\n\n\nResults for tabular_dataset_description Test Plan:Logged the following dataset to the ValidMind platform:\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        training\n                    \n                \n            \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for tabular_data_quality Test Plan:Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Surname', passed=False, values={'n_distinct': 2616, 'p_distinct': 0.327}), TestResult(test_name=None, column='Geography', passed=True, values={'n_distinct': 3, 'p_distinct': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389458}]})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CustomerId', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Geography', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'skewness': -0.005920679739677088}), TestResult(test_name=None, column='CustomerId', passed=True, values={'skewness': 0.010032280260684402}), TestResult(test_name=None, column='CreditScore', passed=True, values={'skewness': -0.06195161237091896}), TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.0245221429799511}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.007692043774702702}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111804}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077728})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='CustomerId', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_unique': 2616, 'p_unique': 0.327}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_unique': 452, 'p_unique': 0.0565}), TestResult(test_name=None, column='Geography', passed=True, values={'n_unique': 3, 'p_unique': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.853125\n\n\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning SHAPGlobalImportance: shap:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 12/13 [00:00<00:00, 33.23it/s]ntree_limit is deprecated, use `iteration_range` or model slicing instead.\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n                                                                                                                                                                                                                 \n\n\nResults for sklearn_classifier_metrics Test Plan:Logged the following model to the ValidMind platform:\n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.879375\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.6053169734151329\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.783068783068783\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.49333333333333335\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7308974358974359\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'Gender': 1.4e-05, 'Age': 0.000816, 'Tenure': 0.000625, 'Balance': 0.000977, 'NumOfProducts': 2.6e-05, 'HasCrCard': 2e-06, 'IsActiveMember': 6.3e-05, 'EstimatedSalary': 0.000863, 'Geography_France': 0.00021, 'Geography_Germany': 2.9e-05, 'Geography_Spain': 0.000131}\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial  new  percent_new       psi\nbin                                                      \n1       2607         0.543125  858        0.536  0.000088\n2        748         0.155833  253        0.158  0.000033\n3        398         0.082917  148        0.092  0.001048\n4        267         0.055625   97        0.061  0.000430\n5        177         0.036875   58        0.036  0.000011\n6        118         0.024583   48        0.030  0.001079\n7        101         0.021042   24        0.015  0.002045\n8         89         0.018542   31        0.019  0.000037\n9        109         0.022708   24        0.015  0.003197\n10       186         0.038750   59        0.037  0.000093\n                \n            \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for sklearn_classifier_validation Test Plan:Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.733125, 'threshold': 0.7})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.12678936605316973, 'threshold': 0.5})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.49089743589743584, 'threshold': 0.5})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=False, values={'test_score': 0.733125, 'train_score': 0.7170833333333333, 'degradation': -0.022370714700755467}), TestResult(test_name='precision', column=None, passed=True, values={'test_score': 0.164021164021164, 'train_score': 0.18407960199004975, 'degradation': 0.10896610896610899}), TestResult(test_name='recall', column=None, passed=True, values={'test_score': 0.10333333333333333, 'train_score': 0.11361310133060389, 'degradation': 0.09048048048048046}), TestResult(test_name='f1', column=None, passed=True, values={'test_score': 0.12678936605316973, 'train_score': 0.14050632911392405, 'degradation': 0.09762523259455776})]"
  },
  {
    "objectID": "notebooks/log_image.html",
    "href": "notebooks/log_image.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialize ValidMind SDK\nimport validmind as vm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(os.getcwd())\n\n/Users/panchicore/www/validmind/validmind-sdk\n\n\n\n\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0\n\n\n\n\n\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  }
]