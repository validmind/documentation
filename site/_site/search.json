[
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html",
    "href": "notebooks/how_to/implementing_custom_tests.html",
    "title": "Implementing Custom Metrics and Threshold Tests",
    "section": "",
    "text": "Custom metrics offer added flexibility by extending the default metrics provided by ValidMind, enabling you to document any type of model or use case. Both metrics and threshold tests assess models but they differ in approach: metrics measure a range of dataset or model behaviors, while threshold tests yield a pass or fail result based on specific criteria. These instructions include the code required to:"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#before-you-begin",
    "href": "notebooks/how_to/implementing_custom_tests.html#before-you-begin",
    "title": "Implementing Custom Metrics and Threshold Tests",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nTo access the ValidMind Platform UI, you’ll need an account.\nSigning up is FREE — Create your account.\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#install-the-client-library",
    "href": "notebooks/how_to/implementing_custom_tests.html#install-the-client-library",
    "title": "Implementing Custom Metrics and Threshold Tests",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#initialize-the-client-library",
    "href": "notebooks/how_to/implementing_custom_tests.html#initialize-the-client-library",
    "title": "Implementing Custom Metrics and Threshold Tests",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\nCreate a metric class signature\nIn order to implement a custom metric or threshold test, you must create a class that inherits from the Metric or ThresholdTest class. The class signatures below show the different methods that need to be implemented in order to provide the required documentation elements:\n@dataclass\nclass ExampleMetric(Metric):\n    name = \"mean_of_values\"\n\n    # Markdown compatible description of the metric\n    def description(self):\n\n    # Code to compute the metric and cache its results and Figures\n    def run(self):\n\n    # Code to build a list of ResultSummaries that form the results tables\n    def summary(self, metric_values):\nWe’ll now implement a sample metric to illustrate their different documentation components.\n\n\nImplement a custom metric\nThe following example shows how to implement a custom metric that calculates the mean of a list of numbers.\n\nBasic metric implementation\nAt its most basic, a metric implementation requires a run() method that computes the metric and caches its results and Figures. The run() method is called by the ValidMind client when the metric is executed. The run() should return any value that can be serialized to JSON.\nIn the example below we also provide a simple description for the metric:\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        return self.cache_results(mean)\n\n\n\nTest the custom metric\nWe should run a metric first without running an entire test suite and test its behavior.\nThe only requirement to run a metric is build a TestContext object and pass it to the metric initializer. Test context objects allow metrics and tests to access data inside their class methods in a predictable way. By default, ValidMind provides support for the following special keys in a test context objects:\n\ndataset\nmodel\nmodels\n\nWhen a test context object is build with one of these keys, the corresponding value is automatically added to the object as an attribute. For example, if you build a test context object with the dataset key, you can access the dataset inside the metric’s run() method as self.dataset. We’ll illustrate this in detail in the next section.\nIn our simple example, we don’t need to pass any arguments to the TestContext initializer.\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\nYou can also inspect the results of the metric by accessing the result variable:\n\nmean_metric.result.show()\n\n\n\n\nAdd a summary() method to the custom metric\nThe summary() method is used to build a ResultSummary object that can display the results of our test as a list of one or more summray tables. The ResultSummary class takes a results argument that is a list of ResultTable objects.\nEach ResultTable object is composed of a data and metadata attribute. The data attribute is any valid Pandas tabular DataFrame and metadata is a ResultTableMetadata instance that takes title as the table description.\n\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata\n\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def summary(self, metric_value):\n        # Create a dataframe structure that can be rendered as a table\n        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n\n        return ResultSummary(\n            results=[\n                ResultTable(\n                    data=simple_df,\n                    metadata=ResultTableMetadata(title=\"Example Table\"),\n                ),\n            ]\n        )\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        return self.cache_results(mean)\n\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\n\nmean_metric.result.show()\n\n\n\nAdd figures to a metric\nYou can also add figures to a metric by passing a figures list to cache_results(). Each figure is a Figure object that takes the following arguments:\n\nfor_object: The name of the object that the figure is for. Usually defaults to self\nfigure: A Matplotlib or Plotly figure object\nkey: A unique key for the figure\n\nThe developer framework uses for_object and key to link figures to the corresponding metric or test.\n\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom validmind.vm_models import Figure, Metric, ResultSummary, ResultTable, ResultTableMetadata\n\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def summary(self, metric_value):\n        # Create a dataframe structure that can be rendered as a table\n        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n\n        return ResultSummary(\n            results=[\n                ResultTable(\n                    data=simple_df,\n                    metadata=ResultTableMetadata(title=\"Example Table\"),\n                ),\n            ]\n        )\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n\n        figure = Figure(\n            for_object=self,\n            key=self.key,\n            figure=fig\n        )\n\n        return self.cache_results(mean, figures=[figure])\n\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\n\nmean_metric.result.show()\n\n\nfrom validmind.vm_models import TestPlan\n\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test suite\n    \"\"\"\n\n    name = \"my_custom_test_suite\"\n    required_inputs = []\n    tests = [MeanMetric]\n\n\nmy_custom_test_suite = MyCustomTestPlan(config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n})\nresults = my_custom_test_suite.run()"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html",
    "href": "notebooks/how_to/run_a_test.html",
    "title": "Running an Individual Test",
    "section": "",
    "text": "This notebook shows how to run individual metrics or thresholds tests that is part of the ValidMind Developer Framework. These instructions include the code required to:"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#before-you-begin",
    "href": "notebooks/how_to/run_a_test.html#before-you-begin",
    "title": "Running an Individual Test",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#install-the-client-library",
    "href": "notebooks/how_to/run_a_test.html#install-the-client-library",
    "title": "Running an Individual Test",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_a_test.html#initialize-the-client-library",
    "title": "Running an Individual Test",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"...\",\n  api_secret = \"...\",\n  project = \"...\"\n)\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "title": "Running an Individual Test",
    "section": "Load the demo dataset",
    "text": "Load the demo dataset\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\n# You can also import customer_churn like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the raw dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "title": "Running an Individual Test",
    "section": "Train a model for testing",
    "text": "Train a model for testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#set-up-test-inputs-and-run-the-test",
    "href": "notebooks/how_to/run_a_test.html#set-up-test-inputs-and-run-the-test",
    "title": "Running an Individual Test",
    "section": "Set up test inputs and run the test",
    "text": "Set up test inputs and run the test\n\nInitialize ValidMind objects\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the test\nIndividual tests can be easily run by calling the run_test function provided by the validmind.tests module. The function takes the following arguments:\n\ntest_id: The ID of the test to run. To find a particular test and get its, refer to the explore_tests.ipynb notebook\nparams: A dictionary of parameters for the test. These will override any default_params set in the test definition. Refer to the explore_tests.ipynb notebook to find the default parameters for a test.\n\nYou can then pass in any inputs for the test as keyword arguments. Most likely, these will be dataset and model objects. Again, you may refer to the explore_tests.ipynb notebook to find the required inputs for a test.\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.model_validation.sklearn.TrainingTestDegradation\",\n    params={}, # can be used to set overrides to the test's default parameters\n    model=vm_model,\n)\n\n\n\nLog the test results to ValidMind\nAfter the test has been run, you can save the results to ValidMind by calling the log method of the test object returned after running the test:\n\ntest.log()"
  },
  {
    "objectID": "notebooks/how_to/configure_parameters_demo.html",
    "href": "notebooks/how_to/configure_parameters_demo.html",
    "title": "Configure Parameters for a Specific Test",
    "section": "",
    "text": "This notebook guides model developers through using a simple classification model for bank customer churn dataset. It shows you how to set up the ValidMind Developer Framework and guide you through documenting a model using the ValidMind Developer framework. It shows how user can configure parameters for a test or set of tests in a specific section of document. For this simple demonstration, we use a bank customer churn dataset from Kaggle (https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data).\nWe will train a sample model and demonstrate the following documentation functionalities:"
  },
  {
    "objectID": "notebooks/how_to/configure_parameters_demo.html#before-you-begin",
    "href": "notebooks/how_to/configure_parameters_demo.html#before-you-begin",
    "title": "Configure Parameters for a Specific Test",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\nInstall ValidMind Developer Framework\n\n%pip install -q validmind\n\n\n\nInitializing the Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/how_to/configure_parameters_demo.html#initialize-the-client-library",
    "href": "notebooks/how_to/configure_parameters_demo.html#initialize-the-client-library",
    "title": "Configure Parameters for a Specific Test",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace the code below with the code snippet from your project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\nPreview the model’s documentation template\nAll models are assigned a documentation template when registered. The template defines a list of sections that are used to document the model. Each section can contain any number of rich text and test driven blocks that populate the documentation. Test driven blocks are populated by running tests against the model.\nWe can preview the model documentation template for this project by running the following code:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/configure_parameters_demo.html#load-the-demo-dataset",
    "href": "notebooks/how_to/configure_parameters_demo.html#load-the-demo-dataset",
    "title": "Configure Parameters for a Specific Test",
    "section": "Load the demo dataset",
    "text": "Load the demo dataset\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\n\n\nDocumenting the model\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the raw dataset\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nWe can now initialize the training and test datasets into dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    target_column=demo_dataset.target_column\n)\n\nWe also initialize a model object using vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the template documentation suite\nWe are now ready to run the model’s documentation tests as defined in its template. The following function runs every test in the template and sends all documentation artifacts to the ValidMind platform.\n\nfull_suite = vm.run_documentation_tests(\n    dataset=vm_dataset,\n    model=vm_model\n)\n\n\n\nConfiguration of parameters for model diagnosis tests\nEach test has its default parameters and their values depending on the use case you are trying to solve. ValidMind’s developer framework exposes these parameters at the user level so that they can be adjusted based on requirements.\nThe config can be applied to a specific test to override the default configuration parameters.\nThe format of the config is:\nconfig = {\n    \"&lt;test1_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n     \"&lt;test2_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n}\nUsers can input the configuration to run_documentation_tests() and run_test_suite() using config, allowing fine-tuning the suite according to the specific configuration requirements.\n\nconfig = {\n    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n        \"cut_off_percentage\": 3,\n        \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n    },\n    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n        \"features_columns\": [\"Age\", \"Balance\"],\n        \"accuracy_gap_threshold\": 85,\n    },\n    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n        \"features_columns\": [\"Balance\", \"Tenure\"],\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        \"accuracy_decay_threshold\": 4,\n    }\n}\n\nfull_suite = vm.run_documentation_tests(\n    dataset=vm_dataset,\n    model=vm_model,\n    section=\"model_diagnosis\",\n    config=config,\n)\n\n\n\n\nNext steps\nYou can look at the results of this test plan right in the notebook where you ran the code, as you would expect. But there is a better way: view the test results as part of your model documentation right in the ValidMind Platform UI:\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand ** Model Development**\nWhat you can see now is a more easily consumable version of the model diagnosis tests you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at  How do I use the framework? ."
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "",
    "text": "This interactive notebook guides you through the process of documenting a model with the ValidMind Developer Framework. It uses the California Housing Price Prediction sample dataset from Sklearn to train a simple regression model.\nAs part of the notebook, you will learn how to train a sample model while exploring how the documentation process works:"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#validmind-at-a-glance",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, you can make use of the following resources alongside this notebook:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#before-you-begin",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#before-you-begin",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#install-the-client-library",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#install-the-client-library",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-client-library",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-client-library",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace this placeholder with the code snippet from your own project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-python-environment",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-python-environment",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#load-the-sample-dataset",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#load-the-sample-dataset",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nTo be able to use a sample dataset, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\nInitialize a ValidMind dataset object\nBefore you can run a test suite, which are just a collection of tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to analyze\ntarget_column — the name of the target column in the dataset\n\n\nfrom validmind.datasets.regression import california_housing as demo_dataset\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\n\nfull_suite = vm.run_test_suite(\n    \"tabular_dataset\",\n    dataset=vm_dataset,\n)"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#document-the-model",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#document-the-model",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\nPrepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\nInitialize RandomForestRegressor regressor: Creates an RandomForestRegressor object with random state set to 0.\nSet evaluation metrics: Specifies metrics for model evaluation as “errors” and “r2”.\nFit the model: Trains the model on x_train and y_train using the validation set (x_val, y_val). Verbose output is disabled.\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nHere we create two regression models so that the performance of the model can be compared through ValidMind test suite.\n\nscale = False\nif scale:\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_val = scaler.fit_transform(x_val)\n\nmodel = RandomForestRegressor(random_state=0)\nmodel.fit(x_train, y_train)\ns1 = model.score(x_train, y_train)\ns2 = model.score(x_val, y_val)\nprint(\"R² of Support Vector Regressor on training set: {:.3f}\".format(s1))\nprint(\"R² of Support Vector Regressor on test set: {:.3f}\".format(s2))\n\nmodel_1 = GradientBoostingRegressor(random_state=0, max_depth=4)\nmodel_1.fit(x_train, y_train)\nmodel1_s1 = model_1.score(x_train, y_train)\nmodel1_s2 = model_1.score(x_val, y_val)\nprint(\n    \"R² of Support Gradient Boosting Regressor on training set: {:.3f}\".format(model1_s1))\nprint(\"R² of Support Gradient Boosting Regressor on test set: {:.3f}\".format(model1_s2))\n\n\n\nInitialize the training and test datasets\nWith the datasets ready, you can now initialize the training and test datasets (train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    target_column=demo_dataset.target_column\n)\n\n\n\nInitialize a model object\nAdditionally, you need to initialize a ValidMind model objects (vm_model and vm_model_1) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\nvm_model_1 = vm.init_model(\n    model_1,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the full suite of tests\nThis is where it all comes together: you are now ready to run the documentation tests for the model as defined by the documentation template you looked at earlier.\nThe vm.run_test_suite function finds and runs every tests specified in the test suites and then uploads all the documentation and test artifacts that get generated to the ValidMind AI Risk Platform.\nThe function takes two arguments:\n\ndataset: The data to be tested, specified as vm_dataset.\nmodel: The candidate model to be used for testing, specified as vm_model. -models: The list of models that can be compare with candidate model.\n\nThe variable full_suite then holds the result of these tests.\n\nfull_suite_model_performance = vm.run_test_suite(\n    \"regression_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model,\n    models=[vm_model_1]\n)"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#next-steps",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#next-steps",
    "title": "Quickstart for California Housing Regression Model Documentation — Full Suite",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Documentation.\nExpand the following sections and take a look around:\n\n2. Data Preparation\n3. Model Development\n\n\nWhat you can see now is a much more easily consumable version of the documentation, including the results of the tests you just performed, along with other parts of your documentation project that still need to be completed. There is a wealth of information that gets uploaded when you run the full test suite, so take a closer look around, especially at test results that might need attention (hint: some of the tests in 2.1 Data description look like they need some attention).\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html",
    "title": "Summarization of Financial Data Using a Large Language Model (LLM)",
    "section": "",
    "text": "This notebook shows model developers how to document a large language model (LLM) using the ValidMind Developer Framework. The use case is a summarization of financial news based on a dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. It shows you how to set up the ValidMind Developer Framework, initialize the client library, and load the dataset, followed by running the model validation tests provided by the framework to quickly generate documentation about the data and model."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#before-you-begin",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#before-you-begin",
    "title": "Summarization of Financial Data Using a Large Language Model (LLM)",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nThis notebook requires an OpenAI API secret key to run. If you don’t have one, visit API keys on OpenAI’s site to create a new key for yourself. Note that API usage charges may apply.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#validmind-at-a-glance",
    "title": "Summarization of Financial Data Using a Large Language Model (LLM)",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#install-the-client-library",
    "title": "Summarization of Financial Data Using a Large Language Model (LLM)",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#initialize-the-client-library",
    "title": "Summarization of Financial Data Using a Large Language Model (LLM)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Go to Documentation Projects and click Create new project.\nSelect [Demo] Foundation Model - Text Summarization and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace the code below with the code snippet from your project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nHelper functions\nLet’s define the following functions to help visualize datasets with long text fields:\n\nimport textwrap\n\nfrom IPython.display import display, HTML\nfrom tabulate import tabulate\n\n\ndef _format_cell_text(text, width=50):\n    \"\"\"Private function to format a cell's text.\"\"\"\n    return '\\n'.join([textwrap.fill(line, width=width) for line in text.split('\\n')])\n\n\ndef _format_dataframe_for_tabulate(df):\n    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n    df_out = df.copy()\n\n    # Format all string columns\n    for column in df_out.columns:\n        # Check if column is of type object (likely strings)\n        if df_out[column].dtype == object:\n            df_out[column] = df_out[column].apply(_format_cell_text)\n    return df_out\n\n\ndef _dataframe_to_html_table(df):\n    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n    headers = df.columns.tolist()\n    table_data = df.values.tolist()\n    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n\n\ndef display_formatted_dataframe(df, num_rows=None):\n    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n    if num_rows is not None:\n        df = df.head(num_rows)\n    formatted_df = _format_dataframe_for_tabulate(df)\n    html_table = _dataframe_to_html_table(formatted_df)\n    display(HTML(html_table))\n\n\n\nLoad the dataset\nThe CNN Dailymail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail (https://huggingface.co/datasets/cnn_dailymail). The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n\nfrom datasets import load_dataset\n\ncnn_dataset = load_dataset('cnn_dailymail', '3.0.0')\ntrain_df = cnn_dataset.data['train'].to_pandas()\nval_df = cnn_dataset.data['validation'].to_pandas()\ntest_df = cnn_dataset.data['test'].to_pandas()\ntrain_df = train_df[['article', 'highlights']]\ntrain_df = train_df.head(20)\n\ndisplay_formatted_dataframe(train_df, num_rows=5)\n\n\n\nGet ready to run the analysis\nImport the ValidMind FoundationModel and Prompt classes needed for the sentiment analysis later on:\n\nfrom validmind.models import FoundationModel, Prompt\n\nCheck your access to the OpenAI API:\n\nimport os\n\nimport dotenv\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nimport openai\n\n\ndef call_model(prompt):\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n    ).choices[0].message[\"content\"]\n\nSet the prompt guidelines for the sentiment analysis:\n\nprompt_template = \"\"\"\nYou are an AI with expertise in summarizing financial news.\nYour task is to provide a concise summary of the specific news article provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the article.\n\nArticle to Summarize:\n\n```\n{article}\n```\n\nPlease respond with a concise summary of the article's main points.\nEnsure that your summary is based on the content of the article and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"article\"]\n\n\ndf_test = train_df.head(10)\n\nvm_test_ds = vm.init_dataset(\n    dataset=df_test,\n    text_column=\"article\",\n    target_column=\"highlights\",\n)\n\nvm_model = FoundationModel(\n    predict_fn=call_model,\n    prompt=Prompt(\n        template=prompt_template,\n        variables=prompt_variables,\n    ),\n    test_ds=vm_test_ds,\n)\n\n\n\nRun model validation tests\nIt’s possible to run a subset of tests on the documentation template by passing a section parameter to run_documentation_tests(). Let’s run the tests that evaluate the model’s overall performance (including summarization metrics), by selecting the “model development” section of the template:\n\nconfig = {\n    \"rouge_metric\": {\n        \"rouge_metrics\": [\"rouge-1\", \"rouge-2\", \"rouge-l\"],\n    },\n}\n\nsummarization_results = vm.run_documentation_tests(\n    section=\"model_development\",\n    model=vm_model,\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#next-steps",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_summarization_demo.html#next-steps",
    "title": "Summarization of Financial Data Using a Large Language Model (LLM)",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "",
    "text": "This notebook guides model developers through the process of using a large language model (LLM) specialized in sentiment analysis for financial news. It shows you how to set up the ValidMind Developer Framework, initializes the client library, and uses a specific prompt template for analyzing the sentiment of sentences in a dataset. The notebook also includes example data to test the model’s ability to correctly identify sentiment as positive, negative, or neutral."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#validmind-at-a-glance",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#before-you-begin",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#before-you-begin",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nThis notebook requires an OpenAI API secret key to run. If you don’t have one, visit API keys on OpenAI’s site to create a new key for yourself. Note that API usage charges may apply.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#install-the-client-library",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#initialize-the-client-library",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Go to Documentation Projects and click Create new project.\nSelect [Demo] Foundation Model - Text Sentiment Analysis and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace the code below with the code snippet from your project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key = \"...\",\n    api_secret = \"...\",\n    project = \"...\"\n)\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nDownload the test dataset\nTo perform the sentiment analysis for financial news, you need a sample dataset:\n\nDownload the sample dataset from https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\nThis dataset contains two columns, Sentiment and News Headline. The sentiment can be negative, neutral or positive.\nMove the CSV file that contains the dataset into the current directory.\n\n\n\nGet ready to run the analysis\nImport the ValidMind FoundationModel and Prompt classes needed for the sentiment analysis later on:\n\nfrom validmind.models import FoundationModel, Prompt\n\nCheck your access to the OpenAI API:\n\nimport os\n\nimport dotenv\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nimport openai\n\ndef call_model(prompt):\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n    ).choices[0].message[\"content\"]\n\nSet the prompt guidelines for the sentiment analysis:\n\nprompt_template = \"\"\"\nYou are an AI with expertise in sentiment analysis, particularly in the context of financial news.\nYour task is to analyze the sentiment of a specific sentence provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the sentence.\n\nSentence to Analyze:\n```\n{Sentence}\n```\n\nPlease respond with the sentiment of the sentence denoted by one of either 'positive', 'negative', or 'neutral'.\nPlease respond only with the sentiment enum value. Do not include any other text in your response.\n\nNote: Ensure that your analysis is based on the content of the sentence and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"Sentence\"]\n\nGet your sample dataset ready for analysis:\n\nimport pandas as pd\n\ndf = pd.read_csv('./datasets/sentiments.csv')\n\ndf_test = df[:10].reset_index(drop=True)\ndf_test"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#run-the-model-documentation-tests",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#run-the-model-documentation-tests",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "Run the model documentation tests",
    "text": "Run the model documentation tests\nFirst, use the ValidMind Developer Framework to initialize the dataset and model objects necessary for documentation. The ValidMind predict_fn function allows the model to be tested and evaluated in a standardized manner:\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=df_test,\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\nvm_model = FoundationModel(\n    predict_fn=call_model,\n    prompt=Prompt(\n        template=prompt_template,\n        variables=prompt_variables,\n    ),\n    test_ds=vm_test_ds,\n)\n\nNext, use the ValidMind Developer Framework to run validation tests on the model. The vm.run_documentation_tests function analyzes the current project’s documentation template and collects all the tests associated with it into a test suite.\nThe function then runs the test suite, logs the results to the ValidMind API and displays them to you.\n\ntest_suite = vm.run_documentation_tests(\n    model=vm_model,\n    dataset=vm_dataset,\n)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#next-steps",
    "href": "notebooks/code_samples/LLM_and_NLP/foundation_models_integration_demo.html#next-steps",
    "title": "Sentiment Analysis of Financial Data Using a Large Language Model (LLM)",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "",
    "text": "This notebook introduces model developers to documenting a natural language processing (NLP) model with the ValidMind Developer Framework. The use case is sentiment analysis of COVID-19-related tweets, categorized as positive or negative. The model employs binary text classification using the CatBoost library. The notebook guides you through setting up the ValidMind Developer Framework, initializing the client library, and loading a sample dataset for training. It then runs the framework’s model validation tests to generate documentation on the data and model."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#validmind-at-a-glance",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#before-you-begin",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#before-you-begin",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#install-the-client-library",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#initialize-the-client-library",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Go to Documentation Projects and click Create new project.\n\n\n\nSelect NLP-based Text Classification and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\nˇ\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#explorary-data-analysis-of-covid-19-tweets-data",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#explorary-data-analysis-of-covid-19-tweets-data",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "1. Explorary data analysis of COVID-19 tweets data",
    "text": "1. Explorary data analysis of COVID-19 tweets data\nThe emphasis in this section is on the in-depth analysis and preprocessing of the text data (tweets). In this section, we introduce the manually tagged COVID-19 tweets, which range from Highly Negative to Highly Positive, representing five distinct classes. In this Exploratory Data Analysis (EDA), these five classes will be simplified to two classes: Positive and Negative.\n\nInitialize the Python environment\nNext, let’s initialize the environment and imports libraries for data manipulation, machine learning, and plotting, followed by configuring PyTorch:\n\n%set_env PYTORCH_MPS_HIGH_WATERMARK_RATIO 0.8\n\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\n\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\ndevice = \"cpu\"\n\ntrain_model = True\n\n\n\nLoad COVID-19 tweets data\n\nfrom validmind.datasets.nlp import twitter_covid_19 as demo_data\ndf = demo_data.load_data()\ndf.head(10)\n\n\n\nRun text data quality test suite\nIn this section, we use the ValidMind Developer Framework to run various data quality checks on the dataset, and send the results to the model document on the ValidMind Platform UI:\n\nvm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='OriginalTweet', target_column=\"Sentiment\")\n\n\nconfig = {\n    \"class_imbalance\":{\"min_percent_threshold\": 3}\n}\ntext_data_test_suite = vm.run_test_suite(\"text_data_quality\",\n                                       dataset=vm_ds,\n                                       config=config)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#preprocess-data",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#preprocess-data",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "2. Preprocess data",
    "text": "2. Preprocess data\n\nHandle class bias\nOne way to handle class bias is to merge a specific class data with related class. Here, we copy the text and class lables in separate columns so that the original text is also there for comparison:\n\nprint(\"Original Classes:\", df.Sentiment.unique())\n\ndf['text'] = df.OriginalTweet\ndf[\"text\"] = df[\"text\"].astype(str)\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"positive\"\n    elif x == \"Extremely Negative\":\n        return \"negative\"\n    elif x == \"Negative\":\n        return \"negative\"\n    elif x ==  \"Positive\":\n        return \"positive\"\n    else:\n        return \"neutral\"\n\ndf['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\ntarget=df['sentiment']\n\nprint(df.sentiment.value_counts(normalize= True))\nprint(\"Modified Classes:\", df.sentiment.unique())\n\n\n\nRemove sentiments that are neutral\n\ndf = df[df[\"sentiment\"] != \"neutral\"]\nprint(df.sentiment.unique())\nprint(df.sentiment.value_counts(normalize= True))\nprint(df.shape)\n\n\ndf\n\n\n\nRemove URLs and HTML links\n\nimport re\n\ndef remove_urls(text):\n    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\n\ndf['text']=df['text'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'&lt;.*?&gt;')\n    return html.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x:remove_html(x))\n\n\n\nConvert text to lower case\n\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['text']=df['text'].apply(lambda x:lower(x))\n\n\n\nRemove numbers\n\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['text']=df['text'].apply(lambda x:remove_num(x))\n\n\n\nRemove stopwords\n\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['text']=df['text'].apply(lambda x:remove_stopwords(x))\n\n\n\nRemove punctuation\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['text']=df['text'].apply(lambda x:punct_remove(x))\n\n\n\nRemove mentions\n\ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['text']=df['text'].apply(lambda x:remove_mention(x))\n\n\n\nRemove hashtags\n\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['text']=df['text'].apply(lambda x:remove_hash(x))\n\n\n\nRemove extra whitespace left while removing other text\n\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['text']=df['text'].apply(lambda x:remove_space(x))\n\n\ndf\n\n\n\nRun text data quality tests again\nHere, we are checking the quality of the data again by running the data quality tests again to verify that we have preprocessed the data to a sufficient standard and that tests are passing according to our requirements:\n\nvm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='text', target_column=\"sentiment\")\n\nconfig = {\n    \"class_imbalance\":{\"min_percent_threshold\": 3}\n}\ntext_data_test_suite = vm.run_test_suite(\"text_data_quality\",\n                                       dataset=vm_ds,\n                                       config=config)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#modeling",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#modeling",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "Modeling",
    "text": "Modeling\n\nCreate training, validation, and test data sets\nWith our data in nice shape, we’ll split it into training, validation, and test sets:\n\n\ndf = df[df['sentiment'] != \"neutral\"]\ndf.loc[df['sentiment'] == \"positive\", 'sentiment'] = 1\ndf.loc[df['sentiment'] == \"negative\", 'sentiment'] = 0\nprint(np.unique(df['sentiment']))\n\nprint(df.head())\ntrain, test = train_test_split(df[['text','sentiment']], test_size=0.33, random_state=42)\ntrain = train[['text','sentiment']]\ntest = test[['text','sentiment']]\n\ntrain, valid = train_test_split(\n    train,\n    train_size=0.7,\n    random_state=0,\n    stratify=train['sentiment'])\ny_train, X_train = \\\n    train['sentiment'], train.drop(['sentiment'], axis=1)\ny_valid, X_valid = \\\n    valid['sentiment'], valid.drop(['sentiment'], axis=1)\ny_test, X_test= \\\n    test['sentiment'], test.drop(['sentiment'], axis=1)\n\n\n\nBuild the model\n\ndef fit_model(X_train, y_train,val_data, **kwargs):\n    model = CatBoostClassifier(\n        task_type='CPU',\n        iterations=5000,\n        eval_metric='Accuracy',\n        od_type='Iter',\n        od_wait=500,\n        **kwargs\n    )\n    return model.fit(\n        X=X_train,\n        y=y_train,\n        eval_set=val_data,\n        verbose=100,\n        plot=True,\n        use_best_model=True\n        )\n\n\nmodel = fit_model(\n    X_train, y_train,\n    val_data=(X_valid,y_valid),\n    text_features=['text'],\n    learning_rate=0.35,\n    tokenizers=[\n        {\n            'tokenizer_id': 'Sense',\n            'separator_type': 'BySense',\n            'lowercasing': 'True',\n            'token_types':['Word', 'Number', 'SentenceBreak'],\n            'sub_tokens_policy':'SeveralTokens'\n        }\n    ],\n    dictionaries = [\n        {\n            'dictionary_id': 'Word',\n            'max_dictionary_size': '5000'\n        }\n    ],\n    feature_calcers = [\n        'BoW:top_tokens_count=10000'\n    ]\n)\n\n\n\nInitialize ValidMind objects\nWith the model ready, we can now initialize the training and testing datasets, as well as the model, for sentiment analysis using vm.init_dataset() and vm.init_model:\n\nvm_train_ds = vm.init_dataset(dataset=pd.concat([X_train, y_train], axis=1), type=\"generic\", target_column=\"sentiment\")\nvm_test_ds = vm.init_dataset(dataset=pd.concat([X_test, y_test], axis=1), type=\"generic\",target_column=\"sentiment\")\nvm_model = vm.init_model(model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n\nRun model metrics test suite\nNext, we run the binary_classifier_metrics test suite on the initialized model to collect performance metrics for binary classification:\n\nmodel_metrics_test_suite = vm.run_test_suite(\"binary_classifier_metrics\",\n                                             model=vm_model\n                                            )\n\n\n\nRun model validation test suite\nAnd finally, let’s runs the binary_classifier_validation test suite on the initialized model to validate the model’s binary classification performance:\n\nmodel_validation_test_suite = vm.run_test_suite(\"binary_classifier_validation\",\n                                             model=vm_model\n                                            )"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#next-steps",
    "href": "notebooks/code_samples/LLM_and_NLP/nlp_sentiment_analysis_catboost_demo.html#next-steps",
    "title": "NLP Sentiment Analysis with CatBoost",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 3. Model Development &gt; 3.2. Prompt Evaluation.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html",
    "title": "Integrate an External Test Provider",
    "section": "",
    "text": "This notebook shows model developers how to integrate a custom test provider into the Validmind Developer Framework. The notebook loads two of sample test providers and registers them with the framework which enables you to run a template that utilizes the custom tests made available by the providers."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#validmind-at-a-glance",
    "title": "Integrate an External Test Provider",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#before-you-begin",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#before-you-begin",
    "title": "Integrate an External Test Provider",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#install-the-client-library",
    "title": "Integrate an External Test Provider",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#before-you-initialize-the-client-library-update-the-customer-churn-template-template",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#before-you-initialize-the-client-library-update-the-customer-churn-template-template",
    "title": "Integrate an External Test Provider",
    "section": "Before you initialize the client library: update the customer churn template template",
    "text": "Before you initialize the client library: update the customer churn template template\nFirst, let’s edit the Binary classification template and register test blocks for the demo test providers we will implement below.\n\nGo to Settings &gt; Templates and click on the Binary classification template. Let’s add a new top level section called test_providers_demo with some test driven content blocks like below:\n\n- id: test_providers_demo\n  title: Test providers demo\n  contents:\n  - content_type: metric\n    content_id: my_local_provider.tests.MyCustomTest\n  - content_type: metric\n    content_id: my_inline_provider.tests.MyCustomTest\n\nClick on Prepare new version, provide some version notes and click con Save new version to save a new version of this template\nNow we need to swap our project to this new version of the template. Follow the steps on this guide to swap the template of our customer churn model: https://docs.validmind.ai/guide/swap-documentation-project-templates.html\n\nIn the sections below we provide more context on how these content_ids above get mapped to the actual test providers and tests."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#initialize-the-client-library",
    "title": "Integrate an External Test Provider",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Go to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\nPreview the documentation template and validate that it contains the new test blocks\nWe should see two custom content blocks in the template whose IDs are under the namespaces registered below (my_inline_provider and my_local_provider).\n::: {.callout-tip}\npreview_template() will show an error when loading the new tests since we haven’t registered our new test providers yet. This is expected.\n:::\n\nvm.preview_template()\n\n\n\nRegister external test providers\nWe will now instantiate and register test provider classes that include the tests we included in the template.\nWe will build an inline test provider that allows creating tests directly in the notebook and a local filesystem test provider that allows loading tests from a local folder.\nFor the local filesystem provider, we just need to specify the root folder under which the provider class will look for tests. For this demo, it is the ./tests/ directory.\nImport the Local File System Test Provider from the validmind.tests module\n\nfrom validmind.tests import LocalTestProvider\n\nCreate an inline TestProvider Class that just returns a single test\n\nimport pandas as pd\n\nclass MySecondCustomTest(vm.vm_models.Metric):\n    # The test name should match the content ID on the template\n    name = \"my_inline_provider.tests.MyCustomTest\"\n\n    def description(self):\n        return \"This is a custom test from an external test provider.\"\n\n    def run(self):\n        return self.cache_results([{\"foo\": \"bar\"}])\n\n    def summary(self, results):\n        return vm.vm_models.ResultSummary(\n            results=[\n                vm.vm_models.ResultTable(\n                    data=pd.DataFrame(results),\n                    metadata=vm.vm_models.ResultTableMetadata(\n                        title=\"Results from Test Provider Inside Notebook\"\n                    ),\n                )\n            ]\n        )\n\n\nclass TestProviderInline:\n    def load_test(self, test_id):\n        # ignore the test_id and just return the single test above\n        return MySecondCustomTest\n\n\n# instantiate the test provider\ninline_test_provider = TestProviderInline()\nlocal_test_provider = LocalTestProvider(root_folder=\".\")\n\n# register the test providers\nvm.tests.register_test_provider(\n    namespace=\"my_inline_provider\",\n    test_provider=inline_test_provider,\n) # validmind will now call the `TestProviderInline.load_test` method whenever it encounters a test ID that starts with `my_inline_provider`\n\nvm.tests.register_test_provider(\n    namespace=\"my_local_provider\",\n    test_provider=local_test_provider,\n) # validmind will now call the `LocalTestProvider.load_test` method whenever it encounters a test ID that starts with `my_local_provider`\n\n\n\nVerify that preview_template() now loads the tests from the test providers\nAfter registering the test providers with vm.tests.register_test_provider(), the developer framework can now locate the code that will execute the tests when we run the documentation tests on the template. We can verify this by running preview_template() again and seeing that the tests are now loaded correctly.\n\nvm.preview_template()\n\n\n\nRunning the template\nNow we can run the template as usual and it will use the external test providers to load the appropriate tests. Note that we’re not passing any inputs such as dataset and model to run_template(). This is because our demo test providers do not have any required inputs and we’re scoping the template execution to the test_providers_demo section by using the section keyword argument.\n\nsuite_results = vm.run_documentation_tests(section='test_providers_demo')"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#next-steps",
    "href": "notebooks/code_samples/custom_tests/external_test_providers_demo.html#next-steps",
    "title": "Integrate an External Test Provider",
    "section": "Next steps",
    "text": "Next steps\nYou can now view the results of the external test providers in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; [Demo] Customer Churn Model.\nExpand the Test Providers Demo section as defined in the template.\n\nWhat you can see now is a more easily consumable version of the test results that were uploaded by the ValidMind Developer Framework, along with other parts of your documentation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The guide to elevating your AI model risk workflow",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Documentation\n                            The guide to elevating your AI model risk workflow\n                            Need help? Find all the information you need to use our platform for model risk management (AI model risk).\n                            \n                            \n                                \n                                    \n                                        \n                                            \n                                        \n                                        \n                                                                \n                                        \n                                    \n                                \n                            \n\n                        \n                    \n                \n                \n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating key aspects of the model risk management process, ValidMind is an AI model risk solution designed for the unique documentation and validation needs of model developers and validators.\n                    Model Documentation Automation\n                    AI Model Risk & Lifecycle Management\n                    Communication & TrackingInstructional GuidesGet Started\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Automate your model documentation and testing tasks with our Developer Framework.Get started\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and documentation to ensure they comply with organizational & regulatory requirements.Collaborate on projects\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "releases/2023-sep-27/enhancement.html",
    "href": "releases/2023-sep-27/enhancement.html",
    "title": "Enhancements – September 26, 2023",
    "section": "",
    "text": "Multi-class test improvements. We made a number of changes to metrics to improve the developer experience:\n\nA new fail_fast argument can be passed to run_test_plan, run_test_suite and run_documentation_tests, used to fail and raise an exception on the first error encountered. This change is useful for debugging.\nClassifierPerformance test now determines if you are testing a binary or a multi-class model. When testing a multi-class model, we now report additional per-class, macro and weighted average metrics.\nFixed F1 score metric so it works correctly for binary and multi-class models.\n\n\n\n\nAdded multi-class classification support. The Developer Framework now supports a multi-class version of some the existing metrics, such as confusion matrix, accuracy, precision, recall, and more. Also, the dataset and model interfaces now support dealing with multiple targets.\n\n\n\nImplemented classification model comparison metrics. Added a model performance comparison test for classification tasks. The test includes metrics such as accuracy, F1, precision, recall, and roc_auc score.\n\n\n\nTrack additional test metadata. Added a metadata property to every ValidMind test class. The metadata property includes a task_types field and a tags field which both serve to categorize the tests based on what data and model types they work with, what category of test they fall into, and more.\n\n\n\nFilter tests by task type and tags. We added a new search feature to the validmind.tests.list_tests function to allow for better test discoverability. The list_tests function in the tests module now supports the following arguments:\n\nfilter: If set, will match tests by ID, task_types or tags using a combination of substring and fuzzy string matching. Defaults to None.\ntask: If set, will further narrow matching tests (assuming filter has been passed) by exact matching the task to the test’s task_type metadata. Defaults to None.\ntags: If a list is passed, will again narrow the matched tests by exact matching on tags. Defaults to None."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html",
    "href": "releases/2023-aug-15/highlights.html",
    "title": "August 15, 2023",
    "section": "",
    "text": "This release includes a number of improvements for the developer experience when using the ValidMind Developer Framework, along with a visual redesign of the Platform UI.\n\n\n\n\n\nImproved developer experience for the Metric test class. We made a number of enhancements to the ValidMind Developer Framework to improve the experience for developers:\n\nBetter test organization: Added a metadata and tags attribute to the Test base class for better categorization and filtering of tests.\nBetter filtering: Added a new task_type argument to the list_tests() function to allow simple filtering by task_type set in a test.\nClearer naming: Renamed required_context to required_inputs across the whole ValidMind SDK.\n\n\n\n\nEnhancements to test configuration discovery: To make the discovery of required context and default configuration easier, this update includes a number of changes to how required_context and default_parameters are presented:\n\nPreview template expansion: With vm.preview_template(), you can now view detailed, expandable information for each content block driven by a test. This includes the test’s name, description, required context, and default parameters.\nSimplified test listing: The vm.tests.list_tests() function has been refined to show only the ID, Name, and Description for each test, making it more user-friendly.\nDetailed test descriptions: You can now obtain complete test details, including required_context and default_params, using the vm.tests.describe_test() function.\nRetrieve test suite: The new function vm.get_test_suite() allows you to access a specific test suite instance via the high-level Validmind API.\nAccess to default configuration: A new method, get_default_config(), is now part of both the TestPlan and TestSuite classes, letting you obtain a dictionary containing all the configuration parameters used by the tests.\nEasier required context: A complementary new method, get_required_context(), now returns a list of all the context variables needed for tests to run, making it easier for you to ensure that everything is set up correctly.\n\n\n\n\n\n\nRedesigned application and navigation bar layout. The new application design offers a cleaner, more intuitive interface that enhances user experience. The updated navigation bar layout specifically contributes to easier navigation, allowing users to find what they need with greater efficiency."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html#release-highlights",
    "href": "releases/2023-aug-15/highlights.html#release-highlights",
    "title": "August 15, 2023",
    "section": "",
    "text": "This release includes a number of improvements for the developer experience when using the ValidMind Developer Framework, along with a visual redesign of the Platform UI.\n\n\n\n\n\nImproved developer experience for the Metric test class. We made a number of enhancements to the ValidMind Developer Framework to improve the experience for developers:\n\nBetter test organization: Added a metadata and tags attribute to the Test base class for better categorization and filtering of tests.\nBetter filtering: Added a new task_type argument to the list_tests() function to allow simple filtering by task_type set in a test.\nClearer naming: Renamed required_context to required_inputs across the whole ValidMind SDK.\n\n\n\n\nEnhancements to test configuration discovery: To make the discovery of required context and default configuration easier, this update includes a number of changes to how required_context and default_parameters are presented:\n\nPreview template expansion: With vm.preview_template(), you can now view detailed, expandable information for each content block driven by a test. This includes the test’s name, description, required context, and default parameters.\nSimplified test listing: The vm.tests.list_tests() function has been refined to show only the ID, Name, and Description for each test, making it more user-friendly.\nDetailed test descriptions: You can now obtain complete test details, including required_context and default_params, using the vm.tests.describe_test() function.\nRetrieve test suite: The new function vm.get_test_suite() allows you to access a specific test suite instance via the high-level Validmind API.\nAccess to default configuration: A new method, get_default_config(), is now part of both the TestPlan and TestSuite classes, letting you obtain a dictionary containing all the configuration parameters used by the tests.\nEasier required context: A complementary new method, get_required_context(), now returns a list of all the context variables needed for tests to run, making it easier for you to ensure that everything is set up correctly.\n\n\n\n\n\n\nRedesigned application and navigation bar layout. The new application design offers a cleaner, more intuitive interface that enhances user experience. The updated navigation bar layout specifically contributes to easier navigation, allowing users to find what they need with greater efficiency."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html#how-to-upgrade",
    "href": "releases/2023-aug-15/highlights.html#how-to-upgrade",
    "title": "August 15, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "href": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases? Our documentation includes how-to instructions for the following user roles:\n\nFor platform administrators — Learn how to configure the platform, from setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your existing workflows, and more.\nFor model developers — Find information for ValidMind tests and test suites, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nWe have more code samples available that you can download and try out yourself.\nAlso check the Guides for how to integrate the Developer Framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers.\nCollaborate on documentation projects\n\n\nHave more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human.\n\n\nNeed help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework in Jupyter Hub and to explore the ValidMind Platform UI online."
  },
  {
    "objectID": "guide/quickstart.html#before-you-begin",
    "href": "guide/quickstart.html#before-you-begin",
    "title": "Quickstart",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\n\nValidMind Developer Framework\nTo try the ValidMind Developer Framework, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\nValidMind Platform UI\nWe support most modern browsers, such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox.\nTo upload from the Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstart",
    "section": "Steps",
    "text": "Steps\n\nTry the Developer Framework (10 minutes)\nTry our introductory Jupyter notebook to see the Developer Framework in action.\nExplore the Platform UI (15 minutes):\n\nExplore an Example Documentation Project\nCreate Your First Documentation Project\nUpload to Your Documentation Project\n\n\n\nWhat’s next\nIf you’re ready to do more with ValidMind, check out Next steps."
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "Need an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "",
    "text": "Need an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#whats-next",
    "href": "guide/before-you-begin.html#whats-next",
    "title": "Before you begin",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Developer Framework. We recommend using Jupyter Hub."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s next",
    "text": "What’s next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/troubleshooting.html",
    "href": "guide/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Learn how to resolve commonly encountered issues with the Developer Framework."
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-install-the-validmind-developer-framework",
    "href": "guide/troubleshooting.html#cannot-install-the-validmind-developer-framework",
    "title": "Troubleshooting",
    "section": "Cannot install the ValidMind Developer Framework",
    "text": "Cannot install the ValidMind Developer Framework\nIssue: You cannot run pip install validmind or import validmind as vm in the ValidMind Developer Framework notebooks.\nFix: Make sure you are installing the latest version of the Developer Framework by running this command:\n%pip install --upgrade validmind"
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "href": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "title": "Troubleshooting",
    "section": "Cannot initialize ValidMind client library",
    "text": "Cannot initialize ValidMind client library\nIssue: When you run vm.init(), you encounter an error message like this:\nMissingAPICredentialsError: API key and secret must be provided either as environment variables or as arguments to init.\nor\nInvalidProjectError: Invalid project ID. Please ensure that you have provided a project ID that belongs to your organization.\nFix: Make sure that you are using the correct initialization credentials for the project you are trying to connect to.\nFollow the steps in Install and initialize the Developer Framework for detailed instructions on how to integrate the Developer Framework and upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/troubleshooting.html#additional-resources",
    "href": "guide/troubleshooting.html#additional-resources",
    "title": "Troubleshooting",
    "section": "Additional resources",
    "text": "Additional resources\nCheck out our FAQ page to browse through common questions, or contact our support team for more help troubleshooting technical issues."
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\nLocating the project identifier, API key and secret:\nOn the Getting started page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/create-documentation-project.html#related-topics",
    "href": "guide/create-documentation-project.html#related-topics",
    "title": "Create documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nInstall and initialize the Developer Framework\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/testing-overview.html",
    "href": "guide/testing-overview.html",
    "title": "Tests and test suites",
    "section": "",
    "text": "This topic provides an overview for what tests and test suites are in the ValidMind Developer Framework, when to use tests and test suites, and typical scenarios."
  },
  {
    "objectID": "guide/testing-overview.html#what-are-tests-and-test-suites",
    "href": "guide/testing-overview.html#what-are-tests-and-test-suites",
    "title": "Tests and test suites",
    "section": "What are tests and test suites?",
    "text": "What are tests and test suites?\n\nTests\n\nTests are designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest suites\n\nTest suites are collections of tests which are run together to automate generating model documentation end-to-end for specific use-cases.\n\n\nExample: the classifier_full_suite test suite runs the tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/testing-overview.html#when-do-i-use-tests-and-test-suites",
    "href": "guide/testing-overview.html#when-do-i-use-tests-and-test-suites",
    "title": "Tests and test suites",
    "section": "When do I use tests and test suites?",
    "text": "When do I use tests and test suites?\nValidMind provides many built-in tests and test suites which make it easy for a model developer to document their work at any point during the model development lifecycle when they need to validate that their work satisfies model risk management requirements.\nWhile you, as a model developer, have the flexibility to decide when to use which ValidMind tests, we have identified a few typical scenarios with their own characteristics and needs:\n\n\nDataset testing\nTo document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test suite.\nFor time-series datasets: use the time_series_dataset test suite.\n\n\n\nModel testing\nTo document and validate your model:\n\nFor binary classification models: use the classifier test suite.\nFor time series models: use the timeseries test suite.\n\n\n\nEnd-to-end testing\nTo document a binary classification model and the relevant dataset end-to-end:\nUse the classifier_full_suite test suite."
  },
  {
    "objectID": "guide/testing-overview.html#can-i-use-my-own-tests",
    "href": "guide/testing-overview.html#can-i-use-my-own-tests",
    "title": "Tests and test suites",
    "section": "Can I use my own tests?",
    "text": "Can I use my own tests?\nYes, ValidMind supports custom tests that you develop yourself or that are provided by third-party test libraries, also referred to as test providers. We provide instructions with code examples that you can adapt:\n\nImplement Custom Metrics and Threshold Tests\nIntegrate an External Test Provider"
  },
  {
    "objectID": "guide/testing-overview.html#is-there-an-api-reference-available",
    "href": "guide/testing-overview.html#is-there-an-api-reference-available",
    "title": "Tests and test suites",
    "section": "Is there an API reference available?",
    "text": "Is there an API reference available?\nSee the ValidMind Developer Framework API Reference for a list of all of the built-in tests and test suites for datasets and models."
  },
  {
    "objectID": "guide/testing-overview.html#how-tos-for-testing",
    "href": "guide/testing-overview.html#how-tos-for-testing",
    "title": "Tests and test suites",
    "section": "How-tos for testing",
    "text": "How-tos for testing"
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html",
    "href": "guide/create-your-first-documentation-project.html",
    "title": "Create Your First Documentation Project",
    "section": "",
    "text": "Let’s learn how to create your own documentation project. You can use this project to upload tests and documentation and then add that to the Quickstart notebook you looked at earlier."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html#steps",
    "href": "guide/create-your-first-documentation-project.html#steps",
    "title": "Create Your First Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the landing page by clicking on the ValidMind logo or Log in to the ValidMind UI.\nFrom the left sidebar, select Documentation Projects and on the page that opens, click the Create new Project button at top right of the screen.\nSelect the right options in the form:\n\n\nModel: [Quickstart] Customer Churn Model\nType: Initial Validation (selected automatically) \nProject name: Enter your preferred name\n\nClick Create Project.\nValidMind will create an empty documentation project associated with the customer churn model.\nYou can now access this project from the UI on the Documentation Projects page or by navigating to the relevant model - [Quickstart] Customer Churn Model - in the Model Inventory page.\nFrom the left sidebar, select Getting started.\nThe page that opens provides you with the credentials for the newly created project to use with the ValidMind Developer Framework.\nLocate the project identifier, API key, and secret:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\n\n\n\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nTry this: Use the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html#whats-next",
    "href": "guide/create-your-first-documentation-project.html#whats-next",
    "title": "Create Your First Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nContinue with Upload to your documentation project to learn about how you can use the ValidMind Platform for your projects."
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "Our Supported Models provide an array of templates for testing and documentation, aimed at specific use-cases. This topic is relevant for model developers who want to know what functionality the Developer Framework provides and how they can extend it."
  },
  {
    "objectID": "guide/supported-models.html#what-is-a-supported-model",
    "href": "guide/supported-models.html#what-is-a-supported-model",
    "title": "Supported models",
    "section": "What is a supported model?",
    "text": "What is a supported model?\nA supported model refers to a model for which predefined testing or documentation functions exist in the ValidMind Developer Framework, provided that the model you are developing is documented using a supported version of our client library.\n\nPlease note the inherent dependency on data types, such as tabular, time series, and text data types. This distinction isn’t always immediately evident. For example: while binary classification is possible on text datasets, only the tabular_dataset suite is explicitly mentioned in our list of supported models.\nAdditionally, the framework might accommodate additional models not explicitly listed. The absence of pre-configured tests for these models doesn’t necessarily indicate a lack of support. You always have the flexibility to integrate your own tests or connect to an external test provider, if needed."
  },
  {
    "objectID": "guide/supported-models.html#currently-supported-models",
    "href": "guide/supported-models.html#currently-supported-models",
    "title": "Supported models",
    "section": "Currently supported models",
    "text": "Currently supported models\nAs of release (v1.20.0), the Developer Framework supports the following model types:\n\n\n\n\n\n\n\n\n\n\n\nProblem type\nGoal\nTest Suite\nSupported Libraries\nTests\n\n\n\n\nBinary classification\nData validation\ntabular_dataset\nnumpy, pandas\nDatasetMetadata, DatasetDescription, DescriptiveStatistics, DatasetCorrelations\n\n\nBinary classification\nData validation\ntabular_dataset\nnumpy, pandas\nClassImbalance, Duplicates, HighCardinality, HighPearsonCorrelation, MissingValues, Skewness, UniqueRows, TooManyZeroValues\n\n\nBinary classification\nModel validation\nbinary_classifier_model_validation\nsklearn.RandomForestClassifier, sklearn.LogisticRegression, xgboost.XGBClassifier, CatBoostClassifier\nModelMetadata, DatasetSplit, ConfusionMatrix, ClassifierInSamplePerformance, ClassifierOutOfSamplePerformance, PermutationFeatureImportance, PrecisionRecallCurve, ROCCurve, CharacteristicStabilityIndex, PopulationStabilityIndex, SHAPGlobalImportance\n\n\nBinary classification\nModel validation\nbinary_classifier_model_validation\nsklearn.RandomForestClassifier, sklearn.LogisticRegression, xgboost.XGBClassifier, CatBoostClassifier\nMinimumAccuracy, MinimumF1Score, MinimumROCAUCScore, TrainingTestDegradation\n\n\nBinary classification\nModel validation\nbinary_classifier_model_validation\nsklearn.RandomForestClassifier, sklearn.LogisticRegression, xgboost.XGBClassifier, CatBoostClassifier\nOverfitDiagnosis, WeakspotsDiagnosis, RobustnessDiagnosis\n\n\nTime series\nData validation\ntime_series_dataset\nnumpy, pandas\nTimeSeriesOutliers, TimeSeriesMissingValues, TimeSeriesFrequency\n\n\nTime series\nData validation\ntime_series_dataset\nnumpy, pandas\nTimeSeriesLinePlot, TimeSeriesHistogram, ACFandPACFPlot, SeasonalDecompose, AutoSeasonality, AutoStationarity, RollingStatsPlot, AutoAR, AutoMA\n\n\nTime series\nData validation\ntime_series_dataset\nnumpy, pandas\nScatterPlot, LaggedCorrelationHeatmap, EngleGrangerCoint, SpreadPlot\n\n\nTime series\nModel validation\ntime_series_model_validation\nstatsmodels.regression.linear_model.OLS, statsmodels.regression.linear_model.GLS, sklearn.linear_model.LinearRegression, xgboost.XGBRegressor\nRegressionModelSummary\n\n\nTime series\nModel validation\ntime_series_model_validation\nstatsmodels.regression.linear_model.OLS, statsmodels.regression.linear_model.GLS, sklearn.linear_model.LinearRegression, xgboost.XGBRegressor\nRegressionModelInsampleComparison, RegressionModelOutsampleComparison\n\n\nTime series\nModel validation\ntime_series_model_validation\nstatsmodels.regression.linear_model.OLS, statsmodels.regression.linear_model.GLS, sklearn.linear_model.LinearRegression, xgboost.XGBRegressor\nRegressionModelForecastPlot"
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\n\nCheck out our Developer Framework documentation for more details on how to use our documentation and testing functions with supported models.\nFor additional tests that are not supported, refer to our documentation on Implementing Custom Tests and Integrating External Test Providers."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html",
    "href": "guide/release-notes-2023-jun-22.html",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the Platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our Developer Framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with Validmind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test suites. Learn more …\n\n\n\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. Learn more …\n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the Platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#release-highlights",
    "href": "guide/release-notes-2023-jun-22.html#release-highlights",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the Platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our Developer Framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with Validmind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test suites. Learn more …\n\n\n\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. Learn more …\n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the Platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#enhancements",
    "href": "guide/release-notes-2023-jun-22.html#enhancements",
    "title": "June 22, 2023",
    "section": "Enhancements",
    "text": "Enhancements\nWe revised our Quickstart guide to be more modular and to highlight that our suggested starting point with the ValidMind Developer Framework is now Jupyter Hub. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "title": "June 22, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/license-agreement.html",
    "href": "guide/license-agreement.html",
    "title": "SOFTWARE LICENSE AGREEMENT",
    "section": "",
    "text": "IMPORTANT: READ THIS SOFTWARE LICENSE AGREEMENT (THIS “AGREEMENT”) CAREFULLY BEFORE USING THE SOFTWARE. BY USING THE SOFTWARE, YOU ARE AGREEING TO BE BOUND BY THE TERMS OF THIS LICENSE AGREEMENT:\nAs between you and Licensor, this software and associated media, printed materials, and “online” or electronic documentation files (collectively, the “Software”), is the proprietary information of ValidMind Inc. (“Licensor”), and is protected under copyright and other intellectual property laws.\nNo part of this Software may be copied, reproduced, distributed, republished, displayed, posted or transmitted in any form or by any means, including, but not limited to, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of Licensor.\nBy installing, copying, or otherwise using the Software, you as a user of the Software (“you”) agree to be bound by the terms of this Agreement. If you do not agree to the terms of this Agreement, do not install or use the Software.\n\nLICENSE GRANT. Subject to the terms and conditions of this Agreement, Licensor grants you a non-exclusive, non-transferable, limited license (without the right to sublicense) during the term of this Agreement to install and use the Software only in object code or byte code form for the sole purpose of testing its functionality.\nOWNERSHIP. The Software is owned by Licensor and its licensors and is protected by copyright laws and international copyright treaties, as well as other intellectual property laws and treaties. As between you and Licensor, Licensor retains all right, title, and interest in and to the Software, including all intellectual property rights. This Agreement does not transfer any ownership of the Software to you.\nRESTRICTIONS. You will not use the Software for any commercial, production, or operational purposes. You will not (a) modify, adapt, translate, reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code of the Software including (without limitation) for the purpose of obtaining unauthorized access to the Software, (b) modify, translate, copy, or create derivative works based on the Software, (c) use the Software to create or develop a competitive product or service, (d) circumvent, remove, alter, or thwart any technological measure or content protections of the Software, (e) distribute, sublicense, rent, lease, or lend the Software to any third party, (f) remove or alter any copyright, trademark, or proprietary rights notice contained in the Software or (g) use the Software for any purpose except as expressly permitted under this Agreement.\nCONFIDENTIALITY. You agree to safeguard the Software and its related materials with the same care and precaution as you use to protect your own proprietary information and trade secrets, but in no event less than a reasonable degree of care so that no unauthorized use is made of them and no disclosure of any part of their contents is made to anyone other than your employees, agents or consultants whose duties reasonably require such disclosure, or as necessary in the ordinary course of business. You shall make all such persons fully aware of their responsibility to fulfill your obligations under this Agreement and agree to be responsible for any breach of this Agreement by such persons. You agree to promptly notify Licensor if you obtain information as to any unauthorized possession, use or disclosure of the Software by any person or entity, and further agrees to cooperate with Licensor in protecting Licensor’s proprietary rights. Upon the request of Licensor, copies and embodiments of the Software and its related materials shall be promptly returned to Licensor by you or destroyed by you, and you agree to certify such destruction in writing.\nTERM & TERMINATION. This Agreement is effective until terminated. The Licensee may terminate this Agreement at any time by destroying all copies of the Software and its related materials. The Licensor may terminate this License if you fail to comply with any term or condition of this Agreement. Upon termination, you must destroy all copies of the Software and its related materials in your possession.\nDISCLAIMER OF WARRANTY. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.\nLIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT SHALL LICENSOR BE LIABLE FOR ANY (A) INDIRECT, PUNITIVE, EXEMPLARY, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES (INCLUDING LOST PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR INABILITY TO USE THE SOFTWARE, EVEN IF LICENSEE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR (B) ANY AMOUNTS IN EXCESS OF THE AMOUNT PAID BY YOU FOR THE SOFTWARE.\nINDEMNIFICATION. You agree to indemnify and hold Licensor harmless from and against all loss, cost, expense or liability (including reasonable attorney’s fees) (i) arising out of a claim by a third party against Licensor based upon your use of the Software, or (ii) related to, or associated with your customizations, updates and/or corrections to the Software. Licensor shall have no liability to you for any damage sustained by you as a result of your use of the Software, whether such damages would arise as a result of breach of contract, tort or otherwise.\nGOVERNING LAW. This Agreement and any dispute arising hereunder shall be governed by and construed in accordance with the laws of the State of California and the federal laws of the United States of America, without giving effect to any principles of conflicts of law.\nENTIRE AGREEMENT. This Agreement constitutes the entire agreement between you and Licensor with respect to the Software and supersedes all prior or contemporaneous communications and proposals, whether oral or written, between you and Licensor.\nASSIGNMENT. You will not assign your rights and obligations under this Agreement without prior written consent of the Licensor. Licensor may freely assign its rights and obligations without your consent.\nMISCELLANEOUS. If any provision of this Agreement is found to be unenforceable or invalid, that provision will be limited or eliminated to the minimum extent necessary so that this Agreement will otherwise remain in full force and effect and enforceable. Without limiting anything herein, and except for your payment obligations, neither party shall have any liability for any failure or delay resulting from any condition beyond the reasonable control of such party, including but not limited to governmental action or acts of terrorism, earthquake or other acts of God, labor conditions, epidemics, pandemics, and power failures. For all purposes under this Agreement each party shall be and act as an independent contractor and shall not bind nor attempt to bind the other to any contract. Any notices in connection with this Agreement will be in writing.\n\nBy installing or using the Software, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.\nCopyright © 2023 ValidMind Inc. All rights reserved."
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html",
    "href": "guide/release-notes-2023-jul-24.html",
    "title": "July 24, 2023",
    "section": "",
    "text": "This release improves the developer experience within the ValidMind Developer Framework and introduces an updated notebook to demonstrate support for NLP models, now using CatBoost for greater performance. Moreover, we’ve upgraded our Platform UI with several major new features, including an all-new text editor, the capability to insert new content blocks into documentation projects, template swapping for keeping your documentation projects current, and much more.\n\n\n\n\n\nUpdated notebook for NLP use case. Now with enhanced performance by using CatBoost compared to the original PyTorch-based notebook, this updated notebook showcases natural language processing (NLP) data analysis and text classification using the ValidMind Developer Framework with emphasis on the in-depth analysis and preprocessing of COVID-19 tweets. Try it …\nImproved developer experience when you encounter error conditions. Unhandled errors now return an error code with a more descriptive message that can help you with looking them up in the troubleshooting section of our documentation. We also improved error handling when tests fail to run and the test results summary widget now display error information for each test that fails to run.\n\n\n\n\n\nNew full-featured text editor. When collaborating on documentation projects in the Platform UI with others, you now have a new text editor at your disposal. The interface should be intuitive and familiar, allowing you to manage model documentation and validation reports with greater ease. In addition to the new editing features, you can use the editor to participate in comment threads, track changes, see the revision history, enter math formulas, and more. Try it …\n\n\n\nAdd new content blocks to documentation projects. For model developers and model validators who want to add new sections to a documentation project, you can now do just that. The new content blocks feature provide you with additional sections that you fill in with text, metrics, and test results. Supported content block types:\nSimple text block: Can be added anywhere on model documentation pages and edited to include additional documentation in text format.\nTest-driven block: Can be added to display one of the supported metrics or threshold test results collected by the Developer Framework.\n\nYou can add new content block to an existing documentation project simply by hovering over the dashed line between sections. Try it …\n\n\nSwap templates for documentation projects. This feature is designed to enhance the flexibility of documenting models. With it, you can seamlessly update an existing documentation project by either switching to an entirely different template or by simply upgrading to a more recent version of the template currently in use. This new feature ensures that your documentation projects can remain up-to-date and align with the latest preferred template without the need to start from scratch. Try it …\nPlatform UI enhancements: \n\nRenamed Client Integration page to Getting Started. To better reflect when you use the credentials for a newly created documentation project with the ValidMind Developer Framework, we have renamed the Client Integration page to Getting Started.\nImproved activity widget: We enhanced the user interface with new tabs, enabling you to filter events based on activity type.\nImproved Findings section and Findings Details page: We made UI enhancements for both project findings and the findings details sections for a better look and feel.\nRefined Models and Projects lists : We refined the user interface, featuring more intuitive default sorting and more user-friendly filtering."
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#release-highlights",
    "href": "guide/release-notes-2023-jul-24.html#release-highlights",
    "title": "July 24, 2023",
    "section": "",
    "text": "This release improves the developer experience within the ValidMind Developer Framework and introduces an updated notebook to demonstrate support for NLP models, now using CatBoost for greater performance. Moreover, we’ve upgraded our Platform UI with several major new features, including an all-new text editor, the capability to insert new content blocks into documentation projects, template swapping for keeping your documentation projects current, and much more.\n\n\n\n\n\nUpdated notebook for NLP use case. Now with enhanced performance by using CatBoost compared to the original PyTorch-based notebook, this updated notebook showcases natural language processing (NLP) data analysis and text classification using the ValidMind Developer Framework with emphasis on the in-depth analysis and preprocessing of COVID-19 tweets. Try it …\nImproved developer experience when you encounter error conditions. Unhandled errors now return an error code with a more descriptive message that can help you with looking them up in the troubleshooting section of our documentation. We also improved error handling when tests fail to run and the test results summary widget now display error information for each test that fails to run.\n\n\n\n\n\nNew full-featured text editor. When collaborating on documentation projects in the Platform UI with others, you now have a new text editor at your disposal. The interface should be intuitive and familiar, allowing you to manage model documentation and validation reports with greater ease. In addition to the new editing features, you can use the editor to participate in comment threads, track changes, see the revision history, enter math formulas, and more. Try it …\n\n\n\nAdd new content blocks to documentation projects. For model developers and model validators who want to add new sections to a documentation project, you can now do just that. The new content blocks feature provide you with additional sections that you fill in with text, metrics, and test results. Supported content block types:\nSimple text block: Can be added anywhere on model documentation pages and edited to include additional documentation in text format.\nTest-driven block: Can be added to display one of the supported metrics or threshold test results collected by the Developer Framework.\n\nYou can add new content block to an existing documentation project simply by hovering over the dashed line between sections. Try it …\n\n\nSwap templates for documentation projects. This feature is designed to enhance the flexibility of documenting models. With it, you can seamlessly update an existing documentation project by either switching to an entirely different template or by simply upgrading to a more recent version of the template currently in use. This new feature ensures that your documentation projects can remain up-to-date and align with the latest preferred template without the need to start from scratch. Try it …\nPlatform UI enhancements: \n\nRenamed Client Integration page to Getting Started. To better reflect when you use the credentials for a newly created documentation project with the ValidMind Developer Framework, we have renamed the Client Integration page to Getting Started.\nImproved activity widget: We enhanced the user interface with new tabs, enabling you to filter events based on activity type.\nImproved Findings section and Findings Details page: We made UI enhancements for both project findings and the findings details sections for a better look and feel.\nRefined Models and Projects lists : We refined the user interface, featuring more intuitive default sorting and more user-friendly filtering."
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#documentation",
    "href": "guide/release-notes-2023-jul-24.html#documentation",
    "title": "July 24, 2023",
    "section": "Documentation",
    "text": "Documentation\nTo make it easier to try out our Jupyter notebooks, we now provide a download button for all notebooks used in our documentation:\n\n\nDownload Notebooks\n\n\nThis download includes:\n\nQuickstart notebooks\nUse case notebooks\nTesting notebooks"
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jul-24.html#how-to-upgrade",
    "title": "July 24, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Getting started page.\nRun the {…} test suite.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Getting started page.\nRun the {…} test suite.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html",
    "href": "guide/review-data-streams-and-audit-trails.html",
    "title": "Review Audit Trail",
    "section": "",
    "text": "Learn how to access and use the audit trail functionality in the ValidMind Platform. This topic matters for for model developers, model validators, and auditors who are looking to track or audit all the information events associated with a specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "href": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "title": "Review Audit Trail",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#steps",
    "href": "guide/review-data-streams-and-audit-trails.html#steps",
    "title": "Review Audit Trail",
    "section": "Steps",
    "text": "Steps\n\nIn the ValidMind platform, navigate to the relevant model documentation project.\nFrom the Overview page, select Audit Trail on the left.\n\nThe table in this page shows a record of all activities generated from the Developer Framework and actions performed by users in the organization related to this specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "href": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "title": "Review Audit Trail",
    "section": "What’s next",
    "text": "What’s next\n\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html",
    "href": "guide/try-developer-framework-with-colab.html",
    "title": "Try it with Google Colaboratory",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Google Colaboratory."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#prerequisites",
    "href": "guide/try-developer-framework-with-colab.html#prerequisites",
    "title": "Try it with Google Colaboratory",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to Google Colaboratory (Colab).\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#steps",
    "href": "guide/try-developer-framework-with-colab.html#steps",
    "title": "Try it with Google Colaboratory",
    "section": "Steps",
    "text": "Steps\n\n\n\n\n\n\n\n\nAbout our Jupyter notebooks\n\n\n\nNotebooks from ValidMind are safe to run — If you get a warning that this notebook was not authored by Google, we welcome you to inspect the notebook source.  Runtime errors — We recommend that you not use the Run all option. Run each cell individually to see what is happening in the notebook. If you do see errors, re-run the notebook cells.\n\n\n\nOpen the Quickstart notebook in Google Colaboratory: \n\nClick File &gt; Save a copy in Drive to make a copy of the Quickstart notebook so that you can modify it later.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#whats-next",
    "href": "guide/try-developer-framework-with-colab.html#whats-next",
    "title": "Try it with Google Colaboratory",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/explore-example-documentation-project.html",
    "href": "guide/explore-example-documentation-project.html",
    "title": "Explore an Example Documentation Project",
    "section": "",
    "text": "Let’s take a look at how the Developer Framework works hand-in-hand with the ValidMind Platform and how documentation and test results get uploaded.\nThe ValidMind Platform is the central place to:"
  },
  {
    "objectID": "guide/explore-example-documentation-project.html#steps",
    "href": "guide/explore-example-documentation-project.html#steps",
    "title": "Explore an Example Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the [Quickstart] Customer Churn Model - Initial Validation and select it.\nOn the model details page that open, you can find important information about the model, such as:\n\nThe ID of the model and its specific use case\nThe owners, developers, validators, and business unit associated with the model\nThe risk tier and current version\nAnd more\n\nScroll down to Documentation Project History and select the model.\nOn the project overview page that opens, you can see what is included, such as model, project findings, recent activity, and project stakeholders, and more. In the left sidebar, you can find links to the documentation, project findings, validation report, audit trail, and client integration.\nFor this Quickstart, we will focus on the Documentation section to show you how content from the Developer Framework gets uploaded.\nNote that the model status is In Documentation. This is the status that a model starts in as part of a documentation project. You can click See workflow to look at what the full workflow is, from documentation, to validation, to review, and finally approval.\nFrom the left sidebar, select Documentation &gt; 2. Data preparation &gt; 2.1. Data description.\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically by the Developer Framework whenever a test result is above or below a certain threshold.\nFrom the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the Developer Framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nThe Documentation Guidelines in the ValidMind Insights right sidebar can tell you more about what these sections mean and help you with the task of documenting the model.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the Developer Framework, but they get added by the model developer in the Platform UI."
  },
  {
    "objectID": "guide/explore-example-documentation-project.html#whats-next",
    "href": "guide/explore-example-documentation-project.html#whats-next",
    "title": "Explore an Example Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nReady to learn about how you can use the ValidMind Platform? Continue with Create your first documentation project."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html",
    "href": "guide/load-credentials-to-env-file.html",
    "title": "Load project credentials to a .env file",
    "section": "",
    "text": "Learn how to store project identifier credentials in a .env file instead of using inline credentials. This topic is relevant for model developers who want to follow best practices for security when running notebooks."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#why-is-this-recommended",
    "href": "guide/load-credentials-to-env-file.html#why-is-this-recommended",
    "title": "Load project credentials to a .env file",
    "section": "Why is this recommended?",
    "text": "Why is this recommended?\nStoring credentials in a .env file is considered a best practice for security. Embedding credentials directly within the code makes them more susceptible to accidental exposure when sharing code or collaborating on projects. Keeing project credentials in a separate file also allows for precise access control and ensures that sensitive credentials are not publically accessible."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#prerequisites",
    "href": "guide/load-credentials-to-env-file.html#prerequisites",
    "title": "Load project credentials to a .env file",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory"
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#steps",
    "href": "guide/load-credentials-to-env-file.html#steps",
    "title": "Load project credentials to a .env file",
    "section": "Steps",
    "text": "Steps\n\nCreate a new file in the same folder as your notebook and name it .env. This is a hidden file, so you may need to change your settings to view it.\nLocate the project identifier credentials for your documentation project. These credentials can be found on the Getting started page. Copy the values from this page and paste them into your .env file in the following format:\n\n``` VM_API_PROJECT= VM_API_HOST= VM_API_KEY= VM_API_SECRET= ```\n\nInsert this code snippet above your project identifier credentials:\n\n``` %load_ext dotenv %dotenv dev.env ```\nThe updated notebook should look like this:\n``` %load_ext dotenv %dotenv .env\nimport validmind as vm\nvm.init( api_host = “http://localhost:3000/api/v1/tracking”, project = “…” ) ```\n\nRun the cell. Instead of using inline credentials, this cell will now load your project credentials from a .env file."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#related-topics",
    "href": "guide/load-credentials-to-env-file.html#related-topics",
    "title": "Load project credentials to a .env file",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the ValidMind UI\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html",
    "href": "guide/release-notes-2023-may-30.html",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#release-highlights",
    "href": "guide/release-notes-2023-may-30.html#release-highlights",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#bugfixes",
    "href": "guide/release-notes-2023-may-30.html#bugfixes",
    "title": "May 30, 2023",
    "section": "Bugfixes",
    "text": "Bugfixes\n\nFixed the display alignment in certain pages of the UI.\nFixed display issues related to Helvetica Neue font not available for Windows users.\nFixed an issue preventing users to drag & drop image files directly in the online editor.\nAdjusted filters for Model Inventory and Documentation Projects search boxes."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "href": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "title": "May 30, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, refresh your browser.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/get-started-closed-beta.html",
    "href": "guide/get-started-closed-beta.html",
    "title": "Welcome to the Closed Beta!",
    "section": "",
    "text": "Thank you for joining ValidMind’s closed beta! We hope that you will find our beta program an engaging experience. Let us know what we can do to help you make the most of your time with us."
  },
  {
    "objectID": "guide/get-started-closed-beta.html#whats-in-the-closed-beta",
    "href": "guide/get-started-closed-beta.html#whats-in-the-closed-beta",
    "title": "Welcome to the Closed Beta!",
    "section": "What’s in the closed beta?",
    "text": "What’s in the closed beta?\nMost of our large language model (LLM) testing and documentation generation features will be available for you to test during the closed beta. This includes both the ValidMind Model Documentation Automation and AI Risk Management platform.\nPractically, this means you will interact with our Developer Framework through Jupyter notebooks to run sample code. You will also log into our cloud-based Platform UI to look at the output the Developer Framework generated as part of running the sample code. The Jupyter notebooks we provide are meant to be self-documenting, meaning they include most everything you need to know to run them.\nIf you have never tried our products, you can also spend a few minutes reading What do I use the ValidMind platform for?. And for the beta specifically, these blog posts provide a bit more context:\n\nAnnouncing Our Closed Beta for Large Language Model Testing — Coming Soon!\nClosed Beta Preview: Prompt Validation for LLMs"
  },
  {
    "objectID": "guide/get-started-closed-beta.html#timeline",
    "href": "guide/get-started-closed-beta.html#timeline",
    "title": "Welcome to the Closed Beta!",
    "section": "Timeline",
    "text": "Timeline\nOur closed beta program is running in two major phases:\n\n\n\nEarly Access: Friends & family\n\nSign-up is CLOSED\n\n\nStarting in early October, we are inviting a small group of select closed beta participants to try out our LLM functionality hot off the press.\nExpect regular closed beta updates, individual kick-off sessions, and some rough edges as we continue to refine our LLM functionality.\n\n\n\nWider audience: All closed beta participants\n\nSign-up is OPEN\n\n\nA larger group of closed beta participants that will benefit from the improvements made during the first phase of testing. If you know of anyone who might be interested in joining our closed beta, please refer them to our signup page:\nJoin our Closed Beta for LLM Testing"
  },
  {
    "objectID": "guide/get-started-closed-beta.html#getting-started",
    "href": "guide/get-started-closed-beta.html#getting-started",
    "title": "Welcome to the Closed Beta!",
    "section": "Getting started",
    "text": "Getting started\nStep-by-step instructions for testing:\n\n\nGet your credentials for the Platform UI by signing up with ValidMind: https://app.prod.validmind.ai/\nThis is the login URL for our Platform UI where you can look at the model inventory and documentation projects for the different models.\nWhen you receive your access credentials, make sure you can log into the Platform UI.\nBook a kick-off session with ValidMind by emailing info@validmind.ai.\nTry the closed beta Jupyter notebooks for LLM functionality:\n\nWhat to test\nWhere to test\n\nProvide feedback or get help:\n\nJoin our community Slack\nOpen a support ticket\n\n\n\nWhat to test\nWe provide Jupyter notebooks that include sample code you can run directly with the ValidMind Developer Framework:\n\n\n\n\n\n\nWhat is “Yes”?\n\n\n\nThroughout the early access for friends and family, we will update this table, so check back on a weekly basis.Can’t wait? Much of the sample code in the notebooks is already on Jupyter Hub, though it may not be as easily consumable.\n\n\n\n\n\nReady\nNotebook\nModel Inventory\n\n\n\n\nYes\nquickstart_customer_churn_full_suite.ipynbUseful if you have never tried ValidMind before. Walks you through the basics of our Developer Framework and the Platform UI.\n[Demo] Customer Churn Model Documentation — Full Suite\n\n\nYes\nnlp_and_llm/prompt_validation_demo.ipynbGuides you through running and documenting prompt validation tests for a large language model (LLM) specialized in sentiment analysis for financial news.\n[Demo] Foundation Model - Text Sentiment Analysis\n\n\nYes\nnlp_and_llm/foundation_models_integration_demo.ipynbGuides you through the process of documenting a large language model (LLM) specialized in sentiment analysis for financial news.\n[Demo] Foundation Model - Text Sentiment Analysis\n\n\nYes\nnlp_and_llm/foundation_models_summarization_demo.ipynbGuides you through the process of documenting a large language model (LLM) specialized in summarization of financial news.\n[Demo] Foundation Model - Text Summarization\n\n\nYes\nnlp_and_llm/hugging_face_integration_demo.ipynbIntroduces you to the process of documenting an NLP model specialized in sentiment analysis of financial news.\n[Demo] Hugging Face - Text Sentiment Analysis\n\n\nYes\nnlp_and_llm/hugging_face_summarization_demo.ipynbIntroduces you to the process of documenting an NLP model specialized in summarization of financial news.\n[Demo] Hugging Face - Text Summarization\n\n\nYes\nnlp_and_llm/tutorial_time_series_forecasting.ipynbGuides you through the process of documenting and testing time series forecasting models\n[Demo] Interest Rate Time Series Forecasting Model\n\n\n\n\n\n\nWhere to test\n\nDeveloper Framework\nJupyter notebooks (code samples) are available in: Jupyter Hub\nAlternatively, you can download the notebooks locally and try them out in your own developer environment.\n\n\nPlatform UI\nJupyter notebooks will often tell you where to look specifically after you run the sample code, but the login for the Platform UI is always the same: https://app.prod.validmind.ai/"
  },
  {
    "objectID": "guide/get-started-closed-beta.html#provide-feedback",
    "href": "guide/get-started-closed-beta.html#provide-feedback",
    "title": "Welcome to the Closed Beta!",
    "section": "Provide feedback",
    "text": "Provide feedback\n\nJoin our community Slack\nHave feedback or questions? We sponsor a Slack community and have set up a dedicated channel for the closed beta: #community-beta. We welcome your input.\nOur Slack community is not just for our products but also aims to foster discussions between AI risk practitioners and those involved in model risk management (MRM). Feel free to take a look around the other channels that are available and stay a while.\n\n\nOpen a support ticket\nTo get help from a human during the closed beta, send an email to support@validmind.ai."
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html",
    "href": "guide/try-developer-framework-with-docker.html",
    "title": "Try it with Docker Desktop",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with the ValidMind Docker image."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#prerequisites",
    "href": "guide/try-developer-framework-with-docker.html#prerequisites",
    "title": "Try it with Docker Desktop",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have Docker Desktop installed on your machine."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#steps",
    "href": "guide/try-developer-framework-with-docker.html#steps",
    "title": "Try it with Docker Desktop",
    "section": "Steps",
    "text": "Steps\n\nFrom the command line, pull the latest ValidMind Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nRun the ValidMind Docker image:\ndocker run -it -p 8888:8888 validmind/validmind-jupyter-demo\nAfter the command completes, you should see a message that Jupyter Server is running similar to this:\n[I 2023-05-18 21:53:06.030 ServerApp] Serving notebooks from local directory: /app\n    1 active kernel\n    Jupyter Server 2.5.0 is running at:\n    http://032c824982aa:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\n        http://127.0.0.1:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\nCopy the browser URL that starts with http://127.0.0.1:8888 from the message and paste it into a new browser tab.\nAfter JupyterLab opens in your browser, you should see a link for our Quickstart_Customer Churn_full_suite.ipynb notebook.\nDouble click the notebook to open it:"
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#whats-next",
    "href": "guide/try-developer-framework-with-docker.html#whats-next",
    "title": "Try it with Docker Desktop",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in machine learning (ML), natural language processing (NLP), and large language models (LLMs). The platform offers tools that cater to both model developers and validators, simplifying key aspects of model risk management."
  },
  {
    "objectID": "guide/get-started.html#welcome-to-validmind",
    "href": "guide/get-started.html#welcome-to-validmind",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in machine learning (ML), natural language processing (NLP), and large language models (LLMs). The platform offers tools that cater to both model developers and validators, simplifying key aspects of model risk management."
  },
  {
    "objectID": "guide/get-started.html#what-do-i-use-the-validmind-platform-for",
    "href": "guide/get-started.html#what-do-i-use-the-validmind-platform-for",
    "title": "Get started",
    "section": "What do I use the ValidMind platform for?",
    "text": "What do I use the ValidMind platform for?\nModel developers and validators play important roles in managing model risk, including risk that stems from generative AI and machine learning models. From complying with regulations to ensuring that institutional standards are followed, your team members are tasked with the careful documentation, testing, and independent validation of models.\nThe purpose of these efforts is to ensure that good risk management principles are followed throughout the model lifecycle. To assist you with these processes of documenting and validating models, ValidMind provides a number of tools that you can employ regardless of the technology used to build your models.\n\n\n\n\n\nThe ValidMind AI risk platform provides two main products components:\n\nThe Developer Framework is a library of tools and methods designed to automate generating model documentation and running validation tests. The framework is designed to be platform agnostic and integrates with your existing development environment.\nFor Python developers, a single installation command provides access to all the functions:\npip install validmind\nThe ValidMind AI Risk Platform is an easy-to-use web-based UI that enables you to track the model lifecycle:\n\n\nCustomize workflows to manage the model documentation and validation process.\nReview and edit the documentation and test metrics generated by the Developer Framework.\nCollaborate with and capture feedback from model developers and model validators.\nGenerate validation reports and approvals\n\nFor more information about the benefits that ValidMind can offer, check out the ValidMind overview.\n\n\n\n\n\n\n Key ValidMind concepts\n\n\n\n\n\n\n\nModel documentation\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model. Provides a comprehensive record and description of a quantitative model. Should encompass all relevant information about the model in accordance with:\n\n\n\nIntended use\nRegulatory requirements set by regulatory bodies\nModel risk policies set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\n\n\nDocumentation project\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s lifecycle constitutes a new project and can be configured with its own templates and workflows.\n\nTemplate, Documentation Template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results.\n\n\nModel documentation is populated when the tests contained in a template run successfully, completing the test suite. This process ensures that the model meets all the specified requirements. At the same time, template placeholders get populated with content that documents the model. This content is generated by the ValidMind Developer Framework, providing a coherent structure for model information.\nEssentially, our platform scans these templates, identifies all tests, and systematically organizes them into a well-structured test suite. This automation enhances the efficiency and consistency of the validation process.\nThe criteria for these templates are typically provided by your model risk management team. We provide some templates out of the box that you can use and programmatically customize to suit the requirements of each model use case. This task of customization is usually performed by an administrator, who ensures that the templates align with the organizational standards and specific needs of each model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest suites\n\nA collection of tests which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\n\n\nExample: the classifier_full_suite test suite runs the tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nOn the ValidMind platform, everything starts with the model inventory: you first register a new model and then manage the model lifecycle through the different activities that are part of your existing model risk management processes.\n\nApproval workflow\nA typical high-level model approval workflow looks like this:\n\n\n\n\ngraph LR\n    A[Model&lt;br&gt;registration] --&gt; B[Initial&lt;br&gt;validation]\n    B --&gt; C[Validation&lt;br&gt;approval]\n    C --&gt; D[In production]\n    D --&gt; E[Periodic review&lt;br&gt;and revalidation]\n    E --&gt; B\n\n\n\n\n\n\n\nNew model registration\n\nSelect a documentation template when registering a new inventory model to start a documentation project. You then use the model inventory to manage the metadata associated with the model, including all compliance and regulatory attributes.\n\nInitial validation\n\nTriggers a new documentation workflow to yield a model that will be ready for production deployment after its documentation and validation reports have been approved.\n\nValidation approval\n\nPerform validation of the model to ensure that it meets the needs for which it was designed. You can also connect to third-party systems to send events when a model has been approved for production.\n\nIn production\n\nUse the model in production while ensuring its ongoing reliability, accuracy, and compliance with regulations by monitoring the model’s performance.\n\nPeriodic review and revalidation\n\nAs part of regular performance monitoring or change management, you follow a process similar to that seen in the Initial validation step.\n\n\n\n\nDocumentation workflow\nOut of the box, the documentation project workflow is configured like this:\n\n\n\n\ngraph LR\n    A[In documentation] --&gt; B[In validation]\n    B --&gt; C[In review /&lt;br&gt;under approval]\n    C --&gt; D[Approved]\n\n\n\n\n\n\n\nIn documentation\n\nYour model developers use the ValidMind Developer Framework to run validation tests and generate automated model documentation. They then use the ValidMind UI to populate qualitative documentation sections. This phase is the most critical and involved for model developers.\n\n\nTo learn more about documenting models, see Get started with the Developer Framework.\n\nIn validation\n\nYour model validators review the model documentation and challenge the model:\n\n\n\nCollaborate in the ValidMind UI to facilitate question and answer threads to collect more information from model developers.\nChallenge the model with the ValidMind Developer Framework to replicate test results and conduct your own testing.\n\nValidators then use the ValidMind UI to generate a validation report and manage the findings discovered during the model review process.\n\nIn review / under approval\n\nSenior risk managers and executives use the ValidMind UI to review validation report and findings to make a final decision.\n\n\nThe generated validation report contains detailed information about the models being evaluated, including their underlying assumptions, methodologies, and performance metrics. It also highlights any potential weaknesses or vulnerabilities that may be present in the models.\n\nApproved\n\nApproval signifies the final endorsement and authorization for a model to be used in production. Approval occurs after careful examination and validation of the model, ensuring that it meets institutional standards and guidelines, and regulatory requirements."
  },
  {
    "objectID": "guide/get-started.html#next-steps",
    "href": "guide/get-started.html#next-steps",
    "title": "Get started",
    "section": "Next steps",
    "text": "Next steps\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\nThe fastest way to explore what ValidMind can offer is with our Quickstart, where you can:\n\nTry out our Developer Framework with a code sample\nExplore the ValidMind Platform UI\n\nIf you have already tried the Quickstart, more how-to instructions and links to our FAQs can be found under Next steps."
  },
  {
    "objectID": "guide/overview-product-tour.html",
    "href": "guide/overview-product-tour.html",
    "title": "Product tour",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "guide/overview-product-tour.html#related-topics",
    "href": "guide/overview-product-tour.html#related-topics",
    "title": "Product tour",
    "section": "Related topics",
    "text": "Related topics\nReady to try out ValidMind? Try the Quickstart."
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developer Framework section."
  },
  {
    "objectID": "guide/view-templates.html",
    "href": "guide/view-templates.html",
    "title": "View templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-templates.html#prerequisites",
    "href": "guide/view-templates.html#prerequisites",
    "title": "View templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-templates.html#steps",
    "href": "guide/view-templates.html#steps",
    "title": "View templates",
    "section": "Steps",
    "text": "Steps\n\nFrom the ValidMind Platform homepage, go to Templates on the left.\nClick on one of the available templates to view the YAML configuration file.\nIn the configuration file that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the Developer Framework that feed into the template\n\n\n\n\n\n\n\n\n\n\n\nTemplates can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-templates.html#related-topics",
    "href": "guide/view-templates.html#related-topics",
    "title": "View templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/upload-to-documentation-project.html",
    "href": "guide/upload-to-documentation-project.html",
    "title": "Upload to Your Documentation Project",
    "section": "",
    "text": "You are now ready to modify the Quickstart notebook to upload to your own project that you created earlier."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html#steps",
    "href": "guide/upload-to-documentation-project.html#steps",
    "title": "Upload to Your Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nReopen the Quickstart notebook you accessed earlier.\nIn the Quickstart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you created your new documentation project:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html#whats-next",
    "href": "guide/upload-to-documentation-project.html#whats-next",
    "title": "Upload to Your Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nReady to learn more about how you can use ValidMind? Check out Next Steps."
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "What is ValidMind?",
    "section": "",
    "text": "The ValidMind platform is a suite of tools helping developers, data scientists and risk & compliance stakeholders identify potential risks in their AI and large language models, and generate robust, high-quality model documentation that meets regulatory requirements.\nThe platform is adept at handling many use cases, including models compatible with the Hugging Face Transformers API, and GPT 3.5, GPT 4, and hosted LLama2 and Falcon-based models (focused on text classification and text summarization use cases).\nIn addition to LLMs, ValidMind can also handle testing and documentation generation for a wide variety of models, including:\nWhat sets ValidMind apart is its focus on simplifying complex tasks for both model developers and validators. By automating critical and often tedious aspects of the model lifecycle, such as documentation, validation, and testing, we enable model developers to concentrate on building better models.\nWe do all of this while making it easy to align with regulatory guidelines on model risk management in the United States, the United Kingdom, and Canada. These regulations include the Federal Reserve’s SR 11-7, the UK’s SS1/23 and CP6/22), and Canada’s Guideline E-23."
  },
  {
    "objectID": "guide/overview.html#automated-model-testing-documentation",
    "href": "guide/overview.html#automated-model-testing-documentation",
    "title": "What is ValidMind?",
    "section": "Automated model testing & documentation",
    "text": "Automated model testing & documentation\nOur developer framework streamlines the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence or machine learning models, and large language models (LLMs).\nIts main purpose is to automate the documentation process, ensuring that it aligns with regulatory and compliance standards.\nRead more …"
  },
  {
    "objectID": "guide/overview.html#model-risk-governance-management",
    "href": "guide/overview.html#model-risk-governance-management",
    "title": "What is ValidMind?",
    "section": "Model risk governance management",
    "text": "Model risk governance management\nOur ValidMind model risk management platform offers an integrated platform to manage validation reports, track findings, and report on model risk compliance across your model portfolio.\nIts main purpose is to enable your organization to monitor and manage models effectively, focusing on mitigating risks, maintaining governance, and ensuring compliance throughout the entire enterprise.\nRead more …"
  },
  {
    "objectID": "guide/overview.html#ready-to-try-out-validmind",
    "href": "guide/overview.html#ready-to-try-out-validmind",
    "title": "What is ValidMind?",
    "section": "Ready to try out ValidMind?",
    "text": "Ready to try out ValidMind?\nOur Quickstart is the quickest and easiest way to try out our product features."
  },
  {
    "objectID": "guide/join-community.html",
    "href": "guide/join-community.html",
    "title": "ValidMind",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nThe validation guidelines for each template can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s next",
    "text": "What’s next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "href": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/view-all-test-plans.html",
    "href": "guide/view-all-test-plans.html",
    "title": "View all test suites",
    "section": "",
    "text": "Learn how to use list_suites(), list_tests(), and describe_suites() methods to view and describe test suites and tests available in the Developer Framework."
  },
  {
    "objectID": "guide/view-all-test-plans.html#prerequisites",
    "href": "guide/view-all-test-plans.html#prerequisites",
    "title": "View all test suites",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are working on an active documentation project\nYou have already installed the ValidMind client library in your developer environment"
  },
  {
    "objectID": "guide/view-all-test-plans.html#steps",
    "href": "guide/view-all-test-plans.html#steps",
    "title": "View all test suites",
    "section": "Steps",
    "text": "Steps\n\nInitialize the client library.\nUse list_suites() and list_tests() to view the list of all available test suites and tests.\nExamples:\n\nList all available test suites currently available in the the Developer Framework:\nvm.test_suites.list_suites()\nList all available individual tests currently available in the Developer Framework:\nvm.tests.list_tests() \n\nUse describe_testsuite() to list all the tests included in a specific test suite:\nExample: The following code will list tests included in the tabular_data_quality test suite:\nvm.test_suites.describe_suite(\"tabular_data_quality\")"
  },
  {
    "objectID": "guide/view-all-test-plans.html#related-topics",
    "href": "guide/view-all-test-plans.html#related-topics",
    "title": "View all test suites",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the Developer Framework\nValidMind Developer Framework API Reference"
  },
  {
    "objectID": "guide/editions-and-features.html",
    "href": "guide/editions-and-features.html",
    "title": "Editions and features",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions-and-features.html#editions",
    "href": "guide/editions-and-features.html#editions",
    "title": "Editions and features",
    "section": "Editions",
    "text": "Editions\n\nDeveloper Edition\nThe Developer Edition is the ideal training ground for developers to play around with ValidMind’s automated model documentation and to test the robustness of our developer framework, documentation, and testing features. The Developer Edition is free, allowing developers who are new to model documentation and model risk management to build, implement, test, and maintain higher quality models and model documentation.\nThe Developer Edition is only for personal testing purposes and cannot be used as a commercial model documentation or model risk management solution.\n\n\nEssential Edition\nWith the Essential Edition, you get an advanced model risk management (MRM) solution. It offers your organization all the features and services of the Developer Edition, plus additional features tailored to the needs of larger-scale organizations.\n\n\nBusiness Critical\nProvides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. This edition encompasses all features and services of the Essential Edition but within a separate ValidMind environment, isolated from other ValidMind accounts via Virtual Private ValidMind (VPV). VPV accounts do not share resources with non-VPV accounts."
  },
  {
    "objectID": "guide/editions-and-features.html#features",
    "href": "guide/editions-and-features.html#features",
    "title": "Editions and features",
    "section": "Features",
    "text": "Features\n\n\n\n\nModel development & documentation\nDeveloper\nEssential\nBusiness Critical\n\n\n\n\nAutomated model documentation\n\n\n\n\n\nPlatform-independent developer framework\n\n\n\n\n\nOnline documentation editing\n\n\n\n\n\nAdvanced editing & readability assistance\n\n\n\n\n\nDocumentation quality measurement\n\n\n\n\n\nOffline document ingestion\n\n\n\n\n\nFeedback capture on online document\n\n\n\n\n\nDocumentation version history management\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nStandard tests & validation libraries\n\n\n\n\n\nConfigure / customize tests & validation libraries\n\n\n\n\n\nSupport for customer-provided tests\n\n\n\n\n\nDeveloper workflow management\n\n\n\n\n\nPre-configured documentation templates & boilerplates\n\n\n\n\n\nConfigurable documentation templates & boilerplates\n\n\n\n\n\nModel validation & audit\n\n\n\n\n\nModel validation report automation\n\n\n\n\n\nFindings / issues & remediation actions tracking\n\n\n\n\n\nConfigurable approval workflows\n\n\n\n\n\nMRM workflows & validation lifecycle tracking\n\n\n\n\n\nMRM resource & workflow management\n\n\n\n\n\nCentral model inventory\n\n\n\n\n\nHistorical documentation repository /documentation CMS\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nExecutive reporting\n\n\n\n\n\nPlatform integration & support\n\n\n\n\n\nData lake integration, such as Evidence Storeand monitoring data\n\n\n\n\n\nSSO integration\n\n\n\n\n\nCustomer managed encryption\n\n\n\n\n\nSupport 8/5 (one timezone)\n\n\n\n\n\nSupport 24/7 (global)\n\n\n\n\n\nPlatform deployment\n\n\n\n\n\nMulti-tenant SaaS\n\n\n\n\n\nVirtual private ValidMind (VPV)\n\n\n\n\n\nSelf-managed VPV\n\n\n\n\n\n\nContact Us\nContact Us\nContact Us"
  },
  {
    "objectID": "guide/add-content-blocks.html",
    "href": "guide/add-content-blocks.html",
    "title": "Add content blocks to documentation",
    "section": "",
    "text": "Learn how to add new content blocks to your documentation project, to write and update your model’s documentation. This topic is relevant for model developers and model validators who want to add new sections to a documentation project."
  },
  {
    "objectID": "guide/add-content-blocks.html#what-are-content-blocks",
    "href": "guide/add-content-blocks.html#what-are-content-blocks",
    "title": "Add content blocks to documentation",
    "section": "What are content blocks?",
    "text": "What are content blocks?\nContent blocks provide you with sections that are part of a template. You can think of these sections as an empty canvas that you fill in with text, metrics, and test results. Multiple sections are joined to create a longer document with a table of contents that has different heading and subheading levels, such as 1., 1.1., and so on.\nTypes of content blocks:\n\nSimple text block\n\nCan be added anywhere on model documentation pages and edited to include additional documentation in text format.\n\nTest-driven block\n\nCan be added to display one of the supported metrics or threshold test results collected by the Developer Framework."
  },
  {
    "objectID": "guide/add-content-blocks.html#prerequisites",
    "href": "guide/add-content-blocks.html#prerequisites",
    "title": "Add content blocks to documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nYou are logged into the ValidMind Platform as a model developer"
  },
  {
    "objectID": "guide/add-content-blocks.html#steps",
    "href": "guide/add-content-blocks.html#steps",
    "title": "Add content blocks to documentation",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project you want to edit.\nInside your documentation project, navigate to the Documentation page.\nSelect one of the numbered sections, such as 1.1 Model Overview.\n\nIn your documentation, hover your mouse over the space where you want your new block to go until a horizontal dashed line with a  sign appears that indicates you can insert a new block:\n\nClick  and then select one of the available options:\n\nSimple text block: Adds a new section with a blank content block. After the new content block has been added, click  to edit the contents of the section like any other.\nTest-driven block: Select one of the options:\n\nMetric: Select one of the available metrics, such as Confusion Matrix.\nThreshold test: Select one of the available threshold tests, such as Data Quality: Skewness or Model Diagnosis: Overfit Regions.\n\n\nFor test-driven blocks, a preview of the available metrics or threshold test gets shown. Click Insert module when you are ready.\n\nAfter you have completed these steps, the new content block becomes a part of your model documentation."
  },
  {
    "objectID": "guide/add-content-blocks.html#related-topics",
    "href": "guide/add-content-blocks.html#related-topics",
    "title": "Add content blocks to documentation",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Validmind UI"
  },
  {
    "objectID": "guide/join-closed-beta.html",
    "href": "guide/join-closed-beta.html",
    "title": "ValidMind",
    "section": "",
    "text": "Image and Iframe\n\n\n    \n        \n        Loading…"
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.prod.validmind.ai.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "href": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test suites and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/edit-templates.html",
    "href": "guide/edit-templates.html",
    "title": "Edit templates",
    "section": "",
    "text": "Learn how to edit templates that get used for model documentation or for validation reports. This topic is relevant for administrators who need to configure templates for specific use cases or where the existing templates supplied by ValidMind need to be customized.\nDocumentation templates are stored as YAML files that you edit directly in the online editor. These templates are versioned and saving a documentation template after making changes or reverting to a previous version state always creates a new version."
  },
  {
    "objectID": "guide/edit-templates.html#prerequisites",
    "href": "guide/edit-templates.html#prerequisites",
    "title": "Edit templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe template you want to edit must have been added to the ValidMind Platform already.\nIf you are not sure which template or which version of a template a documentation project is using, check the Project Overview page of your documentation project. The Documentation Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/edit-templates.html#template-schema",
    "href": "guide/edit-templates.html#template-schema",
    "title": "Edit templates",
    "section": "Template schema",
    "text": "Template schema\n Schema Docs\n\n\n\n\n\n\n\n\n  Type: object     template_id Required     root    template_idType: string Unique identifier for the template.          template_name Required     root    template_nameType: string Name of the template.          version Required     root    versionType: string Version of the template.          description     root    descriptionType: string Description of the template.          sections Required     root    sectionsType: array Documentation sections of the template.  Each item of this array must be:   root    sections    sectionType: object     id Required     root    sections    sections items    idType: string Unique identifier for the section.          title Required     root    sections    sections items    titleType: string Title of the section.          description     root    sections    sections items    descriptionType: string Description of the section.          parent_section     root    sections    sections items    parent_sectionType: string ID of the parent section.          order     root    sections    sections items    orderType: integer Order of the section in the navigation menu. By default sections are ordered alphabetically. If order is specified, sections will be ordered by the order value, and then alphabetically.          default_text     root    sections    sections items    default_textType: string Default text for the section. If set, a metadata content row will be created with this text when installing the template on a given project          index_only     root    sections    sections items    index_onlyType: boolean If true, the section will be displayed in the navigation menu, but it will not be accessible via direct link.          condensed     root    sections    sections items    condensedType: boolean If true, the section will condense all of its subsections into a single section.          guidelines     root    sections    sections items    guidelinesType: array of string Documentation or validation guidelines for the section.  Each item of this array must be:   root    sections    sections items    guidelines    guidelines itemsType: string           contents     root    sections    sections items    contentsType: array Contents to be displayed on the section.  Each item of this array must be:   root    sections    sections items    contents    section_contentsType: object Single content block of the module.      content_type Required     root    sections    sections items    contents    contents items    content_typeType: enum (of string) Default: \"metadata_text\"  Must be one of: \"metadata_text\"\"dynamic\"\"metric\"\"test\"   Examples: \"metadata_text\"\n \"test\"\n          content_id     root    sections    sections items    contents    contents items    content_idType: string ID of the content to be displayed for the given content type (text, metric, testm, etc.).   Examples: \"sample_text\"\n \"section_intro\"\n          options     root    sections    sections items    contents    contents items    optionsType: object Options for the content block.   Examples: {\n    \"default_text\": \"This is a sample text block.\"\n}\n {\n    \"metric_id\": \"metric_1\",\n    \"title\": \"Custom Title for Metric 1\"\n}\n {\n    \"test_id\": \"adf_test\"\n}\n      default_text     root    sections    sections items    contents    contents items    options    default_textType: string Default text for the content block. Only applicable for metadata_text content blocks.          title     root    sections    sections items    contents    contents items    options    titleType: string Title of the content block. Only applicable for metric and test content blocks.                       Generated using json-schema-for-humans on 2023-06-08 at 14:44:40 -0700"
  },
  {
    "objectID": "guide/edit-templates.html#steps",
    "href": "guide/edit-templates.html#steps",
    "title": "Edit templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Templates.\nSelect one of the tabs for the type of template you want to edit:\n\nDocumentation Templates\nValidation Report Templates\n\nLocate the template to edit and, at the bottom of the template card, click Edit Template.\nIn the YAML editor that opens, make your changes.\n\nUse See changes to view a side-by-side comparison of your changes with the latest version of the template.\nUse Reset changes to delete your changes and return to the latest version of the template.\n\nClick Prepare new version to save your changes.\n\nAdd a description in Version notes to track the changes that were made once the version is saved.\n\n\nAfter you have saved a new version, it becomes available for use with model documentation or validation reports."
  },
  {
    "objectID": "guide/edit-templates.html#troubleshooting",
    "href": "guide/edit-templates.html#troubleshooting",
    "title": "Edit templates",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nThe documentation template editor validates the YAML changes you make and flags any errors that it finds. If you make a change that the editor cannot parse correctly, the editor will not let you save the changes until you correct the YAML.\nCommon issues with YAML include incorrect indenting, imbalanced quotes, or missing colons between keys and values. If you run into issues with incorrect YAML, check the error message provided by the template editor, as it might provide a line and column number where the error occurs."
  },
  {
    "objectID": "guide/edit-templates.html#whats-next",
    "href": "guide/edit-templates.html#whats-next",
    "title": "Edit templates",
    "section": "What’s next",
    "text": "What’s next\nYou can see which version of a template is used in the Project Overview page of each documentation project."
  },
  {
    "objectID": "guide/overview-model-documentation.html",
    "href": "guide/overview-model-documentation.html",
    "title": "Automated model testing & documentation",
    "section": "",
    "text": "The ValidMind Developer Framework is a developer framework and documentation engine designed to streamline the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence/machine learning models, and large language models (LLMs). It offers model developers a systematic approach to documenting and testing risk models with repeatability and consistency, ensuring alignment with regulatory and compliance standards.\n\n\n\n\n\nThe Developer Framework consists of a client-side library, API integration for models and testing, and validation tests that streamline the model development process. Implemented as a series of independent libraries in Python and R, our framework ensures compatibility and flexibility with diverse sets of developer environments and requirements.\nWith the Developer Framework, you can:\n\nAutomate documentation — Add comprehensive documentation as metadata while you build models to be shared with model validators, streamlining and speeding up the process.\nRun test suites — Identify potential risks for a diverse range of statistical and AI/LLM/ML models by assessing data quality, model outcomes, robustness, and explainability.\nIntegrate with your development environment — Seamlessly incorporate the framework into your existing model development environment, connecting to your existing model code and data sets.\nUpload documentation data — Send qualitative and quantitative test data to the AI risk platform to generate the model documentation for review and approval, fostering effective collaboration with model reviewers and validators."
  },
  {
    "objectID": "guide/overview-model-documentation.html#the-validmind-developer-framework",
    "href": "guide/overview-model-documentation.html#the-validmind-developer-framework",
    "title": "Automated model testing & documentation",
    "section": "",
    "text": "The ValidMind Developer Framework is a developer framework and documentation engine designed to streamline the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence/machine learning models, and large language models (LLMs). It offers model developers a systematic approach to documenting and testing risk models with repeatability and consistency, ensuring alignment with regulatory and compliance standards.\n\n\n\n\n\nThe Developer Framework consists of a client-side library, API integration for models and testing, and validation tests that streamline the model development process. Implemented as a series of independent libraries in Python and R, our framework ensures compatibility and flexibility with diverse sets of developer environments and requirements.\nWith the Developer Framework, you can:\n\nAutomate documentation — Add comprehensive documentation as metadata while you build models to be shared with model validators, streamlining and speeding up the process.\nRun test suites — Identify potential risks for a diverse range of statistical and AI/LLM/ML models by assessing data quality, model outcomes, robustness, and explainability.\nIntegrate with your development environment — Seamlessly incorporate the framework into your existing model development environment, connecting to your existing model code and data sets.\nUpload documentation data — Send qualitative and quantitative test data to the AI risk platform to generate the model documentation for review and approval, fostering effective collaboration with model reviewers and validators."
  },
  {
    "objectID": "guide/overview-model-documentation.html#simple-installation",
    "href": "guide/overview-model-documentation.html#simple-installation",
    "title": "Automated model testing & documentation",
    "section": "Simple installation",
    "text": "Simple installation\nInstall the Developer Framework with: pip install validmind"
  },
  {
    "objectID": "guide/overview-model-documentation.html#docs-as-code",
    "href": "guide/overview-model-documentation.html#docs-as-code",
    "title": "Automated model testing & documentation",
    "section": "Docs-as-code",
    "text": "Docs-as-code\n\n\nWhat the Developer Framework offers:\n\nGenerates documentation artifacts utilizing the context of the model and dataset, the model’s metadata, and the chosen documentation template.\nCan be easily imported into your local model development environment. The supported platforms include Python and R.\nDual-licensed: the framework is available as open-source under AGPL v3 license and also with a commercial software license.\n\n\n\nimport validmind as vm\n\nvm.init(project=\"PROJECT_IDENTIFIER\")\nvm_dataset = vm. log_dataset(\n      df,\n      \"training\",\n      targets=targets,\n)\nvm. run_dataset_tests(df, vm_dataset=vm_dataset)\nvm. Log_model (model)\nvm. log_training_metrics (model, x_train, y_train)\nvm. run_model_tests (model, x_test, y_test)\n\n\nHow the Developer Framework works:\n\nThe tests and functions are executed automatically, following pre-configured templates tailored for specific model use cases. This ensures that minimum documentation requirements are consistently fulfilled.\nThe framework integrates with ETL/data processing pipelines using connector interfaces. This enables the extraction of relationships between raw data sources and their corresponding post-processed datasets, such as those preloaded session instances received from platforms like Spark and Snowflake."
  },
  {
    "objectID": "guide/overview-model-documentation.html#extensible-by-design",
    "href": "guide/overview-model-documentation.html#extensible-by-design",
    "title": "Automated model testing & documentation",
    "section": "Extensible by design",
    "text": "Extensible by design\n\n\nIn Financial Services, our platform supports various model types, including:\n\nTraditional machine learning models (ML) such as tree-based models and neural network models.\nNatural language processing models (NLP) for text analysis and understanding.\nLarge language models (LLMs) in beta testing phase, offering advanced language capabilities.\nTraditional statistical models like Ordinary Least Squares (OLS) regression, Logistic regression, Time Series models, and more.\n\nRead more …\n\n\nOur platform is designed to be highly extensible to cater to our customers’ specific requirements. You can expand its functionality in the following ways:\n\nYou can easily add support for new models and data types by defining new classes within the framework. We provide templates to guide you through this process. Read more …\nTo include custom tests in the library, you can define new functions. We offer templates to help you create these custom tests. Read more …\nYou have the flexibility to integrate third-party test libraries seamlessly. These libraries can be hosted either locally within your infrastructure or remotely, for example, on GitHub. Leverage additional testing capabilities and resources as needed. Read more …"
  },
  {
    "objectID": "guide/overview-model-documentation.html#api-integration",
    "href": "guide/overview-model-documentation.html#api-integration",
    "title": "Automated model testing & documentation",
    "section": "API integration",
    "text": "API integration\n\n\nValidMind imports the following artifacts into the documentation via SaaS API endpoint integration:\n\nMetadata about datasets and models, used to lookup programmatic documentation content, such as the stored definition for common logistic regression limitations when a logistic regression model has been passed to the ValidMind test plan to be run.\nQuality and performance metrics collected from datasets and models.\nOutput from test and test suites that have been run.\nImages, plots, visuals that were generated as part of extracting metrics and running tests.\n\n\n\n\n\n\n\n\n\n\n\n\nValidMind does NOT:\n\nSend any personal identifiable information (PII) when generating documentation reports.\nStore any customer datasets or models."
  },
  {
    "objectID": "guide/overview-model-documentation.html#ready-to-try-out-validmind",
    "href": "guide/overview-model-documentation.html#ready-to-try-out-validmind",
    "title": "Automated model testing & documentation",
    "section": "Ready to try out ValidMind?",
    "text": "Ready to try out ValidMind?\nOur Quickstart is the quickest and easiest way to try out our product features."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\nEmail support@validmind.com\nEmail support@validmind.com"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s next",
    "text": "What’s next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "href": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/glossary.html",
    "href": "guide/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This glossary of terms provides short definitions for technical terms you find commonly used in our product documentation grouped by terms related to:"
  },
  {
    "objectID": "guide/glossary.html#validmind-platform",
    "href": "guide/glossary.html#validmind-platform",
    "title": "Glossary",
    "section": "ValidMind platform",
    "text": "ValidMind platform\n\nclient library, Python client library\n\nEnables the interaction of your development environment with the ValidMind platform as part of the Developer Framework.\n\ndocumentation automation\n\nA core benefit of the ValidMind platform that allows for the automatic creation of model documentation using predefined templates and test suites.\n\ndocumentation project, project\n\nActs as a container for the model documentation and validation report of your model into which all documentation work in ValidMind is organized. Enables developers and validators to collaborate on model documentation review, status tracking, and generating validation reports.\n\nmodel inventory \n\nA feature of the ValidMind platform where you can track, manage, and oversee the lifecycle of models. Covers the full model lifecycle, including customizable documentation and approval workflows for different user roles, status and activity tracking, and periodic revalidation.\n\ntemplate, documentation template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results. When rendered, produces a document that model developers can use for model validation.\n\ntest\n\nRuns a specific quantitative test provided by the ValidMind Developer Framework on the dataset or model. Test results are sent to the ValidMind platform to generate the model documentation according to the template that is associated with the documentation project.\n\ntest suite\n\nA collection of tests which are run together to generate model documentation end-to-end for specific use-cases. For example, the classifier_full_suite test suite runs the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases.\n\nValidMind Developer Framework\n\nA suite of documentation tools and test suites designed to document models, test models for weaknesses, and identify overfit areas. Enables automating the generation of model documentation by uploading documentation, metrics, and test results to the ValidMind platform.\n\nValidMind Platform UI \n\nA hosted multi-tenant architecture that includes the cloud-based web interface, APIs, databases, documentation and validation engine, and various internal services."
  },
  {
    "objectID": "guide/glossary.html#developer-tools",
    "href": "guide/glossary.html#developer-tools",
    "title": "Glossary",
    "section": "Developer tools",
    "text": "Developer tools\n\npip\n\nA package manager for Python, used to install and manage software packages written in the Python programming language. ValidMind uses the pip command to install the Python client library that is part of the ValidMind Developer Framework so that model developers can make use of its features.\n\nJupyterHub\n\nA multi-user server provides a platform for users to interactively work with data science and scientific computing tools in a collaborative environment. ValidMind uses JupyterHub to share live code, how-to instructions, and visualizations via notebooks as part of our getting started experience for new users.\n\nJupyter notebook\n\nAllows users to create and share documents containing live code, data visualizations, and narrative text. Supports various programming languages, most notably Python, and is widely used for data analysis, machine learning, scientific research, and educational purposes. ValidMind uses notebooks to share sample code and how-to instructions with users that you can adapt to your own use case.\n\nGitHub\n\nA cloud-based platform that provides hosting for software development and version control using Git. GitHub offers collaboration tools such as bug tracking, feature requests, task management, and continuous integration pipelines. ValidMind uses GitHub to share open-source software with you."
  },
  {
    "objectID": "guide/glossary.html#ai-and-model-risk-management",
    "href": "guide/glossary.html#ai-and-model-risk-management",
    "title": "Glossary",
    "section": "AI and model risk management",
    "text": "AI and model risk management\n\n1st line of defense\n\nIn the context of model risk, the business unit(s) responsible for model development, validation, and implementation during the model lifecycle. As the 1st line of defense, model developers must document and test models to ensure that they are accurate, robust, and fit for purpose.\n\n2nd line of defense\n\nIn the context of model risk, an independent oversight function that provides a governance framework for the model lifecycle. As the 2nd line of defense, model validators must independently validate and challenge models created by model developers to ensure that model risk management principles are followed.\n\nmodel developer\n\nResponsible for the design, implementation, and maintenance of models to ensure they are fit-for-purpose, accurate, and aligned with business requirements. As subject matter experts, they collaborate with model validators and other business units, ensuring the models are conceptually sound and robust.\n\nmodel documentation\n\nA structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. Within the realm of model risk management, this documentation serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n\nmodel inventory\n\nA systematic and organized record of all quantitative and qualitative models used within an organization. This inventory facilitates oversight, tracking, and assessment by listing each model’s purpose, characteristics, owners, validation status, and associated risks. Also see ValidMind model inventory.\n\nmodel risk management (MRM)\n\nA structured approach to identifying, assessing, mitigating, and monitoring risks arising from the use of quantitative and qualitative models within an organization. Ensures that models are developed, validated, and used appropriately, with robust controls in place. Encompasses practices such as maintaining a model inventory, conducting periodic validations, and ensuring proper documentation.\n\nmodel risk\n\nThe potential for financial loss, incorrect decisions, or unintended consequences resulting from errors or inaccuracies in AI or machine learning models. Model risk typically arises from incorrect or inappropriate use of models, inaccurate assumptions, or limitations in data quality. Consequences of unmitigated model risk can include adverse outcomes such as financial loss, damage to reputation, and regulatory penalties, for example.\n\nmodel governance\n\nA framework of policies, procedures, and standards established to oversee the lifecycle of models within an organization. Ensures that models are developed, validated, implemented, and retired in a controlled and consistent manner, promoting accountability, transparency, and adherence to regulatory requirements.\n\nmodel validation\n\nA systematic process to evaluate and verify that a model is performing as intended, accurately represents the phenomena it is designed to capture, and is appropriate for its specified purpose. This assessment encompasses a review of the model’s conceptual soundness, data integrity, calibration, and performance outcomes, as well as testing against out-of-sample datasets. Within model risk management, model validation ensures that potential risks associated with model errors, misuse, or misunderstanding are identified and mitigated.\n\nmodel validator\n\nresponsible for conducting independent assessments of models to ensure their accuracy, reliability, and appropriateness for intended purposes. The role involves evaluating a model’s conceptual soundness, data integrity, calibration methods, and overall performance, typically using out-of-sample datasets. Model validators identify potential risks and weaknesses, ensuring that models within an organization meet established standards and regulatory requirements, and provide recommendations to model developers for improvements or modifications.\n\nthree lines of defense\n\nA structured approach to model risk management, consisting of three independent functions. The first line consists of business units responsible for model development, validation, and implementation. They ensure that models are accurate, robust, and fit for purpose. The second line is an independent model risk oversight function that provides a governance framework and guidance for model risk management. The third line is the internal or external audit function, which assesses the effectiveness of model risk management practices and controls.\n\nvalidation report\n\nA formal document produced after a model validation process, outlining the findings, assessments, and recommendations related to a specific model’s performance, appropriateness, and limitations. Provides a comprehensive review of the model’s conceptual framework, data sources and integrity, calibration methods, and performance outcomes. Within model risk management, the validation report is crucial for ensuring transparency, demonstrating regulatory compliance, and offering actionable insights for model refinement or adjustments."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nView validation guidelines"
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html",
    "href": "guide/try-developer-framework-with-jupyterhub.html",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Jupyter Hub (recommended)."
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html#steps",
    "href": "guide/try-developer-framework-with-jupyterhub.html#steps",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "Steps",
    "text": "Steps\n\nIn a web browser, go to https://jupyterhub.validmind.ai.\nClick Sign in with Auth0, enter your ValidMind email address and password credentials, and click Continue.\nIn the sidebar, double click the Quickstart_Customer Churn_full_suite.ipynb notebook:\n\nAfter the notebook opens, run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will guide you through installing the ValidMind Developer Framework, initializing the Python environment, and finally initializing the ValidMind Client Library by connecting to your own documentation project in the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the Developer Framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the Platform UI."
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html#whats-next",
    "href": "guide/try-developer-framework-with-jupyterhub.html#whats-next",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "href": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html",
    "href": "guide/samples-jupyter-notebooks.html",
    "title": "Code samples",
    "section": "",
    "text": "Our code samples, based on Jupyter notebooks, are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases."
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html#quickstart-for-customer-churn-model-documentation-full-suite",
    "href": "guide/samples-jupyter-notebooks.html#quickstart-for-customer-churn-model-documentation-full-suite",
    "title": "Code samples",
    "section": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "text": "Quickstart for Customer Churn Model Documentation — Full Suite\nLearn how to use dataset and model documentation function on a simple customer churn model. The easiest way to try the Quickstart is on JupyterHub or Google Colaboratory:\n\n\n\nJupyter Hub\n\nA a web-based platform when you can interact with Jupyter Notebook instances on a shared server. It is commonly used as a collaborative and interactive computing environment for data analysis, scientific research, and programming.\n\n\n\n\nTry it on JupyterHub:\n\n\n\n\n\n\nGoogle Colaboratory (Colab)\n\nA free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time.\n\n\n\n\nTry it on Colab:"
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html#code-samples-for-your-use-case",
    "href": "guide/samples-jupyter-notebooks.html#code-samples-for-your-use-case",
    "title": "Code samples",
    "section": "Code samples for your use case",
    "text": "Code samples for your use case"
  },
  {
    "objectID": "guide/key-validmind-concepts.html",
    "href": "guide/key-validmind-concepts.html",
    "title": "ValidMind",
    "section": "",
    "text": "Model documentation\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model. Provides a comprehensive record and description of a quantitative model. Should encompass all relevant information about the model in accordance with:\n\n\n\nIntended use\nRegulatory requirements set by regulatory bodies\nModel risk policies set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\n\n\nDocumentation project\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s lifecycle constitutes a new project and can be configured with its own templates and workflows.\n\nTemplate, Documentation Template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results.\n\n\nModel documentation is populated when the tests contained in a template run successfully, completing the test suite. This process ensures that the model meets all the specified requirements. At the same time, template placeholders get populated with content that documents the model. This content is generated by the ValidMind Developer Framework, providing a coherent structure for model information.\nEssentially, our platform scans these templates, identifies all tests, and systematically organizes them into a well-structured test suite. This automation enhances the efficiency and consistency of the validation process.\nThe criteria for these templates are typically provided by your model risk management team. We provide some templates out of the box that you can use and programmatically customize to suit the requirements of each model use case. This task of customization is usually performed by an administrator, who ensures that the templates align with the organizational standards and specific needs of each model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest suites\n\nA collection of tests which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\n\n\nExample: the classifier_full_suite test suite runs the tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/tutorials.html",
    "href": "guide/tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our tutorials provide a more targeted learning experience and cover specific scenarios or use cases."
  },
  {
    "objectID": "guide/tutorials.html#related-topics",
    "href": "guide/tutorials.html#related-topics",
    "title": "Tutorials",
    "section": "Related topics",
    "text": "Related topics\nBesides our tutorials, we also offer a Quickstart that walks you through the full experience from the Developer Framework to the Platform UI."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "href": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test suite execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test suites\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test suites and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enable their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#locate-the-framework-integration-instructions",
    "href": "guide/install-and-initialize-developer-framework.html#locate-the-framework-integration-instructions",
    "title": "Install and initialize the Developer Framework",
    "section": "Locate the framework integration instructions",
    "text": "Locate the framework integration instructions\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick Client integration and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"&lt;project-identifier&gt;\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html",
    "href": "guide/swap-documentation-project-templates.html",
    "title": "Swap Documentation Project Templates",
    "section": "",
    "text": "Learn how to swap documentation templates for projects in the ValidMind Platform UI. By swapping templates, you can keep your documentation projects up-to-date without starting a new project from scratch. This topic is useful for:\nSwapping templates allows you to switch to a completely different template, upgrade to a more recent version of your current template, or make changes to both the template and its version at the same time."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#prerequisites",
    "href": "guide/swap-documentation-project-templates.html#prerequisites",
    "title": "Swap Documentation Project Templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you are not sure which template or which version of a template a documentation project is using, check the Project Overview page of your documentation project. The Documentation Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#steps",
    "href": "guide/swap-documentation-project-templates.html#steps",
    "title": "Swap Documentation Project Templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Documentation Projects.\nOpen the documentation project that you want to update the template for. The right sidebar lists the templates currently in use.\nSelect the template currently in use under one of the following:\n\nDocumentation Template\nValidation Report Template\n\nThe window pane that opens shows the JSON for the current template along with other information, such as the name and the current version.\nClick Swap Template.\nThe window pane now shows the JON for two templates side-by-side:\n\nOn the left, your current template is shown.\nOn the right, you can select a different template and version.\n\nInitially, both templates are the same.\nOn the right, select a different template or version:\n\nTemplate: Change to a different template entirely\nVersion: Change to a different version of the template you selected\n\nFor example: Select a previous version of the template currently in use to revert to that version.\nAfter you select a different template or version, the JSON differences between the templates are highlighted.\nClick Prepare Swap.\nEnter a note to enable completing the swap and click Swap Template.\n\nAfter your model documentation template has been swapped successfully, you can now continue to work on your documentation project."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#about-swapping-templates",
    "href": "guide/swap-documentation-project-templates.html#about-swapping-templates",
    "title": "Swap Documentation Project Templates",
    "section": "About swapping templates",
    "text": "About swapping templates\n\nWhen swapping templates, only the document structure is changed. Any modifications that you might have made to content will be preserved inside each content block or section.  If you added a simple text block to your old template and want to reuse the content, you can temporarily switch back to the old template, copy the content, swap back to the new template, and then paste in the content."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#whats-next",
    "href": "guide/swap-documentation-project-templates.html#whats-next",
    "title": "Swap Documentation Project Templates",
    "section": "What’s next",
    "text": "What’s next\nYou can see which version of a template is used in the Project Overview page of each documentation project."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#related-topics",
    "href": "guide/swap-documentation-project-templates.html#related-topics",
    "title": "Swap Documentation Project Templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/overview-model-risk-management.html",
    "href": "guide/overview-model-risk-management.html",
    "title": "Model risk governance management",
    "section": "",
    "text": "Our ValidMind model risk management platform offers an integrated platform to manage validation reports, track findings, and report on model risk compliance across your model portfolio. Its main purpose is to enable your organization to monitor and manage models effectively, focusing on mitigating risks, maintaining governance, and ensuring compliance throughout the entire enterprise."
  },
  {
    "objectID": "guide/overview-model-risk-management.html#the-validmind-ai-risk-platform",
    "href": "guide/overview-model-risk-management.html#the-validmind-ai-risk-platform",
    "title": "Model risk governance management",
    "section": "The ValidMind AI Risk Platform",
    "text": "The ValidMind AI Risk Platform\nThe ValidMind Platform UI provides a comprehensive suite of tools, guidelines, and best practices. You use the platform to review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements.\n\n\n\n\n\nThe platform employs a multi-tenant architecture, hosting the cloud-based user interface, APIs, databases, and internal services. The design ensures efficient resource utilization and offers a highly scalable solution for organizations of varying sizes.\nWith the ValidMind Platform, you can:\n\nTrack your model inventory — Manage the model lifecycle, track the workflow status for models, plan for upcoming validation dates, and more.\nWork on validation projects — Collaborate with developers and validators to review documentation, add findings, keep track of review statuses, and generate validation reports.\nConfigure workflows — Set up ValidMind to follow your existing model risk management processes, manage statuses for different parts of the workflow, and get an end-to-end view of workflows and who is involved.\nUse, create, or edit tests, test suites, and templates — Create and/or configure required validation tests, test suites, and documentation templates for specific model use cases, tailoring it to your own specific needs.\nIntegrate with your stack — Import and export model documentation and validation reports."
  },
  {
    "objectID": "guide/overview-model-risk-management.html#regulatory-requirements",
    "href": "guide/overview-model-risk-management.html#regulatory-requirements",
    "title": "Model risk governance management",
    "section": "Regulatory requirements",
    "text": "Regulatory requirements\nValidMind’s platform is designed to cater to the regulatory compliance and model risk management (MRM) requirements of financial institutions, facilitating enhanced compliance with government regulations, policies concerning MRM, and emerging legislations addressing AI model risk, including risks associated with the use of large language models (LLMs).\nExamples of regulations or policies include:\n\n\nSR 11-7: Guidance on Model Risk Management\nThe Supervisory Guidance on model risk management issued by the Board of Governors of the Federal Reserve System and the Office of the Comptroller of the Currency in the United States in 2011. It provides comprehensive guidance to financial institutions on developing and maintaining a robust model risk management framework, covering aspects like model development, implementation, use, and validation. SR 11-7 is widely recognized and has become a benchmark in the industry for model risk management practices.\n\n\n \n\n\nSR 11-7 outlines these core requirements:\n\nModel Risk Management\n\n\nIdentify and mitigate risks associated with incorrect or inappropriate model usage, outputs, or implementation errors.\nEncourage “effective challenge” to identify model limitations and propose necessary changes.\nConsider materiality in model risk management based on the extent of model usage and its impact on the organization’s financial condition.\n\n\nModel Development, Implementation, and Use\n\n\nDevelop with a clear statement of purpose, sound design, theory, and logic.\nAssess rigorously data quality and relevance, robust methodologies, and appropriate documentation.\nTest to ensure accuracy, robustness, stability, and to evaluate limitations and assumptions.\n\n\nModel Validation\n\n\nBe an integral part for managing model risk, ensuring models perform as intended.\nIdentify and address potential errors or misuses.\n\n\nGovernance, Policies, and Controls\n\n\nEstablish a sound governance framework to oversee model risk management.\nImplement policies and controls for appropriate use and validation of models.\n\n\n\nThe regulation also mandates ongoing monitoring and periodic reviews to ensure models remain valid and effective.\nRead more …\n\n\n \n\n\nSS1/23 – Model Risk Management Principles for Banks\nA policy issued by the Prudential Regulation Authority (PRA) in the UK. It encapsulates the final model risk management principles following feedback on the earlier consultation paper CP6/22. The statement provides guidelines for banks in the UK on managing model risk effectively, with particular emphasis on strategic planning and technical capabilities. It outlines principles and amendments, like clarifications on model complexity factors, senior management function responsibilities, and inclusion of dynamic adjustments in model change management, aiming to standardize MRM practices across UK banks and foster the safe adoption of emerging technologies, such as machine learning, artificial intelligence, and large language models (LLMs).\n\n\nSS1/23 outlines these core principles:\n\nModel Identification and Model Risk Classification\n\n\nEnsure a structured approach to accurately identify and categorize models within the model risk management (MRM) framework.\nFacilitate the proper management and oversight of models, aiding in the alignment of model risk management efforts with organizational risks and objectives.\n\n\nGovernance\n\n\nEstablish a structured oversight mechanism for effective model risk management, delineating clear responsibilities and authorities.\nEnsure accountability, transparency, and effective communication within the organization regarding model risks and controls.\n\n\nModel Development, Implementation, and Use\n\n\nEmphasize the correct development, deployment, and utilization of models as per the guidelines laid down in the MRM framework.\nEnsure models are developed and utilized in a manner consistent with their intended purposes and within acceptable risk boundaries.\n\n\nIndependent Model Validation\n\n\nStress the importance of independent validation to ascertain model performance, accuracy, and identify potential issues.\nProvide an objective assessment of models to ensure they are functioning as intended and to identify any potential areas of improvement or correction.\n\n\nModel Risk Mitigants\n\n\nUnderline the necessity for measures to mitigate risks associated with model use, including the identification and implementation of controls.\nHelp in reducing the potential adverse impact of model risks on the organization’s financial condition, reputation, and regulatory compliance.\n\n\n\nThe regulation encourages a proportionate application of these principles based on the size and complexity of the institution.\nRead more …\n\nAround the globe\nOther, similar guidelines and policies that our platform is designed to help you with include:\n\n\nGuideline-E23: Enterprise-Wide Model Risk Management for Deposit-Taking Institutions\nIssued by the Office of the Superintendent of Financial Institutions (OSFI) in Canada, it outlines minimum prudent practices for model development, review, approval, use, and modification​​.\nRead more …\n\n\nPrinciples for Model Risk Management\nIssued by the Financial Services Agency (FSA) in Japan in June 2021, this document was finalized after a consultation period and outlines principles for managing model risk​​​.\nRead more …\n\n\n\n\nMeeting regulatory requirements with ValidMind\nValidMind, as a robust tool for implementing Model Risk Management (MRM) best practices, including the three lines of defense, significantly aids organizations in adhering to the regulatory guidelines set forth by SR 11:7 and SS1/23.\n\n\n\nFirst line of defense — model developers\n\nValidMind offers a suite of tools for model developers, facilitating thorough documentation and rigorous testing of models, aligning with the regulatory expectations of both SR 11:7 and SS1/23, particularly for models under regulatory purview.\n\nSecond line of defense — model validators\n\nThe platform empowers model validators with the ability to independently validate models ensuring adherence to the organization’s MRM principles throughout the model lifecycle, a core requirement of these regulations.\n\nThird line of defense — auditors\n\nEnabling internal and external audits provides an independent and objective assurance to the organization by assessing the effectiveness and efficiency of controls within the model risk management framework. It evaluates how well the first and second lines of defense are functioning, ensuring adherence to regulatory and organizational standards, thereby promoting a robust model risk management environment.\n\nModel inventory\n\nThe Model Inventory feature encapsulates a centralized repository for all models, aiding in streamlined tracking, management, and monitoring, simplifying compliance with the inventory mandates specified in SR 11:7 and SS1/23.\n\nLifecycle management and custom workflows\n\nValidMind’s capabilities extend to effective model lifecycle management through configurable workflows. This structured approach to managing model risks across various lifecycle stages significantly aids in meeting the rigorous management and oversight expectations set by SR 11:7 and SS1/23.\n\n\n\n\n\n\nModel documentation automation\n\nBy automating model documentation through configurable templates and test plans, ValidMind ensures consistent and accurate documentation capture, directly aligning with the documentation standards stipulated in these regulatory guidelines.\n\nModel validation and approval\n\nWith automated validation features and comprehensive risk assessment tools, ValidMind aligns with the effective validation criteria and thorough risk evaluation mandates of SR 11:7 and SS1/23.\n\nCommunication and tracking\n\nThe built-in communication and tracking functionality of ValidMind facilitates seamless collaboration and understanding among stakeholders regarding model usage, limitations, and risks, fostering a collaborative environment as encouraged by these regulations.\n\n\n\n\nBy integrating these features, ValidMind provides a comprehensive platform that not only simplifies the path to compliance with SR 11:7 and SS1/23 but also embeds a culture of rigorous and transparent model risk management within the organization."
  },
  {
    "objectID": "guide/overview-model-risk-management.html#ready-to-try-out-validmind",
    "href": "guide/overview-model-risk-management.html#ready-to-try-out-validmind",
    "title": "Model risk governance management",
    "section": "Ready to try out ValidMind?",
    "text": "Ready to try out ValidMind?\nOur Quickstart is the quickest and easiest way to try out our product features."
  },
  {
    "objectID": "guide/get-started-developer-framework.html",
    "href": "guide/get-started-developer-framework.html",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "",
    "text": "This section introduces you to the ValidMind Developer Framework and its functionalities. This topic is relevant for model developers who want to learn how to use the framework to document and test their models."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#what-is-the-developer-framework",
    "href": "guide/get-started-developer-framework.html#what-is-the-developer-framework",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "What is the Developer Framework?",
    "text": "What is the Developer Framework?\nValidMind’s Developer Framework provides a rich collection of documentation tools and test suites, from documenting descriptions of your dataset to validation testing your models for weak spots and overfit areas.\nYou use the framework to automate the generation of model documentation by uploading documentation artifacts and test results to the ValidMind platform.\n\n\n\n\n\n\nValidMind offers two primary methods for documenting model risk:\n\nBy generating model documentation: Through automation, the framework extracts metadata from associated datasets and models for you and generates model documentation. You can also add more documentation and tests manually using the documentation editing capabilities in the ValidMind UI.\nBy running pre-built validation tests: The framework provides a suite of validation tests for common financial services use cases. For cases where these tests do not cover everything you need, you can also extend existing test suites with your own proprietary tests or testing providers.\n\nThe Developer Framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python client library already provides all the standard functionality you might need without requiring your developers to rewrite any functions.\n\n\n\n\n\n\n Key ValidMind concepts\n\n\n\n\n\n\n\nModel documentation\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model. Provides a comprehensive record and description of a quantitative model. Should encompass all relevant information about the model in accordance with:\n\n\n\nIntended use\nRegulatory requirements set by regulatory bodies\nModel risk policies set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\n\n\nDocumentation project\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s lifecycle constitutes a new project and can be configured with its own templates and workflows.\n\nTemplate, Documentation Template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results.\n\n\nModel documentation is populated when the tests contained in a template run successfully, completing the test suite. This process ensures that the model meets all the specified requirements. At the same time, template placeholders get populated with content that documents the model. This content is generated by the ValidMind Developer Framework, providing a coherent structure for model information.\nEssentially, our platform scans these templates, identifies all tests, and systematically organizes them into a well-structured test suite. This automation enhances the efficiency and consistency of the validation process.\nThe criteria for these templates are typically provided by your model risk management team. We provide some templates out of the box that you can use and programmatically customize to suit the requirements of each model use case. This task of customization is usually performed by an administrator, who ensures that the templates align with the organizational standards and specific needs of each model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest suites\n\nA collection of tests which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\n\n\nExample: the classifier_full_suite test suite runs the tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#how-do-i-use-the-framework",
    "href": "guide/get-started-developer-framework.html#how-do-i-use-the-framework",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "How do I use the framework?",
    "text": "How do I use the framework?\nA typical high-level workflow for model developers consists of four major steps:\n\n\n\n\ngraph LR\n    A[Develop&lt;br&gt;model] --&gt; B[Generate model&lt;br&gt;documentation]\n    B --&gt; C[Refine model&lt;br&gt;documentation]\n    C --&gt; D[Submit for review]\n    C --&gt; B\n\n\n\n\n\n\n\nDevelop model\n\nIn your existing developer environment, build one or more candidate models that need to be validated. This step includes all the usual activities you already follow as a model developer.\n\nGenerate model documentation\n\nWith the ValidMind Developer Framework, generate automated model documentation and run validation tests. This step includes making use of the automation and testing functionality provided by the framework and uploading the output to the Platform UI. You can iteratively regenerate the documentation as you work though the next step of refining your documentation.\n\nRefine model documentation\n\nIn the ValidMind Platform UI, review the generated documentation and test output. Iterate over the documentation and test output to refine your model documentation. Collaborate with other developers and model validators to finalize the model documentation and get it ready for review.\n\nSubmit for review\n\nIn the ValidMind Platform UI, you submit the model documentation for review which moves the documentation workflow moves to the next phase where a model validator will review it."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#how-do-i-generate-model-documentation",
    "href": "guide/get-started-developer-framework.html#how-do-i-generate-model-documentation",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "How do I generate model documentation?",
    "text": "How do I generate model documentation?\n\nBefore you can use the Developer Framework, you need to verify that the current documentation template contains all the necessary tests for the model you are developing:\n\nThe template might already be sufficient and you only need to run the template within the Developer Framework to populate documentation.\nOr, more likely, the template might need additional tests that you can add these tests via the Developer Framework.\n\nThis process of verifying the suitability of the the current documentation template and adding more tests to the template is an iterative process:\n\n\n\n\ngraph LR\n    A[Verify template] --&gt; B[Build template]\n    B --&gt; D[Add tests and&lt;br&gt;content blocks]\n    D --&gt; E[Add external&lt;br&gt;test providers]\n    E --&gt; C[Run template]\n    C --&gt; B\n\n\n\n\n\n\n\n\n\nBuild the template\n\nWhen the documentation template requires more tests to be added, or if the documentation template does not include a specific content or test block you need:\n\n\n\nFor functionality provided by the Developer Framework: Add the relevant tests or content blocks for the model use case.\nFor tests not provided by the framework: Add your own external test provider.\n\nRun the template : When you have registered all the required tests as content blocks in the documentation template, populate the necessary model documentation by adding this call to your model:\nrun_documentation_tests()\n\n\n\n\n\n\nValidMind may not support all potential use cases or provide a universally applicable documentation template. Typically, you initiate the process of putting ValidMind into production by constructing a template specific for your own use case and then refine your the documentation project.\n\n\n\n\nEnd-to end workflow\n\n\n\nIn your modeling environment\n\n\nBuild your model.\nExport the datasets and model.\n\nNext, go to With the Developer Framework, Step 2. \n\n\nWith the Developer Framework\n\nCreate a notebook to select and build the relevant tests.\n From your modeling environment, load the trained datasets and models.\n Use the instructions from In the Platform UI, Step 3, initialize the ValidMind Developer Framework.\nSelect the relevant tests.\nReview if all tests are covered by ValidMind or your external test provider:\n\nIf all tests are NOT covered: Create and register additional tests.\nIf all tests are covered:\n\nRun the selected tests.\nReview your test results.\n\n\n\nNext, go to In the ValidMind Platform UI, Step 5. \n\n\n\nIn the ValidMind Platform UI\n\nRegister a new model.\nReview the template structure.\nLocate the framework integration instructions.\nGo to With the Developer Framework, Step 3. \n After With the Developer Framework, Step 6, add content blocks to your model documentation:\nSelect the block type:\n\nFor test-driven blocks: Select from available test provider results\nFor text blocks:\n\nFor new block:\n\nAdd new editable text content block\nReview and collaborate on the content block \n\nFor existing blocks: Select from available texts from content provider\n\n\nSubmit your documentation project for review."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#related-topics",
    "href": "guide/get-started-developer-framework.html#related-topics",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "Related Topics",
    "text": "Related Topics\n\nCode samples\nValidMind Developer Framework Reference"
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html",
    "href": "guide/collaborate-on-documentation-projects.html",
    "title": "Collaborate on documentation projects",
    "section": "",
    "text": "Learn how ValidMind enhances collaboration between model validators and developers on documentation projects. This topic is relevant for model validators who want to track changes across projects, add comments, and access revision history for real-time collaboration with model developers."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#prerequisites",
    "href": "guide/collaborate-on-documentation-projects.html#prerequisites",
    "title": "Collaborate on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#commenting",
    "href": "guide/collaborate-on-documentation-projects.html#commenting",
    "title": "Collaborate on documentation projects",
    "section": "Commenting",
    "text": "Commenting\n\nPosting comments to the documentation\n\nIn any section of the model documentation, highlight the portion of text you would like to comment on, and click the Comment button in the toolbar.\nEnter your comment and click Comment.\nYou can view the comment by clicking the highlighted text. Comments will also appear in the right sidebar.\n\n\n\nResponding to an existing comment\n\nClick the highlighted text portion to view the comment thread.\nEnter your comment and click Reply.\nYou can view the comment thread by clicking the highlighted text.\n\n\n\nResolving comment threads and viewing archived comments\n\nClick the highlighted text portion to view the thread, then click  to resolve the thread.\nTo view the resolved comment thread, click the Comment archive button in the toolbar. You can view a history of all archived comments in the Comment archive.\nTo reopen a comment thread, reply to the comment thread in the Comment archive or click the Reopen button that appears next to the highlighted text portion.\n\n\n\nEditing and deleting comments\n\nClick the highlighted text portion to access the comment thread.\nTo edit a comment in the thread, click the More options icon for the corresponding comment and click Edit.\nEdit your comment and click Save.\nTo edit a comment in a resolved thread, follow the same steps but click the Comments archive button first to access the resolved thread.\n\n\n\n\n\n\n\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#tracking-changes",
    "href": "guide/collaborate-on-documentation-projects.html#tracking-changes",
    "title": "Collaborate on documentation projects",
    "section": "Tracking changes",
    "text": "Tracking changes\n\nSuggesting a change\n\nClick the Track changes button in the toolbar to turn on suggestion mode.\nMake your changes to the documentation project. When changes tracking is enabled, other project contributers can accept or decline the suggested changes.\n\n\n\nResolving changes\n\nSuggested changes appear in green or red highlighted text, depending on if the change is adding or removing content. To accept or decline a change, click the highlighted text, then click  or . You can also reply to a suggested change.\nTo mass accept or decline suggestions, click the dropdown arror next to the Track changes button and click the desired option."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#revision-history",
    "href": "guide/collaborate-on-documentation-projects.html#revision-history",
    "title": "Collaborate on documentation projects",
    "section": "Revision history",
    "text": "Revision history\n\nSaving a version\n\nClick the Revision history button in the toolbar.\nIn the dropdown, click Save current version. Optionally, enter a version name. The default name is the date and time the latest change was made.\n\n\n\nViewing revision history\n\nClick the Revision history button in the toolbar, then click Open revision history. Here, you can view a history of all saved versions and your current version.\nTo see the the change made with each version, select the version in the right sidebar. Changes made in that version are highlighted. Hover over the highlighted content to see who made the change.\n\n\n\nRestoring a version\n\nTo restore a version, select the desired version and click Restore this version.\nThe restored version will now appear under revision history with the name: “Restored: ‘version name’”. To exit revision history without restoring a version, click Back to editing."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#real-time-collaboration",
    "href": "guide/collaborate-on-documentation-projects.html#real-time-collaboration",
    "title": "Collaborate on documentation projects",
    "section": "Real-time collaboration",
    "text": "Real-time collaboration\nUsers can simultaneously edit the documentation project, leave and respond to comments suggestions, and access revision history. Changes to the documentation project are also automatically added to ValidMind’s activity feed."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#additional-features",
    "href": "guide/collaborate-on-documentation-projects.html#additional-features",
    "title": "Collaborate on documentation projects",
    "section": "Additional features",
    "text": "Additional features\n\nSpell and grammar checker.\nMath formulas. Add math formulas to documentation by clicking the MathType button and using the toolbar, or switch to handwriting."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#related-topics",
    "href": "guide/collaborate-on-documentation-projects.html#related-topics",
    "title": "Collaborate on documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nComment on Documentation Projects"
  },
  {
    "objectID": "releases/2023-sep-27/documentation.html",
    "href": "releases/2023-sep-27/documentation.html",
    "title": "Documentation updates – September 26, 2023",
    "section": "",
    "text": "User journey improvements. We enhanced the architecture and content of our external docs site to make the user journey more efficient for model developers and model validators who are new to our products:\n\nReworked the “Get Started” section to include more conceptual information and an overview of the high-level workflows. Try it …\nRevised the “Developer Framework” section to provide an end-to-end overview of the workflow that model developers should follow as they adopt the framework. Try it …\n\n\n\n\nDocs site improvements. We made a number of incremental improvements to our user guide:\n\nNew dropdown for the Developer Framework that gives faster access to the most important bits, such as our code samples and the reference documentation.\n\nPublication date for each page that reflects the last time the source file was touched.\nPrevious and next topic footers** for related topics that make it easier to keep reading.\nExpanded overview for key ValidMind concepts with some additional information.\nLighter background for diagrams that improves legibility."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html",
    "href": "releases/2023-sep-27/highlights.html",
    "title": "September 27, 2023",
    "section": "",
    "text": "In this release, we’ve added support for large language models (LLMs) to enhance the capabilities of the ValidMind Developer Framework in preparation for the closed beta, along with a number of new demo notebooks that you can try out. Other enhancements provide improvements for the developer experience and with our documentation site."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#release-highlights",
    "href": "releases/2023-sep-27/highlights.html#release-highlights",
    "title": "September 27, 2023",
    "section": "",
    "text": "In this release, we’ve added support for large language models (LLMs) to enhance the capabilities of the ValidMind Developer Framework in preparation for the closed beta, along with a number of new demo notebooks that you can try out. Other enhancements provide improvements for the developer experience and with our documentation site."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#validmind-developer-framework-v1.19.0",
    "href": "releases/2023-sep-27/highlights.html#validmind-developer-framework-v1.19.0",
    "title": "September 27, 2023",
    "section": "ValidMind Developer Framework (v1.19.0)",
    "text": "ValidMind Developer Framework (v1.19.0)\n\n\nLarge language model (LLM) support\nWe added initial support for large language models (LLMs) in ValidMind via the new FoundationModel class. You can now create an instance of a FoundationModel and specify predict_fn and a prompt, and pass that into any test suite, for example. The predict_fn must be defined by the user and implements the logic for calling the Foundation LLM, usually via the API.\nTo demonstrate the capabilities of LLM support, this release also includes new demo notebooks:\n\n\nPrompt validation demo notebook for LLMs. As a proof of concept, we added initial native prompt validation tests to the Developer Framework, including a notebook and simple template to test out these metrics on a sentiment analysis LLM model we built.\n\n\n\n\nText summarization model demo notebook for LLMs. We added a new notebook in the Developer Framework that includes the financial news dataset, initializes a Hugging Face summarization model using the init_model interface, implements relevant metrics for testing, and demonstrates how to run a text summarization metrics test suite for an LLM instructed as a financial news summarizer.\n\n\n\n\n\n\n\nInterested in our LLM support?\n\n\n\nLarge language model support and more will be available in our closed beta. Read the announcement and sign up to take the first step in exploring all that ValidMind has to offer:\n\n\nJoin the waitlist\n\n\n\n\n\n\n\nSupport for Hugging Face models\nValidMind can now validate pre-trained models from the HuggingFace Hub, including any language model compatible with the HF transformers API.\nTo illustrate this new feature, we have included a financial news sentiment analysis demo that runs documentation tests for a Hugging Face model with text classification using the financial_phrasebank. Try it …\n\n\n\n\nA better developer experience with run_test\nWe added a new run_test helper function that streamlines running tests for you. This function allows executing any individual test independent of a test suite or a documentation template. A one-line command can execute a test, making it easier to run tests with various parameters and options. For example:\nrun_test(\"ClassImbalance\", dataset=dataset, params=params, send=True)\n\nWe also updated the Quickstart notebook to have a consistent experience: Try it …\nThis notebook:\n\nNow runs vm.preview_template() after initializing ValidMind\nNow runs vm.run_documentation_tests() instead of running a test suite that is not connected to the template\n\n\nExample usage for run_test\nDiscover existing tests by calling list_tests() or describe_test():\nlist_tests():\n\ndescribe_test():\n\nView the tests associated with a documentation template by running preview_template():\n\nUsing the test ID, run a given test and pass in additional configuration parameters and inputs:\n# No params\ntest_results = vm.tests.run_test(\n    \"class_imbalance\",\n    dataset=vm_dataset\n)\n\n# Custom params\ntest_results = vm.tests.run_test(\n    \"class_imbalance\",\n    params={\"min_percent_threshold\": 30},\n    dataset=vm_dataset\n)\nOutput: \nSend the results of the test to ValidMind by calling .log():\ntest_results.log()"
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#how-to-upgrade",
    "href": "releases/2023-sep-27/highlights.html#how-to-upgrade",
    "title": "September 27, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "",
    "text": "This interactive notebook guides model developers through the process of documenting a model with the ValidMind Developer Framework. It uses the Bank Customer Churn Prediction sample dataset from Kaggle to train a simple classification model.\nAs part of the notebook, you will learn how to train a sample model while exploring how the documentation process works:"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#validmind-at-a-glance",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, you can make use of the following resources alongside this notebook:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#before-you-begin",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#before-you-begin",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#install-the-client-library",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#install-the-client-library",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#initialize-the-client-library",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#initialize-the-client-library",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace this placeholder with the code snippet from your own project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"...\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#initialize-the-python-environment",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#initialize-the-python-environment",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#load-the-sample-dataset",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#load-the-sample-dataset",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library, along with a second, different dataset (taiwan_credit) you can try as well.\nTo be able to use either sample dataset, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\n# You can also try a different dataset with:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nInitialize a ValidMind dataset object\nBefore you can run a test suite, which are just a collection of tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to analyze\ntarget_column — the name of the target column in the dataset\nclass_labels — the list of class labels used for classification model training\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#document-the-model",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#document-the-model",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\nPrepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\nInitialize XGBoost classifier: Creates an XGBClassifier object with early stopping rounds set to 10.\nSet evaluation metrics: Specifies metrics for model evaluation as “error,” “logloss,” and “auc.”\nFit the model: Trains the model on x_train and y_train using the validation set (x_val, y_val). Verbose output is disabled.\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\n\nInitialize the training and test datasets\nWith the datasets ready, you can now initialize the training and test datasets (train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    target_column=demo_dataset.target_column\n)\n\n\n\nInitialize a model object\nAdditionally, you need to initialize a ValidMind model object (vm_model) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the full suite of tests\nThis is where it all comes together: you are now ready to run the documentation tests for the model as defined by the documentation template you looked at earlier.\nThe vm.run_documentation_tests function finds and runs every test specified in the template and then uploads all the documentation and test artifacts that get generated to the ValidMind AI Risk Platform.\nThe function takes two arguments:\n\ndataset: The data to be tested, specified as vm_dataset.\nmodel: The model to be used for testing, specified as vm_model.\n\nThe variable full_suite then holds the result of these tests.\n\nfull_suite = vm.run_documentation_tests(\n    dataset=vm_dataset,\n    model=vm_model\n)"
  },
  {
    "objectID": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#next-steps",
    "href": "notebooks/code_samples/quickstart_customer_churn_full_suite.html#next-steps",
    "title": "Quickstart for Customer Churn Model Documentation — Full Suite",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Documentation.\nExpand the following sections and take a look around:\n\n2. Data Preparation\n3. Model Development\n\n\nWhat you can see now is a much more easily consumable version of the documentation, including the results of the tests you just performed, along with other parts of your documentation project that still need to be completed. There is a wealth of information that gets uploaded when you run the full test suite, so take a closer look around, especially at test results that might need attention (hint: some of the tests in 2.1 Data description look like they need some attention).\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "",
    "text": "This notebook shows model developers how to document a natural language processing (NLP) model using the ValidMind Developer Framework. The use case is a summarization of financial news based on a dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. It shows you how to set up the ValidMind Developer Framework, initialize the client library, and load the dataset, followed by running the model validation tests provided by the framework to quickly generate documentation about the data and model."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#validmind-at-a-glance",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#before-you-begin",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#before-you-begin",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#install-the-client-library",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#initialize-the-client-library",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Go to Documentation Projects and click Create new project.\nSelect [Demo] Hugging Face - Text Summarization and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace the code below with the code snippet from your project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"....\",\n  api_key = \"...\",\n  api_secret = \"...\",\n  project = \"...\"\n)\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nHelper functions\nLet’s define the following functions to help visualize datasets with long text fields:\n\nimport textwrap\n\nfrom IPython.display import display, HTML\nfrom tabulate import tabulate\n\ndef _format_cell_text(text, width=50):\n    \"\"\"Private function to format a cell's text.\"\"\"\n    return '\\n'.join([textwrap.fill(line, width=width) for line in text.split('\\n')])\n\ndef _format_dataframe_for_tabulate(df):\n    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n    df_out = df.copy()\n\n    # Format all string columns\n    for column in df_out.columns:\n        if df_out[column].dtype == object:  # Check if column is of type object (likely strings)\n            df_out[column] = df_out[column].apply(_format_cell_text)\n    return df_out\n\ndef _dataframe_to_html_table(df):\n    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n    headers = df.columns.tolist()\n    table_data = df.values.tolist()\n    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n\ndef display_formatted_dataframe(df, num_rows=None):\n    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n    if num_rows is not None:\n        df = df.head(num_rows)\n    formatted_df = _format_dataframe_for_tabulate(df)\n    html_table = _dataframe_to_html_table(formatted_df)\n    display(HTML(html_table))\n\n\n\nLoad the dataset\nThe CNN Dailymail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail (https://huggingface.co/datasets/cnn_dailymail). The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n\nfrom datasets import load_dataset\n\ncnn_dataset = load_dataset('cnn_dailymail', '3.0.0')\ntrain_df = cnn_dataset.data['train'].to_pandas()\nval_df = cnn_dataset.data['validation'].to_pandas()\ntest_df = cnn_dataset.data['test'].to_pandas()\ntrain_df = train_df[['article','highlights']]\ntrain_df = train_df.head(20)\n\ndisplay_formatted_dataframe(train_df, num_rows=5)\n\n\ndf = train_df.head(100)\n# Load a test dataset with 100 rows only\nvm_ds = vm.init_dataset(\n    dataset=df,\n    text_column=\"article\",\n    target_column=\"highlights\",\n)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#nlp-data-quality-tests",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#nlp-data-quality-tests",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "NLP data quality tests",
    "text": "NLP data quality tests\nBefore we proceed with the analysis, it’s crucial to ensure the quality of our NLP data. We can run the data_preparation section of the template to validate the data’s integrity and suitability:\n\ntext_data_test_plan = vm.run_documentation_tests(section=\"data_preparation\", dataset=vm_ds)\n\n\nfrom transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\nsummarizer_model = pipeline(\n    task=\"summarization\",\n    model=model,\n    tokenizer = tokenizer,\n    min_length=0,\n    max_length=60,\n    truncation=True,\n    model_kwargs={\"cache_dir\": '/Documents/Huggin_Face/'},\n)  # Note: We specify cache_dir to use predownloaded models.\n\n\ndf_test = df.head(10)\n\nvm_test_ds = vm.init_dataset(\n    dataset=train_df,\n    text_column=\"article\",\n    target_column=\"highlights\",\n)\n\n\nvm_model = vm.init_model(\n    summarizer_model,\n    test_ds=vm_test_ds,\n)\n\n\nRun model validation tests\nIt’s possible to run a subset of tests on the documentation template by passing a section parameter to run_documentation_tests(). Let’s run only the tests that evaluate the model’s overall performance, including summarization metrics, by selecting the model_development section of the template:\n\nconfig={\n    \"rouge_metric\": {\n        \"rouge_metrics\": [\"rouge-1\",\"rouge-2\", \"rouge-l\"],\n    },\n}\nsummarization_results = vm.run_documentation_tests(\n    section=\"model_development\",\n    model=vm_model,\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#next-steps",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_summarization_demo.html#next-steps",
    "title": "Summarization of Financial Data Using Hugging Face NLP models",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "",
    "text": "This notebook provides an introduction for model developers on how to document a natural language processing (NLP) model using the ValidMind Developer Framework. It shows you how to set up the ValidMind Developer Framework, initialize the client library, and load the dataset, followed by performing a sentiment analysis of financial news data using several different Hugging Face transformers."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#validmind-at-a-glance",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#before-you-begin",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#before-you-begin",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#install-the-client-library",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#initialize-the-client-library",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Hugging Face - Text Sentiment Analysis and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace the code below with the code snippet from your project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key = \"...\",\n    api_secret = \"...\",\n    project = \"...\"\n)\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nDownload the test dataset\nTo perform the sentiment analysis for financial news, you need a sample dataset:\n\n\nDownload the sample dataset from https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\nThis dataset contains two columns, Sentiment and News Headline. The sentiment can be negative, neutral or positive.\nMove the CSV file that contains the dataset into the current directory.\n\n\nimport pandas as pd\n\ndf = pd.read_csv('./datasets/sentiments.csv')\nsample = df.sample(10)\nsample"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#nlp-data-quality-tests",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#nlp-data-quality-tests",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "NLP data quality tests",
    "text": "NLP data quality tests\nBefore we proceed with the analysis, it’s crucial to ensure the quality of our NLP data. We can run the “data preparation” section of the template to validate the data’s integrity and suitability.\n\nvm_ds = vm.init_dataset(\n    dataset=df,\n    text_column='Sentence',\n    target_column=\"Sentiment\"\n)\n\ntext_data_test_plan = vm.run_documentation_tests(section=\"data_preparation\", dataset=vm_ds)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#hugging-face-transformers",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#hugging-face-transformers",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "Hugging Face transformers",
    "text": "Hugging Face transformers\n\n1. Hugging Face: FinancialBERT for Sentiment Analysis\nLet’s now explore integrating and testing FinancialBERT (https://huggingface.co/ahmedrachid/FinancialBERT-Sentiment-Analysis ), a model designed specifically for sentiment analysis in the financial domain:\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nmodel = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\ntokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\nhfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n\nInitialize the ValidMind dataset\n\n# Load a test dataset with 100 rows only\nvm_test_ds = vm.init_dataset(\n    dataset=df.head(100),\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\n\n\nInitialize the ValidMind model\nWhen initializing a ValidMind model, we pre-calculate predictions on the test dataset. This operation can take a long time for large datasets.\n\nvm_model_1 = vm.init_model(\n    hfmodel,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun model validation tests\nIt’s possible to run a subset of tests on the documentation template by passing a section parameter to run_documentation_tests(). Let’s run the tests that correspond to model validation only:\n\nfull_suite = vm.run_documentation_tests(\n    section=\"model_development\",\n    dataset=vm_test_ds,\n    model=vm_model_1,\n)\n\n\n\n\n2. Hugging Face: distilRoberta-financial-sentiment\nThe distilRoberta-financial-sentiment model (https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis) was fine-tuned on the phrasebank dataset: https://huggingface.co/datasets/financial_phrasebank.\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\nhfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n\nInitialize the ValidMind model\n\nvm_model_2 = vm.init_model(\n    hfmodel,\n    test_ds=vm_test_ds,\n)\n\n\nfull_suite = vm.run_documentation_tests(\n    section=\"model_development\",\n    dataset=vm_test_ds,\n    model=vm_model_2,\n    models=[vm_model_1]\n\n)\n\n\n\n\n3. Hugging Face: Financial-RoBERTa\nThe Financial-RoBERTa model (https://huggingface.co/soleimanian/financial-roberta-large-sentiment) is another financial sentiment analysis model trained on large amounts of data including:\n\nFinancial Statements\nEarnings Announcements\nEarnings Call Transcripts\nCorporate Social Responsibility (CSR) Reports\nEnvironmental, Social, and Governance (ESG) News\nFinancial News\nEtc.\n\n\nLoad the model directly\n\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"soleimanian/financial-roberta-large-sentiment\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"soleimanian/financial-roberta-large-sentiment\")\nhfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n\nvm_model_3 = vm.init_model(\n    hfmodel,\n    test_ds=vm_test_ds,\n)\n\n\nfull_suite = vm.run_documentation_tests(\n    section=\"model_development\",\n    dataset=vm_test_ds,\n    model=vm_model_3,\n    models=[vm_model_1, vm_model_2]\n)"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#next-steps",
    "href": "notebooks/code_samples/LLM_and_NLP/hugging_face_integration_demo.html#next-steps",
    "title": "Sentiment Analysis of Financial Data Using Hugging Face NLP Models",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "",
    "text": "This notebook guides model developers through using ValidMind for running and documenting prompt validation tests for a large language model (LLM) specialized in sentiment analysis for financial news. It shows you how to set up the ValidMind Developer Framework, initialize the client library, and use a specific prompt template for analyzing the sentiment of given sentences. The prompt validation covers the initialization of a test dataset and the creation of a foundational model using ValidMind’s framework, followed by the execution of a test suite specifically designed for prompt validation. The notebook also includes example data to test the model’s ability to correctly identify sentiment as positive, negative, or neutral."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#validmind-at-a-glance",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#before-you-begin",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#before-you-begin",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nThis notebook requires an OpenAI API secret key to run. If you don’t have one, visit API keys on OpenAI’s site to create a new key for yourself. Note that API usage charges may apply.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#install-the-client-library",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#initialize-the-client-library",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Foundation Model - Text Sentiment Analysis and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace this placeholder with the code snippet from your own project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"...\",\n  api_secret = \"...\",\n  project = \"...\"\n)\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nDownload the test dataset\nTo perform the sentiment analysis for financial news, you need a sample dataset:\n\nDownload the sample dataset from https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\nThis dataset contains two columns, Sentiment and News Headline. The sentiment can be negative, neutral or positive.\nMove the CSV file that contains the dataset into the current directory.\n\n\n\nGet ready to run the analysis\nImport the ValidMind FoundationModel and Prompt classes needed for the sentiment analysis later on:\n\nfrom validmind.models import FoundationModel, Prompt\n\nCheck your access to the OpenAI API:\n\nimport os\n\nimport dotenv\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nimport openai\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef call_model(prompt):\n    return openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n    ).choices[0].message[\"content\"]\n\nSet the prompt guidelines for the sentiment analysis:\n\nprompt_template = \"\"\"\nYou are an AI with expertise in sentiment analysis, particularly in the context of financial news.\nYour task is to analyze the sentiment of a specific sentence provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the sentence.\n\nSentence to Analyze:\n```\n{Sentence}\n```\n\nPlease respond with the sentiment of the sentence denoted by one of either 'positive', 'negative', or 'neutral'.\nPlease respond only with the sentiment enum value. Do not include any other text in your response.\n\nNote: Ensure that your analysis is based on the content of the sentence and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"Sentence\"]\n\nGet your sample dataset ready for analysis:\n\nimport pandas as pd\n\ndf = pd.read_csv('./datasets/sentiments.csv')\n\ndf_test = df[:10].reset_index(drop=True)\ndf_test"
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#perform-the-prompt-validation",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#perform-the-prompt-validation",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "Perform the prompt validation",
    "text": "Perform the prompt validation\nFirst, use the ValidMind Developer Framework to initialize the dataset and model objects necessary for documentation. The ValidMind predict_fn function allows the model to be tested and evaluated in a standardized manner:\n\nvm_test_ds = vm.init_dataset(\n    dataset=df_test,\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\nvm_model = FoundationModel(\n    predict_fn=call_model,\n    prompt=Prompt(\n        template=prompt_template,\n        variables=prompt_variables,\n    ),\n    test_ds=vm_test_ds,\n)\n\nNext, use the ValidMind Developer Framework to run validation tests on the model. These tests evaluate various aspects of the prompts, including bias, clarity, conciseness, delimitation, negative instruction, and specificity.\nEach test is explained in detail, highlighting its purpose, test mechanism, and the importance of the specific aspect being evaluated. The tests are graded on a scale from 1 to 10, with a predetermined threshold, and the explanations for each test include a score, threshold, and a pass/fail determination.\n\n# Run the full suite with the following command (it will take a while):\n#\n# suite_results = vm.run_documentation_tests(dataset=vm_test_ds, model=vm_model)\n#\n# By default the tests will use GPT-3.5 as the evaluation model, to get better results use GPT:\n# os.environ[\"VM_OPENAI_MODEL\"] = \"gpt4\"\n\ntest_suite_results = vm.run_test_suite(\"prompt_validation\", model=vm_model)\n\nHere, most of the tests pass but the test for conciseness needs further attention, as it fails the threshold. This test is designed to evaluate the brevity and succinctness of prompts provided to a large language model (LLM).\nThe test matters, because a concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed."
  },
  {
    "objectID": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#next-steps",
    "href": "notebooks/code_samples/LLM_and_NLP/prompt_validation_demo.html#next-steps",
    "title": "Prompt Validation for Large Language Models (LLMs)",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 3. Model Development &gt; 3.2. Prompt Evaluation.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This notebook guides model developers through the process of automatically documenting and testing time series forecasting models. It shows you how to use the ValidMind Developer Framework to import and prepare data and before running a data validation test suite, followed by loading a pre-trained model and running a model validation test suite.\nAs part of the notebook, you will learn how to:"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#validmind-at-a-glance",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#validmind-at-a-glance",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "ValidMind at a glance",
    "text": "ValidMind at a glance\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#before-you-begin",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#before-you-begin",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nTo access the ValidMind Platform UI, you’ll need an account.\nSigning up is FREE — Create your account.\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#install-the-client-library",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#install-the-client-library",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#initialize-the-client-library",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#initialize-the-client-library",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Interest Rate Time Series Forecasting Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\nExplore available test suites\nIn this notebook we will run a collection of test suites that are available in the ValidMind Developer Framework. Test suites group together a collection of tests that are relevant for a specific use case. In our case, we will run test different test suites for time series forecasting models. Once a test suite runs successfully, its results will be automatically uploaded to the ValidMind platform.\n\nvm.test_suites.list_suites()\n\nFor our example use case we will run the following test suites:\n\ntime_series_dataset\ntime_series_model_validation"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-1-import-raw-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-1-import-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 1: Import raw data",
    "text": "Step 1: Import raw data\n\nImport FRED dataset\nFederal Reserve Economic Data, or FRED, is a comprehensive database maintained by the Federal Reserve Bank of St. Louis. It offers a wide array of economic data from various sources, including U.S. government agencies and international organizations. The dataset encompasses numerous economic indicators across various categories such as employment, consumer price indices, money supply, and gross domestic product, among others.\nFRED provides a valuable resource for researchers, policymakers, and anyone interested in understanding economic trends and conducting economic analysis. The platform also includes tools for data visualization, which can help users interpret complex economic data and identify trends over time.\nThe following code snippet imports a sample FRED dataset into a Pandas dataframe:\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\ndf = demo_dataset.load_data()\ndf.tail(10)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-2-run-data-validation-test-suite-on-raw-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-2-run-data-validation-test-suite-on-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 2: Run data validation test suite on raw data",
    "text": "Step 2: Run data validation test suite on raw data\n\nExplore the time series dataset test suites\nLet’s see what tests are included on each test suite:\n\nvm.test_suites.describe_suite(\"time_series_data_quality\")\n\n\nvm.test_suites.describe_suite(\"time_series_univariate\")\n\n\n\nInitialize the dataset\nUse the ValidMind Developer Framework to initialize the dataset object:\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\n\n\nRun time series dataset test suite on raw dataset\nNext, use the ValidMind Developer Framework to run the test suite for time series datasets:\n\nconfig = {\n    # TIME SERIES DATA QUALITY PARAMS\n    \"validmind.data_validation.TimeSeriesOutliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"validmind.data_validation.TimeSeriesMissingValues\": {\n        \"min_threshold\": 2,\n    },\n\n    # TIME SERIES UNIVARIATE PARAMS\n    \"validmind.data_validation.RollingStatsPlot\": {\n        \"window_size\": 12\n    },\n    \"validmind.data_validation.SeasonalDecompose\": {\n        \"seasonal_model\": 'additive'\n    },\n    \"validmind.data_validation.AutoSeasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n    \"validmind.data_validation.AutoStationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"validmind.data_validation.AutoAR\": {\n        \"max_ar_order\": 2\n    },\n    \"validmind.data_validation.AutoMA\": {\n        \"max_ma_order\": 2\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS\n    \"validmind.data_validation.LaggedCorrelationHeatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"validmind.data_validation.EngleGrangerCoint\": {\n        \"threshold\": 0.05\n    },\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-3-preprocess-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-3-preprocess-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 3: Preprocess data",
    "text": "Step 3: Preprocess data\n\nHandle frequencies, missing values and stationairty\n\n# Sample frequencies to Monthly\nresampled_df = df.resample(\"MS\").last()\n\n#  Remove all missing values\nnona_df = resampled_df.dropna()\n\n#  Take the first different across all variables\npreprocessed_df = nona_df.diff().dropna()"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-4-run-data-validation-test-suite-on-processed-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-4-run-data-validation-test-suite-on-processed-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 4: Run data validation test suite on processed data",
    "text": "Step 4: Run data validation test suite on processed data\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-5-load-pre-trained-models",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-5-load-pre-trained-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 5: Load pre-trained models",
    "text": "Step 5: Load pre-trained models\n\nLoad pre-trained models\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\n\n\n\nInitialize Validmind models\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(\n    dataset=train_df_A, target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(\n    dataset=test_df_A, target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(\n    dataset=train_df_B, target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(\n    dataset=test_df_B, target_column=demo_dataset.target_column)\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model=model_A,\n    train_ds=vm_train_ds_A,\n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model=model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n\nmodels = [vm_model_A, vm_model_B]"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-6-run-model-validation-test-suite-on-models",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#step-6-run-model-validation-test-suite-on-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 6: Run model validation test suite on models",
    "text": "Step 6: Run model validation test suite on models\n\nExplore the time series model validation test suite\n\nvm.test_suites.describe_test_suite(\"time_series_model_validation\")\n\n\n\nRun model validation test suite on a list of models\n\nconfig = {\n    \"validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\": {\n        \"transformation\": \"integrate\",\n    },\n    \"validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\": {\n        \"transformation\": \"integrate\",\n        \"shocks\": [0.3],\n    }\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model=vm_model_B,\n    models=models,\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#next-steps",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#next-steps",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nLog back into the Platform UI\nGo to Documentation Projects &gt; YOUR_DOCUMENTATION_PROJECT &gt; Documentation.\nExpand 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your documentation project that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html",
    "href": "notebooks/how_to/explore_tests.html",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "",
    "text": "Welcome to this comprehensive guide to the ValidMind Developer Framework tests module! In this notebook, we’ll dive deep into the utilities available for viewing and understanding the various tests that ValidMind provides. Whether you’re just getting started or looking for advanced tips, you’ll find clear examples and explanations to assist you every step of the way.\nBefore we delve into the details, let’s import the describe_test and list_tests functions from the validmind.tests module. These are the two functions that can be used to easily filter through tests and view details for individual tests.\n\nfrom validmind.tests import describe_test, list_tests"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#a-comprehensive-guide-to-finding-and-viewing-tests",
    "href": "notebooks/how_to/explore_tests.html#a-comprehensive-guide-to-finding-and-viewing-tests",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "",
    "text": "Welcome to this comprehensive guide to the ValidMind Developer Framework tests module! In this notebook, we’ll dive deep into the utilities available for viewing and understanding the various tests that ValidMind provides. Whether you’re just getting started or looking for advanced tips, you’ll find clear examples and explanations to assist you every step of the way.\nBefore we delve into the details, let’s import the describe_test and list_tests functions from the validmind.tests module. These are the two functions that can be used to easily filter through tests and view details for individual tests.\n\nfrom validmind.tests import describe_test, list_tests"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#listing-all-tests",
    "href": "notebooks/how_to/explore_tests.html#listing-all-tests",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "Listing All Tests",
    "text": "Listing All Tests\nThe list_tests function provides a convenient way to retrieve all available tests in the validmind.tests module. When invoked without any parameters, it returns a pandas DataFrame containing detailed information about each test.\n\nlist_tests()\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nThresholdTest\nBias\nEvaluates bias in a Large Language Model based on the order and distribution of exemplars in a prompt....\nvalidmind.prompt_validation.Bias\n\n\nThresholdTest\nClarity\nEvaluates and scores the clarity of prompts in a Large Language Model based on specified guidelines....\nvalidmind.prompt_validation.Clarity\n\n\nThresholdTest\nSpecificity\nEvaluates and scores the specificity of prompts provided to a Large Language Model (LLM), based on clarity,...\nvalidmind.prompt_validation.Specificity\n\n\nThresholdTest\nRobustness\nAssesses the robustness of prompts provided to a Large Language Model under varying conditions and contexts....\nvalidmind.prompt_validation.Robustness\n\n\nThresholdTest\nNegative Instruction\nEvaluates and grades the use of affirmative, proactive language over negative instructions in LLM prompts....\nvalidmind.prompt_validation.NegativeInstruction\n\n\nThresholdTest\nConciseness\nAnalyzes and grades the conciseness of prompts provided to a Large Language Model....\nvalidmind.prompt_validation.Conciseness\n\n\nThresholdTest\nDelimitation\nEvaluates the proper use of delimiters in prompts provided to Large Language Models....\nvalidmind.prompt_validation.Delimitation\n\n\nMetric\nBert Score\nEvaluates text generation models' performance by calculating precision, recall, and F1 score based on BERT...\nvalidmind.model_validation.BertScore\n\n\nMetric\nBleu Score\nAssesses translation quality by comparing machine-translated sentences with human-translated ones using BLEU score....\nvalidmind.model_validation.BleuScore\n\n\nMetric\nContextual Recall\nEvaluates a Natural Language Generation model's ability to generate contextually relevant and factually correct...\nvalidmind.model_validation.ContextualRecall\n\n\nMetric\nRouge Metrics\nEvaluates the quality of machine-generated text using various ROUGE metrics, and visualizes the results....\nvalidmind.model_validation.RougeMetrics\n\n\nMetric\nModel Metadata\nExtracts and summarizes critical metadata from a machine learning model instance for comprehensive analysis....\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nToken Disparity\nAssess and visualize token count disparity between model's predicted and actual dataset....\nvalidmind.model_validation.TokenDisparity\n\n\nMetric\nClassifier Out Of Sample Performance\nAssesses ML model's performance on out-of-sample data to measure generalization and guard against overfitting....\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifier In Sample Performance\nEvaluates ML model's in-sample performance using accuracy, precision, recall, and F1 score to assess generalization...\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegression Models Coeffs\nCompares feature importance by evaluating and contrasting coefficients of different regression models....\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs\n\n\nMetric\nBox Pierce\nDetects autocorrelation in time-series data through the Box-Pierce test to validate model performance....\nvalidmind.model_validation.statsmodels.BoxPierce\n\n\nMetric\nRegression Coeffs Plot\nVisualizes regression coefficients with 95% confidence intervals to assess predictor variables' impact on response...\nvalidmind.model_validation.statsmodels.RegressionCoeffsPlot\n\n\nMetric\nRegression Model Sensitivity Plot\nTests the sensitivity of a regression model to variations in independent variables by applying shocks and...\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\nMetric\nRegression Models Performance\nEvaluates and compares regression models' performance using R-squared, Adjusted R-squared, and MSE metrics....\nvalidmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nMetric\nZivot Andrews Arch\nEvaluates the order of integration and stationarity of time series data using Zivot-Andrews unit root test....\nvalidmind.model_validation.statsmodels.ZivotAndrewsArch\n\n\nMetric\nRegression Model Outsample Comparison\nComputes MSE and RMSE for multiple regression models using out-of-sample test to assess model's prediction accuracy...\nvalidmind.model_validation.statsmodels.RegressionModelOutsampleComparison\n\n\nMetric\nRegression Model Forecast Plot Levels\nCompares and visualizes forecasted and actual values of regression models on both raw and transformed datasets....\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nPD Rating Class Plot\nAssesses and visualizes credit risk distribution across different rating classes within a dataset via default...\nvalidmind.model_validation.statsmodels.PDRatingClassPlot\n\n\nMetric\nScorecard Histogram\nCreates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model....\nvalidmind.model_validation.statsmodels.ScorecardHistogram\n\n\nMetric\nFeature Importance And Significance\nEvaluates and visualizes the statistical significance and feature importance using regression and decision tree...\nvalidmind.model_validation.statsmodels.FeatureImportanceAndSignificance\n\n\nMetric\nL Jung Box\nAssesses autocorrelations in dataset features by performing a Ljung-Box test on each feature....\nvalidmind.model_validation.statsmodels.LJungBox\n\n\nMetric\nLogistic Reg Prediction Histogram\nGenerates and visualizes histograms of the Probability of Default predictions for both positive and negative...\nvalidmind.model_validation.statsmodels.LogisticRegPredictionHistogram\n\n\nMetric\nJarque Bera\nAssesses normality of dataset features in an ML model using the Jarque-Bera test....\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nPhillips Perron Arch\nExecutes Phillips-Perron test to assess the stationarity of time series data in each ML model feature....\nvalidmind.model_validation.statsmodels.PhillipsPerronArch\n\n\nMetric\nKolmogorov Smirnov\nExecutes a feature-wise Kolmogorov-Smirnov test to evaluate alignment with normal distribution in datasets....\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nResiduals Visual Inspection\nProvides a comprehensive visual analysis of residuals for regression models utilizing various plot types....\nvalidmind.model_validation.statsmodels.ResidualsVisualInspection\n\n\nMetric\nShapiro Wilk\nEvaluates feature-wise normality of training data using the Shapiro-Wilk test....\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nScorecard Bucket Histogram\nEvaluates and visualizes distribution of risk categories in a classification model's scores, useful in credit risk...\nvalidmind.model_validation.statsmodels.ScorecardBucketHistogram\n\n\nMetric\nRegression Model Insample Comparison\nEvaluates and compares in-sample performance of multiple regression models using R-Squared, Adjusted R-Squared,...\nvalidmind.model_validation.statsmodels.RegressionModelInsampleComparison\n\n\nMetric\nRegression Feature Significance\nAssesses and visualizes the statistical significance of features in a set of regression models....\nvalidmind.model_validation.statsmodels.RegressionFeatureSignificance\n\n\nMetric\nRegression Model Summary\nEvaluates regression model performance using metrics including R-Squared, Adjusted R-Squared, MSE, and RMSE....\nvalidmind.model_validation.statsmodels.RegressionModelSummary\n\n\nMetric\nKPSS\nExecutes KPSS unit root test to validate stationarity of time-series data in machine learning model....\nvalidmind.model_validation.statsmodels.KPSS\n\n\nMetric\nLilliefors\nAssesses the normality of feature distributions in an ML model's training dataset using the Lilliefors test....\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nLogistic Reg Cumulative Prob\nVisualizes cumulative probabilities of positive and negative classes for both training and testing in logistic...\nvalidmind.model_validation.statsmodels.LogisticRegCumulativeProb\n\n\nMetric\nRuns Test\nExecutes Runs Test on ML model to detect non-random patterns in output data sequence....\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nScorecard Probabilities Histogram\nEvaluates risk classification of a model by visualizing the distribution of default probability across score...\nvalidmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram\n\n\nMetric\nDFGLS Arch\nExecutes Dickey-Fuller GLS metric to determine order of integration and check stationarity in time series data....\nvalidmind.model_validation.statsmodels.DFGLSArch\n\n\nMetric\nAuto ARIMA\nEvaluates ARIMA models for time-series forecasting, ranking them using Bayesian and Akaike Information Criteria....\nvalidmind.model_validation.statsmodels.AutoARIMA\n\n\nThresholdTest\nADF Test\nAssesses the stationarity of time series data using the Augmented Dickey-Fuller (ADF) test....\nvalidmind.model_validation.statsmodels.ADFTest\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\nMetric\nRegression Model Forecast Plot\nGenerates plots to visually compare the forecasted outcomes of one or more regression models against actual...\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlot\n\n\nMetric\nADF\nAssesses the stationarity of a time series dataset using the Augmented Dickey-Fuller (ADF) test....\nvalidmind.model_validation.statsmodels.ADF\n\n\nMetric\nDurbin Watson Test\nAssesses autocorrelation in time series data features using the Durbin-Watson statistic....\nvalidmind.model_validation.statsmodels.DurbinWatsonTest\n\n\nMetric\nMissing Values Risk\nAssesses and quantifies the risk related to missing values in a dataset used for training an ML model....\nvalidmind.data_validation.MissingValuesRisk\n\n\nMetric\nIQR Outliers Table\nDetermines and summarizes outliers in numerical features using Interquartile Range method....\nvalidmind.data_validation.IQROutliersTable\n\n\nMetric\nBivariate Features Bar Plots\nGenerates visual bar plots to analyze the relationship between paired features within categorical data in the model....\nvalidmind.data_validation.BivariateFeaturesBarPlots\n\n\nThresholdTest\nSkewness\nEvaluates the skewness of numerical data in a machine learning model and checks if it falls below a set maximum...\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nChecks for and quantifies the presence of duplicate entries in the dataset or a specified column....\nvalidmind.data_validation.Duplicates\n\n\nMetric\nMissing Values Bar Plot\nCreates a bar plot showcasing the percentage of missing values in each column of the dataset with risk...\nvalidmind.data_validation.MissingValuesBarPlot\n\n\nMetric\nDataset Description\nProvides comprehensive descriptive statistics and histograms for each field in a dataset, aiding in data...\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatter Plot\nCreates a scatter plot matrix to visually analyze feature relationships, patterns, and outliers in a dataset....\nvalidmind.data_validation.ScatterPlot\n\n\nThresholdTest\nTime Series Outliers\nIdentifies and visualizes outliers in time-series data using z-score method....\nvalidmind.data_validation.TimeSeriesOutliers\n\n\nMetric\nTabular Categorical Bar Plots\nGenerates and visualizes bar plots for each category in categorical features to evaluate dataset's composition....\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nAuto Stationarity\nAutomates Augmented Dickey-Fuller test to assess stationarity across multiple time series in a DataFrame....\nvalidmind.data_validation.AutoStationarity\n\n\nMetric\nDescriptive Statistics\nPerforms a detailed descriptive statistical analysis of both numerical and categorical data within a model's...\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nANOVA One Way Table\nApplies one-way ANOVA (Analysis of Variance) to identify statistically significant numerical features in the...\nvalidmind.data_validation.ANOVAOneWayTable\n\n\nMetric\nTarget Rate Bar Plots\nGenerates bar plots visualizing the default rates of categorical features for a classification machine learning...\nvalidmind.data_validation.TargetRateBarPlots\n\n\nMetric\nPearson Correlation Matrix\nEvaluates linear dependency between numerical variables in a dataset via a Pearson Correlation coefficient heat map....\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nFeature Target Correlation Plot\nVisualizes the correlation between input features and model's target output in a color-coded horizontal bar plot....\nvalidmind.data_validation.FeatureTargetCorrelationPlot\n\n\nMetric\nTabular Numerical Histograms\nGenerates histograms for each numerical feature in a dataset to provide visual insights into data distribution and...\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nMetric\nIsolation Forest Outliers\nDetects outliers in a dataset using the Isolation Forest algorithm and visualizes results through scatter plots....\nvalidmind.data_validation.IsolationForestOutliers\n\n\nMetric\nChi Squared Features Table\nExecutes Chi-Squared test for each categorical feature against a target column to assess significant association....\nvalidmind.data_validation.ChiSquaredFeaturesTable\n\n\nThresholdTest\nHigh Cardinality\nAssesses the number of unique values in categorical columns to detect high cardinality and potential overfitting....\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissing Values\nEvaluates dataset quality by ensuring missing value ratio across all features does not exceed a set threshold....\nvalidmind.data_validation.MissingValues\n\n\nMetric\nDefault Ratesby Risk Band Plot\nGenerates a bar plot showcasing the distribution of default rates across different risk bands in a dataset....\nvalidmind.data_validation.DefaultRatesbyRiskBandPlot\n\n\nMetric\nRolling Stats Plot\nThis test evaluates the stationarity of time series data by plotting its rolling mean and standard deviation....\nvalidmind.data_validation.RollingStatsPlot\n\n\nMetric\nDataset Correlations\nAssesses correlation and association among features in a dataset, leveraging Pearson's R, Cramer's V, and...\nvalidmind.data_validation.DatasetCorrelations\n\n\nMetric\nTabular Description Tables\nSummarizes key descriptive statistics for numerical, categorical, and datetime variables in a dataset....\nvalidmind.data_validation.TabularDescriptionTables\n\n\nMetric\nAuto MA\nAutomatically selects the optimal Moving Average (MA) order for each variable in a time series dataset based on...\nvalidmind.data_validation.AutoMA\n\n\nThresholdTest\nUnique Rows\nVerifies the diversity of the dataset by ensuring that the count of unique rows exceeds a prescribed threshold....\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nToo Many Zero Values\nIdentifies numerical columns in a dataset that contain an excessive number of zero values, defined by a threshold...\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHigh Pearson Correlation\nIdentifies highly correlated feature pairs in a dataset suggesting feature redundancy or multicollinearity....\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nAC Fand PACF Plot\nAnalyzes time series data using Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to...\nvalidmind.data_validation.ACFandPACFPlot\n\n\nMetric\nBivariate Histograms\nGenerates bivariate histograms for paired features, aiding in visual inspection of categorical variables'...\nvalidmind.data_validation.BivariateHistograms\n\n\nMetric\nWOE Bin Table\nCalculates and assesses the Weight of Evidence (WoE) and Information Value (IV) of each feature in a ML model....\nvalidmind.data_validation.WOEBinTable\n\n\nMetric\nHeatmap Feature Correlations\nCreates a heatmap to visually represent correlation patterns between pairs of numerical features in a dataset....\nvalidmind.data_validation.HeatmapFeatureCorrelations\n\n\nThresholdTest\nTime Series Frequency\nEvaluates consistency of time series data frequency and generates a frequency plot....\nvalidmind.data_validation.TimeSeriesFrequency\n\n\nMetric\nDataset Split\nEvaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML...\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nSpread Plot\nVisualizes the spread relationship between pairs of time-series variables in a dataset, thereby aiding in...\nvalidmind.data_validation.SpreadPlot\n\n\nMetric\nTime Series Line Plot\nGenerates and analyses time-series data through line plots revealing trends, patterns, anomalies over time....\nvalidmind.data_validation.TimeSeriesLinePlot\n\n\nMetric\nPi T Credit Scores Histogram\nGenerates a histogram visualization for observed and predicted credit default scores....\nvalidmind.data_validation.PiTCreditScoresHistogram\n\n\nMetric\nAuto Seasonality\nAutomatically identifies and quantifies optimal seasonality in time series data to improve forecasting model...\nvalidmind.data_validation.AutoSeasonality\n\n\nMetric\nBivariate Scatter Plots\nGenerates bivariate scatterplots to visually inspect relationships between pairs of predictor variables in machine...\nvalidmind.data_validation.BivariateScatterPlots\n\n\nMetric\nEngle Granger Coint\nValidates co-integration in pairs of time series data using the Engle-Granger test and classifies them as...\nvalidmind.data_validation.EngleGrangerCoint\n\n\nThresholdTest\nTime Series Missing Values\nValidates time-series data quality by confirming the count of missing values is below a certain threshold....\nvalidmind.data_validation.TimeSeriesMissingValues\n\n\nDatasetMetadata\nDataset Metadata\nCollects and logs essential metadata of training datasets for transparency in model validation....\nvalidmind.data_validation.DatasetMetadata\n\n\nMetric\nTime Series Histogram\nVisualizes distribution of time-series data using histograms and Kernel Density Estimation (KDE) lines....\nvalidmind.data_validation.TimeSeriesHistogram\n\n\nMetric\nLagged Correlation Heatmap\nAssesses and visualizes correlation between target variable and lagged independent variables in a time-series...\nvalidmind.data_validation.LaggedCorrelationHeatmap\n\n\nMetric\nSeasonal Decompose\nDecomposes dataset features into observed, trend, seasonal, and residual components to identify patterns and...\nvalidmind.data_validation.SeasonalDecompose\n\n\nMetric\nWOE Bin Plots\nGenerates visualizations of Weight of Evidence (WoE) and Information Value (IV) for understanding predictive power...\nvalidmind.data_validation.WOEBinPlots\n\n\nThresholdTest\nClass Imbalance\nEvaluates and quantifies class distribution imbalance in a dataset used by a machine learning model....\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nIQR Outliers Bar Plot\nVisualizes outlier distribution across percentiles in numerical data using Interquartile Range (IQR) method....\nvalidmind.data_validation.IQROutliersBarPlot\n\n\nMetric\nPi TPD Histogram\nAssesses credit risk prediction accuracy of a model by comparing actual and predicted defaults at a chosen point in...\nvalidmind.data_validation.PiTPDHistogram\n\n\nMetric\nAuto AR\nAutomatically identifies the optimal Autoregressive (AR) order for a time series using BIC and AIC criteria....\nvalidmind.data_validation.AutoAR\n\n\nMetric\nTabular Date Time Histograms\nGenerates histograms to provide graphical insight into the distribution of time intervals in model's datetime data....\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\nMetric\nPunctuations\nAnalyzes and visualizes the frequency distribution of punctuation usage in a given text dataset....\nvalidmind.data_validation.nlp.Punctuations\n\n\nMetric\nCommon Words\nIdentifies and visualizes the 40 most frequent non-stopwords in a specified text column within a dataset....\nvalidmind.data_validation.nlp.CommonWords\n\n\nThresholdTest\nHashtags\nAssesses hashtag frequency in a text column, highlighting usage trends and potential dataset bias or spam....\nvalidmind.data_validation.nlp.Hashtags\n\n\nThresholdTest\nMentions\nCalculates and visualizes frequencies of '@' prefixed mentions in a text-based dataset for NLP model analysis....\nvalidmind.data_validation.nlp.Mentions\n\n\nMetric\nText Description\nPerforms comprehensive textual analysis on a dataset using NLTK, evaluating various parameters and generating...\nvalidmind.data_validation.nlp.TextDescription\n\n\nThresholdTest\nStop Words\nEvaluates and visualizes the frequency of English stop words in a text dataset against a defined threshold....\nvalidmind.data_validation.nlp.StopWords"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#understanding-tags-and-task-types",
    "href": "notebooks/how_to/explore_tests.html#understanding-tags-and-task-types",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "Understanding Tags and Task Types",
    "text": "Understanding Tags and Task Types\nEffectively using ValidMind’s tests involves a deep understanding of its ‘tags’ and ‘task types’. Here’s a breakdown:\n\nTask Types: Represent the kind of modeling task associated with a test. For instance:\n\nclassification: Works with Classification Models and Datasets\nregression: Works with Regression Models and Datasets\ntext classification: Works with Text Classification Models and Datasets\ntext summarization: Works with Text Summarization Models and Datasets\n\nTags: Free-form descriptors providing more details about the test, what data and models the test is compatible with and what category the test falls into etc. Some examples include:\n\nllm: Tests that work with Large Language Models\nnlp: Tests relevant for natural language processing.\nbinary_classification: Tests for binary classification tasks.\nforecasting: Tests for forecasting and time-series analysis.\ntabular_data: Tests for tabular data like CSVs and Excel spreadsheets."
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#searching-for-specific-tests-using-tags-and-task_types",
    "href": "notebooks/how_to/explore_tests.html#searching-for-specific-tests-using-tags-and-task_types",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "Searching for Specific Tests using tags and task_types",
    "text": "Searching for Specific Tests using tags and task_types\nWhile listing all tests is valuable, there are times when you need to narrow down your search. The list_tests function offers filter, task, and tags parameters to assist in this.\nIf you’re targeting a specific test or tests that match a particular task type, the filter parameter comes in handy. For example, to list tests that are compatible with ‘sklearn’ models:\n\nlist_tests(filter=\"sklearn\")\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nMetric\nClassifier Out Of Sample Performance\nAssesses ML model's performance on out-of-sample data to measure generalization and guard against overfitting....\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nClassifier In Sample Performance\nEvaluates ML model's in-sample performance using accuracy, precision, recall, and F1 score to assess generalization...\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\n\n\n\nThe task parameter is designed for pinpointing tests that align with a specific task type. For instance, to find tests tailored for ‘classification’ tasks:\n\nlist_tests(task=\"classification\")\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nModel Metadata\nExtracts and summarizes critical metadata from a machine learning model instance for comprehensive analysis....\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nClassifier Out Of Sample Performance\nAssesses ML model's performance on out-of-sample data to measure generalization and guard against overfitting....\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifier In Sample Performance\nEvaluates ML model's in-sample performance using accuracy, precision, recall, and F1 score to assess generalization...\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nPD Rating Class Plot\nAssesses and visualizes credit risk distribution across different rating classes within a dataset via default...\nvalidmind.model_validation.statsmodels.PDRatingClassPlot\n\n\nMetric\nScorecard Histogram\nCreates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model....\nvalidmind.model_validation.statsmodels.ScorecardHistogram\n\n\nMetric\nLogistic Reg Prediction Histogram\nGenerates and visualizes histograms of the Probability of Default predictions for both positive and negative...\nvalidmind.model_validation.statsmodels.LogisticRegPredictionHistogram\n\n\nMetric\nJarque Bera\nAssesses normality of dataset features in an ML model using the Jarque-Bera test....\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nKolmogorov Smirnov\nExecutes a feature-wise Kolmogorov-Smirnov test to evaluate alignment with normal distribution in datasets....\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nShapiro Wilk\nEvaluates feature-wise normality of training data using the Shapiro-Wilk test....\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nScorecard Bucket Histogram\nEvaluates and visualizes distribution of risk categories in a classification model's scores, useful in credit risk...\nvalidmind.model_validation.statsmodels.ScorecardBucketHistogram\n\n\nMetric\nLilliefors\nAssesses the normality of feature distributions in an ML model's training dataset using the Lilliefors test....\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nLogistic Reg Cumulative Prob\nVisualizes cumulative probabilities of positive and negative classes for both training and testing in logistic...\nvalidmind.model_validation.statsmodels.LogisticRegCumulativeProb\n\n\nMetric\nRuns Test\nExecutes Runs Test on ML model to detect non-random patterns in output data sequence....\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nScorecard Probabilities Histogram\nEvaluates risk classification of a model by visualizing the distribution of default probability across score...\nvalidmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\nMetric\nMissing Values Risk\nAssesses and quantifies the risk related to missing values in a dataset used for training an ML model....\nvalidmind.data_validation.MissingValuesRisk\n\n\nMetric\nIQR Outliers Table\nDetermines and summarizes outliers in numerical features using Interquartile Range method....\nvalidmind.data_validation.IQROutliersTable\n\n\nMetric\nBivariate Features Bar Plots\nGenerates visual bar plots to analyze the relationship between paired features within categorical data in the model....\nvalidmind.data_validation.BivariateFeaturesBarPlots\n\n\nThresholdTest\nSkewness\nEvaluates the skewness of numerical data in a machine learning model and checks if it falls below a set maximum...\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nChecks for and quantifies the presence of duplicate entries in the dataset or a specified column....\nvalidmind.data_validation.Duplicates\n\n\nMetric\nMissing Values Bar Plot\nCreates a bar plot showcasing the percentage of missing values in each column of the dataset with risk...\nvalidmind.data_validation.MissingValuesBarPlot\n\n\nMetric\nDataset Description\nProvides comprehensive descriptive statistics and histograms for each field in a dataset, aiding in data...\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatter Plot\nCreates a scatter plot matrix to visually analyze feature relationships, patterns, and outliers in a dataset....\nvalidmind.data_validation.ScatterPlot\n\n\nMetric\nTabular Categorical Bar Plots\nGenerates and visualizes bar plots for each category in categorical features to evaluate dataset's composition....\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nDescriptive Statistics\nPerforms a detailed descriptive statistical analysis of both numerical and categorical data within a model's...\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nANOVA One Way Table\nApplies one-way ANOVA (Analysis of Variance) to identify statistically significant numerical features in the...\nvalidmind.data_validation.ANOVAOneWayTable\n\n\nMetric\nTarget Rate Bar Plots\nGenerates bar plots visualizing the default rates of categorical features for a classification machine learning...\nvalidmind.data_validation.TargetRateBarPlots\n\n\nMetric\nPearson Correlation Matrix\nEvaluates linear dependency between numerical variables in a dataset via a Pearson Correlation coefficient heat map....\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nFeature Target Correlation Plot\nVisualizes the correlation between input features and model's target output in a color-coded horizontal bar plot....\nvalidmind.data_validation.FeatureTargetCorrelationPlot\n\n\nMetric\nTabular Numerical Histograms\nGenerates histograms for each numerical feature in a dataset to provide visual insights into data distribution and...\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nMetric\nIsolation Forest Outliers\nDetects outliers in a dataset using the Isolation Forest algorithm and visualizes results through scatter plots....\nvalidmind.data_validation.IsolationForestOutliers\n\n\nMetric\nChi Squared Features Table\nExecutes Chi-Squared test for each categorical feature against a target column to assess significant association....\nvalidmind.data_validation.ChiSquaredFeaturesTable\n\n\nThresholdTest\nHigh Cardinality\nAssesses the number of unique values in categorical columns to detect high cardinality and potential overfitting....\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissing Values\nEvaluates dataset quality by ensuring missing value ratio across all features does not exceed a set threshold....\nvalidmind.data_validation.MissingValues\n\n\nMetric\nDefault Ratesby Risk Band Plot\nGenerates a bar plot showcasing the distribution of default rates across different risk bands in a dataset....\nvalidmind.data_validation.DefaultRatesbyRiskBandPlot\n\n\nMetric\nDataset Correlations\nAssesses correlation and association among features in a dataset, leveraging Pearson's R, Cramer's V, and...\nvalidmind.data_validation.DatasetCorrelations\n\n\nMetric\nTabular Description Tables\nSummarizes key descriptive statistics for numerical, categorical, and datetime variables in a dataset....\nvalidmind.data_validation.TabularDescriptionTables\n\n\nThresholdTest\nUnique Rows\nVerifies the diversity of the dataset by ensuring that the count of unique rows exceeds a prescribed threshold....\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nToo Many Zero Values\nIdentifies numerical columns in a dataset that contain an excessive number of zero values, defined by a threshold...\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHigh Pearson Correlation\nIdentifies highly correlated feature pairs in a dataset suggesting feature redundancy or multicollinearity....\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nBivariate Histograms\nGenerates bivariate histograms for paired features, aiding in visual inspection of categorical variables'...\nvalidmind.data_validation.BivariateHistograms\n\n\nMetric\nWOE Bin Table\nCalculates and assesses the Weight of Evidence (WoE) and Information Value (IV) of each feature in a ML model....\nvalidmind.data_validation.WOEBinTable\n\n\nMetric\nHeatmap Feature Correlations\nCreates a heatmap to visually represent correlation patterns between pairs of numerical features in a dataset....\nvalidmind.data_validation.HeatmapFeatureCorrelations\n\n\nMetric\nDataset Split\nEvaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML...\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nPi T Credit Scores Histogram\nGenerates a histogram visualization for observed and predicted credit default scores....\nvalidmind.data_validation.PiTCreditScoresHistogram\n\n\nMetric\nBivariate Scatter Plots\nGenerates bivariate scatterplots to visually inspect relationships between pairs of predictor variables in machine...\nvalidmind.data_validation.BivariateScatterPlots\n\n\nDatasetMetadata\nDataset Metadata\nCollects and logs essential metadata of training datasets for transparency in model validation....\nvalidmind.data_validation.DatasetMetadata\n\n\nMetric\nWOE Bin Plots\nGenerates visualizations of Weight of Evidence (WoE) and Information Value (IV) for understanding predictive power...\nvalidmind.data_validation.WOEBinPlots\n\n\nThresholdTest\nClass Imbalance\nEvaluates and quantifies class distribution imbalance in a dataset used by a machine learning model....\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nIQR Outliers Bar Plot\nVisualizes outlier distribution across percentiles in numerical data using Interquartile Range (IQR) method....\nvalidmind.data_validation.IQROutliersBarPlot\n\n\nMetric\nPi TPD Histogram\nAssesses credit risk prediction accuracy of a model by comparing actual and predicted defaults at a chosen point in...\nvalidmind.data_validation.PiTPDHistogram\n\n\nMetric\nTabular Date Time Histograms\nGenerates histograms to provide graphical insight into the distribution of time intervals in model's datetime data....\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\n\n\n\nThe tags parameter facilitates searching tests by their tags. For instance, if you’re interested in only tests associated designed for model_performance that produce a plot (denoted by the visualization tag)\n\nlist_tests(tags=[\"model_performance\", \"visualization\"])\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\n\n\n\nThe above parameters can be combined to create complex queries. For instance, to find tests that are compatible with ‘sklearn’ models, designed for ‘classification’ tasks, and produce a plot:\n\nlist_tests(tags=[\"model_performance\", \"visualization\", \"sklearn\"], task=\"classification\")\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#delving-into-test-details-with-describe_test",
    "href": "notebooks/how_to/explore_tests.html#delving-into-test-details-with-describe_test",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "Delving into Test Details with describe_test",
    "text": "Delving into Test Details with describe_test\nAfter identifying a set of potential tests, you might want to explore the specifics of an individual test. The describe_test function provides a deep dive into the details of a test. It reveals the test name, description, ID, test type, and required inputs. Below, we showcase how to describe a test using its ID:\n\ndescribe_test(\"validmind.model_validation.sklearn.OverfitDiagnosis\")"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#conclusion-and-next-steps",
    "href": "notebooks/how_to/explore_tests.html#conclusion-and-next-steps",
    "title": "Exploring Tests in the Developer Framework:",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\nBy harnessing the functionalities presented in this guide, you should be able to easily list and filter through all of ValidMind’s available tests and find those you are interested in running against your model and/or dataset. The next step is to take the IDs of the tests you’d like to run and either create a Test Suite for reuse or just run them directly to try them out. See the other notebooks for a tutorial on how to do both."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html",
    "href": "notebooks/how_to/run_a_test_suite.html",
    "title": "Running an Individual Test Suite",
    "section": "",
    "text": "This notebook shows how to run an individual test suite that is part of the ValidMind Developer Framework. These instructions include the code required to:"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#before-you-begin",
    "href": "notebooks/how_to/run_a_test_suite.html#before-you-begin",
    "title": "Running an Individual Test Suite",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nNew to ValidMind?\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#install-the-client-library",
    "href": "notebooks/how_to/run_a_test_suite.html#install-the-client-library",
    "title": "Running an Individual Test Suite",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_a_test_suite.html#initialize-the-client-library",
    "title": "Running an Individual Test Suite",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nEvery documentation project in the Platform UI comes with a code snippet that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place.\nGet your code snippet by creating a documentation project:\n\nIn a browser, log into the Platform UI.\nGo to Documentation Projects and click Create new project.\nSelect [Demo] Customer Churn Model and Initial Validation for the model name and type, give the project a unique name to make it yours, and then click Create project.\nGo to Documentation Projects &gt; YOUR_UNIQUE_PROJECT_NAME &gt; Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\n%matplotlib inline\n\nimport xgboost as xgb"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "title": "Running an Individual Test Suite",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import taiwan_credit like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "href": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "title": "Running an Individual Test Suite",
    "section": "List available test suites",
    "text": "List available test suites\n\nvm.test_suites.list_suites()"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the data validation test suite",
    "text": "Run the data validation test suite\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the model validation test suite",
    "text": "Run the model validation test suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the raw dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\n\nTrain a model for testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Import and run the individual test suite",
    "text": "Import and run the individual test suite\n\nInitialize ValidMind objects\nWe initize the objects required to run test suites using the ValidMind framework.\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the binary classification test suite\n\nmodel_suite = vm.run_test_suite(\"classifier_model_validation\", model=vm_model)"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html",
    "href": "notebooks/how_to/explore_test_suites.html",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "",
    "text": "This notebook shows model developers how they can learn more about the test suites and tests that are available in the ValidMind Developer Framework. These instructions include the code required to get:"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#before-you-begin",
    "href": "notebooks/how_to/explore_test_suites.html#before-you-begin",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "Before you begin",
    "text": "Before you begin\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#install-the-client-library",
    "href": "notebooks/how_to/explore_test_suites.html#install-the-client-library",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install -q -q validmind"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#initialize-the-client-library",
    "href": "notebooks/how_to/explore_test_suites.html#initialize-the-client-library",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "Initialize the client library",
    "text": "Initialize the client library\n\nimport validmind as vm"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#get-a-list-of-available-test-suites",
    "href": "notebooks/how_to/explore_test_suites.html#get-a-list-of-available-test-suites",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "Get a list of available test suites",
    "text": "Get a list of available test suites\nTo get the list of all test suites available in the ValidMind Developer Framework:\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTests\n\n\n\n\nclassifier_model_diagnosis\nClassifierDiagnosis\nTest suite for sklearn classifier model diagnosis tests\nvalidmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nclassifier_full_suite\nClassifierFullSuite\nFull test suite for binary classification models.\nvalidmind.data_validation.DatasetMetadata, validmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierInSamplePerformance, validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nclassifier_metrics\nClassifierMetrics\nTest suite for sklearn classifier metrics\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierInSamplePerformance, validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nclassifier_model_validation\nClassifierModelValidation\nTest suite for binary classification models.\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierInSamplePerformance, validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nclassifier_validation\nClassifierPerformance\nTest suite for sklearn classifier models\nvalidmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nllm_classifier_full_suite\nLLMClassifierFullSuite\nFull test suite for LLM classification models.\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.nlp.StopWords, validmind.data_validation.nlp.Punctuations, validmind.data_validation.nlp.CommonWords, validmind.data_validation.nlp.TextDescription, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierInSamplePerformance, validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis, validmind.prompt_validation.Bias, validmind.prompt_validation.Clarity, validmind.prompt_validation.Conciseness, validmind.prompt_validation.Delimitation, validmind.prompt_validation.NegativeInstruction, validmind.prompt_validation.Robustness, validmind.prompt_validation.Specificity\n\n\nprompt_validation\nPromptValidation\nTest suite for prompt validation\nvalidmind.prompt_validation.Bias, validmind.prompt_validation.Clarity, validmind.prompt_validation.Conciseness, validmind.prompt_validation.Delimitation, validmind.prompt_validation.NegativeInstruction, validmind.prompt_validation.Robustness, validmind.prompt_validation.Specificity\n\n\nnlp_classifier_full_suite\nNLPClassifierFullSuite\nFull test suite for NLP classification models.\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.nlp.StopWords, validmind.data_validation.nlp.Punctuations, validmind.data_validation.nlp.CommonWords, validmind.data_validation.nlp.TextDescription, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierInSamplePerformance, validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nregression_model_description\nRegressionModelDescription\nTest suite for performance metric of regression model of statsmodels library\nvalidmind.data_validation.DatasetSplit, validmind.model_validation.ModelMetadata\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest suite for metrics comparison of regression model of statsmodels library\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs, validmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\nvalidmind.data_validation.DatasetMetadata, validmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest suite to extract metadata and descriptive statistics from a tabular dataset\nvalidmind.data_validation.DatasetMetadata, validmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix\n\n\ntabular_data_quality\nTabularDataQuality\nTest suite for data quality on tabular datasets\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues\n\n\ntext_data_quality\nTextDataQuality\nTest suite for data quality on text data\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.nlp.StopWords, validmind.data_validation.nlp.Punctuations, validmind.data_validation.nlp.CommonWords, validmind.data_validation.nlp.TextDescription\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest suite for data quality on time series datasets\nvalidmind.data_validation.TimeSeriesOutliers, validmind.data_validation.TimeSeriesMissingValues, validmind.data_validation.TimeSeriesFrequency\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\nvalidmind.data_validation.TimeSeriesOutliers, validmind.data_validation.TimeSeriesMissingValues, validmind.data_validation.TimeSeriesFrequency, validmind.data_validation.TimeSeriesLinePlot, validmind.data_validation.TimeSeriesHistogram, validmind.data_validation.ACFandPACFPlot, validmind.data_validation.SeasonalDecompose, validmind.data_validation.AutoSeasonality, validmind.data_validation.AutoStationarity, validmind.data_validation.RollingStatsPlot, validmind.data_validation.AutoAR, validmind.data_validation.AutoMA, validmind.data_validation.ScatterPlot, validmind.data_validation.LaggedCorrelationHeatmap, validmind.data_validation.EngleGrangerCoint, validmind.data_validation.SpreadPlot\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nvalidmind.data_validation.DatasetSplit, validmind.model_validation.ModelMetadata, validmind.model_validation.statsmodels.RegressionModelsCoeffs, validmind.model_validation.statsmodels.RegressionModelsPerformance, validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels, validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\ntime_series_multivariate\nTimeSeriesMultivariate\nThis test suite provides a preliminary understanding of the features and relationship in multivariate dataset. It presents various multivariate visualizations that can help identify patterns, trends, and relationships between pairs of variables. The visualizations are designed to explore the relationships between multiple features simultaneously. They allow you to quickly identify any patterns or trends in the data, as well as any potential outliers or anomalies. The individual feature distribution can also be explored to provide insight into the range and frequency of values observed in the data. This multivariate analysis test suite aims to provide an overview of the data structure and guide further exploration and modeling.\nvalidmind.data_validation.ScatterPlot, validmind.data_validation.LaggedCorrelationHeatmap, validmind.data_validation.EngleGrangerCoint, validmind.data_validation.SpreadPlot\n\n\ntime_series_sensitivity\nTimeSeriesSensitivity\nThis test suite performs sensitivity analysis on a statsmodels OLS linear regression model by applying distinct shocks to each input variable individually and then computing the model's predictions. The aim of this test suite is to investigate the model's responsiveness to variations in its inputs. By juxtaposing the model's predictions under baseline and shocked conditions, users can visually evaluate the sensitivity of the model to changes in each variable. This kind of analysis can also shed light on potential model limitations, including over-reliance on specific variables or insufficient responsiveness to changes in inputs. As a result, this test suite can provide insights that may be beneficial for refining the model structure, improving its robustness, and ensuring a more reliable prediction performance.\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nThis test suite provides a preliminary understanding of the target variable(s) used in the time series dataset. It visualizations that present the raw time series data and a histogram of the target variable(s). The raw time series data provides a visual inspection of the target variable's behavior over time. This helps to identify any patterns or trends in the data, as well as any potential outliers or anomalies. The histogram of the target variable displays the distribution of values, providing insight into the range and frequency of values observed in the data.\nvalidmind.data_validation.TimeSeriesLinePlot, validmind.data_validation.TimeSeriesHistogram, validmind.data_validation.ACFandPACFPlot, validmind.data_validation.SeasonalDecompose, validmind.data_validation.AutoSeasonality, validmind.data_validation.AutoStationarity, validmind.data_validation.RollingStatsPlot, validmind.data_validation.AutoAR, validmind.data_validation.AutoMA"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#get-details-for-a-test-suite",
    "href": "notebooks/how_to/explore_test_suites.html#get-details-for-a-test-suite",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "Get details for a test suite",
    "text": "Get details for a test suite\nTo get the list of tests available in a given test suite:\n\nvm.test_suites.describe_suite(\"classifier_full_suite\")\n\n\n\n\n\n\nID\nName\nDescription\nTests\n\n\n\n\nclassifier_full_suite\nClassifierFullSuite\nFull test suite for binary classification models.\nvalidmind.data_validation.DatasetMetadata, validmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierInSamplePerformance, validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\n\n\n\n\nGet details for a test\nTo get the details for a given test:\n\nvm.tests.describe_test('DescriptiveStatistics')\n\n\n\n\n\n\n\n\n\n\n\n\nID:\nvalidmind.data_validation.DescriptiveStatistics\n\n\nName:\nDescriptive Statistics\n\n\nDescription:\n**Purpose**: The purpose of the Descriptive Statistics metric is to provide a comprehensive summary of both numerical and categorical data within a dataset. For numerical data, it gathers statistics such as count, mean, standard deviation, minimum and maximum values, as well as certain percentiles. For categorical data, it calculates the count, number of unique values, most frequent value, frequency of the most common value, and the proportion of the most frequent value relative to the total. This metric aids in visualizing the overall distribution of the variables in the dataset, which in turn assists in understanding the model's behavior and predicting its performance. **Test Mechanism**: The test mechanism involves using the describe() function for numerical fields which computes several summary statistics and value_counts() for categorical fields which counts unique values. Both of these functions are built-in methods of pandas dataframes. The results are then formatted to create two separate tables, one for numerical and one for categorical variable summaries. These tables provide a clear summary of the main characteristics of these variables, which can be crucial in assessing the model's performance. **Signs of High Risk**: High risks can be found in evidence of skewed data or notable outliers. This could be reflected in the mean and median (50% percentile) having a significant difference, in the case of numerical data. For categorical data, high risk can be indicated by a lack of diversity (low count of unique values), or overdominance of a single category (high frequency of the top value). **Strengths**: The primary strength of this metric lies in its ability to provide a comprehensive summary of the dataset, providing insights on the distribution and characteristics of the variables under consideration. It is a flexible and robust method, relevant to both numerical and categorical data. It can help highlight anomalies such as outliers, extreme skewness, or lack of diversity which can be essential in understanding model behavior during testing and validation. **Limitations**: While this metric provides a high-level overview of the data, it may not be sufficient to detect subtle correlations or complex patterns in the data. It does not provide any info on the relationship between variables. Plus, descriptive statistics alone cannot be used to infer properties about future unseen data. It should be used in conjunction with other statistical tests to provide a thorough understanding of the model's data.\n\n\nTest Type:\nMetric\n\n\nRequired Inputs:\n['dataset']\n\n\nParams:\n{}\n\n\n\n\n\n\n\nGet a verbose details view of a test suite and its tests\nTo get more comprehensive details for test suites and tests:\n\nvm.test_suites.describe_suite(\"classifier_full_suite\", verbose=True)\n\n\n\n\n\n\nTest Suite ID\nTest Suite Name\nTest Suite Section\nTest ID\nTest Name\nTest Type\n\n\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.DatasetMetadata\nDataset Metadata\nDatasetMetadata\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.DatasetDescription\nDataset Description\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.DescriptiveStatistics\nDescriptive Statistics\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.PearsonCorrelationMatrix\nPearson Correlation Matrix\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.ClassImbalance\nClass Imbalance\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.Duplicates\nDuplicates\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.HighCardinality\nHigh Cardinality\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.HighPearsonCorrelation\nHigh Pearson Correlation\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.MissingValues\nMissing Values\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.Skewness\nSkewness\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.UniqueRows\nUnique Rows\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.TooManyZeroValues\nToo Many Zero Values\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.ModelMetadata\nModel Metadata\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.data_validation.DatasetSplit\nDataset Split\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ConfusionMatrix\nConfusion Matrix\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\nClassifier In Sample Performance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\nClassifier Out Of Sample Performance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\nPermutation Feature Importance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\nPrecision Recall Curve\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ROCCurve\nROC Curve\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\nPopulation Stability Index\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\nSHAP Global Importance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.MinimumAccuracy\nMinimum Accuracy\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.MinimumF1Score\nMinimum Score\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\nMinimum ROCAUC Score\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.TrainingTestDegradation\nTraining Test Degradation\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\nModels Performance Comparison\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_model_diagnosis\nvalidmind.model_validation.sklearn.OverfitDiagnosis\nOverfit Diagnosis\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_model_diagnosis\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\nWeakspots Diagnosis\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_model_diagnosis\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\nRobustness Diagnosis\nThresholdTest"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#list-all-tests",
    "href": "notebooks/how_to/explore_test_suites.html#list-all-tests",
    "title": "Viewing All Available Test Suites and Tests",
    "section": "List all tests",
    "text": "List all tests\nTo get the list of tests and their purpose:\n\nvm.tests.list_tests()\n\n\n\n\n\n\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nThresholdTest\nBias\n**Purpose:** Bias Evaluation is aimed at assessing if and how the distribution and order of exemplars (examples) within a few-shot learning prompt affect the Large Language Model's (LLM) output, potentially introducing biases. By examining these influences, we can optimize the model's performance and mitigate unintended biases in its responses. **Test Mechanism:** 1. **Distribution of Exemplars:** Check how varying the number of positive vs. negative examples in a prompt impacts the LLM's classification of a neutral or ambiguous statement. 2. **Order of Exemplars:** Examine if the sequence in which positive and negative examples are presented can sway the LLM's response. For each test case, an LLM is used to grade the input prompt on a scale from 1 to 10, based on whether the examples in the prompt may lead to biased responses. A minimum threshold must be met in order for the test to pass. By default, this threshold is set to 7, but it can be adjusted as needed via the test parameters.\nvalidmind.prompt_validation.Bias\n\n\nThresholdTest\nClarity\n**Purpose:** The Clarity Evaluation is designed to assess whether prompts provided to a Large Language Model (LLM) are unmistakably clear in their instructions. With clear prompts, the LLM is better suited to more accurately and effectively interpret and respond to instructions in the prompt **Test Mechanism:** Using an LLM, prompts are scrutinized for clarity, considering aspects like detail inclusion, persona adoption, step-by-step instructions, use of examples, and desired output length. Each prompt is graded on a scale from 1 to 10 based on its clarity. Prompts scoring at or above a predetermined threshold (default is 7) are marked as clear. This threshold can be adjusted via the test parameters. **Why Clarity Matters:** Clear prompts minimize the room for misinterpretation, allowing the LLM to generate more relevant and accurate responses. Ambiguous or vague instructions might leave the model guessing, leading to suboptimal outputs. **Tactics for Ensuring Clarity that will be referenced during evaluation:** 1. **Detail Inclusion:** Provide essential details or context to prevent the LLM from making assumptions. 2. **Adopt a Persona:** Use system messages to specify the desired persona for the LLM's responses. 3. **Specify Steps:** For certain tasks, delineate the required steps explicitly, helping the model in sequential understanding. 4. **Provide Examples:** While general instructions are efficient, in some scenarios, \"few-shot\" prompting or style examples can guide the LLM more effectively. 5. **Determine Output Length:** Define the targeted length of the response, whether in terms of paragraphs, bullet points, or other units. While word counts aren't always precise, specifying formats like paragraphs can offer more predictable results.\nvalidmind.prompt_validation.Clarity\n\n\nThresholdTest\nSpecificity\n**Purpose:** The Specificity Test aims to assess the clarity, precision, and effectiveness of prompts provided to a Large Language Model (LLM). Ensuring specificity in the prompts given to an LLM can significantly influence the accuracy and relevance of its outputs. The goal of this test is to ascertain that the instructions in a prompt are unmistakably clear and relevant, eliminating ambiguity and steering the LLM toward desired outcomes. **Test Mechanism:** Utilizing an LLM, each prompt is graded on a specificity scale ranging from 1 to 10. The grade reflects how well the prompt adheres to principles of clarity, detail, and relevancy without being overly verbose. Prompts that achieve a grade equal to or exceeding a predefined threshold (default set to 7) are deemed to pass the evaluation, while those falling below are marked as failing. This threshold can be adjusted as needed. **Why Specificity Matters:** Prompts that are detailed and descriptive often yield better and more accurate results from an LLM. Rather than relying on specific keywords or tokens, it's crucial to have a well-structured and descriptive prompt. Including relevant examples within the prompt can be particularly effective, guiding the LLM to produce outputs in desired formats. However, it's essential to strike a balance. While prompts need to be detailed, they shouldn't be overloaded with unnecessary information. The emphasis should always be on relevancy and conciseness, considering there are limitations to how long a prompt can be. **Example:** Imagine wanting an LLM to extract specific details from a given text. A vague prompt might yield varied results. However, with a prompt like, \"Extract the names of all characters and the cities they visited from the text\", the LLM is guided more precisely towards the desired information extraction.\nvalidmind.prompt_validation.Specificity\n\n\nThresholdTest\nRobustness\n**Purpose:** The Robustness Integrity Assessment evaluates the resilience and reliability of prompts provided to a Large Language Model (LLM). The primary objective is to ensure that prompts consistently produce accurate and desired outputs, even in diverse or challenging scenarios. **Test Mechanism:** Prompts are subjected to various conditions, alterations, and contexts to check their stability in eliciting consistent responses from the LLM. Factors such as different phrasings, inclusion of potential distractors, and varied input complexities are introduced to test the robustness of the prompt. By default, the test generates 10 inputs for the prompt but this can be adjusted via the test parameters. **Why Robustness Matters:** A robust prompt ensures consistent performance and reduces the likelihood of unexpected or off-tangent outputs. This consistency is vital for applications where predictability and reliability of the LLM's response are paramount.\nvalidmind.prompt_validation.Robustness\n\n\nThresholdTest\nNegative Instruction\n**Purpose:** The Positive Instructional Assessment evaluates prompts provided to a Large Language Model (LLM) to ensure they are framed using affirmative and proactive language. By focusing on what should be done rather than what should be avoided, prompts can guide the LLM more effectively towards generating appropriate and desired outputs. **Test Mechanism:** Employing an LLM as an evaluator, each prompt is meticulously analyzed and graded on use of positive instructions on a scale from 1 to 10. The grade indicates how well the prompt employs affirmative language while avoiding negative or prohibitive instructions. Prompts that achieve a grade equal to or exceeding a predetermined threshold (default set to 7) are recognized as adhering to positive instruction best practices. This threshold can be adjusted via the test parameters. **Why Positive Instructions Matter:** Prompts that are phrased in the affirmative, emphasizing what to do, tend to direct the LLM more clearly than those that focus on what not to do. Negative instructions can lead to ambiguities and undesired model responses. By emphasizing clarity and proactive guidance, we optimize the chances of obtaining relevant and targeted responses from the LLM. **Example:** Consider a scenario involving a chatbot designed to recommend movies. An instruction framed as, \"Don't recommend movies that are horror or thriller\" might cause the LLM to fixate on the genres mentioned, inadvertently producing undesired results. On the other hand, a positively-framed prompt like, \"Recommend family-friendly movies or romantic comedies\" provides clear guidance on the desired output.\nvalidmind.prompt_validation.NegativeInstruction\n\n\nThresholdTest\nConciseness\n**Purpose:** The Conciseness Assessment is designed to evaluate the brevity and succinctness of prompts provided to a Large Language Model (LLM). A concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed. **Test Mechanism:** Using an LLM, this test puts input prompts through a conciseness analysis where it's graded on a scale from 1 to 10. The grade reflects how well the prompt maintains clarity while avoiding verbosity. Prompts that achieve a grade equal to or surpassing a predefined threshold (default set to 7) are considered successful in being concise. This threshold can be adjusted based on specific requirements. **Why Conciseness Matters:** While detailed prompts can guide an LLM towards accurate results, excessive details can clutter the instruction and potentially lead to undesired outputs. Concise prompts are straightforward, reducing ambiguity and focusing the LLM's attention on the primary task. This is especially important considering there are limitations to the length of prompts that can be fed to an LLM. **Example:** For an LLM tasked with summarizing a document, a verbose prompt might introduce unnecessary constraints or biases. A concise, effective prompt like, \"Provide a brief summary highlighting the main points of the document\" ensures that the LLM captures the essence of the content without being sidetracked.\nvalidmind.prompt_validation.Conciseness\n\n\nThresholdTest\nDelimitation\n**Purpose:** The Delimitation Test ensures that prompts provided to the Large Language Model (LLM) use delimiters correctly to distinctly mark sections of the input. Properly delimited prompts simplify the LLM's interpretation process, ensuring accurate and precise responses. **Test Mechanism:** Using an LLM, prompts are checked for their appropriate use of delimiters such as triple quotation marks, XML tags, and section titles. Each prompt receives a score from 1 to 10 based on its delimitation integrity. Prompts scoring at or above a set threshold (default is 7) pass the check. This threshold can be modified as needed. **Why Proper Delimitation Matters:** Delimiters play a crucial role in segmenting and organizing prompts, especially when diverse data or multiple tasks are involved. They help in clearly distinguishing between different parts of the input, reducing ambiguity for the LLM. As task complexity increases, the correct use of delimiters becomes even more critical to ensure the LLM understands the prompt's intent. **Example:** When given a prompt like: ```USER: Summarize the text delimited by triple quotes. '''insert text here'''``` or: ```USER:\ninsert first article here\ninsert second article here\n``` The LLM can more accurately discern sections of the text to be treated differently, thanks to the clear delimitation.\nvalidmind.prompt_validation.Delimitation\n\n\nMetric\nBert Score\n**Purpose**: BERTScore is a metric used to evaluate the quality of text generation models, focusing on the similarity between the reference text and the generated text. It leverages the contextual embeddings from BERT models to evaluate the similarity of the contents. Therefore, it distinctively measures how well a model has learned and can generate context-relevant results. **Test Mechanism**: This metric derives the true values from the model's testing set and the model's predictions. BERTScore evaluates the precision, recall, and F1 score of the model according to the contextual similarity between the reference and the generated text. These scores are calculated for each token in the candidate sentences as compared to the reference sentences, taking into account cosine similarity with BERT embeddings. A line plot is generated for each metric (Precision, Recall and F1 Score), which visualizes the score changes across row indexes. **Signs of High Risk**: If there's a general downward trend in Precision, Recall, or F1 Score or any noticeable instability/fluctuation in these metrics, it indicates high risk. A low Precision implies that predictions often include irrelevant contexts. A low Recall implies the models miss relevant contexts in predictions. A low F1 score indicates poor overall performance in both precision and recall. **Strengths**: BERTScore is potent in detecting the quality of text that requires understanding the context, which is a typical demand in natural language processing tasks. This metric goes beyond simple n-gram matching and considers the semantic similarity in the context, which provides more meaningful evaluation results. The visualization allows observing the performance trends across different sets of predictions. **Limitations**: BERTScore is dependent on BERT model embeddings; therefore, if the base BERT model is not well-suited for a specific task, it may impact the accuracy of BERTScore. In addition, although good at semantic understanding, it might not capture some nuances in text similarity that other metrics like BLEU or ROUGE can detect. Finally, it can be computationally expensive due to the use of BERT embeddings.\nvalidmind.model_validation.BertScore\n\n\nMetric\nBleu Score\n**Purpose**: The Bilingual Evaluation Understudy (BLEU) metric measures the quality of machine-translated text by comparing it to human-translated text. This comparison is done at the sentence level and is designed to bring machine translations closer to the quality of a professional human translation. It is commonly used in the field of translation evaluation, and its purpose is to assess the accuracy of a model's output against that of a benchmark. **Test Mechanism**: The implementation of the BLEU score involves using the NLTK's word_tokenize function to split the text into individual words, upon which the BLEU score can be calculated. The method employs the evaluate library's BLEU metric to compute the BLEU score for each translated sentence, comparing the model's translations (predictions) against the actual, correct translations (references). The results are returned as a single score which indicates the average 'distance' between the generated translations and the human translations across the entire test set. **Signs of High Risk**: Low BLEU scores indicate high model risk. This occurs when there is a significant discrepancy between the machine translation and its human equivalent. This may be due to the model not learning effectively, overfitting the training data, or not handling the nuances of the language effectively. Machine bias toward a certain language style or mode of translation can also result in lower scores. **Strengths**: The main strength of the BLEU score is its simplicity and interpretability. It provides an intuitive way to assess the quality of translated texts that can correlate well with human judgements. Additionally, it operates at a granular level (sentence level), enabling more precise assessment of errors. Lastly, it encapsulates the performance of the model with a single, comprehensive score that is easy to compare and monitor. **Limitations**: The BLEU score places emphasis on exact matches, which can lead to bias in favor of literal translations. As a result, it may not fully capture the quality of more complex or flexible translations that don't adhere strictly to a word-for-word structure. Another limitation is that it doesn't directly evaluate the intelligibility or grammatical correctness of the translations. Moreover, it may not identify errors that arise from subtle nuances in language, cultural contexts, or ambiguities.\nvalidmind.model_validation.BleuScore\n\n\nMetric\nContextual Recall\n**Purpose**: The Contextual Recall metric is utilized to gauge the proficiency of a natural language generation (NLG) model in crafting textual content that remains coherent and consistent with the presented context or prompt. It quantifies the model's aptitude to recollect and mirror the foundational context in its resultant output. In the realm of NLP tasks, it's paramount that models generate text reflecting contextual relevance. **Test Mechanism**: 1. **Prepare Reference and Candidate Texts**: - **Reference Texts**: Gather your reference text(s) which epitomize the expected or ideal output for a designated context or prompt. - **Candidate Texts**: Using the same context, generate candidate text(s) from the NLG model under evaluation. 2. **Tokenization and Preprocessing**: - Tokenize both the reference and candidate texts into discernible words or tokens, using established libraries like NLTK. 3. **Compute Contextual Recall**: - Ascertain the overlap of tokens between the reference and candidate texts. - The Contextual Recall score is deduced by dividing the number of overlapping tokens by the total token count of the reference text. The assessment is made for each test dataset instance, resulting in an array of scores. This series of scores is then illustrated via a line plot, showing score variations across rows. **Signs of High Risk**: Low contextual recall scores are red flags. They imply that the model isn't efficiently echoing the original context in its generated content, culminating in outputs that are incoherent or contextually off-track. A persistent trend of low recall scores could presage model underperformance. **Strengths**: The standout attribute of the contextual recall metric is its ability to quantifiably gauge a model's adherence to, and recollection of, both the context and factual elements in the generated narrative. This proves invaluable in applications demanding profound context comprehension, such as text continuation or interactive dialogue systems. The accompanying line plot visualization offers a lucid and intuitive representation of score variations. **Limitations**: Although potent, the Contextual Recall metric might not holistically represent all dimensions of NLG model performance. Its predominant focus on word overlap might award high scores to texts that employ numerous common words, even if they lack coherence or meaningful context. It also bypasses the significance of word order, which might lead to inflated scores for scrambled outputs. Lastly, infrequent words, despite being used aptly, might not match as often, potentially undervaluing models proficient in their application.\nvalidmind.model_validation.ContextualRecall\n\n\nMetric\nRouge Metrics\n**Purpose**: ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, serves as a robust metric framework aimed at evaluating the caliber of machine-generated text. Especially pertinent in the arena of natural language generation tasks, such as text summarization, machine translation, and text generation, its overarching goal is to ascertain how effectively the machine-rendered text mirrors key information and concepts present in reference human-crafted text. Owing to its efficacy, the ROUGE metrics have become staples in the NLP community for the assessment of text generation systems. **Test Mechanism**: 1. **Comparison Basis**: At its core, ROUGE operates by juxtaposing machine-generated content with a reference human-constructed text. 2. **Incorporated Metrics**: - **ROUGE-N (N-gram Overlap)**: Focuses on the alignment of n-grams (sequential sets of n words) between the generated and reference texts. Typical n-values include 1 (unigrams), 2 (bigrams), and 3 (trigrams). Each metric calculates precision, recall, and F1-score components. - **ROUGE-L (Longest Common Subsequence)**: Pinpoints the most extended shared word sequence present in both the machine and reference texts, assessing the generated text's ability to encapsulate pivotal phrases. - **ROUGE-S (Skip-bigram)**: Measures the concurrence of skip-bigrams, which are word pairings appearing within a designated word window in the text. This metric is sensitive to word ordering but tolerates sporadic word omissions. 3. **Visual Representation**: The determined precision, recall, and F1-score for each metric are plotted visually, ensuring streamlined comprehension of the results. **Signs of High Risk**: Warning signs under this metric umbrella encompass low scores across the ROUGE suite. A diminished precision could insinuate the presence of redundant information in machine-produced text, while a low recall might denote the omission of salient data from the reference text. A decreased F1 score pinpoints a suboptimal harmony between precision and recall. Persistent low scores, especially across diverse test sets, may herald intrinsic flaws in the model's prowess. **Strengths**: ROUGE's primary asset is its multifaceted view on text quality, enabled by an array of evaluation metrics. It graciously accommodates synonyms and rephrasing due to its n-gram-based approach, and it champions the retention of salient word sequences via the longest common subsequence tactic. The visual representation of precision, recall, and F1-scores facilitates an intuitive grasp of model efficacy. **Limitations**: In spite of its advantages, certain constraints tether ROUGE. It might not adequately address the semantic coherence, fluency, or grammatical integrity of the generated narrative, leaning more towards evaluating isolated phrases or n-grams. The metric can be less discerning when critiquing elaborate sentences due to its fragmentary nature. Moreover, as it banks on comparisons with human-made references, procuring such benchmarks can be challenging or even impractical at times.\nvalidmind.model_validation.RougeMetrics\n\n\nMetric\nModel Metadata\n**Purpose:** This test is designed to collect and summarize important metadata related to a particular machine learning model. Such metadata includes the model's architecture (modeling technique), the version and type of modeling framework used, and the programming language the model is written in. **Test Mechanism:** The mechanism of this test consists of extracting information from the model instance. It tries to extract the model information such as the modeling technique used, the modeling framework version, and the programming language. It decorates this information into a data frame and returns a summary of the results. **Signs of High Risk:** High risk could be determined by a lack of documentation or inscrutable metadata for the model. For instance, unidentifiable language, outdated or unsupported versions of modeling frameworks, or undisclosed model architectures reflect risky situations, as they could hinder future reproducibility, support, and debugging of the model. **Strengths:** The strengths of this test lie in the increased transparency and understanding it brings regarding the model's setup. Knowing the model's architecture, the specific modeling framework version used, and the language involved, provides multiple benefits: supports better error understanding and debugging, facilitates model reuse, aids compliance of software policies, and assists in planning for model obsolescence due to evolving or discontinuing software and dependencies. **Limitations:** Notably, this test is largely dependent on the compliance and correctness of information provided by the model or the model developer. If the model's built-in methods for describing its architecture, framework or language are incorrect or lack necessary information, this test will hold limitations. Moreover, it is not designed to directly evaluate the performance or accuracy of the model, rather it provides supplementary information which aids in comprehensive analysis.\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nToken Disparity\n**Purpose**: The purpose of the Token Disparity metric is to evaluate the distributional match between the predicted and actual outputs (tokens) of the model. This is done by creating a comparison through histograms that outline the disparity in the number of tokens in both columns. The metric is also used to assess the verbosity of the model's predictions in comparison to the actual data. **Test Mechanism**: The test is implemented by tokenizing the two columns: one for the real data and the other for the generated or predicted data. It uses the BERT tokenizer to tokenize the content of each column. Then, it counts the tokens in each column. These counts are then arranged into two different histograms to visualize the distribution of token counts in the real data and the generated data. The metric quantifies the distribution disparity by comparing the histogram of the true tokens with the histogram of predicted tokens. **Signs of High Risk**: High risk or failures might be indicated by significant differences in distribution typologies between the two histograms, especially if the predicted histogram considerably diverges from the reference histogram. It may signify that the model is generating outputs with unexpected verbosity, resulting in either far too many or too few tokens than expected. **Strengths**: The primary strength of this metric is that it provides a clear and visual comparison of predicted versus actual token distributions in the model. It helps in understanding the consistency and quality of the model's output in terms of length and verbosity. It also allows detection of potential issues in the model's output generation capabilities, such as over-generation or under-generation of tokens compared to the actual data. **Limitations**: This metric focuses strictly on the count of tokens without considering the semantics behind the tokens. Therefore, it may overlook issues related to the meaningfulness or relevance of the produced tokens. Furthermore, it assumes that a similar distribution of token counts between predicted and actual data implies accurate output, which may not always hold true. Also, it depends on the BERT tokenizer which may not be the best tokenizer for all kinds of text data.\nvalidmind.model_validation.TokenDisparity\n\n\nMetric\nClassifier Out Of Sample Performance\n**Purpose**: This test is designed to assess the performance of a Machine Learning model on out-of-sample data - i.e., data not used during the training phase. This test uses a variety of metrics (e.g., accuracy, precision, recall, F1 score) to evaluate how well the trained model generalizes to unseen data. It helps to ensure that the model is not overfitting the training data and is capable of making accurate predictions on new, unseen data. **Test Mechanism**: The test invokes the metrics on the model's predicted outcomes for the testing data set and compares those predictions to the actual outcomes. If the testing data set is properly set aside during the model training and is not used in any way in the model learning process, it should provide a fair and unbiased measure of how well the model can generalize. The metrics used typically include accuracy (proportion of correct predictions), precision (proportion of positive predictions that were correct), recall (proportion of actual positives that were predicted correctly), and F1 score (a single statistic that balances precision and recall). **Signs of High Risk**: Indications of high risk might include a low accuracy rate, low precision and recall rates, a low F1 score, or significant discrepancies between the model's performance on training data and on testing data. The latter may point to overfitting, meaning the model may not generalize well to new data. **Strengths**: The major strength of this test is its ability to measure a model's predictive performance on unseen data, thereby giving a fair estimate of its generalizability. This test takes into account various performance metrics to provide a comprehensive performance evaluation. It also helps detect overfitting, an important aspect to consider for any machine learning model. **Limitations**: This test's limitations are centered around the dependability of the test dataset. If the test dataset is not a good representative of the real-world data the model will be applied to, the performance metrics may not accurately reflect the true performance of the model. It's also worth noting that all the metrics used (accuracy, precision, recall and F1 score) assume that all errors or misclassifications are equally important, which is not always the case in real-world scenarios.\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nRobustness Diagnosis\n**Purpose**: The purpose of this test is to evaluate the robustness of a machine learning model. Robustness refers to a model's ability to maintain a high level of performance in the face of perturbations or changes—particularly noise—added to its input data. Such simulated scenarios help to assess if the model can handle situations where the input data might be incomplete or corrupted. **Test Mechanism**: This test method works by adding Gaussian noise, proportional to a certain standard deviation scale, to input features (only numerical) of both the training and testing datasets. The performance of the model with perturbed features is then computed based on a metric (default is 'accuracy'). This process is iterated over a list of defined scale factors. The results are then visualized using a line chart, depicting the accuracy trend against the amount of noise introduced. A threshold is set to determine the decay in accuracy due to perturbation that is deemed acceptable. **Signs of High Risk**: Signs of high risk include significant drops in accuracy when noise is introduced to feature inputs. If the decay in accuracy breaches the configured threshold, it is a strong indicator of a high-risk condition. High risk may also pertain to a situation where one or more elements provided in the features list do not match with the training dataset's numerical feature columns. **Strengths**: The robustness diagnosis test offers the following advantages: - It provides an empirical measure of the model's performance in handling noise or data perturbations, offering insights into the model's reliability and stability. - The test is flexible and customizable with the ability to select specific features to perturb and to control the noise level applied. - Detailed results visualization aids in the interpretability of robustness testing. **Limitations**: Despite the benefits, the test also exhibits some limitations: - It only perturbs the numerical features while leaving out features of non-numerical types, potentially providing an incomplete robustness analysis. - Default metric used is accuracy which might not always provide the best measure of a model's success, especially for imbalanced datasets. - The test is reliant on the assumption that the injected Gaussian noise is an adequate representation of potential corruption or incompleteness in real-world data. - The set decay threshold for accuracy might need to be fine-tuned or adjusted based on domain knowledge or specific project requirements. - The test might not perform as expected for datasets with a text column.\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAP Global Importance\n**Purpose:** The SHAP (SHapley Additive exPlanations) Global Importance metric illuminates the outcomes of machine learning models by attributing them to the contributing features. It assigns a quantifiable global importance to features via their absolute Shapley values, making it applicable for tasks such as classification (both binary and multiclass). This metric is pivotal in the model risk management strategy. **Test Mechanism:** The first step involves choosing an appropriate explainer that matches the model type: TreeExplainer for tree-based models (like XGBClassifier, RandomForestClassifier, CatBoostClassifier) and LinearExplainer for linear ones (such as LogisticRegression, XGBRegressor, LinearRegression). Once the explainer calculates the Shapley values, they are visualized through two specific plots: 1. **Mean Importance Plot**: This graph denotes the significance of each feature grounded in its absolute Shapley values. By computing the average of these absolute Shapley values across the entire dataset, the global importance of features is elucidated. 2. **Summary Plot**: This visual representation amalgamates the importance of each feature with their effects. Each dot on this plot symbolizes a Shapley value for a distinct feature in a specific instance. The vertical axis represents the feature, while the horizontal axis corresponds to the Shapley value. A color gradient, shifting from low to high, marks the feature's value. Overlapping points experience a slight vertical dispersion, offering a snapshot of the Shapley values' distribution for each attribute. Features are methodically aligned in accordance with their prominence. The function `_generate_shap_plot()` renders these plots with the aforementioned types. **Signs of High Risk:** Overly dominant features in SHAP importance plots hint at potential model overfitting. Anomalies, like unexpected or speculative features flaunting high importance, could suggest that the model's decisions are rooted in incorrect or undesirable reasoning. Moreover, a SHAP summary plot teeming with high variability or scattered data points is a cause for concern. **Strengths:** Beyond delineating global feature significance, SHAP offers a granular perspective on how individual attributes shape the model's decision logic for each instance. This advanced method unravels model behavior with clarity. Its flexibility is evident in its support for a diverse array of model types, ensuring uniform interpretations across different models. **Limitations:** For large datasets or intricate models, SHAP's computations might demand substantial time and resources. Moreover, its compatibility does not extend to every model class, especially models from libraries like \"statsmodels\", \"pytorch\", \"catboost\", \"transformers\", \"FoundationModel\", and \"R\". High-dimensional data can muddle interpretations, and linking importance to tangible real-world impact retains a degree of subjectivity.\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusion Matrix\n**Purpose**: The Confusion Matrix tester is tasked with evaluating the performance of a classification Machine Learning model. The matrix indicates how well the model can accurately distinguish true and false positives and negatives - basic indicators of model accuracy. **Test Mechanism**: The classification model's results (`y_test_predict`) are compared against the actual results (`y_test_true`). The unique labels identified from `y_test_true` are used to create a confusion matrix using scikit-learn's metrics. The matrix is then rendered using Plotly's `create_annotated_heatmap` function. Outcomes such as True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) are plotted in a color-coded heatmap, providing a visualized 2D representation of the model's performance. **Signs of High Risk**: High risks associated with the model are signaled by high numbers of False Positives (FP) and False Negatives (FN), which demonstrates the model's inability to accurately classify. On the other hand, low numbers of True Positives (TP) and True Negatives (TN) also indicate problematic situation, meaning the model is unable to correctly identify the class labels. **Strengths**: Utilizing a Confusion Matrix has various strengths: - It allows a clear and simple visual summary of the classification model's prediction accuracy. - It provides explicit metrics for True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), making it easy to highlight potential improvements. - It's beneficial for multi-class classification problems, since it can provide readability for complex models in a simplified manner. - It assists in understanding the different types of errors your model might make, by providing both Type-I and Type-II errors. **Limitations**: However, the Confusion Matrix also comes with limitations: - In conditions of unbalanced classes, the utility of the confusion matrix can be limited. It might quite erroneously perceive a model to perform well when it's merely predicting the majority class. - It does not deliver a single unified statistic that can appraise model performance. It merely indicates different facets of model performance separately. - It's largely a descriptive technique and does not allow for statistical hypothesis testing. - Misinterpretation risks also exist, as it doesn't directly provide precision, recall or F1-score data. These metrics need to be computed separately.\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifier In Sample Performance\n**Purpose**: The purpose of this metric is to evaluate the performance of the machine learning model on the training data. It measures commonly-used metrics such as accuracy, precision, recall, and F1 score. This test is typically used to gauge the model's ability to generalize its predictions to new, unseen data. It also measures the model's level of overfitting on the training set. **Test Mechanism**: This test uses metrics like accuracy, precision, recall, and F1 score. It applies these metrics to the model's predictions on the training set, and compares them with the true values. The accuracy measures the ratio of correct predictions to the total number of predictions. Precision gauges the number of true positive predictions relative to the total number of positive predictions. Recall indicates the number of true positive predictions relative to the total number of actual positives in the dataset. Finally, F1 score is the harmonic mean of precision and recall, providing an overall measure of the model's performance. **Signs of High Risk**: If the model has near perfect performance on all metrics on the training data but performs poorly on unseen data, it could be a sign of overfitting and hence, a high-risk scenario. Also, low values on any of these metrics can indicate an underperforming model, which may pose risk in production-grade applications. **Strengths**: Analyzing the model's performance using standard metrics like accuracy, precision, recall, and F1 score allows for a well-rounded assessment. The output is easy-to-interpret because they are widely used and understood in the machine learning community. Besides, since this test is applied to the training set, it can help detect instances of overfitting early in the model's development. **Limitations**: While these metrics provide useful insights, they are prone to biases in the training data. The model may perform well on the training set and yet perform poorly on new, unseen data. Therefore, this test should be complemented with additional validation techniques, such as k-fold cross-validation or out-of-sample testing, to ensure a less biased evaluation of the model's performance.\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nThresholdTest\nOverfit Diagnosis\n**Purpose**: The OverfitDiagnosis is a specialized component designed to identify regions prone to overfitting through the utilization of histogram slicing techniques. The code purports to measure the discrepancies between the performance of the model on the training dataset and the testing dataset, segmented into distinct regions established based on the feature space. By identifying the overfitted regions or high residuals, the mechanism guides towards the subsequent application of regularization techniques such as L1 or L2 regularization, dropout, or early stopping, or data augmentation to enhance the overall performance of the model and its generalization ability. **Test Mechanism**: The OverfitDiagnosis test is initiated by running the `run` method with pre-determined `default_params` and `default_metrics`, with 'accuracy' as the specified metric. The method executes the test by formulation distinct regions of the feature space based on binning techniques applied on feature columns from both training and testing datasets, further appending the predicted values. Each formed region is analyzed for performance discrepancies against actual values and the predictions of the model for both the datasets. The performance difference, referred to as the 'gap', is calculated and compared against a cut-off percentage indicating overfitting. In case of overfitting, the `run` method returns a **failed** status, while a successful test results in a **passed** status. Meanwhile, the function also prepares figures illustrating the overfitting regions. **Signs of High Risk**: The signs of high-risk associated with this model include: - A high value in the 'gap' between training and testing data accuracy - the larger the gap, the more overfit the model. - A high number and/or large regions of overfitting within the feature space. **Strengths**: - The technique allows for an insightful visual understanding by plotting overfitting regions. - Evaluates potential overfitting on a per-feature basis, allowing for more direct interventions. - Flexible test parameters such as 'cut_off_percentage' and 'features_column' for custom-tailored analysis. - Can handle both categorical and numerical features. **Limitations**: - Currently, this test only applies to classification tasks. Regression tasks are not supported. - Does not perform well with text-based features, limiting the utility in Natural Language Processing models. - Highly dependent on 'bins' setting which determines how the feature space is segmented. Different bin settings may produce different results. - Cut-off percentage for overfitting is arbitrary and no guideline is provided to set it optimally. - Performance metrics are limited to accuracy, limiting precision in the case of imbalanced datasets.\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutation Feature Importance\n**Purpose**: The purpose of the \"Permutation Feature Importance\" (PFI) metric is to quantify the importance of each feature used by the ML model. This is measured depending on how much the model's performance decreases when the values of each feature are permuted. **Test Mechanism**: The PFI metric is calculated using the `permutation_importance` method from the `sklearn.inspection` module. This function randomly permutes the columns of the feature dataset fed into the ML model and measures how the model's performance changes. If the model's performance worsens significantly after permuting a feature's values, that feature is deemed important. If the performance remains relatively constant, the feature is considered unimportant. At the end of the execution, PFI metric returns a figure displaying the feature importance of each feature. **Signs of High Risk**: An indication of risk would be if the model primarily depends on a feature whose values can easily be permuted or have high variance, suggesting instability. Another risk could be if there's a feature predicted as of no importance but is known to have significant influence on the expected outcome based on domain knowledge. **Strengths**: The PFI metric has several key advantages. It can provide an understanding of which features are crucial for the model's predictions, potentially revealing insights about the data's structure. This can also help determine if the model is overfitting on a particular feature or set of features. Moreover, PFI is model-agnostic and can be used for any classifier which can provide a measure of prediction accuracy before and after permuting features. **Limitations**: Although this metric is particularly helpful, there are some limitations too. The primary limitation is that feature importance calculated from this method does not imply causality—it only speaks to the amount of information the feature provides about the prediction task. Besides, permutation importance does not consider the interaction between features. If two features are correlated, the permutation importance might allocate importance to only one at the expense of the other. Finally, it cannot handle models from certain libraries like statsmodels, pytorch, catboost etc., thus limiting its applicability.\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimum ROCAUC Score\n**Purpose**: This test metric, Minimum ROC AUC Score, is used to determine the model's performance by ensuring that the Receiver Operating Characteristic Area Under the Curve (ROC AUC) score on the validation dataset meets or exceeds a predefined threshold. The ROC AUC score is an indicator of how well the model is capable of distinguishing between different classes, making it a crucial measure in binary and multiclass classification tasks. **Test Mechanism**: This test implementation calculates the multiclass ROC AUC score on the true target values and the model's prediction. The test converts the multi-class target variables into binary format using `LabelBinarizer` before computing the score. If this ROC AUC score is higher than the predefined threshold (defaulted to 0.5), the test passes; otherwise, it fails. The results, including the ROC AUC score, the threshold, and whether the test passed or failed, are then stored in a `ThresholdTestResult` object. **Signs of High Risk**: A high risk or failure in the model's performance as related to this metric would be represented by a low ROC AUC score, specifically any score lower than the predefined minimum threshold. This suggests that the model is struggling to distinguish between different classes effectively. **Strengths**: This metric is advantageous for the following reasons: - The test considers both the true positive rate and false positive rate, providing a comprehensive performance measure. - ROC AUC score is threshold-independent meaning it measures the model's quality across various classification thresholds. - Works robustly with binary as well as multi-class classification problems. **Limitations**: Despite its strengths, the metric presents certain limitations: - ROC AUC may not be useful if the class distribution is highly imbalanced; it could perform well in terms of AUC but still fail to predict the minority class. - The test does not provide insight into what specific aspects of the model are causing poor performance if the ROC AUC score is unsatisfactory. - The use of macro average for multiclass ROC AUC score implies equal weightage to each class, which might not be appropriate if the classes are imbalanced.\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecision Recall Curve\n**Purpose**: The Precision Recall Curve metric is intended to evaluate the trade-off between precision and recall in classification models, particularly binary classification models. It assesses the model's capacity to produce accurate results (high precision), as well as its ability to capture a majority of all positive instances (high recall). **Test Mechanism**: The test extracts ground truth labels and prediction probabilities from the model's test dataset. It applies the precision_recall_curve method from the sklearn metrics module to these extracted labels and predictions, which computes a precision-recall pair for each possible threshold. This calculation results in an array of precision and recall scores that can be plotted against each other to form the Precision-Recall Curve. This curve is then visually represented by using Plotly's scatter plot. **Signs of High Risk**: A lower area under the Precision-Recall Curve signifies high risk. This corresponds to a model yielding a high amount of false positives (low precision) and/or false negatives (low recall). If the curve is closer to the bottom left of the plot, rather than being closer to the top right corner, it can be a sign of high risk. **Strengths**: This metric aptly represents the balance between precision (minimizing false positives) and recall (minimizing false negatives), which is especially critical in scenarios where both values are significant. Through the graphic representation, it enables an intuitive understanding of the model's performance across different threshold levels. **Limitations**: This metric is only applicable to binary classification models – it raises errors for multiclass classification models or Foundation models. Also, it may not fully represent the overall accuracy of the model if the cost of false positives and false negatives are extremely different, or if the dataset is heavily imbalanced.\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifier Performance\n**Purpose**: The provided script assesses the performance of Machine Learning classification models. The performance is calculated by determining the precision, recall, F1-Score, accuracy, and the ROC AUC (Receiver operating characteristic - Area under the curve) scores. The test accepts both binary and multiclass models. **Test Mechanism**: The script generates a report that includes precision, recall, F1-Score, and accuracy using the `classification_report` method from the scikit-learn's metrics module. For multiclass models, weighted and macro averages for these scores are also computed. Additionally, the script calculates and includes the ROC AUC scores for the model using a custom method, `multiclass_roc_auc_score`. The output of the test is a structured report that differs depending on whether the model is binary or multiclass. **Signs of High Risk**: Indicators of high risk or a failing model include low values for the metrics used - precision, recall, F1-Score, accuracy, and ROC AUC. An imbalanced precision and recall score can also indicate a high-risk model, as precision focuses on the correct prediction of the positive class, while recall focuses on the number of correctly identified actual positive cases. A low ROC AUC score, particularly when it is close to 0.5 or below, is a strong indicator of a failing model. **Strengths**: This metric excels in its versatility by being capable of handling both binary and multiclass models. It computes several commonly used performance metrics, providing a comprehensive view of the model's performance. Using ROC-AUC as a metric can help in identifying the best threshold for classification, especially when datasets are unbalanced. **Limitations**: The test relies on labels being correctly identified for binary classification models, raising an exception if the positive class is not labeled as \"1\", which might not always be the case in practical applications. Furthermore, this script is designed only for classification models and cannot evaluate regression models. The calculated metrics might not be informative in situations where the test dataset is not representative of the data the model will encounter.\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimum F1 Score\n**Purpose**: The primary purpose of this test is to ensure that the model's F1 score - a balanced reflection of both precision and recall - meets or surpasses a predetermined threshold on the validation dataset. This F1 score is a valuable measure of model performance in classification tasks, particularly where the proportion of positive and negative classes is imbalanced. **Test Mechanism**: The model's F1 score is calculated for the validation dataset via python's scikit-learn metrics. The scoring differs according to the classification problem: for multi-class, it uses macro averaging (calculating metrics separately then finding their unweighted mean), and for binary classification, it uses inherent f1_score calculation. The computed F1 score is then compared against the predetermined threshold - a minimum F1 score the model is expected to achieve. **Signs of High Risk**: Any model that returns an F1 score lower than the set threshold is considered high risk. A low F1 score could indicate that the model is not striking a desirable balance between precision and recall, or in simpler terms, it is not doing well in correctly identifying positive classes and limiting false positives. **Strengths**: This metric provides the advantage of being a balanced measure of a model's performance by considering both false positives and false negatives. It is particularly useful in scenarios with imbalanced class distribution, where accuracy can be misleading. It also allows for customization of the minimum acceptable performance by setting the threshold value. **Limitations**: This testing method may not be appropriate for all types of models and machine learning tasks. Also, while the F1 score captures a balanced view of a model's performance, it assumes equal cost for false positives and false negatives, which may not always be the case in certain real world scenarios. This limitation may compel practitioners to choose other metrics such as precision, recall, or the ROC-AUC score that align better with their specific needs.\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROC Curve\n**Purpose**: The Receiver Operating Characteristic (ROC) curve is a critical analysis tool for the performance of binary classification models. The ROC curve displays the trade-off between the True Positive Rate (TPR) and False Positive Rate (FPR) at varying threshold levels. The ROC curve, together with the Area Under the Curve (AUC), is designed to provide a measure of how well the model can discriminate between the two classes in a binary classification problem (e.g., default vs non-default). The higher the AUC score, the better the model is at correctly distinguishing between the positive and negative classes. **Test Mechanism**: This script extracts the target model and datasets, where binary classification is a requirement. Next, it computes predicted probabilities for the test set. It then calculates and plots the ROC curve using the true outcomes and predicted probabilities, along with the line representing randomness (AUC of 0.5). The AUC score for the ROC curve of the model is also computed, giving a numeric estimate of the model's performance. Any Infinite values in the ROC threshold are identified and removed in the process. The resulting ROC curve, AUC score, and thresholds are subsequently saved. **Signs of High Risk**: There would be a high risk associated with the performance of the model if the AUC score is below or close to 0.5, or if the ROC curve is observed to be closer to the line of randomness (indicating no discriminative power). It's essential to note that the AUC score must be significantly greater than 0.5 for the model to be considered effective at its classification task. **Strengths**: The ROC Curve provides a comprehensive visual representation of a model’s discriminative power over all possible classification thresholds, unlike metrics that only reveal model performance at a single set threshold. The AUC Score, which summarizes the ROC curve into a single value, remains consistent in the face of imbalanced datasets, making it an ideal choice for such cases. **Limitations**: This test is designed specifically for binary classification tasks, limiting its application to other model types. Additionally, it might not perform well for models that output probabilities severely skewed towards 0 or 1. In an extreme case, the ROC curve can exhibit high performance even in situations where the majority of classifications are incorrect, if the model's score ranking is preserved. This is known as the \"class imbalance problem.\"\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\n**Purpose**: The 'TrainingTestDegradation' class serves as a test to verify that the degradation in performance between the training and test datasets does not exceed a predefined threshold. This test serves as a measure to check the model's ability to generalize from its training data to unseen test data. It assesses key classification metric scores such as accuracy, precision, recall and f1 score, to verify the model's robustness and reliability. **Test Mechanism**: The code applies several predefined metrics including accuracy, precision, recall and f1 scores to the model's predictions for both the training and test datasets. It calculates the degradation as the difference between the training score and test score divided by the training score. The test is considered successful if the degradation for each metric is less than the preset maximum threshold of 10%. The results are summarized in a table showing each metric's train score, test score, degradation percentage, and pass/fail status. **Signs of High Risk**: High risk or failure in the model's performance related to this test can be indicated by one or more of the following: 1. A degradation percentage that exceeds the maximum allowed threshold of 10% for any of the evaluated metrics. 2. A high difference or gap between the metric scores on the training and the test datasets. 3. The 'Pass/Fail' column displaying 'Fail' for any of the evaluated metrics. **Strengths**: 1. This test provides a quantitative measure of the model's ability to generalize to unseen data, which is key for predicting its practical real-world performance. 2. By evaluating multiple metrics, it takes into account different facets of model performance and enables a more holistic evaluation. 3. The use of a variable predefined threshold allows the flexibility to adjust the acceptability criteria for different scenarios. **Limitations**: 1. The test compares raw performance on training and test data, but does not factor in the nature of the data. Areas with less representation in the training set, for instance, might still perform poorly on unseen data. 2. It requires good coverage and balance in the test and training datasets to produce reliable results, which may not always be available. 3. The test is currently only designed for classification tasks.\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nModels Performance Comparison\n**Purpose**: This metric test is intended to evaluate and compare the performance of multiple Machine Learning models on test data. It uses a number of metrics including accuracy, precision, recall, and F1 score among others, to measure model performance and aid in the selection of the most effective model for the given task. **Test Mechanism**: This test involves using Scikit-learn’s performance metrics to assess model's performance in both binary and multiclass classification tasks. To compare the performances, it evaluates each model on the test dataset and produces a detailed classification report. This includes the aforesaid metrics, along with the roc_auc score. Depending on whether the task at hand is binary or multiclass classification, it calculates metrics globally for the \"positive\" class or their weighted averages, macro averages, and per class metrics respectively. If no models are provided, the test is skipped. **Signs of High Risk**: High risk or poor model performance might be indicated by low accuracy, precision, recall, and/or F1 scores, or a low area under the Receiver Operating Characteristic (ROC) curve (roc_auc). If the metrics' scores are significantly lower than alternative models, it might suggest a high failure risk. **Strengths**: This test allows for straightforward performance comparison of multiple models, accommodating both binary and multiclass classification tasks. It provides a comprehensive report of key performance metrics, offering a holistic view of model performance. It also includes ROC AUC, a robust performance metric that is capable of dealing effectively with class imbalance issues. **Limitations**: This test may not be comprehensive for more complex performance evaluations that consider other factors, such as speed of prediction, computational cost, or specific business constraints. It's also reliant on the provided test dataset, so the chosen models' performance could differ on unseen data or when the data distribution changes. The ROC AUC score isn't necessarily meaningful for multilabel/multiclass tasks, and can be hard to interpret in such contexts.\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nThresholdTest\nWeakspots Diagnosis\n**Purpose:** The weak spots test is designed to evaluate the performance of a machine learning model in specific regions of the feature space. This test consists of dividing the feature space into various sections or slices, evaluating the model's output within each of these sections, and identifying regions where the model's performance metrics fall below specified thresholds. Performance metrics include accuracy, precision, recall, and F1 scores. This diagnostic test helps identify areas where the machine learning model may not perform well, potentially exposing its limitations and weaknesses. **Test Mechanism:** The test is performed by slicing the feature space of the training data set into multiple bins. For each bin, the model's performance metrics are computed for both the training and test data sets. If any of the model's performance metrics fall below the predetermined threshold for a particular bin on the test dataset, it is identified as a \"weak spot\". The results are visually represented in a bar chart for each performance metric, marking the bins failing the threshold. **Signs of High Risk:** High risk or failure in the model's performance is indicated when any of the model's performance metrics fall below the set thresholds. If any bin performed significantly worse in the test dataset compared to the training dataset, it might indicate overfitting in that region. Further, if a region or slice has low performance metrics, it suggests that the model does not handle that type of input data well, which may lead to inaccurate predictions. **Strengths:** - The weak spots test helps identify specific regions of the feature space where the model's performance is subpar, which can guide further refinement of the model. - Plotting the performance metrics provides an intuitive way to understand the model's performance across different regions. - The test can be customizable, allowing users to specify various thresholds for multiple performance metrics based on the needs of the specific application. **Limitations:** - By binning the feature space, the test could potentially oversimplify the model's behavior in each region. The granular control of this slicing depends on the bins parameter, and can be coincidentally arbitrary. - The test's effectiveness relies upon the chosen thresholds for the performance metrics, which may not be universally applicable and subject to the model's specification and the application. - The test does not handle datasets with a text column, thus limiting its applicability to only numerical or categorical data. - The test does not directly provide suggestions for model improvement, only highlighting potentially problematic regions.\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulation Stability Index\n**Purpose:** The Population Stability Index (PSI) is used to evaluate how stable a predictive model's score distribution is when two different datasets are compared - typically a development and a validation dataset or two separate time periods. This assessment aids in understanding if a significant shift has occurred in the model's performance over time or if there has been a severe change in population characteristics. **Test Mechanism:** In this script, the test mechanism involves comparing the PSI between the training and test datasets. The data from each dataset is sorted and placed into either fixed bins or quantiles. The bin boundaries are established based on the initial population in the case of quantiles. The values in the bins are counted and the proportions calculated. The PSI is then calculated for each bin using a logarithmic transformation of the ratio of the proportions. A summarising table and a grouped bar chart alongside a scatter plot are created showing the PSI, and percentage of original and new data in each bin. **Signs of High Risk:** Signs of high risk would be captured by a high PSI value. A high PSI indicates a dramatic shift in the model's performance or a significant change in population attributes, suggesting that the model may not be performing as expected. **Strengths:** The PSI approach helps assess the stability of a model over time or across samples, which is critical for understanding if the model's performance has changed significantly. It provides a quantitative measure (the PSI value) that allows for direct comparisons across features. The calculation and interpretation of PSI is straightforward making it an effective tool for model validation. The visual display further aids for better understanding. **Limitations:** One potential drawback is that the PSI test doesn't take into account the interdependence between features. It also doesn't provide insights into why the distributions are different. Additionally, PSI may not adequately handle features with significant outliers. Finally, the test is performed here on the model predictions, not on the underlying data distributions - differences in PSI could be due to feature drift, model drift, or both combined, without a clear way to distinguish between them.\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimum Accuracy\n**Purpose**: The purpose of the Minimum Accuracy test is to validate that the model's prediction accuracy on a given dataset meets or exceeds a predefined minimum threshold. Accuracy is the fraction of predictions our model got right and is an essential metric to understand how well our model is performing. In the context of both binary and multiclass classifications, accurate labeling is crucial. **Test Mechanism**: The mechanism of this threshold test involves comparing the model's accuracy score against the minimum threshold value specified (default value is 0.7). The accuracy score is calculated by using the sklearn's `accuracy_score` method between true label `y_true` and predicted label `class_pred`. If the accuracy score exceeds the threshold, the test is marked as passed. The test result is returned along with the accuracy score and the threshold used for the test. **Signs of High Risk**: Signs of high risk within this test present themselves when the model is unable to meet or exceed the specified score threshold. If the model's accuracy score is consistently falling below the set threshold, it indicates a high risk of inaccurate predictions, which can reduce the model's effectiveness and dependability. **Strengths**: The strength of the Minimum Accuracy test lies in its simplicity. It provides a straightforward measure of overall model performance across all classes. It's particularly advantageous when the classes are balanced. The test is also versatile and can be used with both binary and multiclass classification tasks. **Limitations**: This test's limitations surface when dealing with imbalanced datasets. If the classes in the dataset are highly skewed, the accuracy score could be misleading, potentially favoring the majority class and providing a false sense of model performance. Another limitation is it does not provide a measure of the model's precision, recall, or ability to manage false positives and false negatives. It primarily focuses on overall correctness, which might not be sufficient for all types of model analytics.\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegression Models Coeffs\n**Purpose**: The 'RegressionModelsCoeffs' test is designed to evaluate and compare the coefficients of different regression models that were trained on the same dataset. This metric assesses how each model weighted the importance of various features during training, which is useful for analyzing which features influenced the model's outputs the most, and how these influence patterns vary between different models. **Test Mechanism**: This test measures the coefficients of each regression model by calling the 'regression_coefficients()' method on the model. These coefficients are then compiled into a summary dataframe for all models. Each row in the dataframe corresponds to a model, with a column for each feature's coefficient. Note, this test is only applicable to 'statsmodels' and 'R' models, and will raise a 'SkipTestError' for models from other libraries. **Signs of High Risk**: Instances that might suggest high risk associated with this test include: discrepancies in the way different models weigh features, if any coefficient is unexpectedly high or low, or the test could not be applied to certain models because they are not of the 'statsmodels' or 'R' types. **Strengths**: This test is highly valuable for gaining insight into the training process of different models and for comparing how the models considered the importance of various features. By monitoring the feature coefficients, this test provides a more transparent evaluation of the model and surfaces crucial weights and biases in the training process. **Limitations**: The primary limitation of this test is its compatibility: it is only designed for use with 'statsmodels' and 'R' regression models. Beyond this, while the test contrasts the ways various models weigh features, it does not indicate which weighting is most appropriate or accurate, leaving room for interpretation. Lastly, this test does not account for potential overfitting or underfitting of models, and the coefficients it produces might not translate to effective performance on unseen data.\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs\n\n\nMetric\nBox Pierce\n**Purpose:** The Box-Pierce test is a diagnostic tool employed to check if any form of autocorrelation (serial correlation) is present in the time series dataset, a set of data points indexed in time order, being tested. Autocorrelation is the similarity between observations as a function of the time lag between them. It evaluates the independence of residuals in regression types of tasks in Machine Learning. This is essentially a measure to establish the quality of a time-series model, ensuring the error terms in the model do not follow any specific pattern and are random. **Test Mechanism:** The Box-Pierce test calculates a test statistic and a corresponding p-value from the data features. The computed statistical test checks the null hypothesis that the data are independently distributed. This test is implemented by looping over each feature column in the time-series dataset and applying the `acorr_ljungbox` function from the statsmodels library, which returns both the Box-Pierce test statistic and its corresponding p-value. These values are cached as results. **Signs of High Risk:** A low p-value (typically less than 0.05) in the box-pierce test reflects evidence against the null hypothesis suggesting that there are autocorrelations in the dataset, pointing to a potentially high-risk situation in terms of model performance. Additionally, large Box-Pierce test statistic values may signal the presence of autocorrelation. **Strengths:** The Box-Pierce test is beneficial as it aids in detecting patterns or trends in data that are meant to be random, essentially ensuring the absence of autocorrelation. It can be implemented efficiently due to low computational complexity. Moreover, it can be widely applied for most types of regression problems. **Limitations:** This test assumes homoscedasticity and normality of residuals, assumptions which may not always hold true. It may have reduced power for detecting complex autocorrelation schemes, such as higher order or negative correlations. Also, it does not provide specific insights into the nature or patterns of the autocorrelation if detected; it is solely a general test for the existence of autocorrelation. Furthermore, if the time series data has a trend or seasonality, the Box-Pierce test could give misleading results. Lastly, it's worth stressing it's only applicable to time-series data, limiting its overall utility.\nvalidmind.model_validation.statsmodels.BoxPierce\n\n\nMetric\nRegression Coeffs Plot\n**Purpose**: The Regression Coefficients with Confidence Intervals plot and metric is used to gain an understanding of the impacts of predictor variables on the response variable under examination. It enables the visualization and analysis of the regression model by displaying the set of coefficients derived from the model and their associated 95% confidence intervals. This provides insight into the variability and uncertainty associated with the model's estimates. **Test Mechanism**: The test extracts the estimated coefficients and their associated standard errors from the regression model. Then, it calculates and draws confidence intervals using a 95% confidence level (standard convention in statistics), which provides an indication of the range where the true value can be expected to fall in 95% of the samples. For instance, if the same regression were run multiple times with samples taken from the same population. The process then visualizes these as a bar plot, with predictor variables on the x-axis and their corresponding coefficients on the y-axis, and the calculated upper and lower margins of the confidence intervals being illustrated as error bars. **Signs of High Risk**: If the zero value (indicating no effect) is within the calculated confidence interval, it suggests that that particular feature/coefficient may not significantly contribute to prediction in the model. If multiple coefficients demonstrate this behavior, the overall reliability of the model could be questioned. Additionally, very wide confidence intervals could suggest high uncertainty in the associated coefficient estimates. **Strengths**: This metric provides a straightforward and easily understood visualization of the significance and impact of the individual predictor variables in a regression model. The inclusion of confidence intervals allows for an examination of the uncertainty around each coefficient estimate. **Limitations**: This test is reliant on certain assumptions about the data: it assumes normality of the residuals and independence of observations, which might not be the case in all types of datasets. Additionally, it does not take into account multi-collinearity (correlation between predictor variables), which can distort the model and make interpretation of the coefficients difficult. Finally, this test is only applicable to regression tasks and tabular data sets, and is not suitable for other types of machine learning tasks or data structures.\nvalidmind.model_validation.statsmodels.RegressionCoeffsPlot\n\n\nMetric\nRegression Model Sensitivity Plot\n**Purpose**: The Regression Sensitivity Plot metric is designed to facilitate sensitivity analysis for regression models. It is primarily utilized for examining the outcome change of a system when alterations(shocks) are applied to one variable at a time, keeping all other variables constant. Its core focus is to analyze the effect of each independent variable on the dependent variable in the regression model, and thus aids in identifying critical risk factors in the model that could significantly influence the output. **Test Mechanism**: This test begins by applying shocks of varying magnitudes (as per defined parameters) to each feature of the model, one at a time. A new prediction is then made for each shocked dataset. Since these shocks are applied while keeping all other variables constant, the resulting changes in the model's predictions can be attributed to the shocks. If a transformation parameter is set to \"integrate\", the initial predictions and target values are transformed using an integration function before being plotted. In the end, a plot of observed values versus predicted values is generated for each model, with a distinct line graph illustrating predictions for each shock. **Signs of High Risk**: If the plot shows drastic changes in model predictions with small shocks to an individual variable, it may be an indication of high risk. This could suggest a high sensitivity of the model to changes in that variable, potentially signaling over-reliance on the variable to make predictions. Unusually high or erratic shifts in the response of the model concerning shocks may also suggest potential model instability or overfitting. **Strengths**: This metric holds the value by enabling the identification of those variables that have a major effect on the model outcomes, thus paving the way towards feature importance understanding. It also generates visual plots, making the results understandable, clear, and interpretable even to non-technical stakeholders. It's beneficial for spotting overfitting and detecting unstable models that too responsive to small changes in variables. **Limitations**: This metric assumes that all other variables remain constant when a shock is applied, which may not always reflect real-world conditions where variables might have complex interdependencies. It is also best suited for linear models and may not always appropriately evaluate the sensitivity of non-linear models. Additionally, it does not provide a numerical measure of risk, it only provides a visual indication, which might be subjective.\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\nMetric\nRegression Models Performance\n**Purpose**: This metric is intended to assess and compare the performance of different regression models. It offers an insightful look into the models' ability to predict dependent variables, both on the data used for training (in-sample) and new data (out-of-sample). Metrics such as R-squared, Adjusted R-squared, and Mean Squared Error (MSE) are employed. **Test Mechanism**: This test iterates over various regression models supplied as an input list. For each model, it evaluates their in-sample and out-of-sample performance by capturing predictions for the training and test datasets respectively, and comparing these predictions against actual values to calculate R-squared, Adjusted R-squared, and Mean Squared Error (MSE). The models are individually evaluated in this manner, with their results being stored and returned for comparison purposes. **Signs of High Risk**: High risk or potential failures in the model's performance might be indicated by excessively large MSE values, or unreasonably low R-squared and Adjusted R-squared values. If the model's performance drastically declines when moving from in-sample to out-of-sample evaluations, this could also signify a high risk of overfitting. **Strengths**: This metric is versatile and allows for the comparison of multiple models at once, providing an objective means of identifying the best-performing model. It provides both in-sample and out-of-sample evaluations, thus informing about the model's performance on unseen data. The use of R-squared and Adjusted R-squared values in addition to MSE provides a comprehensive view of the model's explainability and error rate. **Limitations**: This test presupposes that the regression model's residuals are normally distributed, *i.e.*, a key assumption of Ordinary Least Squares (OLS) regression. Thus, it may not appropriately assess models where this assumption is violated. Additionally, it does not account for situations where predictive performance might not directly correlate with higher R-squared or lower MSE values, especially in the event of excessively complex models.\nvalidmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nMetric\nZivot Andrews Arch\n**Purpose**: The Zivot-Andrews Arch metric is used to evaluate the order of integration for a time series data in a machine learning model. It's designed to test for stationarity, a crucial aspect in time series analysis where data points are assumed not to be dependent on time. Stationarity is an indication that the statistical properties such as mean, variance and autocorrelation are all constant over time. **Test Mechanism**: For each feature in the dataset, the Zivot-Andrews unit root test is performed using ZivotAndrews function imported from the arch.unitroot module. It returns the zivot-andrews metric for each feature which consists of the statistical value, p-value (probability value), the number of used lags and the number of observations. The p-value is then later utilized to make a decision on the null hypothesis (unit root exists, series is non-stationary) based on a chosen significance level. **Signs of High Risk**: High risk can be suggested by a high p-value. This could imply that there's an insufficient basis to reject the null hypothesis, which states that the time series has a unit root and is therefore non-stationary. Non-stationary time series data may lead to misleading statistics and unreliable Machine Learning models. **Strengths**: The Zivot-Andrews Arch metric can dynamically test for stationarity against structural breaks in time series data, which provides a robust evaluation of stationarity of features. It becomes very useful in the cases of financial, economic or any time-series data where data observations don't have a consistent pattern, and structural breaks might occur. **Limitations**: The Zivot-Andrews Arch metric test assumes that data under consideration comes from a single-equation, autoregressive model. Thus, it may not be suitable for multivariate time series data or data that doesn't follow the autoregressive model assumption. Additionally, it might not account for sudden shocks or changes in the trend of the time series data which can significantly impact the stationarity of the data.\nvalidmind.model_validation.statsmodels.ZivotAndrewsArch\n\n\nMetric\nRegression Model Outsample Comparison\n**Purpose**: The RegressionModelOutsampleComparison test is designed to evaluate the predictive performance of multiple regression models by means of an out-of-sample test. Crucially, the aim of this test is to validate the model's ability to generalize to unseen data, a need that arises from the challenge of overfitting. Two key metrics, Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), are computed for this purpose to provide a quantifiable measure of the model's accuracy on the testing dataset. **Test Mechanism**: To perform this test, multiple models (in the form of Ordinary Least Squares, or OLS regression models) and a test dataset are required as inputs. For each model, predictions are made on the test dataset, following which, the residuals are calculated. These residuals are then used to compute the MSE and RMSE for each model. The outcomes of the test, including the model's descriptive name, its MSE, and RMSE, are stored and outputted in a structured dataframe format. **Signs of High Risk**: High values of MSE or RMSE indicate elevated risk, signifying that the model's predictions significantly deviate from the actual values in the test dataset. Furthermore, persistently significant discrepancies between training and testing performance across various models might suggest an issue with the input data or with the model selection strategies. **Strengths**: This test effectively provides a comparative evaluation of multiple models' out-of-sample performance, enabling the identification of the best performing model. Moreover, by leveraging both MSE and RMSE, one can gain insights about the model's prediction error. While MSE is sensitive to outliers, emphasising larger errors, RMSE (being in the same unit as the dependent variable) provides a more interpretable measurement of average prediction error. **Limitations**: While this test provides valuable insights about model generalization, its applicability is constrained to regression tasks, and specifically OLS models. Furthermore, it assumes that the test dataset is a representative sample of the population that the built model is intended to be generalized to, which might not always be the case. Lastly, the RMSE and MSE might be less meaningful when the dependent variable scale varies significantly, or the residuals' distribution is heavily skewed or contains outliers.\nvalidmind.model_validation.statsmodels.RegressionModelOutsampleComparison\n\n\nMetric\nRegression Model Forecast Plot Levels\n**Purpose:** The purpose of `RegressionModelForecastPlotLevels` is to visually evaluate the performance of a given list of regression models. It does this by plotting the forecasts of these models against the observed data in their training and test datasets. It evaluates the model's ability to produce accurate and reliable forecasts when faced with certain input features. The measure of accuracy here is the proximity of the forecasted values to the actual observed values. In case of any transformations specified, the metric also handles transforming the data before the comparison. **Test Mechanism:** The Python class in consideration accepts `transformation` as a parameter, which defaults to None. First, it checks for the presence of model objects and raises a `ValueError` if none are provided. Next, it loops through each model, generating predictive forecasts for the model's training and testing datasets. These forecasts are then plotted against the actual (observed) values. If a transformation, such as \"integrate\", is specified, the class carries out the transformation operation (i.e., it performs cumulative sums to create a new series). Finally, plots are created comparing observed and forecasted values for both the original and transformed datasets. **Signs of High Risk:** High risk or failure in the model's performance can be inferred from the generated plots. If the forecasted values deviate significantly from the observed values in either the training or test datasets, it suggests high risk. A significant deviation could be a sign of overfitting or underfitting, which would be a cause for concern. Such discrepancies could limit the model's ability to produce accurate and generalizable results. **Strengths:** - Visual evaluation: The metric provides a graphical way of evaluating the regression models, allowing easier interpretation and assessment of the forecasting accuracy. - Handles multiple models: The metric enables evaluation of multiple models at once, providing a comparative overview of all models. - Handles transformations: Ability to handle transformations such as \"integrate\" allows for broader scope and flexibility in model evaluations. - Detailed insight: The metric provides a detailed perspective by looking at the performance on both training and testing datasets. **Limitations:** - Visual subjectivity: The metric relies heavily on visual evaluations, and interpretation can vary from person to person. - Limitation in transformations: Currently supports \"integrate\" transformation only. More complex transformation might not be covered. - Overhead: Plotting for large datasets might be computationally expensive and could increase runtime. - Lack of numerical metrics: While visualization is useful, a corresponding numerical measure to support observations would be beneficial.\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\n\n\nMetric\nLog Regression Confusion Matrix\n**Purpose**: The Logistic Regression Confusion Matrix metric is designed to evaluate the performance of a logistic regression classification model by accounting for true positives, true negatives, false positives, and false negatives. By using this confusion matrix, one can visually determine how effectively the model is distinguishing between correct and incorrect classifications. The metric is particularly useful in scenarios where predictions are generated by thresholding probabilities. **Test Mechanism**: This metric makes use of Python's `sklearn.metrics.confusion_matrix` function to generate the confusion matrix. Initially, the model's predicted probabilities are adjusted to binary predictions using a provided cut-off threshold, by default set to 0.5. Afterwards, the confusion matrix is constructed using the actual and predicted classes. The confusion matrix is designed in such a way that predicted class labels form the x-axis and actual class labels form the y-axis. Each cell of the matrix contains the count of true positives, true negatives, false positives and false negatives respectively. **Signs of High Risk**: High risk or indications of model failure would be suggested by large numbers of false positives and false negatives. This would suggest that the model is incorrectly classifying a substantial number of instances. Also, if the true positive and true negative counts are significantly lower than expected, it can be seen as a high-risk indication. **Strengths**: The Logistic Regression Confusion Matrix is a simple and intuitive way to understand model performance. It not only provides a clear picture of how many instances are correctly and incorrectly classified, but also gives a breakdown of the types of errors. By adjusting the cut-off threshold parameter, users can explore trade-offs between precision (minimizing false positives) and recall (minimizing false negatives), making it flexible and adaptable for various prediction scenarios. **Limitations**: Despite its simplicity and clarity, the confusion matrix has its limitations. For imbalanced datasets, confusion matrix might give misleading results. For instance, a model might have a high accuracy by predicting majority classes, yet perform poorly on minority classes. It also does not give any insight into the severity of the mistakes and the cost trade-off between different kinds of misclassifications. Lastly, the choice of the cut-off threshold can significantly impact the interpretation, and an improperly chosen threshold may lead to flawed conclusions.\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nPD Rating Class Plot\n**Purpose**: The Probability of Default (PD) Rating Class Plot is used to understand the distribution of default probabilities across different rating classes. This test is critical in inferring credit risk, as it shows the likelihood of default for different rating classes based on the predictions made by the model. The test is designed to visualize how effectively the model is able to differentiate between different risk levels in the credit data set. **Test Mechanism**: This visualization metric classifies predicted probabilities of defaults into user-defined rating classes (defined in \"rating_classes\" in default parameters), performs a calculation of average default rate within each rating class, and then generates bar plots for each rating class. The generated bar plots aim to illustrate the average probability of default within each class, for both the training and testing data sets. The classification of predicted probabilities is performed using the pandas \"cut\" function, which segments and sorts the data values into bins. **Signs of High Risk**: If the bar plots show that lower rating classes have higher average probabilities of default than higher rating classes, or there is little differentiation between the averages across rating classes, this indicates high risk in the model's ability to accurately predict default levels. Also, a model that produces a vast difference between probabilities for the training and testing set may indicate overfitting, which is also a sign of high risk. **Strengths**: The strength of this metric lies in its ability to visually represent the effectiveness of the model in predicting credit default risk across different risk levels, allowing for easy interpretation. By visualizing the model's effectiveness for each rating class separately, it allows the analyst to quickly identify where the model is struggling. The inclusion of both training and testing data sets helps to quickly highlight issues of overfitting. **Limitations**: The main limitation of this metric comes from predetermining the number of rating classes. Incorrect choices could either oversimplify or overcomplicate the picture of default rates. The metric relies heavily on the assumptions that the rating classes effectively separate different risk levels and that the defined boundaries between rating classes accurately represent the true risk distribution. Furthermore, the tests don't account for class imbalance in the data set that could skew the average probabilities. This metric alone cannot be used for determining the overall performance of the model, and should be used in combination with other evaluation metrics.\nvalidmind.model_validation.statsmodels.PDRatingClassPlot\n\n\nMetric\nScorecard Histogram\n**Purpose**: The **Scorecard Histogram** test provides a visual representation of the distribution of credit scores produced by an ML model for classification tasks involving credit risk. The metric is primarily designed to analyze how the model's scoring decision aligns with actual outcomes of credit loan applications. In addition, it helps identify potential discrepancies between model predictions and real-world risk levels. **Test Mechanism**: This model-specific metric uses the provided training and test datasets to generate a histogram of credit scores for both default (negative class) and non-default (positive class) instances. It works by calculating the credit score for each instance in the dataset using a logistic regression model's scorecard method which takes into account the contributions of different features to the odds of being a default. It uses a default point to odds (PDO) scaling factor and predefined target score and odds settings. The score distribution is calculated and plotted separately for the training and test sets to facilitate insights into how well the model generalizes to unseen data. **Signs of High Risk**: Indications of potential risk or performance issues in relation to this metric include significant discrepancies between the distributions of training and testing data, skewed distributions favoring a particular score or class, or an abnormal distribution of scores that does not align with expected real-world patterns. If positive and negative classes tend to have similar scores or their distributions overlap significantly, it might suggest the model is not effective at differentiating between the classes. **Strengths**: The Scorecard Histogram metric is useful for visually interpreting the credit scoring system of a machine learning model and can provide a greater understanding of model behavior. It allows for direct comparison of actual and predicted scores for both training and testing data, and provides a way to intuitively visualize model's ability to differentiate between positive and negative classes. It also aids in uncovering patterns or anomalies that might not be evident from numerical metrics alone. **Limitations**: Although valuable for visual interpretation, this method doesn't quantify model's performance, and hence might lack precision when it comes to complete model evaluation. It is also susceptible to the quality of the input data—undue bias or noise in the data will impact both the score calculation and resultant histogram. Additionally, the test is specific to credit scoring models, which limits its usefulness across a broader range of machine learning tasks and models. Finally, the efficacy of the metric is somewhat tied to subjective interpretation, as it relies on the analyst's assessment of the plot's characteristics and implications.\nvalidmind.model_validation.statsmodels.ScorecardHistogram\n\n\nMetric\nFeature Importance And Significance\n**Purpose**: The 'FeatureImportanceAndSignificance' test in the given script aims to calculate the importance and statistical significance of features within the model's context. It achieves this by comparing the p-values from a regression model and the feature importances from a decision tree model. This test aids in feature selection during the model development process by identifying the most significant variables. **Test Mechanism**: During this test, an initial comparison is made between the p-values from a regression model and the importance of features from a decision tree model. Both the p-values and feature importances are then normalized to ensure uniform comparison. The 'p_threshold' parameter is used to determine what p-value is considered statistically significant. If 'significant_only' parameter is true, features with p-values below the threshold will be included in the final output. The output includes interactive plots displaying Normalized p-values and the corresponding Normalized Feature Importance. Errors are thrown if two models aren't provided (one regression and one decision tree). **Signs of High Risk**: High risk or warning signs in the context of this test might include very high or very low p-values that stand out, suggesting a feature might not be meaningful. The risk may also exist if many unimportant variables (with low feature importance) have significant p-values, which might indicate that the model is possibly overfitted. **Strengths**: It is an excellent test for feature selection mechanisms as it combines two perspectives: statistical significance through p-values and feature importance using a machine learning model (decision tree). Additionally, it includes an interactive visualization, facilitating comprehension of the results easily. **Limitations**: This test only accepts two models - a regression model and a decision tree. Therefore, its application may be limited if other model types are used. Furthermore, the test might not account for potential correlative or causative relationships between the input features, which potentially can lead to inaccurate importance and significance readings. Lastly, it heavily relies on the p-value as a cut-off for feature significance, which critics argue can be arbitrary and might not reflect the true significance of the feature.\nvalidmind.model_validation.statsmodels.FeatureImportanceAndSignificance\n\n\nMetric\nL Jung Box\n**Purpose**: The Ljung-Box test is a type of statistical test utilized to ascertain whether there are autocorrelations within a given dataset that differ significantly from zero. In the context of a machine learning model, this test is primarily used to evaluate data utilized in regression tasks, especially those involving time series and forecasting. **Test Mechanism**: The test operates by iterating over each feature within the training dataset and applying the `acorr_ljungbox` function from the `statsmodels.stats.diagnostic` library. This function calculates the Ljung-Box statistic and p-value for each feature. These results are then stored in a dictionary where the keys are the feature names and the values are dictionaries containing the statistic and p-value respectively. Generally, a lower p-value indicates a higher likelihood of significant autocorrelations within the feature. **Signs of High Risk**: A high risk or failure in the model's performance relating to this test might be indicated by high Ljung-Box statistic values or low p-values. These outcomes suggest the presence of significant autocorrelations in the respective features, which, if not properly considered or handled in the machine learning model, can negatively affect model performance or bias. **Strengths**: The Ljung-Box test is a powerful tool for detecting autocorrelations within datasets, especially in time series data. It provides quantitative measures (statistic and p-value) that allow for precise evaluation of autocorrelation and can be instrumental in avoiding issues related to autoregressive residuals and other challenges in regression models. **Limitations**: Despite its utility, the Ljung-Box test is not without its limitations. For instance, it cannot detect all types of non-linearity or complex interrelationships among variables. Additionally, testing individual features may not fully encapsulate the dynamics of the data if features interact with each other. Lastly, it is designed more for traditional statistical models and may not be fully compatible with certain types of complex machine learning models.\nvalidmind.model_validation.statsmodels.LJungBox\n\n\nMetric\nLogistic Reg Prediction Histogram\n**Purpose**: The Logistic Regression Prediction Histogram is designed to generate two overlaid histograms for each of the training and test datasets, to evaluate and visualize the Probability of Default (PD) for positive and negative classes. This serves to analyze the performance of a logistic regression model, especially one related to credit risk prediction. **Test Mechanism**: The operation of this test involves the extraction of the target column from the train and test datasets, followed by the calculation of probabilities using the model's predict function. These probabilities are added as a new column to the training and testing dataframes. The histograms are generated by extracting subsets from these dataframes according to the target column and actual classes they represent (0 or 1 for binary classification scenarios), with different opacities set for better visualization. The four histograms (two for training data, two for test data) are overlaid on two subplot frames (one for training, one for testing), and the result is returned as a plotly graph object. **Signs of High Risk**: The possibility of high risk or failure in the model's performance arises if the histograms show significant discrepancies between the training and testing data, or between the positive and negative classes. These discrepancies could indicate overfitting or potential bias in the model. Additionally, if the probabilities are distributed unevenly, it may suggest that the model is not accurately predicting outcomes. **Strengths**: This test offers numerous strengths. First, it provides a visual representation of the PD generated by a machine learning model, which helps analyze and understand the model's behavior. Second, it can assess both the training and testing datasets, adding layers of validation to the model. Third, it highlights the disparity between multiple classes, offering potential insights into class imbalance or data skewness. Lastly, in the context of credit risk prediction, it is especially beneficial for visualizing the spread of risk across different classes. **Limitations**: Despite its strengths, the Logistic Regression Prediction Histogram has a few limitations. First, it is specifically tailored for binary classification scenarios, where the target variable only has two classes - making it inappropriate for multi-class classification tasks. Second, it is applicable mostly for logistic regression models and might not be as effective or accurate for testing other model types. Finally, while it gives a good visual representation of the model's PD predictions, it does not provide a quantifiable measure or score for model performance.\nvalidmind.model_validation.statsmodels.LogisticRegPredictionHistogram\n\n\nMetric\nJarque Bera\n**Purpose**: The purpose of the Jarque-Bera test as implemented in this metric is to determine if the features in the dataset of a given Machine Learning model follows a normal distribution. This is essential in understanding the distribution and behavior of the model's features, as numerous statistical methods assume normal distribution of the data. **Test Mechanism**: The test mechanism involves computing the Jarque-Bera statistic and the associated p-value, skew and kurtosis for each feature in the dataset. Necessary variables like the Jarque-Bera statistic, p-value, skew and kurtosis are computed using the 'jarque_bera' function from the 'statsmodels' library in python and stored in a dictionary. The jarque_bera function evaluates the skewness and kurtosis of the input dataset to determine if it follows a normal distribution. A significant p-value (usually less than 0.05) suggests that the feature does not possess normal distribution. **Signs of High Risk**: High risk associated with this test would be a significantly high Jarque-Bera statistic and a low p-value (typically &lt; 0.05). These indicate that the data deviates significantly from a normal distribution, thereby implying that a machine learning model may not function as intended as many models expect feature data to be normally distributed. **Strengths**: The strength of this test lies in its ability to provide insights into the shape of the data distribution. It helps determine whether a given set of data follows a normal distribution or not, enabling risk assessment, especially for models that assume normal distribution of data. By measuring skewness and kurtosis, it also provides extra insights on the nature and magnitude of distribution deviation. **Limitations**: The Jarque-Bera test only checks for normality of the data distribution and will not provide insights about other types of distributions. Datasets which are not normally distributed but may follow some other distribution, may lead to incorrect risk assessments. Additionally, this test is very sensitive to large sample sizes, often rejecting the null hypothesis (that data is normally distributed) for even slight deviations in larger datasets.\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nPhillips Perron Arch\n**Purpose**: The Phillips-Perron (PP) test is being used to establish the order of integration in time series data. It is a method for testing a null hypothesis that a time series is unit-root non-stationary. When applied to machine learning models, it helps in forecasting and understanding the stochastic behavior of data. In sense, it is used to ensure the robustness of the results and make valid predictions out of regression analysis models. **Test Mechanism**: The PP test is applied to each feature present in the dataset. A data frame is acquired from the dataset and for each column in this data frame, the PhillipsPerron method is used to calculate its statistic value, p-value, used lags and number of observations. This method is calculating the PP metric for each feature and caching the results. **Signs of High Risk**: Indicators of high risk related to this metric could include: - High P-value, which might suggest that the series has a unit root and thus is non-stationary. - Test statistic values that exceed the critical values, providing further evidence of non-stationarity. - If the 'usedlag' value is high for a series, there may be autocorrelation issues which can further complicate model performance. **Strengths**: The strengths of the PP test are as follows: - It is robust against heteroskedasticity in the error term. - It examines relatively long time series. - It helps to identify if the time series is stationary or not, which affects the selection of appropriate models for forecasting. **Limitations**: However, the PP test has some limitations: - It can only be utilized in a univariate time series framework. - The PP test does rely on asymptotic theory, therefore, for small sample sizes the power of the test can substantially reduce. - Non-stationary time series might require differencing to convert them into stationary series, which might lead to loss of important data points.\nvalidmind.model_validation.statsmodels.PhillipsPerronArch\n\n\nMetric\nKolmogorov Smirnov\n**Purpose**: The Kolmogorov-Smirnov (KS) test featured in this metric is aimed at evaluating the distribution of a dataset's features. Particularly, it assesses whether each feature's data follows a normal distribution, which is a crucial assumption in many statistical methods and machine learning models. **Test Mechanism**: The KS test implemented here computes the KS statistic and the corresponding p-value for each column in a dataset. These values are determined by comparing the cumulative distribution function of the dataset's feature with a theoretical normal distribution. A feature-wise KS statistic and p-value are then stored in a dictionary. The threshold p-value, below which the null hypothesis (that the data comes from a normal distribution) can be rejected, is not explicitly set in this implementation and can be defined in accordance with the specific application. **Signs of High Risk**: A higher KS statistic for a feature and a low p-value would suggest a significant discrepancy between that feature's distribution and a normal distribution. Features with substantial deviations might cause problems if the model in use assumes a normal distribution of data, hence pose a risk. **Strengths**: The KS test is sensitive to differences in both the location and shape of the empirical cumulative distribution functions of two samples. It's non-parametric and doesn't assume any specific distribution for the data, making it versatile for various datasets. Being a feature-oriented test, it provides granular insights about the data distribution. **Limitations**: The KS test may be overly sensitive to deviations in the tails of the data distribution, potentially flagging them as non-normal even in cases where these deviations are not necessarily a concern for the model. Moreover, the test could become less powerful with multivariate distributions since it's primarily used for univariate distributions. Also, as it's a goodness-of-fit test, it does not point out specific forms of non-normality such as skewness or kurtosis that could be relevant for model fitting.\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nResiduals Visual Inspection\n**Purpose**: The main purpose of this metric is to visualize and analyze the residuals (the differences between the observed and predicted values) of a regression problem. It allows for a graphical exploration of the model's errors, helping to identify statistical patterns or anomalies that may indicate a systematic bias in the model's predictions. By inspecting the residuals, we can check how well the model fits the data and meets the assumptions of the model. **Test Mechanism**: The metric generates four common types of residual plots which are: a histogram with kernel density estimation, a quantile-quantile (Q-Q) plot, a residuals series dot plot, and an autocorrelation function (ACF) plot. - The residuals histogram with kernel density estimation visualizes the distribution of residuals and allows to check if they are normally distributed. - Q-Q plot compares the observed quantiles of the data to the quantiles of a standard normal distribution, helping to assess the normality of residuals. - A residuals dot plot indicates the variation in residuals over time, which helps in identifying any time-related pattern in residuals. - ACF plot visualizes the correlation of an observation with its previous observations, helping to pinpoint any seasonality effect within residuals. **Signs of High Risk**: - Skewness or asymmetry in the histogram or a significant deviation from the straight line in the Q-Q plot, which indicates that the residuals aren't normally distributed. - Large spikes in the ACF plot, indicating that the residuals are correlated, in violation of the assumption that they are independent. - Non-random patterns in the dot plot of residuals, indicating potential model misspecification. **Strengths**: - Visual analysis of residuals is a powerful yet simple way to understand a model’s behavior across the data set and to identify problems with the model's assumptions or its fit to the data. - The test is applicable to any regression model, irrespective of complexity. - By exploring residuals, we might uncover relationships that were not captured by the model, revealing opportunities for model improvement. **Limitations**: - Visual tests are largely subjective and can be open to interpretation. Clear-cut decisions about the model based solely on these plots may not be possible. - The metrics from the test do not directly infer the action based on the results; domain-specific knowledge and expert judgement is often required to interpret the results. - These plots can indicate a problem with the model but they do not necessarily reveal the nature or cause of the problem. - The test assumes that the error terms are identically distributed, which might not always be the case in real-world scenarios.\nvalidmind.model_validation.statsmodels.ResidualsVisualInspection\n\n\nMetric\nShapiro Wilk\n**Purpose**: The Shapiro-Wilk metric is used to assess if a given set of data adheres to the standard normal distribution. This is integral in machine learning modeling as the normality of data can significantly affect model performance. In particular, it is used on different features of the dataset in both classification and regression tasks. **Test Mechanism**: By implementing the Shapiro-Wilk test for each feature column in the training dataset, we determine if the data in these columns conform to the normal distribution. The test returns a statistic and a p-value, where the p-value is used to decipher whether to accept or reject the null hypothesis (the data being tested is normally distributed). **Signs of High Risk**: A lower p-value, typically below 0.05, indicates a high risk as it rejects the null hypothesis, meaning that the data is not normally distributed. For ML models that assume normality of data, this could lead to poor performance or inaccurate predictions. **Strengths**: The Shapiro-Wilk test is known for its precision, making it especially useful on small to moderate-sized datasets. It also works on both classification and regression tasks, providing more versatility in its usage. Since it tests each feature column separately, it can give an alert if any particular feature does not conform to normality. **Limitations**: Since Shapiro-Wilk test is quite sensitive, it often rejects the null hypothesis (i.e., data is normally distributed) even for slight deviations, particularly for large datasets. This means the test can consider data as non-normal even if it's approximately normally distributed, potentially leading to 'false alarms' of high risk. Additionally, handling of missing data or outliers needs to be carefully managed before testing as these can highly influence the results. Lastly, it is not ideally suited for data with strong skewness or kurtosis.\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nScorecard Bucket Histogram\n**Purpose**: The 'Scorecard Bucket Histogram' is employed as a metric to evaluate the performance of a classification model, specifically in credit risk assessment. It categorizes model scores into different rating classes, and visualizes the distribution of scores or probabilities within each class. It essentially measures how different risk categories (classes) are distributed in the model scores and provides an insight into the model's classification ability, making it particularly useful in credit scoring and risk modelling where understanding probability of default is critical. **Test Mechanism**: The test works by computing the probabilities for each record in the test and train dataset using the model's predict function. Subsequently, it calculates the scores using a predefined formula incorporating target score, target odds, and points to double odds (PDO). The scores are then bucketed into pre-defined rating classes (such as 'A', 'B', 'C', 'D') and plotted in a histogram for both the train and test datasets. The target score, target odds, points to double the odds (PDO), and rating classes are all customizable parameters, thus allowing for flexibility in test metrics based on differing model or industry norms. **Signs of High Risk**: Indicators of high risk or failure in relation to this metric might include disproportionate scores within rating classes, excessive overlap between classes, or an inconsistent distribution of scores between the training and testing datasets. If the model is accurately classifying and risk is being evenly distributed, we'd anticipate smooth and relatively balanced histograms within classes. **Strengths**: The strengths of this metric lie in its form of output, it provides a quick visual snap of score distribution; it breaks down complex predictions into simple, understandable classes making it easily interpretable for both technical and business audiences. Also, this metric caters to customization of parameters, but most importantly it gives ownership of the class definitions to the user. It is quite useful in the field of credit risk, giving a clear understanding of which class or 'Bucket' a potential borrower belongs to. **Limitations**: One limitation of this metric is its reliance on manual setting of classes and other parameters (like target score, target odds, and PDO), which may lead to arbitrary classifications if not judiciously performed and could potentially introduce biases. Additionally, its effectiveness can be limited if used on non-tabular data. Furthermore, it does not provide a numerical value which can be easily compared across different models or runs, since the output is primarily visual. Consequently, it may not present the complete view of model performance, and should be used in conjunction with other metrics.\nvalidmind.model_validation.statsmodels.ScorecardBucketHistogram\n\n\nMetric\nRegression Model Insample Comparison\n**Purpose**: The purpose of this test metric, RegressionModelInsampleComparison, is to assess the performance of regression models on the same dataset that they were trained on. The performance of the models is compared against each other to identify which fits the data best. Evaluation metrics include goodness-of-fit statistics such as R-Squared, Adjusted R-Squared, Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). **Test Mechanism**: The test's implementation involves the following steps; - First, a check is done to ensure that the list of models is not empty. - After confirmation, the In-Sample performance of the models is computed by using a private function `_in_sample_performance_ols`. This function: - Loops through each model in the list - For each model, it extracts the features (`X`) and target (`y_true`) from the training dataset and then predicts target values (`y_pred`) - The model's performance metrics are then computed using formulas for R-Squared, Adjusted R-Squared, MSE and RMSE. - The computed metrics, variables of the model and the model's identifier are saved in a dictionary and appended to a list. - The collected results are then saved and returned in form of a pandas dataframe. **Signs of High Risk**: A high risk or failure of the model's performance can be indicated by waiting significantly low values for R-Squared or Adjusted R-Squared and significantly high values for MSE and RMSE. The exact thresholds may vary based on the specific context or domain in which the model is being applied. **Strengths**: - Enables direct comparison of different models' in-sample performance on the same data set, providing a clear picture of which model is better suited to the data. - It computes multiple evaluation methods (R-Squared, Adjusted R-Squared, MSE, RMSE), which provides a comprehensive overview of the model's performance. **Limitations**: - This test only uses in-sample performance, i.e., how well a model fits the data it was trained on. It might not indicate how the model performs on unseen or out-of-sample data, which is a core aspect of modeling tasks. - It might be sensitive to overfitting as better in-sample performance might be a result of the model merely memorizing the training data. - It does not take into account other crucial factors like data's pattern of changes over time, also known as temporal dynamics. - The test doesn't offer a mechanism to automatically determine whether reported metrics are acceptable - human/judgement-based interpretation is needed.\nvalidmind.model_validation.statsmodels.RegressionModelInsampleComparison\n\n\nMetric\nRegression Feature Significance\n**Purpose**: The Regression Feature Significance metric assesses the significance of each feature in a given set of regression models. It creates a visualization displaying p-values for every feature of each model, assisting model developers in understanding which features are most influential in their models. **Test Mechanism**: The test mechanism involves going through each fitted regression model in a given list, extracting the model coefficients and p-values for each feature, and then plotting these values. The x-axis on the plot contains the p-values while the y-axis denotes the coefficients of each feature. A vertical red line is drawn at the threshold for p-value significance, which is 0.05 by default. Any features with p-values to the left of this line are considered statistically significant at the chosen level. **Signs of High Risk**: Any feature with a high p-value (greater than the threshold) is considered a potential high risk, as it suggests the feature is not statistically significant and may not be reliably contributing to the model's predictions. A high number of such features may indicate problems with the model validation, variable selection, and overall reliability of the model predictions. **Strengths**: 1. Helps identify the features that significantly contribute to a model's prediction, providing insights into the feature importance. 2. Provides tangible, easy-to-understand visualizations to interpret the feature significance. 3. Facilitates comparison of feature importance across multiple models. **Limitations**: 1. This metric assumes model features are independent, which may not always be the case. Multicollinearity (high correlation amongst predictors) can cause high variance and unreliable statistical tests of significance. 2. The p-value strategy for feature selection doesn't take into account the magnitude of the effect, focusing solely on whether the feature is likely non-zero. 3. This test is specific to regression models and wouldn't be suitable for other types of ML models. 4. P-value thresholds are somewhat arbitrary and do not always indicate practical significance, only statistical significance.\nvalidmind.model_validation.statsmodels.RegressionFeatureSignificance\n\n\nMetric\nRegression Model Summary\n**Purpose**: This metric test evaluates the performance of regression models, specifically capturing their ability to predict the dependent variable(s) given changes in the independent variable(s). It measures the quality of the model using classic regression metrics such as R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). **Test Mechanism**: This test utilizes the 'train_ds' attribute of the model to collect and assess the training data. It first retrieves the independent variables and uses the model to predict on the given features. The test calculates several standard regression performance metrics including R-Square, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics quantify how close the predicted responses are to the true responses. **Signs of High Risk**: High risk or potentially problematic model performance could be indicated by low R-Squared and Adjusted R-Squared values, or high MSE and RMSE values. Low R-squared and adjusted R-squared values suggest a poor fit between the model predictions and the true responses, indicating the model explains a small portion of the variance in the target variable. High MSE or RMSE represents a high prediction error, which points to poor model performance. **Strengths**: This test offers an extensive evaluation of regression models as it combines four key measures of model accuracy and fit, offering a comprehensive view of the model's performance. Furthermore, both the R-Squared and the Adjusted R-Squared measures are readily interpretable, representing the proportion of total variation in the dependent variable that is explained by the independent variables. **Limitations**: This test only applies to regression models and cannot be used to evaluate binary classification models or time series models, limiting its scope. Furthermore, while RMSE and MSE are good measures of prediction error, they can be sensitive to outliers, which might lead to an overestimation of the model's prediction error. Lastly, high R-squared or adjusted R-squared doesn't necessarily imply a good model, especially in cases where the model is overfitting the data.\nvalidmind.model_validation.statsmodels.RegressionModelSummary\n\n\nMetric\nKPSS\n**Purpose**: The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test is utilized to check the stationarity of data within the machine learning model. It is used on time-series data specifically, to establish the order of integration, which in turn helps in providing accurate forecasting. The baseline condition for any time series model is that the series should be stationary. **Test Mechanism**: Given a dataset, this metric calculates the KPSS score for each feature in the dataset. Each KPSS score consists of a statistic, a p-value, a used lag, and critical values. The KPSS score calculates the hypothesis that an observable time series is stationary around a deterministic trend. If the calculated statistic is greater than the critical value, the null hypothesis is rejected, indicating that the series is not stationary. **Signs of High Risk**: High risk in relation to this metric would be signified by a high KPSS score, specifically if the calculated statistic is greater than the critical value. This implies that the null hypothesis is rejected and the series is not stationary, making the model ineffective for forecasting. **Strengths**: The KPSS test has several advantages. It assesses directly the stationarity of a series, which is the basic condition of many time-series models and is therefore a crucial step in model validation. Furthermore, the underlying intuition of the test is intuitive and straightforward, making it easily understandable for developers and risk management teams. **Limitations**: Despite its strengths, KPSS does have some limitations. The test assumes the absence of a unit root in the series and does not differentiate between series that are stationary and series that are near-stationary. The test may also have limited power against certain alternatives, and the validity of the test depends on the number of lags chosen.\nvalidmind.model_validation.statsmodels.KPSS\n\n\nMetric\nLilliefors\n**Purpose**: The Lilliefors test, named after the Swedish statistician Hubert Lilliefors, is used in this context to determine whether the features of the machine learning model's training dataset follow a normal distribution. Normality of distribution is a critical assumption in many statistical procedures and machine learning models. If the features do not follow a normal distribution, certain types of models may not function optimally, potentially leading to inaccurate predictions. **Test Mechanism**: The test is applied to every feature column in the training dataset. For each feature, the Lilliefors test computes a test statistic and a p-value. The test statistic quantifies how far the distribution of the feature is from a perfect normal distribution, while the p-value provides a measure of the statistical significance of this deviation. The results are stored in a dictionary where keys are feature column names, and values are another dictionary containing the test statistic and p-value. **Signs of High Risk**: If the p-value for a feature is less than a predetermined significance level (typically 0.05), this would indicate that the distribution of the feature significantly deviates from a normal distribution. This suggests a high risk as models assuming normality may not work accurately or efficiently with such a feature. **Strengths**: This Lilliefors test is particularly useful because it does not require the mean and variance of the normal distribution to be known in advance, making it a robust choice for real-world data where these values are often unknown. The test is capable of screening every feature column, providing a comprehensive view of the dataset. **Limitations**: While the Lilliefors test is a practical tool to gauge normality, it has a few limitations. It can only test one-dimensional data, so it is not suited for datasets with interactions between features or multi-dimensional phenomena. Additionally, the test may have lower power compared to other tests (like Anderson-Darling test) which means it's slightly less sensitive at detecting departures from normality. Finally, like any statistical test, it can yield false positives or negatives, so relying on it alone without considering other characteristics of the data could be risky.\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nLogistic Reg Cumulative Prob\n**Purpose**: This metric is used to evaluate the distribution of predicted probabilities for positive and negative classes in a logistic regression model. It's purpose is not only to measure the performance of the model but also to visually assess the behavior of the model by plotting the cumulative probabilities for positive and negative classes across the training and test datasets. **Test Mechanism**: To test the logistic regression model, the metric computes the predicted probabilities for each instance in training and test datasets and adds it as a new column in these sets. The cumulative probabilities for positive and negative classes are then calculated and sorted in ascending order. Subsequently, cumulative distributions of these probabilities are created for both positive and negative classes across both training and test datasets. The cumulative probabilities are presented visually in a plot. The plot has two subplots - one for the training data and the other for the test data, with lines representing cumulative distributions of positive and negative classes. **Signs of High Risk**: Indicators of high risk include: 1. Imbalanced distribution of probabilities for either positive or negative classes. 2. Inconsistencies or significant differences between the cumulative probability distributions for the training data versus the test data. 3. Inconsistencies or significant differences between the cumulative probability distributions for positive and negative classes. **Strengths**: Some major merits of using this metric are: 1. It not only returns numerical probabilities but also provides a visual representation of data which makes it easier to understand and interpret the model's behavior. 2. It allows for comparison of model's behavior across training and testing datasets, which can provide insights into how well the model is generalized. 3. It also distinguishes between positive and negative classes and their respective distribution patterns which can aid in problem diagnosis. **Limitations**: Despite its strengths, this metric also has few limitations: 1. It only applies to classification tasks and specifically to logistic regression models. 2. The graphical results require human interpretation and may not be directly applicable for automated risk detection. 3. The method does not provide a single quantifiable measure of model risk, but rather a visual representation and broad distributional information. 4. If the training and test datasets are not representative of the overall data distribution, the metric could provide misleading results.\nvalidmind.model_validation.statsmodels.LogisticRegCumulativeProb\n\n\nMetric\nRuns Test\n**Purpose**: The Runs Test is a statistical procedure used to determine whether the sequence of data extracted from the ML model behaves randomly or not. Specifically, it analyzes runs, sequences of consecutive positives or negatives, in the data to check if there are more or fewer runs than expected under the assumption of randomness. This can be an indication of some pattern, trend, or cycle in the model's output which may need attention. **Test Mechanism**: The testing mechanism applies the Runs Test from the statsmodels module on each column of the training dataset. For every feature in the dataset, a Runs Test is executed, whose output includes a Runs Statistic and P-value. A low P-value suggests that data arrangement in the feature is not likely to be random. The results are stored in a dictionary where the keys are the feature names, and the values are another dictionary storing the test statistic and the P-value for each feature. **Signs of High Risk**: High risk is indicated when the P-value is close to zero. If the p-value is less than a predefined significance level (like 0.05), it suggests that the runs (series of positive or negative values) in the model's output are not random and are longer or shorter than what is expected under a random scenario. This would mean there's a high risk of non-random distribution of errors or model outcomes, suggesting potential issues with the model. **Strengths**: The strength of the Runs Test is that it's straightforward and fast for detecting non-random patterns in data sequence. It can validate assumptions of randomness, which is particularly valuable for checking error distributions in regression models, trendless time series data, and making sure a classifier doesn't favour one class over another. Moreover, it can be applied to both classification and regression tasks, making it versatile. **Limitations**: The test assumes that the data is independently and identically distributed (i.i.d.), which might not be the case for many real-world datasets. The conclusion drawn from the low p-value indicating non-randomness does not provide information about the type or the source of the detected pattern. Also, it is sensitive to extreme values (outliers), and overly large or small run sequences can influence the results. Furthermore, this test does not provide model performance evaluation; it is used to detect patterns in the sequence of outputs only.\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nScorecard Probabilities Histogram\n**Purpose**: This metric, referred to as the Scorecard Probabilities Histogram, is used to visualise and evaluate the risk classification of a model especially designed for the credit risk domain. It does this by examining the distribution of the probability of default across different score buckets. The score buckets are categories in which entities (like loan applicants) are placed based on their predicted default risk. Specifically, this metric is used to ensure the model correctly classifies entities into the appropriate risk categories (score buckets) and also properly represents their default probabilities. **Test Mechanism**: The Scorecard Probabilities Histogram first calculates default probabilities using the 'compute_probabilities' method where the output probability is added as a new column to the input dataset. Next, scores are computed using these probabilities, a target score, target odds and Points to Double the odds (pdo) factor in the 'compute_scores' method. These scores are further processed into buckets using the 'compute_buckets' method. Once data is bucketed, a histogram is plotted for each score bucket. This histogram takes the probabilities of default serious as the x-axis and their frequency as the y-axis using the 'plot_probabilities_histogram' method. The process is run separately for both training and testing data. **Signs of High Risk**: If there is a significant overlap of different score buckets in the histogram, this can indicate that the model is not effectively distinguishing between different risk categories. Additionally, if extremely high or low probabilities are common across all buckets, there may be a skew in the model's predictions. **Strengths**: The Scorecard Probabilities Histogram is an advantageous tool for observing and analysing the predicted default risk distribution across different risk categories. This metric allows us to visually inspect the model's performance and its calibration for different risk categories. Moreover, it also provides a mechanism to visualize how these classifications are distributed on the training and testing datasets separately which is beneficial for understanding model generalization. **Limitations**: The Scorecard Probabilities Histogram does have limitations. For one, it assumes that the risk categories are linear and equally spaced, which may not always be the case. Secondly, the visualization may not provide adequate information if there are too few or too many score buckets. Lastly, while this metric effectively visualizes the distribution of probabilities, it provides no concrete numerical metric or threshold to definitively gauge the model's performance. For a more precise evaluation, it should be used in combination with other metrics and tools such as confusion matrix, AUC-ROC, Precision, Recall etc.\nvalidmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram\n\n\nMetric\nDFGLS Arch\n**Purpose**: The purpose of the Dickey-Fuller GLS (DFGLS) Arch metric is to establish the order of integration of time series data. In machine learning models that deal with time series and forecasting, this metric checks for the presence of a unit root i.e., it tests whether a time series is non-stationary, which is an integral first step in such analyses. **Test Mechanism**: In this code, the Dickey-Fuller GLS unit root test is employed on each feature of the dataset. In other words, it iterates over every individual column of the dataset and applies the DFGLS test to evaluate and report on the presence of a unit root. This information is then stored as output values such as 'stat' for the test statistic, 'pvalue' for the p-value, 'usedlag' for the number of lagged differences used in the regression and 'nobs' for the number of observations. **Signs of High Risk**: High risk in this context would be determined by a high p-value. If the p-value for the DFGLS test is high (e.g., a common threshold is above 0.05), this indicates that the time series data is likely non-stationary, and thus poses a risk for generating unreliable forecasts or analyses. **Strengths**: The Dickey-Fuller GLS is a powerful technique to check the stationarity of times series data. It helps in confirming that the assumptions of the models are met before the actual building of the machine learning models can take place. The results given by this metric provides a clear perspective on whether the data is suitable for certain machine learning models, mainly those that require time-series to be stationary. **Limitations**: Despite its advantages, the DFGLS test also has some limitations. It can lead to incorrect conclusions if the time series has a structural break, or if the time series follows a trend but is otherwise stationary (in which case detrending might be necessary). Also, it doesn't work well with shorter time series data or with volatile data.\nvalidmind.model_validation.statsmodels.DFGLSArch\n\n\nMetric\nAuto ARIMA\n**Purpose**: This validation test is designed to evaluate and rank ARIMA models, which are popular forecasting models used, amongst other things, for time series prediction tasks. The script automatically fits multiple ARIMA models (with varying orders of differencing, autoregression, and moving average parameters) for each variable in the provided dataset. It then ranks them based on their Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) values. This allows efficient selection of the most suitable model for forecasting, based on these standard statistical measures. **Test Mechanism**: The metric works by generating a range of possible combinations of ARIMA model parameters within a predefined limit (max_p, max_d, max_q representing autoregressive, differencing, and moving average components respectively). It fits all these models on the provided time-series data. For each ARIMA model, it calculates and records the BIC and AIC value, which serve as performance measures for the model fit. Furthermore, prior to the parameter fitting, it runs an Augmented Dickey-Fuller test for stationarity on the series. If the series isn't stationary, it sends out a warning, as ARIMA models require the input series to be stationary. **Signs of High Risk**: If the p-value of the Augmented Dickey-Fuller test for any variable is greater than 0.05, a warning is logged indicating that the series might not be stationary, and this can lead to inaccurate results. Secondly, if there is consistent failure in fitting ARIMA models (evident through logged errors), it might indicate issues with the data or model stability. **Strengths**: This mechanism streamlines the potentially complex task of selecting the most suitable ARIMA model based on BIC and AIC criteria. It checks for and warns about non-stationarity in the data, a crucial assumption for ARIMA models. The exhaustive approach in checking all possible combinations of model parameters maximizes the potential for finding the best-fit model. **Limitations**: The approach can be computationally expensive given that it creates and fits multiple ARIMA models per variable. Secondly, a high p-value in the Augmented Dickey-Fuller test indicates a lack of stationarity, but it doesn't make any transformations on the data to rectify this. Additionally, model selection is purely based on BIC and AIC criteria, which might not yield the best predictive model in all scenarios. Also, it's limited to regression tasks on time series data and won't work effectively for other types of Machine Learning tasks.\nvalidmind.model_validation.statsmodels.AutoARIMA\n\n\nThresholdTest\nADF Test\n**Purpose**: The purpose of the Augmented Dickey-Fuller (ADF) metric test is to test the null hypothesis that a unit root is present in a time series. In other words, the ADF test aims to check the stationarity of a given time-series dataset. It plays a crucial role in the phases of time series analysis, regression tasks, and forecasting, as these techniques often require the data to be stationary. **Test Mechanism**: The ADF test applies the Python “statsmodels” library's function \"adfuller\" to each column of the training dataset. By doing so, it calculates the ADF statistic, p-value, the number of lags used, and the number of observations in the sample for each column. If the p-value for a column is smaller than the predefined threshold (typically 0.05), then it is considered as having passed the test, thus concluding that the series is stationary. **Signs of High Risk**: If the p-value exceeds the threshold value, the test for that column is considered as failed, which indicates a high risk or potential model performance issue. A high p-value implies that the null hypothesis (indicating a unit root) cannot be rejected, meaning that the series is non-stationary and may cause unreliable and spurious results in the model's performance and forecast. **Strengths**: The ADF test holds several key advantages: 1. Test for stationarity: The ADF test provides a rigorous mechanism for testing the stationarity of time series data, which is critical for many machine learning and statistical models. 2. Comprehensive output: The function provides rich output, including the number of lags used and the number of observations, which can be useful for understanding the behavior of the series. **Limitations**: However, the ADF test also presents some limitations: 1. Dependence on threshold: The result of the test is highly dependent on the threshold chosen. An inappropriate threshold may lead to incorrect rejection or acceptance of the null hypothesis. 2. Inefficient for trending data: The ADF test assumes no deterministic trend in the data. If a deterministic trend exists, the test may identify a stationary series as non-stationary. 3. May lead to false positive: Particularly for larger datasets, the ADF test tends to reject the null hypothesis (indicating a unit root), which can lead to a high propensity for false positives.\nvalidmind.model_validation.statsmodels.ADFTest\n\n\nMetric\nGINI Table\n**Purpose**: This metric 'GINITable' is used to calculate and display the AUC (Area under the ROC Curve), GINI coefficient, and KS (Kolmogorov-Smirnov) statistic for both training and testing datasets. These metrics are used to evaluate the performance of a classification model and quantify its discriminatory power, i.e., its ability to distinguish between different classes. **Test Mechanism**: A dict is created to store performance metrics for both training and testing datasets. The algorithm loops over these datasets and computes the metrics for each. The Area under the ROC (Receiver Operating Characteristic) Curve (AUC) is calculated by invoking the compute_auc method, which uses the roc_auc_score function from Scikit-Learn. The GINI coefficient, a measure of statistical dispersion, is computed by doubling the AUC and subtracting 1. The Kolmogorov-Smirnov (KS) statistic is computed using the roc_curve function of Scikit-Learn, subtracting False Positive Rate (FPR) from True Positive Rate (TPR), and then finding the maximum value from the resulting dataset. The calculated metrics are stored in a pandas DataFrame for neat display. **Signs of High Risk**: High risk is indicated when performance metrics values are low or there is a substantial discrepancy between the performance in training and testing datasets. A low AUC indicates poor classification performance, while a low GINI coefficient suggests low discriminatory power. High KS value, on the other hand, may indicate potential overfitting as it signifies a substantial gap between positive and negative distributions. **Strengths**: 1. This test provides multiple metrics (AUC, GINI, and KS) giving a broader perspective on the model's performance. 2. It compares the performance of the model on both training and testing datasets, offering insight into potential overfitting or underfitting. 3. The metrics used are invariant to class distribution and can effectively measure model performance even for imbalanced datasets. 4. Displays the metrics in a user-friendly, neatly formatted table. **Limitations**: 1. The GINI coefficient and KS statistic are reliant on the AUC value, meaning any error in AUC calculation will propagate and affect these metrics too. 2. It is mainly suitable for binary classification models and may require adjustments for multi-class scenarios. 3. The metrics used are threshold-dependent and may vary significantly based on chosen cut-off points. 4. The test does not contain any method to handle missing or inefficient data which might lead to inaccurate metrics if data is not preprocessed efficiently.\nvalidmind.model_validation.statsmodels.GINITable\n\n\nMetric\nRegression Model Forecast Plot\n**Purpose**: The \"regression_forecast_plot\" is intended to visually depict the performance of one or more regression models by comparing the model's forecasted outcomes against actual observed values within a specified date range. This metric is especially useful in time-series models or any model where the outcome changes over time, allowing direct comparison of predicted vs actual values. **Test Mechanism**: This test generates a plot for each fitted model in the list. The x-axis represents the date ranging from the specified \"start_date\" to the \"end_date\", while the y-axis shows the value of the outcome variable. Two lines are plotted: one representing the forecasted values and the other representing the observed values. The \"start_date\" and \"end_date\" can be parameters of this test; if these parameters are not provided, they are set to the minimum and maximum date available in the dataset. The test verifies that the provided date range is within the limits of the available data. **Signs of High Risk**: High risk or failure signs could be deduced visually from the plots if the forecasted line significantly deviates from the observed line, indicating the model's predicted values are not matching actual outcomes. Furthermore, a model that struggles to handle the edge conditions like maximum and minimum data points could also be considered a sign of risk. **Strengths**: 1. Visualization: The plot provides an intuitive and clear illustration of how well the forecast matches the actual values, making it straightforward even for non-technical stakeholders to interpret. 2. Flexibility: It allows comparison for multiple models and for specified time periods. 3. Model Evaluation: It can be useful in identifying overfitting or underfitting situations, as these will manifest as discrepancies between the forecasted and observed values. **Limitations**: 1. Interpretation Bias: Interpretation of the plot is subjective and can lead to different conclusions by different evaluators. 2. Lack of Precision: Visual representation might not provide precise values of the deviation. 3. Inapplicability: Limited to cases where the order of data points (time-series) matters, it might not be of much use in problems that are not related to time series prediction.\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlot\n\n\nMetric\nADF\n**Purpose**: The Augmented Dickey-Fuller (ADF) test is primarily utilized in this scenario to ascertain the order of integration of a given time series. In other words, it’s a metric to assess the stationarity of the time series data employed in the machine learning model. The stationary property of time series data is critical in many models as it directly impacts the reliability and robustness of predictions and forecasts. **Test Mechanism**: This test initiates by executing the ADF function from the statsmodels library for every feature in the dataset. Each run results in numerous outputs, including the ADF test statistic, the corresponding p-value, number of lags used, number of observations included in the test, critical values at different confidence levels, and the maximized information criterion. The results are then cached for each attribute for further analysis. **Signs of High Risk**: Observing a large ADF statistic and high p-value (typically above 0.05) would yet suggest a high risk for the model's performance. As both metrics indicate the presence of a unit root, they thus signify non-stationarity in the data series. Such characteristics can often lead to unreliable or inadequate forecasts. **Strengths**: The ADF test has the advantage of being robust to higher-order correlation within the dataset. This intrinsic merit makes it feasible to use in a variety of instances where data may exhibit complex stochastic behavior. Additionally, providing detailed information such as test statistics, critical values, and information criterion enhances the interpretability and transparency of the model validation process. **Limitations**: However, the ADF test does have its share of limitations. It might exhibit low statistical power, making it difficult for to distinguish between a unit root and a level of near-unit-root. This may lead to false negatives, where the test fails to reject a null hypothesis of a unit root when, in reality, it should. Furthermore, the ADF test assumes that the underlying series follows an autoregressive process, which might not always be the case. Lastly, handling time series with structural breaks can be challenging.\nvalidmind.model_validation.statsmodels.ADF\n\n\nMetric\nDurbin Watson Test\n**Purpose**: The Durbin-Watson Test metric is utilized to detect autocorrelation (i.e., a set of data values are influenced by their predecessors) in time series data. This is especially important in regression tasks, where the independence of residuals is often assumed. If there is a significant autocorrelation, the predictions of the model may not be reliable. **Test Mechanism**: The Durbin-Watson Test metric generates a statistical value for each feature of the training dataset. This is done through the `durbin_watson` function from the `statsmodels` library in Python. The function is looped through all columns of the dataset, calculating and storing the Durbin-Watson (DB) value for each column. The results are then cached and made available for further analysis. A Durbin-Watson metric value close to 2 suggests no autocorrelation, a value toward 0 indicates positive autocorrelation, and a value toward 4 shows negative autocorrelation. **Signs of High Risk**: If the Durbin-Watson value for any of the features is significantly different from 2, it indicates a high risk as it may point to significant autocorrelation issues within the dataset. Particularly, a value closer to '0' might hint at positive autocorrelation, while a value closer to '4' might suggest negative autocorrelation. These autocorrelation issues could lead to unreliable prediction models. **Strengths**: This metric is particularly powerful in identifying autocorrelation in the residuals of prediction models. With this metric, we can diagnose whether residuals have autocorrelation, which could violate the assumptions of various modeling techniques, particularly in regression analysis and time-series data modeling. **Limitations**: The Durbin-Watson Test has several limitations. First, it only detects linear autocorrelation and may miss other types of relationships. Second, it is significantly influenced by the order of data points, and shuffling the order can lead to entirely different results. Lastly, the test only checks for autocorrelation between a variable and its immediate preceding variable, essentially it's of order one, and cannot detect higher order autocorrelation.\nvalidmind.model_validation.statsmodels.DurbinWatsonTest\n\n\nMetric\nMissing Values Risk\n**Purpose**: This metric, Missing Values Risk, is intended to evaluate and quantify the risk associated with missing values in the data set being used for machine learning model training. This metric determines two specific risk measures: 1) what percentage of total data in the dataset are missing values, and 2) what percentage of total variables (i.e., columns) in the dataset have at least some missing values. **Test Mechanism**: The metric tests the dataset by first calculating the total number of datapoints in the data set and the total count of missing values. Then, it examines each column(variable) to find out how many contain at least one missing data point. By meticulously counting missing data points across the whole dataset and then within each variable(column), the metric determines the percentage of missing values in the total dataset and the percentage of columns(variables) with missing values. **Signs of High Risk**: High percentages in either of the risk measures could indicate a high risk. If the dataset has a high percentage of missing values, then it can significantly affect the performance and reliability of the model. Similarly, if a lot of variables (columns) in the dataset have missing values, the model might be prone to bias or overfitting. **Strengths**: This metric provides invaluable insight into the dataset's readiness for model training, as missing values can severely impact both the model's performance and its predictions. Quantifying the risk posed by missing values allows for targeted methods to handle these values properly, either through deletion, imputation, or other strategies. The metric is flexible and can be applied to both classification and regression tasks, thereby maintaining its utility across a broad range of models and scenarios. **Limitations**: This metric only identifies and quantifies the risk of missing values but does not offer any solutions to mitigate this risk. More specifically, it does not infer whether the missingness is random or linked to some underlying issue in data collection or preprocessing. Nonetheless, identifying the presence and extent of missingness is the essential first step in data-quality improvement.\nvalidmind.data_validation.MissingValuesRisk\n\n\nMetric\nIQR Outliers Table\n**Purpose**: The \"Interquartile Range Outliers Table\" (IQROutliersTable) metric is used to detect and summarize outliers in numerical features of a dataset using the Interquartile Range (IQR) method. Outliers, values that deviate substantially from general patterns of the data, are crucial to be identified in data pre-processing as they can lead to serious problems in statistical analyses and may degrade the performance of machine learning models. **Test Mechanism**: This metric works by calculating the IQR (the range between the first quartile (25th percentile) and the third quartile (75th percentile)) of each numerical feature in a given dataset. An outlier is defined as a data point that falls below \"Q1 - 1.5 * IQR\" or above \"Q3 + 1.5 * IQR\". The number of outliers, as well as their minimum, 25th percentile, median, 75th percentile, and maximum values, are computed and collected for each numerical feature. If no specific features are selected, the metric will be applied to all numerical features in the dataset. Users can customize the outlier threshold. By default, it's 1.5, following the standard definition of outliers in statistical analysis. **Signs of High Risk**: High risk is indicated by a large number of outliers in multiple features, as well as outliers that are far from the mean value of variables, which suggest that they deviate significantly from the general patterns of the data. Additionally, extremely high or low outlier values might also point to data entry errors or other data quality issues. **Strengths**: 1. This metric provides a comprehensive summary of outliers for each numerical feature in the dataset, which can help users identify features with potential quality issues. 2. It's versatile and customizable, allowing users to select specific features and set a custom threshold for defining outliers. 3. The IQR method is robust to the influence of extreme outlier values as it's based on quartile calculations. **Limitations**: 1. This metric might generate false positives if the variable of interest does not follow a normal or near-normal distribution, specifically in case of skewed distributions. 2. It does not provide interpretation or recommendations for handling outliers. The results need to be analyzed by users or data scientists. 3. It only works on numerical features and cannot be used for categorical data. 4. If the data has been heavily pre-processed or manipulated, or if it naturally has a high kurtosis (heavy tails), the set threshold may not work optimally for outlier detection.\nvalidmind.data_validation.IQROutliersTable\n\n\nMetric\nBivariate Features Bar Plots\n**Purpose**: The BivariateFeaturesBarPlots metric is used to conduct a visual analysis of categorical data in the model. The primary aim is to evaluate and comprehend the relationship between different feature pairs while emphasizing the model's target variable. These bivariate plots are extremely useful in revealing trends, correlations, patterns or anomalies that might not be immediately obvious in tabular data. **Test Mechanism**: The test constructs bar plots for each feature pair defined in the parameters. It groups the dataset by each feature pair and calculates the mean of the target variable within each pair grouping. Each group is represented as a bar in the plot with the height of the bar corresponding to the calculated mean. The colors of the bars are determined by the specific category they belong to: either taken from a colormap or generated if the number of categories exceeds the colormap's capacity. **Signs of High Risk**: High risk or failure may be indicated if: 1. Any values are missing or inconsistent in the feature pairs. 2. Large differences or variations exist between the mean values of certain categories within the feature pairs. 3. The parameters for feature pairs are not defined or are defined incorrectly. **Strengths**: 1. BivariateFeaturesBarPlots provides a clear, visual understanding of the relationships between feature pairs and the target variable. 2. It allows for easy comparison of different categories within the feature pairs. 3. This metric can deal with a range of categorical data, thus increasing its broad applicability. 4. It's highly customizable, as users can define the feature pairs based on their specific requirements. **Limitations**: 1. It can only be used with categorical data, limiting its utility with numerical or text data. 2. It's dependent on manual input for feature pairs, which may lead to overlooking important feature pairs if not chosen wisely. 3. The generated bar plots can become very cluttered and hard to interpret when dealing with feature pairs with a large number of categories. 4. This metric solely provides a visual analysis and doesn't provide any numerical or statistical measures for quantifying the relationship between feature pairs.\nvalidmind.data_validation.BivariateFeaturesBarPlots\n\n\nThresholdTest\nSkewness\n**Purpose**: The Skewness test is designed to measure the degree to which a given distribution deviates from a normal distribution, indicating any asymmetry in the predictive model's data. The skewness metric can be positive (indicating a distribution with a long tail to the right) or negative (signifying a tail to the left). As consistency and correct data distribution is paramount to good model performance, understanding skewness can assist in identifying data quality issues, making it integral for data profiling in both classic classification and regression machine learning models. **Test Mechanism**: The test is performed by calculating the skewness of numerical columns in the dataset. The skewness is obtained from the DataFrame of the dataset, constrained to numerical data types. An absolute value for the skewness is compared against a maximum threshold (default value of 1). If the calculated value is less than this maximum threshold, the test is marked 'Pass'; otherwise, it's considered 'Fail'. The results are then cached, with the list of result classes for each column indicating the column name, the passed status, and its skewness value compliance. **Signs of High Risk**: Indicators of high risk or potential model performance issues associated with this test include high skewness levels that greatly exceed the max_threshold in a negative or positive direction, representing skewed distribution in data. If data appears consistently skewed, it can show that the underlying assumptions of the Machine Learning model may not apply, and can lead to poor model performance, wrong predictions, or biased inferences. **Strengths**: The skewness test allows for rapid identification of unequal data distributions, which may go unnoticed but may considerably affect the performance of a machine learning model. The test is flexible and can be adapted according to the users' needs, adjusting the maximum threshold parameter. **Limitations**: The test only runs on numeric columns, which means that data in non-numeric columns could still hold bias or problematic skewness not captured by this test. Secondly, it assumes data to follow a normal distribution, which might not always be a realistic expectation in real-world data. Finally, by relying heavily on a manually set threshold, the risk grading might be either too strict or too lenient depending on the chosen threshold, something that might require expertise and iterations to perfect.\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\n**Purpose**: The Duplicates test aims to evaluate the quality of data in an ML model by identifying duplicate entries in the dataset. It specifically targets duplication in a designated text column or among the primary keys of the dataset, which can have significant implications for model performance and integrity. Duplicate entries can potentially distort data distribution and skew model training. **Test Mechanism**: The test works by counting the total number of duplicate entries within the dataset. If a 'text_column' property is specified, the algorithm will count duplicates within this column. If any primary key is declared, the test will be run on the primary keys as well. The number of duplicates ('n_duplicates') is then compared to a predefined minimum threshold (default 'min_threshold' is set to 1) to determine if the test has passed. Results include total number of duplicates, and the percentage of duplicate rows in the overall dataset ('p_duplicates'). **Signs of High Risk**: The existence of a significant number of duplicates, especially exceeding the minimum threshold, indicates high risk. Potential issues include an overrepresentation of certain data (thus skewing results), or the indication of inefficient data collecting methods leading to data redundancy. Models that predominantly fail this test may need to have their data preprocessing methods or source data reviewed. **Strengths**: The Duplicates test is versatile and can be applied to both text data and tabular data formats. It also provides results calculated both as a count and as a percentage of the total dataset, facilitating a comprehensive understanding of the extent of duplication. This metric can effectively flag data quality issues that can skew your model performance and lead to inaccurate predictions. **Limitations**: The Duplicates test only targets exact duplication in entries, which might overlook close 'almost-duplicate' entries, or normalized forms of entries, that may also impact data distribution and model integrity. Data variations due to errors, slight changes in phrasing, or other inconsistencies may not be detected. Furthermore, a high number of duplicates in a dataset may not always indicate poor data quality, depending on the nature of data and the problem being addressed.\nvalidmind.data_validation.Duplicates\n\n\nMetric\nMissing Values Bar Plot\n**Purpose**: The 'MissingValuesBarPlot' metric generates a visual representation that highlights the percentage of missing values in each column of a dataset used in an ML model. This visualization is used to quickly identify and quantify missing data, a critical step in data preprocessing. Missing data can distort the predictions of an ML model and reduce its accuracy. The metric also applies a user-defined threshold to classify columns into those with missing data above (high-risk) and below (less-risk) the threshold. **Test Mechanism**: This metric scans through each column in the input dataset and calculates the percentage of missing values. It preferentially selects columns that have at least some missing data (more than 0%). Each column's missing data percentage is then examined against the predefined threshold. Columns with missing data above the threshold are classified as high-risk. A bar plot is then generated; columns with missing data are displayed on the y-axis, while their corresponding missing data percentages are plotted on the x-axis. Each bar's color is dictated by the missing data's percentage relation to the threshold. Values below the threshold are grey, while ones above are light coral. A red dashed line represents the user-defined threshold on the plot. **Signs of High Risk**: Columns in the dataset represented by light coral bars indicate a high risk. These columns have a higher percentage of missing values than the threshold. If a significant number of columns or crucial features fall into this category, it implies a substantial risk for reliable model performance. **Strengths**: This method offers a snapshot overview of missing data across all the dataset's columns, making it easier to identify problematic areas promptly. It not only helps with quantitative assessment but also with pattern recognition through its visual summary. The user-defined threshold allows a customized level of risk tolerance. It also supports classification and regression tasks, adding to its versatility. **Limitations**: This metric purely looks at the number of missing values and doesn't consider the type of missingness (MCAR, MAR, NMAR) or methods for handling missing entries (like imputation strategies). It also doesn't factor in the potential impact of missing data on the model's accuracy or precision. It might require expert understanding to interpret the findings and take appropriate action.\nvalidmind.data_validation.MissingValuesBarPlot\n\n\nMetric\nDataset Description\n**Purpose**: This test is used to provide a variety of descriptive statistics about the input data used in a machine learning model. Such statistics include measures of central tendency, measures of dispersion, and frequency counts for each field (i.e. feature or variable) in the dataset. This helps in understanding the nature of the data, identifying outliers, and understanding how well the data meets the assumptions of the model. **Test Mechanism**: The mechanism starts by transforming the input dataset to its basic form by reversing any one-hot encoding and extracting each field from the dataset. Descriptive statistics are then computed for each field depending on its data type (Numeric, Categorical, Boolean, Dummy, Text, Null). For numeric fields, statistics like mean, standard deviation, min, max and various percentiles are computed. For categorical and boolean fields, frequency counts of each category and the most frequent category (top) are computed. Missing and distinct values are computed for all fields. Additionally, the test generates histograms of the data for numeric and categorical types, as well as word count histograms for text simply by counting the number of occurrences of every distinct word. **Signs of High Risk**: Areas of potential high risk include a significant proportion of missing or null values which may affect the model's performance. Also, predominance of a single value (low diversity) in a feature, high variance in a numeric feature, or high number of distinct categories in categorical features may signal problems with the model. High frequency of a particular word in text data could also indicate bias. **Strengths**: This test effectively provides a comprehensive, high-level summary of the dataset. It aids in understanding the distribution, dispersion, and central tendency of numeric features, and the frequency distribution in categorical and text features. It effectively handles different data types and provides insight into the variety and distribution of field values. **Limitations**: This metric doesn’t provide insights into the relationships between different fields or features; it only provides univariate analysis. The statistical description for text data is also quite limited, essentially being a bag-of-words model. The descriptive statistics like mean, standard deviation are not meaningful for ordinal categorical data. Lastly, this test does not inform on how the data will influence the model's performance.\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatter Plot\n**Purpose**: The purpose of this metric, ScatterPlot, is to provide a visual analysis of the input dataset by generating a scatter plot matrix. This scatter plot matrix includes all columns (or features) in the dataset and is intended to reveal relationships, patterns, or outliers among different features. This visual representation can give both qualitative and quantitative insights into the multidimensional relationships in the dataset, which can be useful in understanding the suitability and effectiveness of selected features for model training. **Test Mechanism**: The ScatterPlot class utilizes the seaborn library to generate the scatter plot matrix. It first retrieves all columns from the input dataset, ensures that the specified columns exist in the dataset, and then creates a pairplot for these columns. It uses the kernel density estimate (kde) for a smoother, univariate distribution along the diagonal of the grid. The final plot is stored in an array of Figure objects, each encapsulating matplotlib figure instance for storage and later usage. **Signs of High Risk**: Signs of potential risk could be the appearance of non-linear or random patterns across different feature pairs indicating complex relationships not suitable for linear assumptions. Additionally, showing no clear patterns or clusters might indicate weak or non-existent correlations among features, posing a challenge for certain model types. Visualization of outliers may also be a sign of risk as outliers can negatively impact model performance. **Strengths**: The utilization of a scatter plot matrix for data analysis provides various strengths: 1. It provides visual insight into the multidimensional relationships among multiple features. 2. It helps to identify trends, correlations, and outliers that could impact model performance. 3. As a diagnostic tool, it can suggest whether certain assumptions taken during model creation (like linearity) are valid. 4. It is versatile and can be used for both classification and regression type tasks. **Limitations**: Scatter plot matrices, however, also present a few limitations: 1. They can become highly cluttered and difficult to interpret as the number of features (columns) increases. 2. They primarily reveal pairwise relationships and might not expose complex interactions involving more than two features. 3. Since it is a visual tool, quantitative analysis might not be precise. 4. Outliers, if not clearly visible, can be overlooked, affecting model performance. 5. It assumes that the dataset fits into memory, which might not be the case for extremely large datasets.\nvalidmind.data_validation.ScatterPlot\n\n\nThresholdTest\nTime Series Outliers\n**Purpose**: This test is designed to identify outliers in time-series data using the z-score method. It's vital for ensuring data quality before modeling, as outliers can skew predictive models and significantly impact their overall performance. **Test Mechanism**: The test processes a given dataset which must have datetime indexing, checks if a 'zscore_threshold' parameter has been supplied, and identifies columns with numeric data types. After finding numeric columns, the implementer then applies the z-score method to each numeric column, identifying outliers based on the threshold provided. Each outlier is listed together with their variable name, z-score, timestamp and relative threshold in a dictionary and converted to a DataFrame for convenient output. Additionally, it produces visual plots for each time series illustrating outliers in the context of the broader dataset. The 'zscore_threshold' parameter sets the limit beyond which a data point will be labeled as an outlier. The default threshold is set at 3, indicating that any data point that falls 3 standard deviations away from the mean will be marked as an outlier. **Signs of High Risk**: If many or substantial outliers are present within a dataset, this may be an indicator of high risk as it suggests that the dataset contains significant anomalies. This could potentially affect the performance of the machine learning models, if not properly addressed. Data points with z-scores higher than the set threshold would be flagged as outliers and could be considered as high risk. **Strengths**: 1. The z-score method is a popular and robust method for identifying outliers in a dataset. 2. Time series maintenance is simplified through requiring a datetime index. 3. Outliers are identified for each numeric feature individually. 4. Provides an elaborate report which shows variables, date, z-score and whether the test passed or failed. 5. Offers visual inspection for detected outliers in the respective time-series through plots. **Limitations**: 1. This test only identifies outliers in numeric columns, and won't identify outliers in categorical variables. 2. The utility and accuracy of z-scores can be limited if the data doesn’t follow a normal distribution. 3. The method relies on a subjective z-score threshold for deciding what constitutes an outlier, which might not always be suitable depending on the dataset and the use case. 4. It does not address possible ways to handle identified outliers in the data. 5. The necessity for a datetime index could limit the extent of its application.\nvalidmind.data_validation.TimeSeriesOutliers\n\n\nMetric\nTabular Categorical Bar Plots\n**Purpose**: The purpose of this metric is to visually analyze categorical data using bar plots. It is intended to evaluate the dataset's composition by displaying the counts of each category in each categorical feature. **Test Mechanism**: The provided dataset is first checked to determine if it contains any categorical variables. If no categorical columns are found, the tool raises a ValueError. For each categorical variable in the dataset, a separate bar plot is generated. The number of occurrences for each category is calculated and displayed on the plot. If a dataset contains multiple categorical columns, multiple bar plots are produced. **Signs of High Risk**: High risk could occur if the categorical variables exhibit an extreme imbalance, with categories having very few instances possibly being underrepresented in the model, which could affect the model's performance and its ability to generalize. Another sign of risk is if there are too many categories in a single variable, which could lead to overfitting and make the model complex. **Strengths**: This metric provides a visual and intuitively understandable representation of categorical data, which aids in the analysis of variable distributions. By presenting model inputs in this way, we can easily identify imbalances or rare categories that could affect the model's performance. **Limitations**: This method only works with categorical data, meaning it won't apply to numerical variables. In addition, the method does not provide any informative value when there are too many categories, as the bar chart could become cluttered and hard to interpret. It offers no insights into the model's performance or precision, but rather provides a descriptive analysis of the input.\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nAuto Stationarity\n**Purpose**: This test, AutoStationarity, is used to automatically detect and evaluate the stationarity of each time series in a Dataframe, using the Augmented Dickey-Fuller (ADF) test. Stationarity is a crucial property in time series data, which indicates that a series' statistical features such as mean and variance are constant over time. This property is an underlying assumption for many time-series models. **Test Mechanism**: The test performs the Augmented Dickey-Fuller test on each time series in a provided DataFrame to determine if the time series is stationary. A loop is ran over each column (time series) of the DataFrame. For each series, the ADF test is performed up to a maximum order of differencing (default value 5, configurable via params). The p-value of the ADF test is compared against a predefined threshold (default value 0.05, configurable via params). If the p-value is less than the threshold, the series is considered stationary at the current order of differencing. **Signs of High Risk**: The high risk or failure in the model's performance may be indicated by a significant number of series failing to achieve stationarity up to the maximum order of differencing. This might suggest that the series are not well-modeled by a stationary process and other modeling strategies might be needed. **Strengths**: By automating the ADF test, this metric allows for the mass analysis of stationarity across a multitude of time series, greatly increasing the efficiency and reliability of the analysis. The use of the ADF test, a commonly accepted method for testing stationarity, gives credibility to the results. Furthermore, the implementation of max order and threshold parameters allows users to specify their preferred level of strictness in the tests. **Limitations**: The Augmented Dickey-Fuller test and stationarity check generally have limitations. They rely on the assumption that the series are well-modeled by an autoregressive process, which may not be the case for all time series. The stationarity check is also sensitive to the choice of the threshold value for the significance level. Too high or too low threshold may lead to erroneous conclusions about stationary properties. There is also a risk of over-differencing if the maximum order is set too high, leading to unnecessary cycles.\nvalidmind.data_validation.AutoStationarity\n\n\nMetric\nDescriptive Statistics\n**Purpose**: The purpose of the Descriptive Statistics metric is to provide a comprehensive summary of both numerical and categorical data within a dataset. For numerical data, it gathers statistics such as count, mean, standard deviation, minimum and maximum values, as well as certain percentiles. For categorical data, it calculates the count, number of unique values, most frequent value, frequency of the most common value, and the proportion of the most frequent value relative to the total. This metric aids in visualizing the overall distribution of the variables in the dataset, which in turn assists in understanding the model's behavior and predicting its performance. **Test Mechanism**: The test mechanism involves using the describe() function for numerical fields which computes several summary statistics and value_counts() for categorical fields which counts unique values. Both of these functions are built-in methods of pandas dataframes. The results are then formatted to create two separate tables, one for numerical and one for categorical variable summaries. These tables provide a clear summary of the main characteristics of these variables, which can be crucial in assessing the model's performance. **Signs of High Risk**: High risks can be found in evidence of skewed data or notable outliers. This could be reflected in the mean and median (50% percentile) having a significant difference, in the case of numerical data. For categorical data, high risk can be indicated by a lack of diversity (low count of unique values), or overdominance of a single category (high frequency of the top value). **Strengths**: The primary strength of this metric lies in its ability to provide a comprehensive summary of the dataset, providing insights on the distribution and characteristics of the variables under consideration. It is a flexible and robust method, relevant to both numerical and categorical data. It can help highlight anomalies such as outliers, extreme skewness, or lack of diversity which can be essential in understanding model behavior during testing and validation. **Limitations**: While this metric provides a high-level overview of the data, it may not be sufficient to detect subtle correlations or complex patterns in the data. It does not provide any info on the relationship between variables. Plus, descriptive statistics alone cannot be used to infer properties about future unseen data. It should be used in conjunction with other statistical tests to provide a thorough understanding of the model's data.\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nANOVA One Way Table\n**Purpose**: The ANOVA (Analysis of Variance) One Way Table metric is used to evaluate whether the mean of numerical variables differs across different groups identified by the target or categorical variables. It plays a key role in identifying whether categorical variables have a significant effect on the numerical variables of interest. This metric helps in understanding and detecting features that are statistically significant with regard to the target variable in a given dataset. **Test Mechanism**: This metric performs an ANOVA F-test on each of the numerical variables against the target. Should no specific features be indicated, the test is conducted on all numerical features. A p-value is generated for each test and compared against a specified threshold, defaulting to 0.05 if none is given. The feature is marked as 'Pass' if the p-value is less than or equal to the specified threshold, indicating that there is a significant difference in the means of the groups, and 'Fail' otherwise. The test results are returned in a DataFrame indicating the variable name, F statistic value, p-value, threshold, and pass/fail status for each numerical variable. **Signs of High Risk**: High risk or failure in the model may be indicated by a large number of 'Fail' results in the ANOVA F-test. This suggests that many numerical variables in the dataset display no statistically significant difference across the target variable groups, potentially leading to underperforming predictions. Additionally, features with high p-values also indicate a high risk as they display a larger chance of obtaining the observed data given that the null hypothesis is true. **Strengths**: - The ANOVA One Way Table is an efficient tool in identifying statistically significant features as it compares the means of multiple groups simultaneously. - This metric is flexible and can handle situations where numerical features have not been specified by testing all numerical features in the dataset. - It provides a convenient way to measure the statistical significance of numerical variables and assists in picking those variables that significantly influence the classifier's predictions. **Limitations**: - The ANOVA One Way Table metric assumes that the data is normally distributed, which might not be the case, potentially leading to inaccurate test results. - The F-test is sensitive to variance changes, and therefore, this metric might not perform well in datasets with high variance. - The ANOVA One Way test does not indicate which specific group means are statistically different from each other; it merely tells that there is a difference. - The metric does not provide insight into interactions between variables, and it might fail to detect significant effects due to these interactions.\nvalidmind.data_validation.ANOVAOneWayTable\n\n\nMetric\nTarget Rate Bar Plots\n**Purpose**: The purpose of this metric is to visually summarize the distinct categorizations made by a classification-oriented machine learning model. Specifically, it generates bar plots representing target rate ratios for different categorical variables within the input dataset. This makes it easier to evaluate the performance of the classification model and gain quick insights into its accuracy. **Test Mechanism**: The test mechanic involves the creation of two bar plots for each categorical feature in the dataset. The first plot counts the number of times each category appears in the dataset, using a unique color to facilitate identification. On the second plot, it calculates the average target rate for each category and plots these averages. The target rate value comes from the column specified as \"default_column.\" The plots are generated using the Plotly library in Python, with subplots created for each feature. If no specific columns are indicated, all categorical feature columns in the dataset will be used to generate the plot. **Signs of High Risk**: High risk or failure signs might involve inconsistent or non-binary values in the \"default_column,\" making it difficult to calculate a meaningful default rate. Another risk sign could be an unusually low or high default rate for any particular category, suggesting the model might be wrongly classifying data points for those categories. **Strengths**: The main strength of this metric is its ability to provide a clear, visual representation of the model's categorization patterns. This helps stakeholders spot anomalies, inconsistencies, or trends in model behavior quickly. The metric is also flexible, allowing for the examination of a single column or multiple columns in the data. **Limitations**: This metric only works well with categorical data, limiting its application to numeric or continuous variables. It also assumes binary target values (only 0s and 1s), making it less useful for multi-class problems. The bar plots can become confusing and less interpretable if the dataset has too many distinct categories.\nvalidmind.data_validation.TargetRateBarPlots\n\n\nMetric\nPearson Correlation Matrix\n**Purpose**: This test is intended to evaluate the extent of linear dependency between all pairs of numerical variables in the given dataset. It provides the Pearson Correlation coefficient, which reveals any high correlations present. The purpose of doing this is to identify potential redundancy, as variables that are highly correlated can often be removed to reduce the dimensionality of the dataset without significantly impacting the model's performance. **Test Mechanism**: This metric test generates a correlation matrix for all numerical variables in the dataset using the Pearson correlation formula. A heat map is subsequently created to visualize this matrix effectively. The color of each point on the heat map corresponds to the magnitude and direction (positive or negative) of the correlation, with a range from -1 (perfect negative correlation) to 1 (perfect positive correlation). Any correlation coefficients higher than 0.7 (in absolute terms) are indicated in white in the heat map, suggesting a high degree of correlation. **Signs of High Risk**: High-risk signs pertaining to this metric might include a large number of variables in the dataset showing a high degree of correlation (coefficients approaching ±1). This indicates redundancy within the dataset, suggesting that some variables may not be contributing new information to the model. This could potentially lead to overfitting. **Strengths**: The primary strength of this metric test is its ability to detect and quantify the linearity of relationships between variables. This allows for the identification of redundant variables, which in turn can help in simplifying models and potentially improving their performance. The visualization aspect (heatmap) is another strength as it offers an easy-to-understand overview of the correlations, beneficial for those not comfortable navigating numerical matrices. **Limitations**: The primary limitation of Pearson Correlation is its inability to detect non-linear relationships between variables, which can lead to missed opportunities for dimensionality reduction. Furthermore, it only measures the degree of linear relationship and not the strength of effect of one variable on the other. The cutoff value of 0.7 for high correlation is a somewhat arbitrary choice and some valid dependencies might be missed if they have a correlation coefficient less than this value.\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nFeature Target Correlation Plot\n**Purpose**: The purpose of this test is to visually analyze and display the correlations between different input features and the target output of a Machine Learning model. This is important in understanding how each feature impacts the model's predictions. A high correlation means the feature strongly influences the target variable - something particularly useful in feature selection and understanding model behaviour. **Test Mechanism**: The FeatureTargetCorrelationPlot test computes correlations between features and the target variable of a given dataset. The correlations are calculated and then plotted in a horizontal bar graph, with color varying according to the strength of the correlation. An automatic hover template is also provided for descriptive tooltips. The features to be analyzed can be specified, and the graph's height can be adjusted per requirement. **Signs of High Risk**: Absence of any strong correlations (either positive or negative) between features and the target variable might suggest a high risk as it indicates that the given features don't have a significant influence on the prediction output. Also, duplication of correlation values might indicate redundancy in feature selection. **Strengths**: This test provides a visual aid, significantly improving the interpretation of correlations and giving a clear, easy-to-understand overview of how each feature impacts the model's target variable. This aids in feature selection and in understanding the nature of model predictions. Additionally, the hover template provides precise correlation values for each feature, essential for a granular level understanding. **Limitations**: The test only works with numerical data, so variables of other types must be preprocessed. Also, the plot assumes a linear correlation - it cannot efficiently capture non-linear relationships. One more limitation is that it may not accurately reflect importance for models that use complex interactions of features, like Decision Trees or Neural Networks.\nvalidmind.data_validation.FeatureTargetCorrelationPlot\n\n\nMetric\nTabular Numerical Histograms\n**Purpose**: The purpose of this test is to provide visual analysis of numerical data through the generation of histograms for each numerical feature in the dataset. Histograms aid in the exploratory analysis of data, offering insight into the distribution of the data, skewness, presence of outliers, and central tendencies. It helps in understanding if the inputs to the model are normally distributed which is a common assumption in many machine learning algorithms. **Test Mechanism**: This test scans the provided dataset and extracts all the numerical columns. For each numerical column, it constructs a histogram using plotly, with 50 bins. The deployment of histograms offers a robust visual aid, ensuring unruffled identification and understanding of numerical data distribution patterns. **Signs of High Risk**: A high degree of skewness, unexpected data distributions or existence of extreme outliers in the histograms may indicate issues with the data that the model is receiving. If data for a numerical feature is expected to follow a certain distribution (like normal distribution) but does not, it could lead to sub-par performance by the model. As such these instances should be treated as high-risk indicators. **Strengths**: This test offers several advantages: - It provides a simple, easy-to-interpret visualization of how data for each numerical attribute is distributed. - It can help detect skewed values and outliers, that could potentially harm the AI model's performance. - It can be applied to large datasets and multiple numerical variables conveniently. **Limitations**: Despite its effectiveness, this test does have some limitations: - It only works with numerical data, thus ignoring non-numerical or categorical data. - It does not analyze relationships between different features, only the individual feature distributions. - It is a univariate analysis, and may miss patterns or anomalies that only appear when considering multiple variables together. - It does not provide any insight into how these features affect the output of the model; it is purely an input analysis tool.\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nMetric\nIsolation Forest Outliers\n**Purpose**: The purpose of the `IsolationForestOutliers` test is to identify anomalies or outliers in the model's dataset. It assumes anomalous data points, due to their distinctive properties, can be detected more quickly by the isolation forest algorithm. This algorithm evaluates anomalies through the creation of isolation trees and identifying instances with shorter average path lengths in these trees, as these instances are expected to be different from the majority of the data points. **Test Mechanism**: This test uses the isolation forest algorithm, which isolates anomalies rather than identifying normal data points. It runs by building an ensemble of isolation trees, utilizing binary trees created by randomly selecting features and splitting the data based on random thresholds. For each pair of variables, it generates a scatter plot distinguishing the identified outliers from the inliers. The test results are visualized in these scatter plots showing the distinction of outliers from inliers. **Signs of High Risk**: Signs of high risk or failure in the model's performance might be the presence of high contamination which indicates a lot of anomalies, an inability to detect clusters of anomalies that are geographically close in the feature space, detecting normal instances as anomalies or overlooking actual anomalies. **Strengths**: The strengths of the isolation forest algorithm include its ability to handle large high-dimensional datasets, its efficiency in isolating anomalies rather than normal instances tradition, and its insensitivity to the underlying distribution of data. It is able to recognize anomalies even when they are not separated from the data cloud by identifying distinctive properties of the anomalies. Additionally, it visualizes the test results, aiding understanding and interpretability. **Limitations**: Among its limitations, the isolation forest test might find it hard to detect anomalies that are close to each other, or in datasets where anomalies are more prevalent. The results strongly rely on the contamination parameter, which might need fine-tuning to be effective. It may also not be effective in detecting collective anomalies if they behave similarly to normal data. Furthermore, it potentially lacks precision in identifying which features contribute most to the anomalous behavior.\nvalidmind.data_validation.IsolationForestOutliers\n\n\nMetric\nChi Squared Features Table\n**Purpose**: The purpose of this metric, `ChiSquaredFeaturesTable`, is to perform a Chi-Squared test of independence for each categorical feature variable with a given target column. The Chi-Squared test is designed to determine if there's a significant association between the categorical features and the target variable. It is typically used in the context of Model Risk Management to comprehend feature relevance and identify potential bias in a classification model. **Test Mechanism**: This test involves creating a contingency table for each categorical variable and the target variable, and then performing a Chi-Squared test. The Chi-Squared statistic and the p-value for each feature are calculated using this method. The p-value threshold is a parameter that can be adjusted, and a test will pass if the p-value is less than or equal to this threshold. If not, the test will fail. The test result for each feature - consisting of variable name, Chi-squared statistic, p-value, threshold, and pass/fail status - is aggregated into a final summary table. **Signs of High Risk**: Red Flags for high risk include high p-values (greater than the set threshold) for certain variables. These high p-values suggest there is not a statistically significant relationship between the feature and the target variables, resulting in a 'Fail' status. If a categorical feature has no relevant association with the target variable, it can signal that the machine learning model may not be performing well. **Strengths**: This test allows for a thorough examination of the interaction between a model's input features and the target output, helping to validate the relevance of the categorical features. It also provides a clear 'Pass/Fail' output for each categorical feature. The ability to adjust the p-value threshold adds flexibility to suit different statistical standards. **Limitations**: The metric assumes that the data is tabular and categorical, which may not apply to all datasets. It is specifically designed for classification tasks and not suitable for regression scenarios. It's also important to remember that the Chi-squared test, like any test based on hypothesis testing, is not able to identify causal relationships; it can only reveal associations. Moreover, the test relies on an adjustable p-value threshold, and different threshold choices may lead to different conclusions about feature relevance.\nvalidmind.data_validation.ChiSquaredFeaturesTable\n\n\nThresholdTest\nHigh Cardinality\n**Purpose**: The “High Cardinality” test functions to assess and evaluate the number of unique values present in the categorical columns of a dataset. Cardinality represents the measure of distinct elements residing in a set. In context of this test, high cardinality implies presence of a large quantity of unique, non-repetitive values in the dataset. **Test Mechanism**: This test begins by inferring the type of the dataset and initializing numeric threshold based on the test parameters. If the “threshold_type” is set to “percent”, it calculates the numeric threshold as the product of “percent_threshold” and the count of rows in the dataset. Subsequently, the test commences iteration through each column in the dataset. It limits the evaluation to those columns which are categorized as \"Categorical\". For each categorical column, the number of distinct values (n_distinct) and the percentage of distinct values (p_distinct) are computed. The test asserts success if n_distinct is less than the calculated numeric threshold. The summary method compiles results in tabulated form, with details for each column including column name, number of distinct values, percentage of distinct values, and pass/fail status. **Signs of High Risk**: High risk or failure in model performance may be indicated by a large number of distinct values (high cardinality) in one or more categorical columns. If any column fails the test (n_distinct &gt;= num_threshold), it is a sign of high risk. **Strengths**: The High Cardinality test is valuable in detecting overfitting risks and potential noise. High cardinality in categorical variables may trigger overfitting in machine learning models as the models can excessively adapt to the training data. It also helps in identifying potential outliers and inconsistencies, therefore aids in enhancing the data quality. It can be applied to both classification and regression task types, which lends versatility to the test. **Limitations**: Despite its numerous benefits, the High Cardinality test has certain limitations. It is solely applicable to \"Categorical\" data types, and not suitable for numerical or continuous features, limiting its scope. Furthermore, the test does not inherently assert the relevance or importance of unique values in categorical features, thus critical data points may be overlooked. The threshold (both number and percent) used for the test is static and likely to be non-optimal for diverse datasets and applications, it might benefit from additional mechanisms to adapt and fine-tune this dynamic.\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissing Values\n**Purpose**: This test is designed to measure the number of missing values in the dataset across all the features. This offers a way to evaluate the data quality, which is crucial to the predictive strength and reliability of any machine learning model. It aims to ensure that the ratio of missing data to total data is less than a predefined threshold, which in this case defaults to 1. **Test Mechanism**: The test iteratively runs through each column in the dataset, counting the number of missing values (NaNs) and calculating the percentage of missing values compared to the total number of rows. It then checks if the number of missing values is less than the pre-defined `min_threshold`. The results are summarized in a table that lists each column, the number of missing values, the percentage of missing values in each column, and a Pass/Fail status based on the comparison with the threshold. **Signs of High Risk**: A high risk is indicated when the number of missing values in any column exceeds the `min_threshold` value. Another sign of high risk is when there are missing values spread across many columns. In both cases, the test would issue a \"Fail\" mark on the Pass/Fail status. **Strengths**: This test helps to identify the presence and extent of missing data quickly and at a granular level (each feature in the dataset). It provides an effective and straightforward way to maintain data quality, which is crucial for building effective machine learning models. **Limitations**: While the test can efficiently detect missing data, it does not address the root causes of these missing values or suggest ways to impute or handle such values. It also does not consider situations where a feature has a significant amount of missing values, but still less than the `min_threshold`, which might also affect the model in cases where `min_threshold` is set too high. Lastly, this test does not account for data encoded as values (e.g., \"-999\" or \"None\"), which may technically not be missing but could carry the same implications.\nvalidmind.data_validation.MissingValues\n\n\nMetric\nDefault Ratesby Risk Band Plot\n**Purpose**: The Default Rates by Risk Band Plot metric is designed to measure and visualize default rates across different risk bands in a given dataset. This measurement is critical in evaluating the performance of credit risk models, allowing users to understand and compare default rates across diverse categories of risk. **Test Mechanism**: The test is conducted through a computed bar plot. The plot is generated by first calculating the count of accounts in each risk band, converting these counts into percentages by dividing by the total number of accounts, and then visualizing these percentages in the form of a bar plot. The bar plot thereby clearly indicates the percentage of total accounts associated with each risk band, providing a visual summary of default risk across these bands. The plot uses the 'Dark24' color sequence to ensure distinguishable colors for each risk band. **Signs of High Risk**: High risk is indicated with a high percentage of accounts associated with high-risk bands. The higher the percentage of accounts in these bands, the higher the exposure to default risk in the dataset. This could suggest potential failures in the model's ability to effectively manage or predict credit risk. **Strengths**: The main strengths of this metric are its simplicity and its visual impact. By graphically displaying the default rates, the metric provides a clear visual aid for understanding the spread of default risk across risk bands. The use of a bar chart provides an easy comparison between the different risk bands, helping to highlight any imbalances or areas of high risk. This helps to draw attention to any disproportions or anomalies in the data, making it easier to evaluate and compare the performance of credit risk models. **Limitations**: The main limitation of this metric is that it doesn't provide any insights into why certain risk bands have higher default rates. Additionally, it might not accurately represent the distribution of risk if some risk bands have significantly more accounts than others. It also doesn't consider other factors contributing to credit risk outside of the risk bands. Finally, the metric's reliance on visualization means that the results could be misinterpreted if not carefully analyzed, as graphical representations can sometimes be misleading.\nvalidmind.data_validation.DefaultRatesbyRiskBandPlot\n\n\nMetric\nRolling Stats Plot\n**Purpose**: The `RollingStatsPlot` metric is used to assess the stationarity of time series data in a given dataset. More specifically, the metric evaluates the rolling mean and rolling standard deviation of the dataset over a defined window size. The rolling mean is a measure of the average trend in the data, while the rolling standard deviation assesses the data's volatility within the window. These measures are critical for preparing time series data for modeling as they provide insights into the behavior of the data over time. **Test Mechanism**: The testing mechanism is divided into two steps. Firstly, the rolling mean and standard deviation for each column of the dataset are calculated over a window size, specified by the user or defaulted to 12 data points. Secondly, the rolling mean and standard deviation are plotted separately, thus visualizing the trends and volatility in the dataset. A basic check is performed to ensure that the columns exist in the dataset and that the provided dataset is indexed by date and time, which is a requirement for time series analysis. **Signs of High Risk**: Signs that could indicate high risk include: 1. A non-stationary pattern in either the rolling mean or the rolling standard deviation plot. This might mean that the data has trends or seasonality, which could affect the performance of time series models. 2. Missing columns in the dataset, which would prevent the metric from running successfully. 3. The presence of NaN values in the dataset. These might need to be handled before the metric can proceed. **Strengths**: The strengths of this metric include: 1. Providing visualizations of the data's trending behaviour and volatility, which can aid in understanding the overall characteristics of the data. 2. Checking the integrity of the dataset (whether all designated columns exist, and that the index is of datetime type). 3. Adapting to various window sizes, which allows for flexibility in analysing data with different temporal granularities. 4. Accommodating multi-feature datasets by considering each column of the data individually. **Limitations**: Some limitations of the `RollingStatsPlot` metric include: 1. A fixed window size is used for all columns, which may not accurately capture the patterns in datasets where different features have different optimal window sizes. 2. The metric requires the dataset to be indexed by date and time, hence may not be applicable to datasets without a timestamp index. 3. The metrics primarily serve for data visualization. It does not provide any quantitative measures for stationarity, such as statistical tests. Thus, interpretation is subjective and depends on the discretion of the modeler.\nvalidmind.data_validation.RollingStatsPlot\n\n\nMetric\nDataset Correlations\n**Purpose**: The DatasetCorrelations metric is employed to examine the relationship between variables in a dataset, specifically designed for numerical and categorical data types. Using Pearson's R, Cramer's V, and Correlation ratios, it helps in understanding the linear relationship between numerical variables, association between categorical ones, and between numerical-categorical variables respectively. This allows for better awareness regarding dependency between features, which is crucial for optimizing model performance and understanding the model's behavior and predictors. **Test Mechanism**: During its execution, DatasetCorrelations initiates the calculation of the aforementioned correlation coefficients for the provided dataset. It leverages the built-in method 'get_correlations()', populating the 'correlations' attribute in the dataset object. It then invokes 'get_correlation_plots()' to generate graphical representations of these correlations. Finally, the correlation details and figures are cached for further study and analysis. The test does not dictate specific thresholds or grading scales. **Signs of High Risk**: Signs of high risk might involve the presence of high correlation levels between input variables (multicollinearity), which can jeopardize the interpretability of the model and lead to overfitting. Additionally, the absence of any significant correlations, which suggests that the variables might not have predictive power. **Strengths**: The test boasts the advantage of being comprehensive; it covers the correlation study of numerical, categorical, and numerical-categorical variables. Thus, it negates the need for multiple individual tests. Furthermore, it not only provides numerical correlation values, but also visualization plots, aiding in a more intuitive understanding of relationships between variables. **Limitations**: This metric, however, is limited by the fact that it only detects linear relationships and associations; nonlinear relationships may go unnoticed. Also, given the absence of specified thresholds for determining correlation significance, the interpretation of the results is dependent on the user's expertise. Lastly, it doesn't handle missing values in the dataset, which need to be treated beforehand.\nvalidmind.data_validation.DatasetCorrelations\n\n\nMetric\nTabular Description Tables\n**Purpose**: The primarily purpose of this metric is to gather and present descriptive statistics for numerical, categorical and datetime variables from a given dataset. This helps in understanding the core traits of the dataset like the count, mean, min, max, percentage of missing values, data types of fields, unique values in case of categorical fields, amongst other descriptors. **Test Mechanism**: The methodology for this metric involves first segregating the variables in the dataset into their data types - numerical, categorical or datetime. Once they are segregated, the metric gathers the summary statistics for each variable type. The capture of these summary statistics varies depending upon the type of variable. For numerical variables, it extracts descriptors like count, mean, min, max, count of missing values and data type. For categorical variables, it extracts descriptors like count, number of unique values, display of unique values, count of missing values, and data type. For datetime variables, it extracts descriptors like count, number of unique values, earliest and latest date, count of missing values and data type. **Signs of High Risk**: There could be signs of high risk or failure when a substantial number of missing values is observed in the descriptive statistics results. It signals potential issues regarding data collection, integrity, and quality. Other red signals could include seeing inappropriate distributions for numerical variables (like having negative values for a variable that should always be positive), or finding inappropriate data types (like a continuous variable encoded as a categorical type). **Strengths**: This metric provides a comprehensive overview of the dataset. It not only gives a snapshot into the essence of the numerical, categorical, and datetime fields respectively but also identifies potential data quality issues such as missing values or inconsistencies which are crucial for building credible machine learning models. The metadata, notably the data type and missing value information, are vital for anyone including data scientists dealing with the dataset before the modeling process. **Limitations**: However, the limitations of this metric include not performing any deeper statistical analysis or tests on the data. It doesn't handle issues such as outliers, or relationships between variables. It does not offer insights into potential correlations or possible interactions between variables. It also stays silent on the potential impact of missing values on the performance of the machine learning models. Furthermore, it doesn’t investigate potential transformation requirements that may be necessary to enhance the performance of the chosen algorithm.\nvalidmind.data_validation.TabularDescriptionTables\n\n\nMetric\nAuto MA\n**Purpose**: The primary role of the `AutoMA` metric is to automatically determine the optimal Moving Average (MA) order for each variable in the time series dataset. The decision of MA order is based on minimizing BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion); both are statistical tools for model choice. Before the fitting process begins, a stationarity test (Augmented Dickey-Fuller test) is conducted on each series. **Test Mechanism**: First, the `AutoMA` algorithm checks whether the `max_ma_order` parameter is provided. It then loops over all variables in the dataset, performing the Dickey-Fuller test for stationarity. For each variable (if stationary), it fits an ARIMA model for orders running from 0 to `max_ma_order`. The output is a list showing BIC and AIC values of the ARIMA models with different orders. For each variable, the MA order that offers the smallest BIC is selected as the *best MA order*. The final results include a table summarizing the auto MA analysis and a table listing the best MA order for each variable. **Signs of High Risk**: - If a series is non-stationary (p-value&gt;0.05 in the Dickey-Fuller test), the result might be inaccurate. - Any error while fitting the ARIMA models, especially with higher MA order, could indicate possible risks and might require further investigation. **Strengths**: - This metric is beneficial for automating the selection process of the MA order for time series forecasting, thus saving time and effort usually required in manual hyperparameter tuning. - It applies both BIC and AIC, enhancing the chance of selecting the most suitable model. - It checks for stationarity of the series prior to model fitting, ensuring that the underlying assumptions of the MA model are met. **Limitations**: - If the time series is not stationary, the metric would provide inaccurate results. This limitation necessitates pre-processing steps to stabilize the series before ARIMA model fitting. - This metric uses a rudimentary model selection approach based on BIC and does not consider other possible model selection strategies, which might be more appropriate depending on the specific dataset. - The 'max_ma_order' parameter must be manually set, which may not always guarantee the best performance, especially if set too low. - The computation time raises with the increase in `max_ma_order`, thus the metric can be computationally expensive for larger values.\nvalidmind.data_validation.AutoMA\n\n\nThresholdTest\nUnique Rows\n**Purpose**: The purpose of the UniqueRows test is to assess the quality of the data that is being fed into the ML model, specifically by checking that the number of unique rows in the dataset exceeds a certain threshold in order to ensure diversity in the data. This is important as having a diverse range of data is crucial in training an unbiased and robust model that performs well on unseen data. **Test Mechanism**: The testing mechanism first calculates the total number of rows in the dataset. Then, for each column in the dataset, the number of unique rows is counted. The test passes if the percentage of unique rows (calculated as the number of unique rows divided by the total number of rows) is less than the predefined minimum percent threshold provided as a parameter to the function. The results are then cached, and an overall pass or fail is issued based on whether all columns have passed the test. **Signs of High Risk**: Signs of high risk include columns of data where the number of unique rows is less than the predefined minimum percent threshold. This lack of diversity in the data can be seen as an indicator of poor data quality, which could potentially lead to model overfitting and a poor generalization ability, thereby posing a high risk. **Strengths**: The Strengths of the UniqueRows test include its ability to quickly and efficiently evaluate the diversity of data across each column in the dataset. It allows an easy and systematic way to check data quality in terms of uniqueness, which can be a critical aspect in creating an effective and unbiased ML model. **Limitations**: One limitation of the UniqueRows test is that it assumes that the quality of the data is directly proportional to its uniqueness, which may not always be the case. There might be scenarios where certain non-unique rows are important and shouldn't be discounted. Also, it doesn’t take into account the 'importance' of each column in predicting the output and treats all columns equally. Finally, this test will not be useful or appropriate for categorical variables where the number of unique categories is naturally limited.\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nToo Many Zero Values\n**Purpose**: The 'TooManyZeroValues' test is used to identify numerical columns in the data that have an abundance of zero values. If a column has too many zeros, it could indicate data sparsity or potential lack of variation, which may limit its utility in a machine learning model. The test quantifies 'too many' as a percentage of total values, set by default to 3%. **Test Mechanism**: The application of this test involves iterating over each column of the dataset and determining if the column corresponds to numerical data. When a numeric column is identified, the function calculates the total count of zero values and their ratio to the total row number. If the proportion exceeds the preset threshold parameter (default 0.03 or 3%), the column fails the test. The results from each column are summarised in a report that indicates the number and the percentage of zero values for each numeric column, along with whether the column passed or failed the test. **Signs of High Risk**: The indicators of high risk associated with this test include: 1. Numeric columns with a high ratio of zero values to the total number of rows (beyond the set threshold). 2. Columns where all values registered are zero, implying a complete lack of data variation. **Strengths**: This test offers several advantages: 1. Helps in identifying columns with excessive zero values which otherwise might go unnoticed in a large dataset. 2. Offers flexibility to adjust the threshold of what counts as 'too many' zero values, accommodating the specific needs of a given analysis or model. 3. Reports both the count and percentage of zero values, shedding more light on the distribution and proportion of zeros within a column. 4. Focuses specifically on numerical data, preventing the misapplication to non-numerical columns and avoiding false test failures. **Limitations**: Certain limitation of this metric include: 1. It merely checks for zero values and does not assess the relevance or the impact of other potentially problematic values in the dataset such as extremely high or extremely low numbers, null values or outliers. 2. It cannot detect a pattern of zeros, which might be important in time series or longitudinal data. 3. Zeroes might be meaningful in certain contexts, so labelling them as 'too many' could be a misinterpretation of the data. 4. This test does not account for the dataset’s context and disregards that for certain columns, a high presence of zero values could be normal and does not necessarily indicate low data quality. 5. Cannot evaluate non-numerical or categorical columns, that might carry different types of concerns or issues.\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHigh Pearson Correlation\n**Purpose**: The Pearson Correlation test is intended to measure the linear relationship between features in the dataset, specifically ensuring that the pairwise Pearson correlation coefficients do not surpass a certain threshold. High correlation between two variables might indicate redundancy or multicollinearity. Identifying such correlations can alert the risk management team or developers about issues in the dataset which may have an impact on the performance and interpretability of the Machine Learning model. **Test Mechanism**: This Python implementation of the test first generates pairwise Pearson correlations for all the features in the dataset. It then sorts these correlations and eliminates duplicate and self-correlations (where a feature is correlated with itself). A Pass or Fail is assigned based on whether the absolute value of the correlation coefficient exceeds a predefined threshold (default to 0.3). The top 10 strongest correlations, regardless of whether they pass or fail, are returned. **Signs of High Risk**: The presence of correlation coefficients exceeding the specified threshold indicates high risk. This means that the features share a strong linear relationship, leading to potential multicollinearity and model overfitting. Redundancy of variables can undermine the interpretability of the model because it’s unclear which variable's predictive power is true. **Strengths**: - The Pearson Correlation test is a simple and fast way to identify pairwise relationships between features. - Provides clear output: Results show the pairs of correlated variables along with their Pearson correlation coefficient and a Pass or Fail status. - Helps in early identification of potential multicollinearity issues which can impact model training. **Limitations**: - Limited to linear relationships only: Pearson correlation cannot depict non-linear relationships or dependencies. - Sensitive to outliers: A few outliers can significantly change the correlation coefficient. - Redundancy identification only possible for pairwise correlation: If three or more variables are linearly dependent, this method might not identify that complex relationship. - Top 10 result limitation: It only keeps the top 10 high correlations, which might not fully capture the data's complexity. Configuration for the number of kept results needs to be implemented.\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nAC Fand PACF Plot\n**Purpose**: The ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plot test is employed for analyzing time series data in machine learning models. These plots provide valuable insights about the data correlation over a period of time. The ACF plots the correlation of the series with its own lags, while the PACF plots correlations after removing the effects already explained by earlier lags. This helps in identifying the nature of the trend (for example, seasonality), degree of autocorrelation, and selecting order parameters for AutoRegressive Integrated Moving Average (ARIMA) models. **Test Mechanism**: The python class `ACFandPACFPlot` receives a dataset with a time-based index. After checking that the index is of datetime type and dealing with any NaN values, it generates ACF and PACF plots for each column in the dataset, creating a subplot for each. In case the dataset does not include key columns, an error is thrown. **Signs of High Risk**: An indication of model risk in relation to the ACF and PACF plot test includes sudden drops in the correlation at a specific lag or consistent high correlation across multiple lags. Both could indicate non-stationarity in the data, which might suggest that the model estimated on this data might not generalize well to future data. **Strengths**: The ACF and PACF plots provide clear graphical representations of the correlation in time series data. This is particularly useful for identifying important characteristics such as seasonality, trends, and correlation patterns within the data. This allows for better model configuration, more specifically in the selection of parameters for ARIMA models. **Limitations**: ACF and PACF plots can only be used with time series data, and are thus not applicable to every ML model. Additionally, they require a large and consistent dataset, as gaps can lead to misleading results. These plots only show linear correlation, ignoring potential nonlinear relationships. Lastly, they may not be intuitive for non-experts to interpret and are not a substitute for more advanced analyses.\nvalidmind.data_validation.ACFandPACFPlot\n\n\nMetric\nBivariate Histograms\n**Purpose**: This metric is used for visual data analysis, specifically for inspecting the distribution of categorical variables. By presenting bivariate histograms, the metric assists in assessing correlations between variables and distributions within each defined target class, giving us intuitive insights into the data's traits and potential patterns. **Test Mechanism**: The BivariateHistograms module requires an input dataset and a set of feature pairs. For each pair of features in the dataset, a bivariate histogram is drawn using seaborn's histogram plotting function. This histogram separates the data by the status of the target column, optionally restricted to a specific status by the *target_filter* parameter. The module creates two histograms for each pair - one for each variable, where colors distinguish between different target statuses. **Signs of High Risk**: High risk signs would be irregular or unexpected distributions of data among categories. For instance, extremely skewed distributions, differing significantly from normal or expected distributions, or large discrepancies in distribution patterns between different target states, might all be reasons for concern. **Strengths**: The key strength of this test lies in its simplicity and visualization power. It provides a quick, consolidated view of data distributions across different target conditions for each variable pair, highlighting potential correlations and patterns. This can be instrumental in detecting anomalies, understanding feature interactions, and driving exploratory data analysis. **Limitations**: This metric's limitations are inherent in its simplicity. The use of histograms might not be effective in identifying complex patterns or detailed intricacies in the data. There could also be an issue of overplotting with larger datasets. Additionally, this test only works with categorical data and may not provide useful insights for numerical or continuous variables. Also, the interpretation of the visual results depends a lot on the expertise of the observer.\nvalidmind.data_validation.BivariateHistograms\n\n\nMetric\nWOE Bin Table\n**Purpose**: The Weight of Evidence (WoE) and Information Value (IV) test is intended to evaluate the predictive power of each feature in the machine learning model. The test generates binned groups of values from each feature in a dataset, computes the WoE value and the IV value for each bin. These values provide insights on the relationship between each feature and the target variable and their contribution towards the predictive output of the model. **Test Mechanism**: The metric leverages the `scorecardpy.woebin` method to perform WoE based automatic binning on the dataset. Depending on the parameter `breaks_adj`, the method adjusts the cut-off points for binning numeric variables. The bins are then used to calculate the WoE and IV. The metric requires a dataset with the target variable defined. The metric outputs a dataframe that comprises the bin boundaries, WoE, and IV values for each feature. **Signs of High Risk**: Indicators of high risk associated with this metric might include high IV values, which denote variables with too much predictive power which might lead to overfitting. Another sign of risk could be errors during the binning process, which might be due to inappropriate data types or poorly defined bins. **Strengths**: The WoE and IV test is highly effective for feature selection in binary classification problems, as it quantifies how much predictive information is packed within each feature regarding the binary outcome. Furthermore, the WoE transformation creates a monotonic relationship between the target and independent variables. **Limitations**: This method is mainly designed for binary classification tasks, therefore it might not be applicable or reliable for multi-class classification or regression tasks. Additionally, if the dataset has many features or the features are not binnable or they are non-numeric, this process might encounter difficulties. Furthermore, this metric doesn't help in identifying if the predictive factor being observed is a coincidence or a real phenomenon due to data randomness.\nvalidmind.data_validation.WOEBinTable\n\n\nMetric\nHeatmap Feature Correlations\n**Purpose:** The HeatmapFeatureCorrelations metric is utilized to evaluate the degree of interrelationships between pairs of input features within a dataset. This metric allows us to visually comprehend the correlation patterns through a heatmap, which can be essential in understanding which features may contribute most significantly to the performance of the model. Features that have high intercorrelation can potentially reduce the model's ability to learn, thus impacting the overall performance and stability of the machine learning model. **Test Mechanism:** The metric executes the correlation test by computing the Pearson correlations for all pairs of numerical features. It then generates a heatmap plot using seaborn, a Python data visualization library. The colormap ranges from -1 to 1, indicating perfect negative correlation and perfect positive correlation respectively. A 'declutter' option is provided which, if set to true, removes variable names and numerical correlations from the plot to provide a more streamlined view. The size of feature names and correlation coefficients can be controlled through 'fontsize' parameters. **Signs of High Risk:** Indicators of potential risk include features with high absolute correlation values. Since this suggests a significant degree of multicollinearity, it might lead to instabilities in the trained model and can also result in overfitting. Furthermore, the presence of multiple homogeneous blocks of high positive or negative correlation within the plot might indicate redundant or irrelevant features included within the dataset. **Strengths:** The strength of this metric lies in its ability to visually represent the extent and direction of correlation between any two numeric features, which aids in the interpretation and understanding of complex data relationships. The heatmap provides an immediate and intuitively understandable representation, hence, it is extremely useful for high-dimensional datasets where extracting meaningful relationships might be challenging. **Limitations:** The central limitation might be that it can only calculate correlation between numeric features, making it unsuitable for categorical variables unless they are already numerically encoded in a meaningful manner. Also, it uses Pearson's correlation, which only measures linear relationships between features. It may perform poorly in cases where the relationship is non-linear. Moreover, large feature sets might result in cluttered and difficult-to-read correlation heatmaps, especially when the 'declutter' option is set to false.\nvalidmind.data_validation.HeatmapFeatureCorrelations\n\n\nThresholdTest\nTime Series Frequency\n**Purpose**: The TimeSeriesFrequency test is designed to evaluate the consistency in the frequency of data points in a time series dataset. The metric inspects the intervals or duration between each data point in the dataset and helps to determine if there is a fixed pattern (e.g. daily, weekly, monthly). This is important in time-series analysis, as irregularities in the data frequency can lead to erroneous results or may affect the model's ability to identify trends and patterns. **Test Mechanism**: The test examines the dataframe index to verify that it's in datetime format. Afterwards, it identifies the frequency of each data series in the dataframe by employing pandas' `infer_freq` method, which infers the frequency of a given time series. It returns the frequency string as well as a dictionary that matches frequency strings to their respective labels. The frequencies of all variables in the dataset are then compared. If they all share one common frequency, the test passes; otherwise, it fails. A frequency plot that graphically depicts the time differences between consecutive entries in the dataframe index is also generated using Plotly. **Signs of High Risk**: One significant sign of high risk is if the test fails, i.e., when there are multiple unique frequencies in the dataset. This could indicate irregular time intervals between observations, which could disrupt pattern recognition or trend analysis that the model is designed to conduct. Also, missing or null frequencies can signify data inconsistencies or gaps in the data collection process. **Strengths**: This test provides a systematic way to check and visually represent the consistency of data frequency in a time series dataset. It increases reliability of the model by asserting that the timing and consistency of the observations are in line with the requirements of time series analysis. The visual plot generated provides an intuitive representation of the dataset's frequency distribution, catering to visual learners and aiding explanation and interpretation. **Limitations**: This test is restricted to time-series datasets; hence, it is not applicable to all kinds of datasets. Moreover, the `infer_freq` method may not always infer the correct frequency in the presence of missing or irregular data points. Lastly, in some cases, mixed frequencies might be acceptable depending on the context or the model being developed, but this test considers them a failing condition.\nvalidmind.data_validation.TimeSeriesFrequency\n\n\nMetric\nDataset Split\n**Purpose:** The DatasetSplit test is designed to evaluate and visualize the distribution of data among training, testing, and validation datasets, if available, within a given machine learning model. The main purpose is to assess whether the model's datasets are split appropriately, as an imbalanced split might affect the model's ability to learn from the data and generalize to unseen data. **Test Mechanism:** The DatasetSplit test first calculates the total size of all available datasets in the model. Then, for each individual dataset, the methodology involves determining the size of the dataset and its proportion relative to the total size. The results are then conveniently summarized in a table that shows dataset names, sizes, and proportions. Absolute size and proportion of the total dataset size are displayed for each individual dataset. **Signs of High Risk:** High risk signals include: 1. A very small training dataset, which may result in the model not learning enough from the data. 2. A very large training dataset and a small test dataset, which may lead to model overfitting and poor generalization to unseen data. 3. A small or non-existent validation dataset, which might complicate the model's performance assessment. **Strengths:** 1. The DatasetSplit test provides a clear, understandable visualization of dataset split proportions, which can highlight any potential imbalance in dataset splits quickly. 2. It covers a wide range of task types including classification, regression, and text-related tasks. 3. The metric is not tied to any specific data type and is applicable to tabular data, time series data, or text data. **Limitations:** 1. The DatasetSplit test does not provide any insight into the quality or diversity of the data within each split, just the size and proportion. 2. The test does not give any recommendations or adjustments for imbalanced datasets. 3. Potential lack of compatibility with more complex modes of data splitting (for example, stratified or time-based splits) could limit the applicability of this test.\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nSpread Plot\n**Purpose**: The purpose of the SpreadPlot metric is to visually explore and understand the relationships between pairs of time series variables in the given dataset. This understanding aids in identifying and assessing potential time series relationships, such as cointegration, between variables. **Test Mechanism**: The test mechanism for this metric involves calculating and plotting the spread between each pair of time series variables in the dataset. More specifically, the difference between two variables is computed and then plotted as a line graph. This process is repeated for all unique pairs of variables in the dataset. **Signs of High Risk**: Signs of high risk related to this metric include the presence of large fluctuations in the spread over time, unexpected patterns, or trends that might signal a potential risk in the underlying relationships between the variables. Also, the presence of significant missing data or extreme outlier values could potentially distort the spread, indicating a high risk. **Strengths**: The key strengths of using the SpreadPlot metric include: 1. Enables detailed visual inspection and interpretation of the relationships between time-series pairs. 2. Facilitates uncovering complex relationships like cointegration. 3. Enhances interpretability by visualizing the relationships, aiding in identifying outliers and trends. 4. Capable of handling multiple variable pairs from a dataset with a flexible and adaptable process. **Limitations**: Despite its strengths, the SpreadPlot metric also comes with certain limitations: 1. It's primarily a visualization tool and does not provide quantitative measurements or statistics to objectively assess relationships. 2. Highly dependent on the quality and granularity of the data – missing data or outliers can significantly affect the interpretation of the relationships. 3. Can become inefficient or challenging to interpret with a large number of variables due to the large number of plots. 4. Might not fully capture complex non-linear relationships between variables.\nvalidmind.data_validation.SpreadPlot\n\n\nMetric\nTime Series Line Plot\n**1. Purpose**: The \"TimeSeriesLinePlot\" metric is designed to generate and analyze time series data visual plots for model performance inspection purposes. This analysis manifests as line plots that represent various time series in the dataset. By visual plotting, the metric helps in the initial data inspection, which gives a sense of patterns, trends, seasonality, irregularity, and any anomalies that might occur over time in the given dataset. **2. Test Mechanism**: This Python class extracts the column names from the input dataset and iteratively generates line plots for each column. The Python library 'Plotly' is used for creating the line plots. For every column (representing a time series), a plot's time-series values are generated against the dataset's datetime index. Indexes that are not of datetime type will trigger a ValueError. **3. Signs of High Risk**: Red flags that could denote potential high risk scenarios include: * Time-series data lacking datetime indices * Non-existence of provided columns in the dataset * Anomalous patterns or irregularities seen in time-series plots indicating high model instability or predictive error potential **4. Strengths**: This metric comes with several favourable assets: * It succinctly visualizes complex time series data, facilitating easier interpretation and detection of temporal trends, patterns and anomalies. * It is highly adaptable and capable of functioning well with multiple time series within the same dataset. * By visual inspection, it allows the detection of anomalies and irregular patterns that could indicate a potential issue with the data or model performance. **5. Limitations**: Despite its numerous advantages, the metric has a few limitations: * The effectiveness of this metric relies heavily on the quality and patterns of the input time series data. * It's solely a visual inspection tool and does not provide quantitative measurement. This can be a disadvantage when the aim is to compare and rank multiple models or when precise, numeric diagnostics are required. * It assumes that the time specific data has already been transformed into a datetime index and requires the input data to be formatted appropriately. * It inherently lacks the ability to extract any deeper statistical insights from the time series data, making it less effective when dealing with complex data structures and phenomena.\nvalidmind.data_validation.TimeSeriesLinePlot\n\n\nMetric\nPi T Credit Scores Histogram\n**Purpose**: The PiT (Point in Time) Credit Scores Histogram metric is used to assess the performance of a classification model with a focus on credit risk evaluation. It visualizes the distributions of observed and predicted default scores and provides an intuitive comparison allowing for quick model assessment. **Test Mechanism**: This metric leverages histograms to illustrate the differences in score distributions for both observed and predicted results. It separates the scores into defaulted (1) and not defaulted (0) and makes a histogram for each of these categories. The model compares the distributions of observed and predicted classifications simultaneously, furnishing a clear idea about how well the model can predict the credit risk. **Signs of High Risk**: If the observed and predicted histograms are significantly different, there may be risk factors that the model is not adequately addressing. Additionally, if the predicted defaults are concentrated towards one end of the graph or are less evenly distributed than the observed scores, this could indicate potential issues with how the model is understanding the data or predicting outcomes. **Strengths**: This metric provides an intuitive visual representation of model performance which can be easily understood and interpreted even without extensive technical background. By comparing the distributions of observed and predicted scores, it gives a clear picture about the model's ability in distinguishing between defaulting and non-defaulting entities. It's especially tailored for credit risk assessment models and the PiT element takes into consideration time evolution of credit risk. **Limitations**: This visual method may not provide accurate enough information for detailed statistical analysis and doesn't give precise, quantifiable measures of model performance. It relies on manual inspection and comparison, making it subjective to human bias and potentially less reliable for catching subtle discrepancies. Furthermore, this method doesn't work well when the score distributions overlap significantly or when there are too many scores to be plotted, resulting in cluttered or hard-to-read graphs.\nvalidmind.data_validation.PiTCreditScoresHistogram\n\n\nMetric\nAuto Seasonality\n**Purpose:** The AutoSeasonality metric is intended to automatically detect and determine the optimal seasonal order or period for each variable in a time series dataset. The aim here is to quantify the inherent periodic pattern and seasonality that recurs after fixed intervals of time in the dataset. This is especially crucial for forecasting-based models, where understanding the seasonality component of the series can substantially enhance prediction accuracy. **Test Mechanism:** The implementation uses the seasonal decomposition method from Statsmodels Python library, broken down into 'additive' model type for each variable within the defined range of 'min_period' and 'max_period'. The test applies the seasonal decompose function to each seasonality period in the range and calculates the mean residual error for each period. The seasonal period that minimizes these residual errors is selected as the 'Best Period'. The results include the 'Best Period', along with the calculated residual errors and the determination of 'Seasonality' or 'No Seasonality'. **Signs of High Risk:** The following factors may indicate high risk or failure in the model's performance: - If the optimal seasonal period or 'Best Period' is consistently at the maximum or minimum limit of the provided range for a majority of variables, this might signify that the specified range doesn't adequately capture the true seasonality of the series. - A high average 'Residual Error' for the 'Best Period' could imply issues with the performance of the model. **Strengths:** This metric offers several advantages for time series dataset analysis: - It provides a robust, automated approach to identifying and quantifying the optimal seasonality. - It is applicable to multiple variables in a dataset, offering a comprehensive assessment of each variable's seasonality. - It uses concrete and measurable statistical methods to derive seasonality, enhancing the objectivity and reproducibility of the model. **Limitations:** There are also a few limitations of this metric: - Errors may arise if the time series exhibits random walk behaviour or lacks a clear seasonality, as the seasonal decomposition model may not be appropriate. - The defined range for the seasonal period (min_period and max_period) can influence the results. If the true seasonality period lies outside this range, this approach will not be able to identify the actual seasonality. - It may not fully interpret complex patterns that go beyond the simple additive model for seasonal decomposition. - The tool may infer seasonality where none exists if random fluctuations in the data match the defined seasonal period range.\nvalidmind.data_validation.AutoSeasonality\n\n\nMetric\nBivariate Scatter Plots\n**Purpose**: This metric is intended for visual inspection and monitoring of relationships between pairs of variables in a machine learning model targeting classification tasks. It is especially useful for understanding how predictor variables (features) behave in relation to each other and how they are distributed for different classes of the target variable, which could inform feature selection, model-building strategies, and even alert to possible biases and irregularities in the data. **Test Mechanism**: This metric operates by creating a scatter plot for each pair of the selected features in the dataset. If the parameters \"features_pairs\" are not specified, an error will be thrown. The metric offers flexibility by allowing the user to filter on a specific target class - specified by the \"target_filter\" parameter - for more granified insights. Each scatterplot is then color-coded based on the category of the target variable for better visual differentiation. The seaborn scatterplot library is used for generating the plots. **Signs of High Risk**: High risk indicators in this context would be related to visual patterns which might suggest non-linear relationships, substantial skewness, multicollinearity, clustering, or isolated outlier points in the scatter plot. Such issues could affect the assumptions and performance of some models, especially the ones assuming linearity like linear regression or logistic regression. **Strengths**: Scatterplots are simple and intuitive for users to understand, providing a visual tool to pinpoint complex relationships between two variables. It helps in outlier detection, identification of variable associations and trends, including non-linear patterns which can be overlooked by other linear-focused metrics or tests. The implementation also supports visualizing binary or multi-class classification datasets. **Limitations**: Scatterplots are limited to bivariate analysis - the relationship of two variables at a time - and might not reveal the full picture in higher dimensions or where interactions are present. They also are not ideal for very large datasets as points will overlap and render the visualization less informative. Further, scatterplots are more of an exploratory tool rather than a formal statistical test, so they don't provide any quantitative measure of model quality or performance. Interpretation of scatterplots relies heavily on the domain knowledge and judgment of the viewer, which can introduce subjective bias.\nvalidmind.data_validation.BivariateScatterPlots\n\n\nMetric\nEngle Granger Coint\n**Purpose**: The purpose of this metric is to use the Engle-Granger test to examine for cointegration between pairs of time series variables in a given dataset. This can be beneficial for assessing the extent to which two time series variables move together over time. Predictive regressions can be substantially more accurate if the variables involved are co-integrated. **Test Mechanism**: This test first prepares the input dataset by dropping any non-applicable values. It then iterates pairs of variables, applying the Engle-Granger cointegration test to each pair. The test yield a 'p' value, which is then compared to a pre-set threshold to determine whether the corresponding variables are cointegrated or not. If the 'p' value is less than or equal to the threshold, the decision is 'Cointegrated', if not, 'Not cointegrated'. The metric returns a summary table of the cointegration results for each pair of time series variables. **Signs of High Risk**: High risk can be indicated if a large number of variables that are expected to be cointegrated fail the test. Also, if a significant number of 'p' values obtained are near the threshold, it also poses a risk as minor data variations can skew the decision between 'Cointegrated' and 'Not cointegrated'. **Strengths**: This test is a powerful method for analysing the relationship between time series, particularly when it's important to know whether variables are moving together in a statistically significant manner. Objectively evaluating such relationships can drive better forecasting in many domains, especially in finance or economics where prediction models often depend on understanding how different variables move together over time. **Limitations**: Cointegration tests, such as the Engle-Granger, often assume that time series are integrated of the same order, which is not always true in multi-variate time series datasets. Non-stationary characteristics in the series or the presence of structural breaks might give falsely positive or negative cointegration results. Additionally, the method doesn't work very well with small sample sizes due to a lack of power. Thus, the test must be used with caution, and where possible, corroborated with other predictive indicators.\nvalidmind.data_validation.EngleGrangerCoint\n\n\nThresholdTest\nTime Series Missing Values\n**Purpose**: This test is designed to verify that the number of missing values in a historical time-series dataset is below a specified threshold. Since time-series models rely on continuity and temporality of data points, missing values could affect the model's performance. Therefore, this test intends to maintain data quality and readiness for the machine learning model. **Test Mechanism**: The test process begins by checking if the dataset has a datetime index; if not, an error is raised. Next, a lower limit threshold for missing values is set and a missing values check is run on each column of the dataset, with a test result object being generated whether the number of missing values is below the specified threshold. The percentage of missing values is also calculated alongside the raw count. As a visualization aid, the test generates two plots (a bar plot and a heatmap) to better illustrate the distribution and quantity of missing values per variable, if any exist. The test results, missing values count, missing values percentage, and the pass/fail status are returned in a results table. **Signs of High Risk**: If any column in the dataset contains a number of missing values exceeding the threshold, it is considered a failure and a high-risk scenario. This could be due to incomplete data collection, faulty sensors, or data preprocessing errors. A continuous visual 'streak' in the 'heatmap' might also indicate a systematic error during data collection. **Strengths**: This test is beneficial in promptly identifying missing values which might impact the model's performance. It is broadly applicable and customizable through the threshold parameter. Going beyond raw numbers to calculate the percentile of missing values adds a more relative understanding of data scarcity. It also includes a robust mechanism to visually represent data quality, facilitating quicker and friendlier misinterpretation. **Limitations**: Though the test effectively identifies missing values, it does not suggest methods to deal with them. It assumes that the dataset should have datetime index, limiting its use to time series analysis only. The test is sensitive to the 'min_threshold' parameter and might raise false alarms if it's set too strictly, or might miss problematic data if it's set too loosely. Lastly, the test solely focuses on 'missingness' and might overlook other aspects of data quality.\nvalidmind.data_validation.TimeSeriesMissingValues\n\n\nDatasetMetadata\nDataset Metadata\n**Purpose**: The main objective of the `DatasetMetadata` test is to gather and log crucial descriptive statistics about the datasets used during model training. It serves as an important tool for monitoring relevant characteristics of the dataset, such as task types (classification, regression, text_classification, text_summarization) and tags (tabular_data, time_series_data, text_data). This helps ensure that model validation exercises are conducted with full transparency and context by connecting different metrics and test results to the underlying dataset. **Test Mechanism**: This class does not include a specific test or grading scale. Instead, it collects metadata associated with the dataset and logs it for future reference. The metadata is attached to the dataset object during the post-initialization phase. The `run` method initializes a `TestSuiteDatasetResult` object with a unique result ID and dataset. The dataset metadata then gets associated with this ID for future use in more targeted validation efforts. **Signs of High Risk**: The risks associated with this process are not connected with model performance or biases. However, incomplete metadata, incorrect dataset labels, or missing dataset types could lead to inaccuracies in model risk assessment and may constitute signs of risk within the metadata gathering process itself. **Strengths**: A key strength of this class is the transparency it brings to model validation exercises by providing detailed information about the dataset. This can help in diagnosing errors, identifying correlations to the model's behavior, ensuring correct task and data-type associations, and enabling superior model explanations. Also, it can support dataset versioning by logging each dataset's metadata, offering a trackable history of changes. **Limitations**: The `DatasetMetadata` class could lack completeness or accuracy, particularly if metadata is not adequately added or is incorrect. The process does not inherently evaluate the quality of the dataset or directly validate the model's predictions, so it must be combined with other tests for a comprehensive evaluation. Lastly, any potential bias in the dataset won't be recognized using this class. Bias detection would require separate tests tailored specifically towards fairness and bias detection.\nvalidmind.data_validation.DatasetMetadata\n\n\nMetric\nTime Series Histogram\n**Purpose**: This test conducts a histogram analysis on time series data. The goal is to evaluate the distribution of values in the dataset over a given time period, typically for regression tasks. Internet traffic, stock prices, weather data etc. could be time series data. Histograms provide insights into the data’s underlying probability distribution, skewness, peakness(kurtosis) etc. **Test Mechanism**: The test requires a dataset column that must have a datetime type index. It iterates over each column in the provided dataset and generates a histogram using Seaborn's histplot function. If the dataset contains more than one time-series (i.e., more than one column with datetime type index), a separate histogram will be plotted for each. Additionally, a kernel density estimate (KDE) line is drawn for each histogram to indicate the data's underlying probability distribution. The x and y-axis labels are hidden to only focus on the data distribution. **Signs of High Risk**: - The dataset does not contain a column with a datetime type index. - The specified columns do not exist in the dataset. - The distribution of data in the histogram exhibits high skewness or kurtosis, which might induce biases in the model. - Presence of outliers that are far from the main data distribution. **Strengths**: - Provides a visual diagnostic tool, which is a good starting point to understand the overall behavior and distribution trends of the dataset. - Works well for both single and multiple time series data analysis. - The Kernel Density Estimation (KDE) line offers a smooth estimate of the overall trend in data distribution. **Limitations**: - It only provides a high-level overview of data distribution and does not provide specific numeric measures of skewness, kurtosis, etc. - It doesn’t show the precise data values and the actual data is grouped into bins, hence some detail is inherently lost (precision vs. general overview trade-off). - Can't handle non-numeric data columns. - The shape of the histogram can be sensitive to the number of bins used.\nvalidmind.data_validation.TimeSeriesHistogram\n\n\nMetric\nLagged Correlation Heatmap\n**Purpose**: This LaggedCorrelationHeatmap metric is designed to evaluate and visualize the correlation between the target variable and certain delayed copies (lags) of independent variables in a time-series based dataset. This metric aids in uncovering relationships in time-series data where the influence of an input feature on the target variable may not be immediate but occurs after a certain period (lags). **Test Mechanism**: Python's Pandas library is used in conjunction with Plotly to perform computations and generate the heatmap. The target variable and corresponding independent variables are taken from the dataset. Subsequently, lags of independent variables are generated and the correlation between these lagged variables and the target are calculated. The calculated correlations are stored in a matrix that has variables on one axis and the number of lags on the other. This correlation matrix is then visualized as a heatmap, where different color intensities represent the strength of the correlation, making patterns easier to spot. **Signs of High Risk**: High risk associated with this metric may be indicated by insignificant correlations throughout the heatmap or by correlations that break intuition or prior knowledge, indicating potential issues with the data or model. **Strengths**: This metric offers an excellent way to explore and visualize the time-dependent relationships between features and the target variable in a time-series dataset. By looking at correlations with lagged features, it is possible to identify delayed effects that may go unnoticed with more traditional correlation measures. As it's visually represented, the heatmaps are a very intuitive way to present time dependent correlations and influence. **Limitations**: The LaggedCorrelationHeatmap metric has a few limitations. It assumes linear relationships between the variables, meaning relationships that aren't linear may go undetected. As it only considers linear correlation, it may miss out on complex nonlinear interactions. This metric works only with time-series data and would not be applicable for other types of data. Also, the choice of number of lags used can also affect the results; whilst too many lags can make the heatmap hard to interpret and less useful, too few might miss delayed effects. Furthermore, this metric does not consider any causal relationship, it merely presents the correlation.\nvalidmind.data_validation.LaggedCorrelationHeatmap\n\n\nMetric\nSeasonal Decompose\n**Purpose**: This test utilizes the Seasonal Decomposition of Time Series by Loess (STL) method to break down a dataset into its fundamental components: observed, trend, seasonal, and residuals. The main purpose of this method is to identify any hidden or non-intuitive patterns and ascertain certain attributes such as seasonality in the dataset's features, helping to understand and validate the dataset further. **Test Mechanism**: The test uses the seasonal_decompose function from the statsmodels.tsa.seasonal library to assess each feature in the dataset. The seasonal_decompose function breaks down each analyzed feature into four components: observed, trend, seasonal, and residuals. It generates graphical representations, essentially six subplot graphs for each feature, to visually interpret the results. The Seasonal Decompose test also includes a verification step to identify and exclude non-finite values before the seasonal decomposition. **Signs of High Risk**: Signs of high risk associated with this test include: - Non-finiteness: Having too many non-finite values in a dataset might imply a high risk since these values are excluded before performing the seasonal decomposition. - Frequent Warnings: Issue warnings when the test fails to infer frequency for a contested feature. - High Seasonality: If the seasonal component is significantly high, forecasts might be highly unreliable due to excessive seasonal changes. **Strengths**: Strengths of this metric test include: - Ability to Detect Seasonality: This code excels in detecting hidden seasonality patterns within features of datasets. - Visualization: This test provides visualizations, making it easier to interpret and comprehend. - Works with Any Regression Model: It does not restrict its applicability to any specific regression model, ensuring wider usability. **Limitations**: Limitations of this method include: - Dependencies on Assumptions: The test assumes that features in the dataset have a certain frequency. If no frequency could be inferred for a variable, that feature will be excluded from analysis. - Handling of Non-finite Values: The test excludes non-finite values during the analysis which could lead to incomplete representation or understanding of the dataset. - Not Reliable for Noisy Datasets: This test may generate unreliable results in the presence of heavy noise.\nvalidmind.data_validation.SeasonalDecompose\n\n\nMetric\nWOE Bin Plots\n**Purpose**: The purpose of this test is to perform a visual analysis of the Weight of Evidence (WoE) and Information Value (IV) for categorical variables in a provided dataset. This aids in understanding the predictive power of each variable in a classification-based machine learning model by displaying the data distribution over the different categories of each feature. WoE and IV are common metric in credit scoring models and provide reliable statistical measures for variables' predictive power. **Test Mechanism**: The test mechanism proceeds in predefined steps. It first selects non-numeric columns and converts them to string type for proper binning. Afterward, it performs an automatic WoE binning on the selected features in the dataset, which effectively groups the possible values of a variable into bins or categories. Once this is done, the function generates two visual charts for each variable - a Bar chart for IV and a Scatter chart for WoE values. These visualizations are rendered based on the distribution of the respective metric across the different categories of each feature. **Signs of High Risk**: Indicators of high potential risk include an error occurring during the binning process or issues converting non-numeric columns to string data type. Furthermore, an uneven distribution of WoE and IV, especially if certain bins dominate others significantly, might indicate that the model is excessively relying on certain variables or categories for predictions, which could have an adverse impact on the model's generalizability and robustness. **Strengths**: One of the strengths of using this metric is that it provides a detailed visual presentation of how feature categories relate to the target variable, giving an intuitive understanding of the feature's contribution to the model. It also allows easy identification of highly impacting features, which can aid in feature selection and in understanding the decision logic of the model. Furthermore, WoE transformations are monotonic, meaning they preserve the rank ordering of the original data points, which simplifies subsequent analyses. **Limitations**: One of the limitations of this method is that it is largely dependent on the binning process. An inappropriate choice of the number of bins or the binning thresholds can lead to an inadequate representation of the variable's distribution. Also, this method is most suitable for categorical data; encoding continuous variables to categorical might sometimes lead to loss of information. Another limitation is that extreme or outlier values can have a significant impact on WoE and IV calculation. Finally, it requires sufficient events per bin in order to give reliable information value and weight of evidence.\nvalidmind.data_validation.WOEBinPlots\n\n\nThresholdTest\nClass Imbalance\n**Purpose**: The purpose of the ClassImbalance test is to assess the distribution of the target classes in a dataset used for a machine learning model. Particularly, this method is geared towards ensuring that the classes are not too skewed, which could bias the predictions of the model. It is important to have a balanced dataset for training to avoid biased model with high accuracy for the majority class and low accuracy for the minority class. **Test Mechanism**: The ClassImbalance test functions by determining the frequency (in percentage proportion) of each class in the target column of the dataset and checks whether each class appears in at least a particular minimum percentage of the total records. This minimum percentage is a parameter that can be pre-set or changed according to requirements; the default value in this case is set to 10%. **Signs of High Risk**: Any class that constitutes less than the preset minimum percentage threshold is flagged as a high risk, indicating potential class imbalance. The function provides a pass/fail verdict for each class based on this criterion. In essence, if any class fails this test, the dataset is likely to contain an imbalanced class distribution. **Strengths**: 1. Identifies under-represented classes which can impact the performance of a machine learning model. 2. Easy and fast to compute. 3. The test is very informative as it not only detects imbalance but also quantifies the degree of imbalance. 4. Adjustable threshold allows flexibility and adaptation to varying use-cases or domain-specific requirements. 5. The test generates a visually intuitive plot showing classes and their corresponding proportions, aiding in interpretability and understanding of the data. **Limitations**: 1. The test might not perform well, or offer useful insights for datasets with too many classes. The imbalance may be inevitable due to the inherent class distribution. 2. Sensitivity to the threshold value may lead to false discrimination of imbalance when the threshold is set too high. 3. Irrespective of the percentage threshold, it does not account for different costs or impacts of misclassification of different classes, which can vary based on specific application or domain. 4. While it can identify imbalances in class distribution, it does not offer direct ways to handle or correct such imbalances. 5. The test is only applicable for classification tasks and not for regression or clustering tasks.\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nIQR Outliers Bar Plot\n**Purpose**: The InterQuartile Range Outliers Bar Plot (IQROutliersBarPlot) metric aims to visually analyse and evaluate the extent of outliers in numeric variables based on percentiles. This metric is vital for understanding data distribution, identifying abnormalities within a dataset and assessing the risk associated with processing potentially skewed data that might impact the predictive performance of the machine learning model. **Test Mechanism**: The test involves the following steps: 1. For each numeric feature, or column, in the dataset, it computes Q1 (25th percentile) and Q3 (75th percentile), and then the Interquartile Range (IQR) which is the difference between Q3 and Q1. 2. It then calculates the thresholds for lower and upper bounds as Q1 minus `threshold` times IQR and Q3 plus `threshold` times IQR, respectively. The default `threshold` is 1.5. 3. Any value in the feature that is less than the lower bound and greater than the upper bound is considered an outlier. 4. The number of outliers are then calculated for different percentile categories like [0-25], [25-50], [50-75], [75-100]. 5. These counts are then used to prepare a bar plot for the feature showing the distribution of outliers across different percentile ranges. **Signs of High Risk**: High risk or failure in the model's performance could be indicated by: 1. Presence of large number of outliers in the data that will skew the distribution. 2. When the outliers are in higher percentiles (75-100). This indicates extreme values, which can have a more prominent impact on the model's performance. 3. Certain features having a majority of their values as outliers, meaning these features may not contribute positively to model's prediction power. **Strengths**: 1. Identifies outliers in the data visually, which is easy to understand and can help in interpreting the potential impact on the model. 2. Accommodates both total numeric features or a selected subset hence proving its flexibility. 3. Agnostic to the task type: can be used for both classification and regression tasks. 4. This metric can process large datasets as it does not rely on computationally expensive operations. **Limitations**: 1. This metric only works with numerical variables and would not be applicable to categorical variables. 2. It uses a pre-defined threshold (defaulting to 1.5) to determine what constitutes an outlier. This threshold might not be ideal for all cases. 3. This metric does not provide insights about the consequence of these outliers on the predictive performance of the models, but only presents their presence and distribution. 4. It assumes that the data is unimodal and symmetric, which may not be always the case. For non-normal distributions, the results might be misleading.\nvalidmind.data_validation.IQROutliersBarPlot\n\n\nMetric\nPi TPD Histogram\n**Purpose**: The PiTPDHistogram metric is used to compute the Probability of Default (PD) for each instance in both the training and test datasets in order to evaluate the model's performance in credit risk estimation. The PD is examined at a specific point in time (PiT). The resulting distributions for the actual and predicted default classes are then visualized in a histogram, making it easier to understand the model's prediction accuracy. **Test Mechanism**: This metric sorts instances into either observed (actual) or predicted default classes, with a default being represented by '1' and non-default as '0'. Using these classes, the probability of default is computed for each instance and plotted in a histogram. Two histograms are generated: one for the observed defaults and another one for the predicted defaults. If the distribution of predicted defaults closely matches the distribution of observed defaults, this would indicate good model performance. **Signs of High Risk**: Discrepancies between the two histograms (observed and predicted defaults) would suggest model risk. This may include differences in the shapes of the histograms, divergences in the spread of default probabilities, or significant mismatches in the peak default probabilities. These disparities signal that the model may not be accurately predicting defaults, which could pose a high risk especially in the field of credit risk analysis. **Strengths**: This metric excels in its visual interpretation of model performance. By comparing two histograms side by side, it allows for a convenient comparison between the observed and predicted defaults. This visualization can reveal model biases and illuminate areas where the model's performance may fall short, which might not be as evident in purely numerical evaluations or more complex visualization measures. **Limitations**: Despite its strengths, the PiTPDHistogram metric is largely interpretive and subjective. Determining risk based on visual discrepancies requires a certain level of human judgement and may vary between different analysts. If used solely, this metric might overlook other nuances in model performance that are better captured by more quantitative or diversified metrics. Furthermore, the information provided is limited to a specific point in time, which might not fully reflect the model's performance over different periods or under changing conditions.\nvalidmind.data_validation.PiTPDHistogram\n\n\nMetric\nAuto AR\n**Purpose**: The purpose of this test, referred to as AutoAR, is to automatically detect the Autoregressive (AR) order of a time series using both Bayesian information criterion (BIC) and Akaike information criterion (AIC) for regression tasks. The AR order specifies the number of previous terms in the series to use to predict the current term. The goal is to select the appropriate AR model that best captures the trend and seasonality in the time series data. **Test Mechanism**: The test iterates over a range of possible AR orders up to a specified maximum. For each order, an autoregressive model is fitted, and the BIC and AIC are computed. Both these statistical measures penalize models for complexity (higher number of parameters), favoring simpler models that fit the data well. Additionally, the Augmented Dickey-Fuller test is conducted to check stationarity of the time series; non-stationary series might produce inaccurate results. The test results, including AR order, BIC, and AIC, are added into a dataframe for easy comparison. Subsequently, the AR order with minimum BIC is determined as the \"best\" order for each variable. **Signs of High Risk**: - If a time series is not stationary (Augmented Dickey-Fuller test p-value &gt; 0.05), it may lead to inaccurate results. - Issues with the model fitting process, such as computational or convergence problems, suggest a high risk. - If the chosen AR order is consistently at the maximum specified order, this could suggest insufficiency of the maximum set limit. **Strengths**: - The test automatically determines the optimal AR order, reducing potential bias involved in manual selection. - The method attempts to balance goodness-of-fit against model simplicity, preventing overfitting. - It factors in stationarity of the time series, essential for reliable AR modeling. - The test consolidates the results into a clear, easily interpreted table. **Limitations**: - The test requires stationary input time series. - Assumes linear relationship between the series and its lags. - Finding the best model is limited to the maximum AR order provided in the parameters. A low max_ar_order may yield suboptimal results. - AIC and BIC may not always select the same model as the best; interpretation may need to consider the goal and the trade-offs.\nvalidmind.data_validation.AutoAR\n\n\nMetric\nTabular Date Time Histograms\n**Purpose**: The purpose of `TabularDateTimeHistograms` metric is to visually analyze the datetime data within a given machine learning model dataset. It plots histograms for differences between consecutive dates across all datetime variables present in the dataset. This assists in the inspection and understanding of the frequency distribution of time intervals in the dataset, which can be useful in analysing patterns in the data over time. **Test Mechanism**: The test works by first extracting all the datetime columns from the dataset. For each of these datetime columns, the test then calculates the difference between consecutive dates and converts these differences into days. Zero values are filtered out to only plot data points that represent some amount of day difference. Histograms are created for each column using seaborn's histplot function and displayed with relevant labels. The histograms provide a visual representation of the frequency of occurrences of different day intervals in the dataset. **Signs of High Risk**: One high risk associated with this test is if no datetime columns are found in the data set, it would result in a ValueError. A potential sign of risk could also be a skewed or highly irregular distribution in the histogram, indicating potential issues in the data such as incorrect timestamps or anomalies. **Strengths**: The strengths of this metric include its ability to visually represent frequency distributions of time interval changes in the dataset, aiding in understanding inherent patterns. The histogram plots can also assist in identifying potential outliers and anomalies in the data, and can help assess data quality. The metric is applicable to various task types like classification, regression, and works with multiple datetime variables if available. **Limitations**: Limitations of using this metric mainly stem from its reliance on visual interpretation of the data. The metric does not provide a quantitative evaluation of the model and may not capture complexities or multidimensional patterns in data. The test can only be applied to datasets with datetime columns, and would fail in absence of them. Interpretation of the generated histograms also depends on the domain knowledge and experience of the investigator.\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\nMetric\nPunctuations\n**1. Purpose:** The main goal of the Punctuations Metric is to analyze the frequency of punctuation usage within a given text dataset. This heuristic is often used in Natural Language Processing tasks such as text classification and text summarization. **2. Test Mechanism:** The test begins by checking the input's eligibility, specifying that the \"dataset\" must be of a VMDataset type. The next step is to create a corpus of the dataset by splitting the text of the input dataset on spaces. Then, it tallies the usage of each unique punctuation character in the text corpus. The distribution of the frequencies of each punctuation symbol is subsequently depicted as a bar graph. Results are stored as Figures and associated with the main Punctuations object. **3. Signs of High Risk:** Given the nature of the test, risks are typically associated with the quality of text data. One sign of high risk might be an excessive or unusual frequency of certain punctuation marks, which may denote dubious quality, data corruption, or skewed data. **4. Strengths:** The primary strength of this metric is providing insights into the distribution of punctuation use in a text dataset. This could be valuable in validating the quality, consistency, and nature of data. It could also offer hints about the style or tonality of the text corpus (e.g., frequent exclamation use might signify a more informal, emotional context). **5. Limitations:** This test exclusively looks at punctuation usage and thus can overlook other crucial text characteristics. Additionally, it’s crucial not to make broad cultural or tonality assumptions based solely on punctuation distribution since these may vary widely across different languages and contexts. It may also be less effective when dealing with languages that employ non-standard or different punctuation. Finally, the visualized results may lack interpretability when there are many unique punctuation marks in the dataset.\nvalidmind.data_validation.nlp.Punctuations\n\n\nMetric\nCommon Words\n**Purpose**: The CommonWords metric is used to identify and visualize the most prevalent words within a specified text column of a dataset, giving insights into the prevalent language patterns and vocabulary. This is particularly useful for Natural Language Processing (NLP) tasks such as text classification and text summarization. **Test Mechanism**: The methodology for this test involves splitting the specified text column's entries into words and collating them into a corpus. Subsequently, the frequency of each word is counted using Counter functionality. The forty most commonly occurring words that aren't categorized as English stopwords are then visualized in a bar chart. The x-axis shows formulated words and the y-axis indicates their frequency of appearance. **Signs of High Risk**: Indicators of high risk may include a lack of distinct words within the list, or common words primarily encompassing stopwords. In addition, the frequent appearance of irrelevant or inappropriate words could indicate a poorly curated or noisy data set. Lastly, if the test returns an error due to the absence of a valid Dataset object, it signifies a high risk as it can't be effectively implemented. **Strengths**: This metric offers a clear insight into the language features – specifically, the word frequency – of unstructured text data. It can reveal prominent vocabulary and language patterns, which can be vital for effective feature extraction in NLP tasks. The visualization aspect aids in quickly capturing the patterns and getting an intuitive feel of the data. **Limitations**: This test is solely focused on word frequency and disregards semantic or context-related information. It deliberately ignores stopwords, which, in some cases, might carry important significance. Furthermore, it may only be applicable to English language text data as it uses English stopwords for filtering, and does not account for data in other languages. The metric also requires a valid Dataset object, indicating a dependency condition that limits applicability.\nvalidmind.data_validation.nlp.CommonWords\n\n\nThresholdTest\nHashtags\n**Purpose**: The Hashtags test is designed to measure the frequency of hashtags used within a given text column in a dataset. It is particularly useful for natural language processing tasks such as text classification and text summarization. The goal is to identify common trends and patterns in the use of hashtags, which can serve as critical indicators or features within a machine learning model. **Test Mechanism**: The test implements a regular expression (regex) to extract all hashtags from the specified text column. For each hashtag found, it makes a tally of its occurrences. It then outputs a list of the top N hashtags (default is 25, but customizable), sorted by their counts in a descending order. The results are also visualized in a bar-plot that indicates frequency counts on the y-axis and the corresponding hashtags on the x-axis. **Signs of High Risk**: The indications of a potential high-risk situation may include: - A low diversity in the usage of hashtags as indicated by a few hashtags being used disproportionately more than the others. - Repeated usage of one or few hashtags can be indicative of spam or biased dataset. - If there are no or extremely few hashtags found in the dataset, it perhaps signifies that the text data does not contain structured social media data. **Strengths**: The Hashtags test has several strong points: - It provides a concise visual representation of the frequency of hashtags, which can be critical for understanding the trend about a particular topic in text data. - It is instrumental in tasks specifically related to social media text analytics such as opinion analysis, trend discovery, etc. - The test is adaptable, allowing the flexibility to determine the number of top hashtags to be analyzed. **Limitations**: Despite its strengths, the test has some limitations: - The test assumes the presence of hashtags and therefore may not be applicable for text datasets that do not contain hashtags (e.g., formal documents, scientific literature). - Language-specific limitations of hashtag formulations are not explicitly addressed. - It does not account for typographical errors, variations or synonyms in hashtags. - This test does not provide context or sentiment associated with the hashtags. Thus the information provided may have limited utility on its own.\nvalidmind.data_validation.nlp.Hashtags\n\n\nThresholdTest\nMentions\n**Purpose**: The \"Mentions\" test focuses on the aspect of data quality in an NLP (Natural Language Processing) or text-based Machine Learning model. Its main objective is to identify and analyze the frequency of 'mentions' within a given dataset, particularly within a selected text column. 'Mentions' in this context are discrete elements of text that are preceded by '@'. The output would reveal the most frequently mentioned entities or usernames, which could be crucial to certain applications like social media analyses, customer sentiment analyses, etc. **Test Mechanism**: Upon validating the presence of a text column in the provided dataset, this test applies a regular expression pattern to extract these mentions. The number of occurrences for each unique mention is then counted. The top mentions, based on user-specified parameters or by default the top 25, are selected for representation. This thresholding process forms the main mechanism of this test, showcasing high-frequency elements. The results are visually rendered in a treemap plot, wherein each rectangle's size is indicative of the corresponding mention's frequency. **Signs of High Risk**: Execution failure for this test could be linked to the absence of a valid text column in the dataset. One other indicators of high risk includes the absence of mentions in the text data, meaning that there might not be any text preceded by '@'. This conditions could indicate sparse or poor-quality data, affecting the model's ability to generalize or learn appropriately. **Strengths**: This test is specifically optimized for textual datasets, making it particularly powerful in NLP contexts. It allows rapid identification of dominant elements, displaying them visually for easy interpretation. This allows potentially pivotal insights about the most mentioned entities or usernames to be extracted. **Limitations**: This test can be limited by its dependence on '@' for the identification of mentions. Aspects of text not preceded by '@' could be potentially useful, but remain overlooked by this test. Additionally, this test is not suitable for datasets without textual data. The test doesn't present insights on less frequently occurring data or outliers, potentially leaving important patterns unrevealed.\nvalidmind.data_validation.nlp.Mentions\n\n\nMetric\nText Description\n**Purpose**: This is an in-depth textual analysis test used to assess the lexical complexity, structure, and vocabulary of a given dataset. The TextDescription metric measures various parameters such as: - **Total Words**: Assess the length and complexity of the input text. Longer documents might require more sophisticated summarization techniques, while shorter ones may need more concise summaries. - **Total Sentences**: Understand the structural makeup of the content. Longer texts with numerous sentences might require the model to generate longer summaries to capture essential information. - **Avg Sentence Length**: Determine the average length of sentences in the text. This can help the model decide on the appropriate length for generated summaries, ensuring they are coherent and readable. - **Total Paragraphs**: Analyze how the content is organized into paragraphs. The model should be able to maintain the logical structure of the content when producing summaries. - **Total Unique Words**: Measure the diversity of vocabulary in the text. A higher count of unique words could indicate more complex content, which the model needs to capture accurately. - **Most Common Words**: Identify frequently occurring words that likely represent key themes. The model should pay special attention to including these words and concepts in its summaries. - **Total Punctuations**: Evaluate the usage of punctuation marks, which contribute to the tone and structure of the content. The model should be able to maintain appropriate punctuation in summaries. - **Lexical Diversity**: Calculate the richness of vocabulary in relation to the overall text length. A higher lexical diversity suggests a broader range of ideas and concepts that the model needs to capture in its summaries. It is also responsible for paring down unwanted tokens, measuring the average sentence length, and calculating the lexical diversity in the available text. This metric's measures help in understanding the nature of the text and assessing the potential challenges of machine learning algorithms deployed for textual analysis, language processing, or summarization. **Test Mechanism**: The test operates by parsing the dataset and using the NLTK library to tokenize the text into words, sentences, and paragraphs. It dissects the processed text further, removing stopwords declared in 'unwanted_tokens' and punctuations. It then calculates parameters such as the total number of words, sentences, paragraphs, punctuations; average sentence length; and lexical diversity. The metric also condenses these results and plots scatter plots for certain combinations of variables (Total Words vs Total Sentences, Total Words vs Total Unique Words, etc.) to provide a visual depiction of the text structure. **Signs of High Risk**: Anomaly in results or increased complexity in lexical diversity, long sentences and paragraphs, high uniqueness of words, or an excess of unwanted tokens can signify risk in text processing ML models. It suggests that the model's text absorption and processing capability can be compromised. Additionally, missing or erroneous visualizations may also indicate issues with the text description analysis. **Strengths**: This metric is an essential tool for ML model data pre-processing, focusing on textual analysis. It provides a detailed dissection of a text dataset to understand structural and vocabulary complexity, which is helpful to strategize a ML model's textual processing capability. Besides, it visualizes correlations between chosen variables, helping further in grasping the text's structure and complexity. **Limitations**: This metric relies heavily on the NLTK library, rendering it unsuitable for languages unsupported by NLTK. The unwanted tokens and stop words are predefined, limiting customization by context or application. This metric does not consider semantics or grammatical complexities, which can be crucial in language processing. Finally, it assumes that the document is well-structured (with sentences and paragraphs); unstructured or poorly formatted text may skew the results.\nvalidmind.data_validation.nlp.TextDescription\n\n\nThresholdTest\nStop Words\n**Purpose**: The StopWords test is a data quality assessment tool specifically designed for text data. It identifies and analyzes the usage of 'stop words' within a dataset. Stop words are frequent, common words in a language (e.g., \"the\", \"and\", \"is\"), which are typically considered insignificant for in-depth analysis. The test quantifies the frequency of each stop word in the dataset, further calculating their proportional usage compared to the total word count. Ultimately, it aims to highlight the prevailing stop words based on their usage frequency. **Test Mechanism**: The test triggers once it receives a 'VMDataset' object; in absence of which, it raises an error. The text column of the dataset undergoes an inspection for the creation of a 'corpus' (a collection of written texts representing the dataset). With the help of Natural Language Toolkit's (NLTK) stop word repository, the test screens the corpus for any stop words and logs their frequency. Each stop word, expressed as a percentage usage out of the total corpus, is compared against a 'min_percent_threshold'. If the percentage exceeds this threshold, the test is deemed as failed. The test returns the top prevailing stop words, decided by the 'num_words' parameter, and their percentages. It also provides a bar chart visualization of the top stop words and their usage frequency for intuitive understanding. **Signs of High Risk**: Potential signs indicating high risk are if the percentage of any stop words exceeds the predefined 'min_percent_threshold' and the application's context where a major presence of stop words could negatively impact the analytical performance due to noise generation. **Strengths**: The main strengths of the StopWords test include its ability to scrutinize and quantify the usage of stop words in a dataset, which might often be overlooked due to their insignificant semantic meaning. The test gives insights into potential noise in the text data, which could interfere with model training efficiency. Furthermore, the bar chart visualization provided by the test facilitates readily interpretable and actionable insights. **Limitations**: While robust for stop word analysis, the test does exhibit a few limitations. It only works with English stop words and therefore may not perform well on datasets in other languages. The effectiveness of the test depends on the 'min_percent_threshold', which might need to be fine-tuned for different datasets. Additionally, the test does not consider the context of the stop words used within the dataset, and hence, it might overlook their significance if used in certain contexts. Lastly, the test checks only the frequency of stop words and does not offer direct measures of model performance or predictive accuracy.\nvalidmind.data_validation.nlp.StopWords"
  }
]