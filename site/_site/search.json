[
  {
    "objectID": "guide/developer-framework-introduction.html",
    "href": "guide/developer-framework-introduction.html",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "This page provides an introduction for:\n\nWhat the ValidMind Developer Framework is, key concepts, and what functionality it provides\nHow ValidMind documentation projects are structured"
  },
  {
    "objectID": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "href": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "What ValidMind’s Developer Framework is",
    "text": "What ValidMind’s Developer Framework is\n\nValidMind’s Python Developer Framework is a library of developer tools and methods designed to automate the documentation and validation of your models.\nThe Developer Framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python library will provide all the standard functionality without requiring your developers to rewrite any functions.\nThe Developer Framework provides a rich suite of documentation tools and test plans, from documenting descriptions of your dataset to testing your models for weak spots and overfit areas. The Developer Framework helps you automate the generation of model documentation by feeding the ValidMind platform with documentation artifacts and test results to the ValidMind platform."
  },
  {
    "objectID": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "href": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "ValidMind Documentation Project Structure",
    "text": "ValidMind Documentation Project Structure\n\n\nProjects\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s MRM lifecycle will constitute a new project, and may be configured with its own templates and workflows.\n\nModel documentation\n\nA comprehensive record and description of a quantitative model. It should encompass all relevant information about the model in accordance with regulatory requirements (set by regulatory bodies) and model risk policies (set by an institution’s MRM team), assumptions, methodologies, data and inputs, model performance evaluation, limitations, and intended use. The purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest plans\n\nA collection of many tests which are meant to be run simultaneously to validate and document specific aspects of the documentation. For instance, the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind UI.\n\nTest suites\n\nCollection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\n\nTemplates\n\nAn outline of the sections/sub-sections of a ValidMind document (model documentation or validation report) and how they are organized. Templates also contain boilerplates and documentation & test results placeholders for which content will be provided by the Developer Framework. Template requirements are typically provided by the model risk management team, and can be configured programmatically for each model use case, typically by an administrator."
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind overview",
    "section": "",
    "text": "ValidMind is a model risk management (MRM) solution designed for the specific needs of model developers and model validators alike. The platform automates key aspects of the MRM process, including model documentation, validation, and testing. In addition, the platform comes with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date.\nOur solution comprises two primary architectural components: the ValidMind Developer Framework and the cloud-based ValidMind MRM platform."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind overview",
    "section": "Related Topics",
    "text": "Related Topics\nReady to try out ValidMind? Try the Quickstart."
  },
  {
    "objectID": "guide/view-templates.html",
    "href": "guide/view-templates.html",
    "title": "View templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-templates.html#prerequisites",
    "href": "guide/view-templates.html#prerequisites",
    "title": "View templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-templates.html#steps",
    "href": "guide/view-templates.html#steps",
    "title": "View templates",
    "section": "Steps",
    "text": "Steps\n\nFrom the ValidMind Platform homepage, go to Templates on the left.\nClick on one of the available templates to view the YAML configuration file.\nIn the configuration file that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the Developer Framework that feed into the template\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTemplates can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-templates.html#related-topics",
    "href": "guide/view-templates.html#related-topics",
    "title": "View templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developers section."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe validation guidelines for each template can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s Next",
    "text": "What’s Next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/dataset-object.html",
    "href": "guide/dataset-object.html",
    "title": "The dataset object",
    "section": "",
    "text": "The Dataset is a container for the data and relevant metadata such as special column roles (e.g. labels, target variable, etc.). It enables the library to take into account the relevant context when running tests/test plans, and to save that information in a convenient manner."
  },
  {
    "objectID": "guide/dataset-object.html#class-properties",
    "href": "guide/dataset-object.html#class-properties",
    "title": "The dataset object",
    "section": "Class Properties:",
    "text": "Class Properties:\nThe common properties for dataset are: * Need input from engineering here\nCheck out the API Reference for more details."
  },
  {
    "objectID": "guide/dataset-object.html#api-reference",
    "href": "guide/dataset-object.html#api-reference",
    "title": "The dataset object",
    "section": "API Reference:",
    "text": "API Reference:\n{Insert relevant API Reference link here}"
  },
  {
    "objectID": "guide/dataset-object.html#creating-a-dataset-object",
    "href": "guide/dataset-object.html#creating-a-dataset-object",
    "title": "The dataset object",
    "section": "Creating a dataset object:",
    "text": "Creating a dataset object:\n\nFrom a Pandas dataframe:\nThe default Dataset method expects to get a pd.DataFrame\nNeed code example from Engineering here"
  },
  {
    "objectID": "guide/dataset-object.html#related-topics",
    "href": "guide/dataset-object.html#related-topics",
    "title": "The dataset object",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/jupyter-notebooks.html",
    "href": "guide/jupyter-notebooks.html",
    "title": "Example notebooks",
    "section": "",
    "text": "Our example notebooks are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nTry the notebook source yourself:\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time, and download notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/jupyter-notebooks.html#related-topics",
    "href": "guide/jupyter-notebooks.html#related-topics",
    "title": "Example notebooks",
    "section": "Related topics",
    "text": "Related topics\nFor an introduction to how these notebooks get used with ValidMind, take a look at the Quickstart."
  },
  {
    "objectID": "guide/view-all-test-plans.html",
    "href": "guide/view-all-test-plans.html",
    "title": "View all test plans",
    "section": "",
    "text": "Learn how to use list_plans(), list_test(), and describe_plan() methods to view and describe test plans and tests available in the Developer Framework."
  },
  {
    "objectID": "guide/view-all-test-plans.html#prerequisites",
    "href": "guide/view-all-test-plans.html#prerequisites",
    "title": "View all test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are working on an active documentation project\nYou have already installed the ValidMind client library in your developer environment"
  },
  {
    "objectID": "guide/view-all-test-plans.html#steps",
    "href": "guide/view-all-test-plans.html#steps",
    "title": "View all test plans",
    "section": "Steps",
    "text": "Steps\n\nInitialize the client library.\nUse list_plans() and list_tests() to view the list of all available test plans and tests.\nExamples:\n\nList all available test plans currently available in the the Developer Framework:\nvm.test_plans.list_plans()\nList all available individual tests currently available in the Developer Framework:\nvm.test_plans.list_tests() \n\nUse describe_testplan() to list all the tests included in a specific test plan:\nExample: The following code will list tests included in the tabular_data_quality test plan:\nvm.test_plans.describe_plan(\"tabular_data_quality\")"
  },
  {
    "objectID": "guide/view-all-test-plans.html#related-topics",
    "href": "guide/view-all-test-plans.html#related-topics",
    "title": "View all test plans",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a solution designed to help simplify and automate key aspects of model risk management (MRM) activities for model developers and model validators alike. The platform helps automate model documentation, validation, and testing. In addition, the platform offers with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/get-started.html#how-does-it-work",
    "href": "guide/get-started.html#how-does-it-work",
    "title": "Get started",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\nValidMind consists of two main products components:\n\nThe Developer Framework is a library of tools and methods designed to automate model documentation and validation. It is platform agnostic, and integrates with the model development environment.\nThe ValidMind Platform is an easy-to-use web-based UI that enables users to review and edit the documentation generated by the Developer Framework. It also enables collaboration and feedback capture between model developers and model validators, and offers workflow capabilities to manage the model documentation and validation process.\n\nFor more information about what ValidMind offers, check out our ValidMind overview"
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "href": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "title": "Get started",
    "section": "How do I get access to ValidMind?",
    "text": "How do I get access to ValidMind?\nIf you are new to our products, you will need access. You can request it."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nThe fastest way to explore what ValidMind can offer is with our Quickstart.\nThe Quickstart takes about 20 minutes to complete and walks you through the Developer Framework with a sample Jupyter notebook and introduces you to the ValidMind Platform.\nIf you have already tried the Quickstart, how-to instructions for different users are in our Guides:\n\nFor platform administrators — Learn how to configure the platform, from setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your exisiting workflows, and more.\nFor model developers — Find information for ValidMind test plans and tests, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nAlso check the Guides for how you integrate the Developer Framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers."
  },
  {
    "objectID": "guide/get-started.html#have-more-questions",
    "href": "guide/get-started.html#have-more-questions",
    "title": "Get started",
    "section": "Have more questions?",
    "text": "Have more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html",
    "href": "guide/use-test-plans-and-tests.html",
    "title": "When to use test plans and tests",
    "section": "",
    "text": "This topic provides an overview about:"
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "href": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "title": "When to use test plans and tests",
    "section": "What Tests, Test plans, and Test suites are",
    "text": "What Tests, Test plans, and Test suites are\n\nTests are designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\nTest plans are collections of tests which are meant to be run simultaneously to address specific aspects of the documentation.\nExample: the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind Platform.\nTest suites are collection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\nExample: the binary_classifier_full_suite test suite runs the tabular_dataset and binary_classifier test plans to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "href": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "title": "When to use test plans and tests",
    "section": "When to use ValidMind Tests, Test plans, and Test suites",
    "text": "When to use ValidMind Tests, Test plans, and Test suites\nValidMind provides many built-in tests and test plans which make it easy for a model developer to document their work at any point during the model development lifecycle when they need to validate that their work satisfies model risk management requirements.\nWhile model developers have the flexibility to decide when to use ValidMind tests, we have identified a few typical scenarios which have their own characteristics and needs:\n\nWhen you want to document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test plan.\nFor time-series datasets: use the time_series_dataset test plan.\n\nWhen you want to document and validate about your model:\n\nFor binary classification models: use the binary_classifier test plan.\nFor time series models: use the timeseries test plan.\n\nWhen you want to document a binary classification model and the relevant dataset end-to-end: use the binary_classifier_full_suite test suite."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#api-reference",
    "href": "guide/use-test-plans-and-tests.html#api-reference",
    "title": "When to use test plans and tests",
    "section": "API Reference",
    "text": "API Reference\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/editions-and-features.html",
    "href": "guide/editions-and-features.html",
    "title": "Editions and features",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions-and-features.html#editions",
    "href": "guide/editions-and-features.html#editions",
    "title": "Editions and features",
    "section": "Editions",
    "text": "Editions\n\nDeveloper Edition\nThe Developer Edition is the ideal training ground for developers to play around with ValidMind’s automated model documentation and to test the robustness of our developer framework, documentation, and testing features. The Developer Edition is free, allowing developers who are new to model documentation and model risk management to build, implement, test, and maintain higher quality models and model documentation.\nThe Developer Edition is only for personal testing purposes and cannot be used as a commercial model documentation or model risk management solution.\n\n\nEssential Edition\nWith the Essential Edition, you get an advanced model risk management (MRM) solution. It offers your organization all the features and services of the Developer Edition, plus additional features tailored to the needs of larger-scale organizations.\n\n\nBusiness Critical\nProvides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. This edition encompasses all features and services of the Essential Edition but within a separate ValidMind environment, isolated from other ValidMind accounts via Virtual Private ValidMind (VPV). VPV accounts do not share resources with non-VPV accounts."
  },
  {
    "objectID": "guide/editions-and-features.html#features",
    "href": "guide/editions-and-features.html#features",
    "title": "Editions and features",
    "section": "Features",
    "text": "Features\n\n\n\n\nModel development & documentation\nDeveloper\nEssential\nBusiness Critical\n\n\n\n\nAutomated model documentation\n\n\n\n\n\nPlatform-independent developer framework\n\n\n\n\n\nOnline documentation editing\n\n\n\n\n\nAdvanced editing & readability assistance\n\n\n\n\n\nDocumentation quality measurement\n\n\n\n\n\nOffline document ingestion\n\n\n\n\n\nFeedback capture on online document\n\n\n\n\n\nDocumentation version history management\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nStandard tests & validation libraries\n\n\n\n\n\nConfigure / customize tests & validation libraries\n\n\n\n\n\nSupport for customer-provided tests\n\n\n\n\n\nDeveloper workflow management\n\n\n\n\n\nPre-configured documentation templates & boilerplates\n\n\n\n\n\nConfigurable documentation templates & boilerplates\n\n\n\n\n\nModel validation & audit\n\n\n\n\n\nModel validation report automation\n\n\n\n\n\nFindings / issues & remediation actions tracking\n\n\n\n\n\nConfigurable approval workflows\n\n\n\n\n\nMRM workflows & validation lifecycle tracking\n\n\n\n\n\nMRM resource & workflow management\n\n\n\n\n\nCentral model inventory\n\n\n\n\n\nHistorical documentation repository /documentation CMS\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nExecutive reporting\n\n\n\n\n\nPlatform integration & support\n\n\n\n\n\nData lake integration, such as Evidence Storeand monitoring data\n\n\n\n\n\nSSO integration\n\n\n\n\n\nCustomer managed encryption\n\n\n\n\n\nSupport 8/5 (one timezone)\n\n\n\n\n\nSupport 24/7 (global)\n\n\n\n\n\nPlatform deployment\n\n\n\n\n\nMulti-tenant SaaS\n\n\n\n\n\nVirtual private ValidMind (VPV)\n\n\n\n\n\nSelf-managed VPV\n\n\n\n\n\n\nContact Us\nContact Us\nContact Us"
  },
  {
    "objectID": "guide/register-model.html",
    "href": "guide/register-model.html",
    "title": "Register models",
    "section": "",
    "text": "Register a model you are documenting in the model inventory."
  },
  {
    "objectID": "guide/register-model.html#prerequisites",
    "href": "guide/register-model.html#prerequisites",
    "title": "Register models",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/register-model.html#steps",
    "href": "guide/register-model.html#steps",
    "title": "Register models",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/register-model.html#troubleshooting",
    "href": "guide/register-model.html#troubleshooting",
    "title": "Register models",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/register-model.html#whats-next",
    "href": "guide/register-model.html#whats-next",
    "title": "Register models",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework, including:\n\nValidMind Python library\nValidMind models class\nTest plans\nData validation tests\nModel validation tests"
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.prod.validmind.ai.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test plans and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/developer-framework.html",
    "href": "guide/developer-framework.html",
    "title": "Developers",
    "section": "",
    "text": "Geared towards model developers, this section includes information for:"
  },
  {
    "objectID": "guide/developer-framework.html#related-topics",
    "href": "guide/developer-framework.html#related-topics",
    "title": "Developers",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developer tasks related to documentation projects and collaborating with model validators and model owners, refer to our Guides."
  },
  {
    "objectID": "guide/explore-validmind.html",
    "href": "guide/explore-validmind.html",
    "title": "Explore ValidMind",
    "section": "",
    "text": "To see how the ValidMind Developer Framework works, try our introductory Jupyter notebook.\nThis notebook shows you how to initialize the ValidMind Developer Framework and run functions from a sample dataset for a customer churn model that we trained for this demo.\n\n\n\nOpen the Quickstart notebook: \nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there.\nMake a copy of the Quickstart notebook for yourself:\n\nIn Colab, click File > Save a copy in Drive > make your own copy in Google Drive so that you can modify the notebook.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n\n\n\n\n\n\n\n\n\n“Warning: This notebook was not authored by Google”\n\n\n\nNotebooks from ValidMind are safe to run. You can inspect the notebook source yourself.\n\n\n\n\n\n\n\n\n\nGetting runtime errors in Google Colaboratory?\n\n\n\nWe recommend that you NOT use the Run all option. Stepping through each cell individually enables you to see what is happening in the notebook. If you run into errors, re-run the notebook cells.\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework.\nYou should see a message like this near the bottom of the Initialize ValidMind section:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)"
  },
  {
    "objectID": "guide/explore-validmind.html#explore-the-validmind-platform",
    "href": "guide/explore-validmind.html#explore-the-validmind-platform",
    "title": "Explore ValidMind",
    "section": "Explore the ValidMind Platform",
    "text": "Explore the ValidMind Platform\nNext, let’s take a look at how the Developer Framework works hand-in-hand with the ValidMind Platform and how documentation and test results get uploaded.\nThe ValidMind Platform is the central place to:\n\nView results and documentation uploaded via the Developer Framework\nCollaborate with other model developers, model reviewers and validators, and other users involved in the documentation and validation workflow"
  },
  {
    "objectID": "guide/explore-validmind.html#steps-1",
    "href": "guide/explore-validmind.html#steps-1",
    "title": "Explore ValidMind",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the [Quickstart] Customer Churn Model - Initial Validation and select it.\nOn the model details page that open, you can find important information about the model, such as:\n\nThe ID of the model and its specific use case\nThe owners, developers, validators, and business unit associated with the model\nThe risk tier and current version\nAnd more\n\nScroll down to Project History and select the model.\nOn the project overview page that opens, you can see what is included, such as model, project findings, recent activity, and project stakeholders, and more. In the left sidebar, you can find links to the documentation, project findings, validation report, audit trail, and client integration.\nFor this Quickstart, we will focus on the Documentation section to show you how content from the Developer Framework gets uploaded into it.\nNote that the model status is In Documentation. This is the status that a model starts in as part of a documentation project. You can click See workflow to look at what the full workflow is, from documentation, to validation, to review, and finally approval.\nFrom the left sidebar, select Documentation > 2. Data preparation > 2.1. Data description.\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically by the Developer Framework whenever a test result is above or below a certain threshold.\nFrom the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the Developer Framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nThe Documentation Guidelines in the ValidMind Insights right sidebar can tell you more about what these sections mean and help you with the task of documenting the model.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the Developer Framework, but they get added by the model developer in the Platform UI."
  },
  {
    "objectID": "guide/explore-validmind.html#create-a-new-documentation-project",
    "href": "guide/explore-validmind.html#create-a-new-documentation-project",
    "title": "Explore ValidMind",
    "section": "Create a new documentation project",
    "text": "Create a new documentation project\nNext, let’s learn how to create your own documentation project. You can use this project to upload tests and documentation and then add that to your own copy of the Quickstart notebook you looked at earlier.\n\nSteps\n\nNavigate to the landing page by clicking on the ValidMind logo or if you have to, Log in to the ValidMind UI.\nFrom the left sidebar, select Documentation Projects and on the page that opens, click the Create new Project button at top right of the screen.\nSelect the right options in the form:\n\nModel: [Quickstart] Customer Churn Model\nType: Initial validation (selected automatically) \nProject name: Enter your preferred name\n\nClick Create Project.\nValidMind will create an empty documentation project associated with the customer churn model.\nYou can now access this project from the UI on the Documentation Projects page or by navigating to the relevant model - [Quickstart] Customer Churn Model - in the Model Inventory page.\nFrom the left sidebar, select Client Integration.\nThe page that opens provides you with the credentials for the newly created project to use with the ValidMind Developer Framework.\nLocate the project identifier, API key, and secret:\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\n\n\n\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nTry this: Use the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/explore-validmind.html#modify-the-quickstart-notebook-to-upload-to-your-own-project",
    "href": "guide/explore-validmind.html#modify-the-quickstart-notebook-to-upload-to-your-own-project",
    "title": "Explore ValidMind",
    "section": "Modify the Quickstart notebook to upload to your own project",
    "text": "Modify the Quickstart notebook to upload to your own project\nAfter you have completed the above steps, you are ready to use your documentation project with your copy of the Quickstart notebook.\n\nSteps\n\nReopen your copy of the Quickstart notebook in Google Colaboratory.\nIn the Quickstart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you created your new project:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework.\n\n\n\n\n\n\n\n\nGetting runtime errors in Google Colaboratory?\n\n\n\nWe recommend that you NOT use the Run all option. Stepping through each cell individually enables you to see what is happening in the notebook. If you run into errors, re-run the notebook cells."
  },
  {
    "objectID": "guide/explore-validmind.html#whats-next",
    "href": "guide/explore-validmind.html#whats-next",
    "title": "Explore ValidMind",
    "section": "What’s next",
    "text": "What’s next\nReady to use ValidMind for production with your own use cases? Follow our how-to guides."
  },
  {
    "objectID": "guide/explore-validmind.html#related-topics",
    "href": "guide/explore-validmind.html#related-topics",
    "title": "Explore ValidMind",
    "section": "Related topics",
    "text": "Related topics\n\nIntroduction to the ValidMind Developer Framework.\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/tabular-data-tests.html",
    "href": "guide/tabular-data-tests.html",
    "title": "Tabular data tests",
    "section": "",
    "text": "This article provides an example of how to run a Test Plan to validate a tabular dataset, and provides a reference to other dataset tests and test plans available in the Developer Framework."
  },
  {
    "objectID": "guide/tabular-data-tests.html#prerequisites",
    "href": "guide/tabular-data-tests.html#prerequisites",
    "title": "Tabular data tests",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nThe model is already registered in the model inventory\nThere is an active documentation project for the model\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/tabular-data-tests.html#examples",
    "href": "guide/tabular-data-tests.html#examples",
    "title": "Tabular data tests",
    "section": "Examples",
    "text": "Examples\nPlease refer to the following notebooks for examples of how to run dataset test plans:\n\nTabular dataset: Customer Churn Model \nTime series dataset: Time Series Dataset Test Suite"
  },
  {
    "objectID": "guide/tabular-data-tests.html#list-of-all-dataset-test-plans-and-tests",
    "href": "guide/tabular-data-tests.html#list-of-all-dataset-test-plans-and-tests",
    "title": "Tabular data tests",
    "section": "List of all dataset test plans and tests:",
    "text": "List of all dataset test plans and tests:\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\ncom.amazonaws.vpce.us-west-2.vpce-svc-0b956fa3e03afa538\nhttps://private.prod.vm.validmind.ai"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s Next",
    "text": "What’s Next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html",
    "href": "guide/review-data-streams-and-audit-trails.html",
    "title": "Review Audit Trail",
    "section": "",
    "text": "Learn how to access and use the audit trail functionality in the ValidMind Platform. This topic matters for for model developers, model validators, and auditors who are looking to track or audit all the information events associated with a specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "href": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "title": "Review Audit Trail",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#steps",
    "href": "guide/review-data-streams-and-audit-trails.html#steps",
    "title": "Review Audit Trail",
    "section": "Steps",
    "text": "Steps\n\nIn the ValidMind platform, navigate to the relevant model documentation project.\nFrom the Overview page, select Audit Trail on the left.\n\nThe table in this page shows a record of all activities generated from the Developer Framework and actions performed by users in the organization related to this specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "href": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "title": "Review Audit Trail",
    "section": "What’s Next",
    "text": "What’s Next\n\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documetnation"
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nView validation guidelines"
  },
  {
    "objectID": "guide/model-evaluation-tests.html",
    "href": "guide/model-evaluation-tests.html",
    "title": "Model evaluation tests",
    "section": "",
    "text": "This article provides an examples of how to run model evaluation test plans, and provides a reference to other model evaluation tests and test plans available in the Developer Framework."
  },
  {
    "objectID": "guide/model-evaluation-tests.html#prerequisites",
    "href": "guide/model-evaluation-tests.html#prerequisites",
    "title": "Model evaluation tests",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nThe model is already registered in the model inventory\nThere is an active documentation project for the model\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/model-evaluation-tests.html#examples",
    "href": "guide/model-evaluation-tests.html#examples",
    "title": "Model evaluation tests",
    "section": "Examples",
    "text": "Examples\nPlease refer to the following notebooks for examples of how to run model evaluation test plans:\n\nBinary Classifier model example: Customer Churn Model \nTime Series Forecasting model example: Time Series Dataset Test Suite"
  },
  {
    "objectID": "guide/model-evaluation-tests.html#list-of-all-model-evaluation-test-plans-and-tests",
    "href": "guide/model-evaluation-tests.html#list-of-all-model-evaluation-test-plans-and-tests",
    "title": "Model evaluation tests",
    "section": "List of all model evaluation test plans and tests:",
    "text": "List of all model evaluation test plans and tests:\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "As of the current release (v1.9.3), the Developer Framework supports the following model types:"
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\n\nCheck out our Developer Framework documentation for more details on how to use our documentation and testing functions with supported models."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\nCheck Include comment threads to include comment threads in the exported file.\nCheck Section activity logs to include a history of changes in each section of the documentation.\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\nCheck Include comment threads to include comment threads in the exported file.\nCheck Section activity logs to include a history of changes in each section of the documentation.\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test plan execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test plans\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test plans and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\n\nLocating the project identifier, API key and secret:\nOn the Client Integration page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/create-documentation-project.html#related-topics",
    "href": "guide/create-documentation-project.html#related-topics",
    "title": "Create documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nInstall and initialize the Developer Framework\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enbale their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier\n\n\n\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick Client integration and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"<project-identifier>\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s Next",
    "text": "What’s Next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "To try out ValidMind, you need to be a registered user on the ValidMind Platform. If you don’t already have access, you can request it."
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo connect ValidMind’s Developer Framework to the ValidMind platform and to access the ValidMind platform User Interface (Web UI), you must be able to access our domain - validmind.ai . If necessary, ask a network administrator to add this domain to your company’s firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\nQuickStart Requirements\nTo follow the Quickstart, you must be able to access Google Colaboratory (Colab) where you can run our sample notebooks for free.\nNote: If you would like to run our sample notebooks locally, you need to ensure your developer environment is setup with Python 3.8+. \n\n\nAccess to Validmind’s Web UI\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstart — 20 mins",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework and explore the ValidMind Platform.\nThis Quickstart takes about 20 minutes of your time."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstart — 20 mins",
    "section": "Steps",
    "text": "Steps\n\nBefore you begin\nCheck the prerequisites for the Developer Framework and ValidMind Platform UI.\nExplore ValidMind\nTry our introductory Jupyter notebook to see the Developer Framework in action and explore our Platform UI to work with a documentation projects.\nNext steps\nTry some more advanced sample notebooks or set up ValidMind for production with your own use cases."
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases?"
  },
  {
    "objectID": "guide/next-steps.html#additional-resources",
    "href": "guide/next-steps.html#additional-resources",
    "title": "Next steps",
    "section": "Additional resources",
    "text": "Additional resources\nSee our FAQ for a curated list of frequently asked questions about what ValidMind offers."
  },
  {
    "objectID": "guide/next-steps.html#need-help",
    "href": "guide/next-steps.html#need-help",
    "title": "Next steps",
    "section": "Need help?",
    "text": "Need help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "validmind/api.html",
    "href": "validmind/api.html",
    "title": "ValidMind",
    "section": "",
    "text": "Python Library API\nMain entrypoint to the ValidMind Python Library\n\nvalidmind.init\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\n\nParameters\n\nproject (str) – The project CUID\napi_key (str, optional) – The API key. Defaults to None.\napi_secret (str, optional) – The API secret. Defaults to None.\napi_host (str, optional) – The API host. Defaults to None.\n\nRaises\nValueError – If the API key and secret are not provided\nReturns\nTrue if the ping was successful\nReturn type\nbool\n\n\n\nvalidmind.init_dataset\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\nRaises\nValueError – If the dataset type is not supported\nReturns\nA VM Dataset instance\nReturn type\nvm.vm.Dataset\n\n\n\nvalidmind.init_model\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained sklearn model\nRaises\nValueError – If the model type is not supported\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.init_r_model\nInitializes a VM Model for an R model\nR models must be saved to disk and the filetype depends on the model type… Currently we support the following model types:\n\nLogisticRegression glm model in R: saved as an RDS file with saveRDS\nLinearRegression lm model in R: saved as an RDS file with saveRDS\nXGBClassifier: saved as a .json or .bin file with xgb.save\nXGBRegressor: saved as a .json or .bin file with xgb.save\n\nLogisticRegression and LinearRegression models are converted to sklearn models by extracting the coefficients and intercept from the R model. XGB models are loaded using the xgboost since xgb models saved in .json or .bin format can be loaded directly with either Python or R\n\nParameters\n\nmodel_path (str) – The path to the R model saved as an RDS or XGB file\nmodel_type (str) – The type of the model (one of R_MODEL_TYPES)\n\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.run_test_plan\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘binary_classifier’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises\nValueError – If the test plan name is not found or if there is an error initializing the test plan\nReturns\nA dictionary of test results\nReturn type\ndict\n\n\n\nvalidmind.run_test_suite\nHigh Level function for running a test suite\nThis function provides a high level interface for running a test suite. A test suite is a collection of test plans. This function will automatically find the correct test suite class based on the test_suite_name, initialize each of the test plans, and run them.\n\nParameters\n\ntest_suite_name (str) – The test suite name (e.g. ‘binary_classifier_full_suite’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test suite. These will provide the TestSuite instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan, metric or threshold test for more details.\n\nRaises\nValueError – If the test suite name is not found or if there is an error initializing the test suite\nReturns\nthe TestSuite instance\nReturn type\nTestSuite\n\n\n\nvalidmind.log_dataset\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\nvm_dataset (validmind.VMDataset) – A VM dataset object\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options (dict, optional) – Additional dataset options for analysis. Defaults to None.\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset. Defaults to None.\nfeatures (list, optional) – Optional. A list of features metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nThe VMDataset object\nReturn type\nvalidmind.VMDataset\n\n\n\nvalidmind.log_figure\nLogs a figure\n\nParameters\n\ndata_or_path (str or matplotlib.figure.Figure) – The path of the image or the data of the plot\nkey (str) – Identifier of the figure\nmetadata (dict) – Python data structure\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metrics\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics (list) – A list of Metric objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_model\nLogs model metadata and hyperparameters to ValidMind API.\n\nParameters\nvm_model (validmind.VMModel) – A VM model object\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_test_results\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults (list) – A list of TestResults objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nclass validmind.Dataset\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nproperty index()\nReturns the dataset’s index.\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nget_numeric_features_columns()\nReturns list of numeric features columns\n\nReturns\nThe list of numberic features columns\nReturn type\nlist\n\n\n\nget_categorical_features_columns()\nReturns list of categorical features columns\n\nReturns\nThe list of categorical features columns\nReturn type\nlist\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.DatasetTargets\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.Figure\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.Metric\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nTODO: Metric should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\ndescription()\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(metric_value: dict | list | DataFrame | None = None)\nReturn the metric summary. Should be overridden by subclasses. Defaults to None. The metric summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the metric results.\nWe return None here because the metric summary is optional.\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.Model\nBases: object\nA class that wraps a trained model instance and its associated data.\n\nattributes()\nThe attributes of the model. Defaults to None.\n\nType\nModelAttributes, optional\n\n\n\ntask()\nThe task that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nsubtask()\nThe subtask that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nparams()\nThe parameters of the model. Defaults to None.\n\nType\ndict, optional\n\n\n\nmodel_id()\nThe identifier of the model. Defaults to “main”.\n\nType\nstr\n\n\n\nmodel()\nThe trained model instance. Defaults to None.\n\nType\nobject, optional\n\n\n\ntrain_ds()\nThe training dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ntest_ds()\nThe test dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\nvalidation_ds()\nThe validation dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ny_train_predict()\nThe predicted outputs for the training dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_test_predict()\nThe predicted outputs for the test dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_validation_predict()\nThe predicted outputs for the validation dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\nvalidation_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\ny_validation_predict(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic model_library(model)\nReturns the model library name\n\n\nstatic model_class(model)\nReturns the model class name\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod init_vm_model(model, train_ds, test_ds, validation_ds, attributes)\nInitializes a model instance from the provided data.\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.ModelAttributes\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.TestResult\nBases: object\nTestResult model\n\nvalues(: dic )\n\n\ntest_name(: str | Non _ = Non_ )\n\n\ncolumn(: str | Non _ = Non_ )\n\n\npassed(: bool | Non _ = Non_ )\n\n\nserialize()\nSerializes the TestResult to a dictionary so it can be sent to the API\n\n\n\nclass validmind.TestResults\nBases: object\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\nsummary(: ResultSummary | Non )\n\n\nserialize()\nSerializes the TestResults to a dictionary so it can be sent to the API\n\n\n\nclass validmind.ThresholdTest\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nTODO: ThresholdTest should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\ndescription()\nReturn the test description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(test_results: TestResults | None = None)\nReturn the threshold test summary. Should be overridden by subclasses. Defaults to None. The test summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the test results.\nWe return None here because the test summary is optional.\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/vm_models.html",
    "href": "validmind/vm_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Models\nModels entrypoint\n\nclass validmind.vm_models.Dataset\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nproperty index()\nReturns the dataset’s index.\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nget_numeric_features_columns()\nReturns list of numeric features columns\n\nReturns\nThe list of numberic features columns\nReturn type\nlist\n\n\n\nget_categorical_features_columns()\nReturns list of categorical features columns\n\nReturns\nThe list of categorical features columns\nReturn type\nlist\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.vm_models.DatasetTargets\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.vm_models.Figure\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Metric\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nTODO: Metric should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\ndescription()\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(metric_value: dict | list | DataFrame | None = None)\nReturn the metric summary. Should be overridden by subclasses. Defaults to None. The metric summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the metric results.\nWe return None here because the metric summary is optional.\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\nrequired_context(: ClassVar[List[str] )\n\n\n\nclass validmind.vm_models.MetricResult\nBases: object\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\ntype(: st )\n\n\nscope(: st )\n\n\nkey(: dic )\n\n\nvalue(: dict | list | DataFram )\n\n\nsummary(: ResultSummary | Non _ = Non_ )\n\n\nvalue_formatter(: str | Non _ = Non_ )\n\n\nserialize()\nSerializes the Metric to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Model\nBases: object\nA class that wraps a trained model instance and its associated data.\n\nattributes()\nThe attributes of the model. Defaults to None.\n\nType\nModelAttributes, optional\n\n\n\ntask()\nThe task that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nsubtask()\nThe subtask that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nparams()\nThe parameters of the model. Defaults to None.\n\nType\ndict, optional\n\n\n\nmodel_id()\nThe identifier of the model. Defaults to “main”.\n\nType\nstr\n\n\n\nmodel()\nThe trained model instance. Defaults to None.\n\nType\nobject, optional\n\n\n\ntrain_ds()\nThe training dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ntest_ds()\nThe test dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\nvalidation_ds()\nThe validation dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ny_train_predict()\nThe predicted outputs for the training dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_test_predict()\nThe predicted outputs for the test dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_validation_predict()\nThe predicted outputs for the validation dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\nvalidation_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\ny_validation_predict(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic model_library(model)\nReturns the model library name\n\n\nstatic model_class(model)\nReturns the model class name\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod init_vm_model(model, train_ds, test_ds, validation_ds, attributes)\nInitializes a model instance from the provided data.\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.vm_models.ModelAttributes\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.vm_models.ResultSummary\nBases: object\nA dataclass that holds the summary of a metric or threshold test results\n\nresults(: List[ResultTable )\n\n\nadd_result(result: ResultTable)\nAdds a result to the list of results\n\n\nserialize()\nSerializes the ResultSummary to a list of results\n\n\n\nclass validmind.vm_models.ResultTable\nBases: object\nA dataclass that holds the table summary of result\n\ndata(: Dict[str, Any] | DataFram )\n\n\ntype(: st _ = ’table_ )\n\n\nmetadata(: ResultTableMetadat _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.ResultTableMetadata\nBases: object\nA dataclass that holds the metadata of a table summary\n\ntitle(: st )\n\n\n\nclass validmind.vm_models.TestContext\nBases: object\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as shared dataset metrics, etc.\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\nmodels(: List[Model _ = Non_ )\n\n\ncontext_data(: dic _ = Non_ )\n\n\nset_context_data(key, value)\n\n\nget_context_data(key)\n\n\n\nclass validmind.vm_models.TestContextUtils\nBases: object\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\ntest_context(: TestContex )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\nproperty dataset()\n\n\nproperty model()\n\n\nproperty models()\n\n\nproperty df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\n\nclass validmind.vm_models.TestPlan\nBases: object\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\nname(: ClassVar[str )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\ntests(: ClassVar[List[object] _ = [_ )\n\n\ntest_plans(: ClassVar[List[object] _ = [_ )\n\n\nresults(: ClassVar[List[TestPlanResult] _ = [_ )\n\n\nconfig(: { _ = Non_ )\n\n\ntest_context(: TestContex _ = Non_ )\n\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\nmodels(: List[Model _ = Non_ )\n\n\npbar(: tqd _ = Non_ )\n\n\ntitle()\nReturns the title of the test plan. Defaults to the title version of the test plan name\n\n\ndescription()\nReturns the description of the test plan. Defaults to the docstring of the test plan\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\nget_config_params_for_test(test_name)\nReturns the config for a given test, if it exists. The config attribute is a dictionary where the keys are the test names and the values are dictionaries of config values for that test.\nThe key in the config must match the name of the test, i.e. for a test called “time_series_univariate_inspection_raw” we could pass a config like this:\n{\n“time_series_univariate_inspection_raw”: {\n\n    “columns”: [“col1”, “col2”]\n\n}\n}\n\n\nrun(send=True)\nRuns the test plan\n\n\nlog_results()\nLogs the results of the test plan to ValidMind\nThis method will be called after the test plan has been run and all results have been collected. This method will log the results to ValidMind.\n\n\nsummarize()\nSummarizes the results of the test plan\nThis method will be called after the test plan has been run and all results have been logged to ValidMind. It will summarize the results of the test plan by creating an html table with the results of each test. This html table will be displayed in an VS Code, Jupyter or other notebook environment.\n\n\nget_results(result_id: str | None = None)\nReturns one or more results of the test plan. Includes results from sub test plans.\n\n\n\nclass validmind.vm_models.TestPlanDatasetResult\nBases: TestPlanResult\nResult wrapper for datasets that run as part of a test plan\n\ndataset(: Datase _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanMetricResult\nBases: TestPlanResult\nResult wrapper for metrics that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\nmetric(: MetricResult | Non _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanModelResult\nBases: TestPlanResult\nResult wrapper for models that run as part of a test plan\n\nmodel(: Mode _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanTestResult\nBases: TestPlanResult\nResult wrapper for test results produced by the tests that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestResult\nBases: object\nTestResult model\n\nvalues(: dic )\n\n\ntest_name(: str | Non _ = Non_ )\n\n\ncolumn(: str | Non _ = Non_ )\n\n\npassed(: bool | Non _ = Non_ )\n\n\nserialize()\nSerializes the TestResult to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.TestResults\nBases: object\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\nsummary(: ResultSummary | Non )\n\n\nserialize()\nSerializes the TestResults to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.TestSuite\nBases: TestPlan\nBase class for test suites. Test suites are used to define any arbitrary grouping of test plans that will be run on a dataset and/or model.\n\ntest_plans(: ClassVar[List[str] _ = [_ )\n\n\nrun(send=True)\nRuns the test suite.\n\n\nproperty results()\nReturns the results of the test suite.\n\n\n\nclass validmind.vm_models.ThresholdTest\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nTODO: ThresholdTest should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\ndescription()\nReturn the test description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(test_results: TestResults | None = None)\nReturn the threshold test summary. Should be overridden by subclasses. Defaults to None. The test summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the test results.\nWe return None here because the test summary is optional.\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\nrequired_context(: ClassVar[List[str] )"
  },
  {
    "objectID": "validmind/model_validation_tests_sklearn.html",
    "href": "validmind/model_validation_tests_sklearn.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions models trained with sklearn or that provide a sklearn-like API\n\n\n\nBases: Metric\nAccuracy Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCharacteristic Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculates PSI for each of the dataset features\n\n\n\n\nBases: Metric\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nF1 Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPermutation Feature Importance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPrecision Recall Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPrecision Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nRecall Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nROC AUC Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nROC Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nSHAP Global Importance\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPopulation Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the model’s prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the model’s F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the model’s ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that identify weak regions with high residuals by histogram slicing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest robustness of model by perturbing the features column values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\nAdds Gaussian noise to a list of values.\n\nParameters\n\nvalues (*list**[float]*) – A list of numerical values to which noise is added.\nx_std_dev (float) – A scaling factor for the standard deviation of the noise.\n\nReturns\nA tuple containing:\n  * A list of noisy values, where each value is the sum of the corresponding value\n\n  in the input list and a randomly generated value sampled from a Gaussian distribution\n  with mean 0 and standard deviation x_std_dev times the standard deviation of the input list.\n  - The standard deviation of the input list of values.\nReturn type\ntuple[list[float], float]"
  },
  {
    "objectID": "validmind/readme.html",
    "href": "validmind/readme.html",
    "title": "ValidMind",
    "section": "",
    "text": "pip install validmind\n\n\npip install validmind[r-support]\n\n\n\n\n\n\n\n\nEnsure you have poetry installed: https://python-poetry.org/\nAfter cloning this repo run:\n\npoetry shell\npoetry install\n\n\n\nIf you want to use the R support that is provided by the ValidMind Developer Framework, you must have R installed on your machine. You can download R from https://cran.r-project.org/. If you are on a Mac, you can install R using Homebrew:\nbrew install r\nOnce you have R installed, you can install the r-support extra to install the necessary dependencies for R by running:\npoetry install --extras r-support\n\n\n\nMake sure you bump the package version before merging a PR with the following command:\nmake version tag=patch\nThe value of tag corresponds to one of the options provided by Poetry: https://python-poetry.org/docs/cli/#version\n\n\n\n\nIf you want to integate the validmind package to your development environment, you must build the package wheel first, since we have not pushed the package to a public PyPI repository yet. Steps:\n\nRun make build to build a new Python package for the developer framework\nThis will create a new wheel file in the dist folder\nRun pip install <path-to-wheel> to install the newly built package in your environment\n\n\n\n\nAPI documentation can be generated in Markdown or HTML format. Our documentation pipeline uses Markdown documentation before generating the final HTML assets for the documentation site.\nFor local testing, HTML docs can be generated with Sphinx. Note that the output template is different since the documentation pipeline uses the source Markdown files for the final HTML output.\nMarkdown and HTML docs can be generated with the following commands:\n# Navigate to the docs folder\ncd docs/\n\n# Generate HTML and Markdown docs\nmake docs\n\n# Generate Markdown docs only\nmake docs-markdown\n\n# Generate HTML docs only\nmake docs-html\nThe resulting markdown and html under docs/_build folders will contain the generated documentation.\n\n\n\n\n\nIf you run into an error related to the ValidMind wheel, try:\npoetry add wheel\npoetry update wheel\npoetry install\nIf there are lightgbm errors partway through, run remove lightgbm, followed by poetry update wheel and poetry install."
  },
  {
    "objectID": "validmind/test_plans.html",
    "href": "validmind/test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "Test Plans entry point\n\n\nReturns a list of all available test plans\n\n\n\nReturns a list of all available tests.\n\n\n\nReturns the test plan by name\n\n\n\nReturns a description of the test plan\n\n\n\nRegisters a custom test plan\n\n\n\n\n\n\nTest plan for tabular datasets\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on time series datasets"
  },
  {
    "objectID": "validmind/index.html",
    "href": "validmind/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Developer Framework\n\nValidMind Python Client\n\nInstallation\nContributing to ValidMind Developer Framework\nIntegrating the ValidMind Developer Framework to your development environment\nGenerating Docs\nKnown Issues\n\nPython Library API\nCore Library Tests\n\nData Validation Tests\n\nCore Library Tests\n\nModel Validation Tests for SKLearn-Compatible Models\n\nTest Plans\n\nlist_plans()\nlist_tests()\nget_by_name()\ndescribe_plan()\nregister_test_plan()\nTest Plans for SKLearn-Compatible Classifiers\nTest Plans for Tabular Datasets\n\nValidMind Models"
  },
  {
    "objectID": "validmind/data_validation_tests.html",
    "href": "validmind/data_validation_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions for any Pandas-compatible datasets\n\n\n\nBases: TestContextUtils\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function\n\n\n\n\nBases: Metric\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild two tables: one for summarizing numerical variables and one for categorical variables\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\n\nReturns a summarized representation of the dataset split information\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCalculates seasonal_decompose metric for each of the dataset features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStores the seasonal decomposition results in the test context so they can be re-used by other tests. Note we store one sd at a time for every column in the dataset.\n\n\n\nSerializes the seasonal decomposition results for one column into a JSON serializable format that can be sent to the API.\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPlots ACF and PACF for a given time series dataset.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot rolling mean and rolling standard deviation in different subplots for a given series.\n\nParameters\n\nseries – Pandas Series with time-series data\nwindow_size – Window size for the rolling calculations\nax1 – Axis object for the rolling mean plot\nax2 – Axis object for the rolling standard deviation plot\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\n\n\n\n\n\n\n\n\n\nPlot the spread between two time series variables.\n\nParameters\n\nseries1 – Pandas Series with time-series data for the first variable\nseries2 – Pandas Series with time-series data for the second variable\nax – Axis object for the spread plot\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique rows is greater than a threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe zeros test finds columns that have too many zero values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that find outliers for time series data using the z-score method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that detect frequencies in the data\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    Documentation\n                    The guide to elevating your MRM workflow\n                    Need help? Find all the information you need to use our platform for model risk management (MRM).\n\n\n    \n        \n            \n                \n            \n            \n                                    \n            \n        \n    \n\n\n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating the key aspects of the model risk management process, ValidMind is a MRM solution designed for the unique needs of model developers and validators.\n                    Model Documentation Automation\n                    MRM Lifecycle and Workflow\n                    Communication & TrackingInstructional GuidesGet Started\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Collect, manage, and automate your model documentation and testing with our Developer Framework.Collaboration for Model Developers\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements.Collaboration for Model Validators\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "This interactive notebook will guide you through documenting a model using the ValidMind Developer framework. We will use sample datasets provided by the library and train a simple classification model.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nInitializing the ValidMind Developer Framework\nUsing a sample datasets provided by the library to train a simple classification model\nRunning a test suite to quickly generate document about the data and model\n\n\n\nClick File > Save a copy in Drive > to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n\n\n\n\n!pip install validmind\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue with the next cell.\n##Initializing the Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nLog in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n\n\n(Note: if a documentation project has already been created, you can skip this section and head directly “Finding Project API key and secret”)\nClicking on “Create a new project” allows to you to register a new documentation project for our demo model.\nSelect “Customer Churn model” from the Model drop-down, and “Initial Validation” as Type. Finally, click on “Create Project”.\n\n\n\nIn the “Client Integration” page of the newly created project, you will find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\napi_host: Location of the ValidMind API.\napi_key: Account API key.\napi_secret: Account Secret key.\nproject: The project identifier. The project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nThe code snippet can be copied and pasted directly in the cell below to initialize the ValidMind Developer Framework when run:\n\n## Replace the code below with the code snippet from your project ## \n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"0ec28d90ceb3160a8dd16b7a82aaf286\",\n  api_secret = \"7e5399be27bc9b4a0b963256bb575aa104de8217597f272c878c64e2a248fb3f\",\n  project = \"clhogsv0d008x1pil43778rew\"\n)\n  \n  \n  \n\n\n\n\n\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\n from validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with: \n#from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\n\n\n\n\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\n\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nWe can now initialize the training and test datasets into dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nWe also initialize a model object using vm.init_model():\n\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\n\nWe are now ready to run the test suite for binary classifier with tabular datasets. This function will run test plans on the dataset and model objects, and will document the results in the ValidMind UI.\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html",
    "href": "notebooks/Introduction_Customer_Churn.html",
    "title": "ValidMind",
    "section": "",
    "text": "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\n\nClick File > Save a copy in Drive > to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n##Install ValidMind Developer Framework\n\n!pip install validmind\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue on to the next cell.\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nLog in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n\n\n(Note: if a documentation project has already been created, you can skip this section and head directly “Finding Project API key and secret”)\nClicking on “Create a new project” allows to you to register a new documentation project for our demo model.\nSelect “Customer Churn model” from the Model drop-down, and “Initial Validation” as Type. Finally, click on “Create Project”.\n\n\n\nIn the “Client Integration” page of the newly created project, you will find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\napi_host: Location of the ValidMind API.\napi_key: Account API key.\napi_secret: Account Secret key.\nproject: The project identifier. The project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nThe code snippet can be copied and pasted directly in the cell below to initialize the ValidMind Developer Framework when run:\n\n## Replace the code below with the code snippet from your project ## \n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"3b481c4935130773c825014129d62c02\",\n  api_secret = \"61ba7b6d80e1e7b162646d3a29374807dab6b3613757942f12ca11eab7530dcc\",\n  project = \"clhny5uil00071ojtaomas47g\"\n)\n  \n\nThe Developer Framework is now initialized and connected to the correct project on the platform.\n\n\n\n\nWe will now train an example model that will be used to demonstrate the ValidMind Developer Framework functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nNow that we are satisfied with our model, we can begin using the ValidMind Library to generate test and document it.\n\n\n\nWe can find all the test plans and tests available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nList all available tests: vm.test_plans.list_tests()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_data_quality\")\n\nHere is an example:\n\nvm.test_plans.list_plans()\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\n\n\n\nWe can now initialize the TabularDataset test suite. The primary method of doing this is with the run_test_suite function from the vm module. This function takes in a test suite name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Data Preparation” section of the model documentation.\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nWe can now run the BinaryClassifierModelValidation test plan:\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/time_series_model_validation_full_suite.html",
    "href": "notebooks/time_series_model_validation_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Full Suite notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework.\nIn this demo, we will use the time_series_model_validation test suite to run multiple model validation metrics on several pre-trained time series models.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\nWe can find all the test suites and test plans available in the developer framework by calling the following functions:\n\nAll test suites: vm.test_suites.list_suites()\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_suites.list_suites()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_full_suite\n      BinaryClassifierFullSuite\n      Full test suite for binary classification models.\n      tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      binary_classifier_model_validation\n      BinaryClassifierModelValidation\n      Test suite for binary classification models.\n      binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      tabular_dataset\n      TabularDataset\n      Test suite for tabular datasets.\n      tabular_dataset_description, tabular_data_quality\n    \n    \n      time_series_dataset\n      TimeSeriesDataset\n      Test suite for time series datasets.\n      time_series_data_quality, time_series_univariate, time_series_multivariate\n    \n    \n      time_series_model_validation\n      TimeSeriesModelValidation\n      Test suite for time series model validation.\n      regression_model_performance, regression_models_comparison, time_series_forecast\n    \n  \n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\n\n\n\n\n\n\n# Currently only fred pre-trained models are available\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_1')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_C, train_df_C, test_df_C = demo_dataset.load_model('fred_loan_rates_model_4')\nmodel_D, train_df_D, test_df_D = demo_dataset.load_model('fred_loan_rates_model_5')\n\n\n\n\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model C\nvm_train_ds_C = vm.init_dataset(dataset=train_df_C, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_C = vm.init_dataset(dataset=test_df_C, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model D\nvm_train_ds_D = vm.init_dataset(dataset=train_df_D, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_D = vm.init_dataset(dataset=test_df_D, type=\"generic\", target_column=demo_dataset.target_column)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n# Initialize model C\nvm_model_C = vm.init_model(\n    model=model_C, \n    train_ds=vm_train_ds_C, \n    test_ds=vm_test_ds_C\n)\n\n# Initialize model D\nvm_model_D = vm.init_model(\n    model=model_D,\n    train_ds=vm_train_ds_D,\n    test_ds=vm_test_ds_D\n)\n\n\nlist_of_models = [vm_model_A, vm_model_B, vm_model_C, vm_model_D]\n\n\n\n\n\n\n\nUsers can input the configuration to a test suite using config, allowing fine-tuning the suite according to their specific data requirements.\nTime Series Data Quality params - time_series_outliers is set to identify outliers using a specific Z-score threshold - time_series_missing_values defines a minimum threshold to identify missing data points.\nTime Series Univariate params - Visualization: The keys time_series_line_plot and time_series_histogram are designed to generate line and histogram plots respectively for each column in a DataFrame.\n\nSeasonality: The keys seasonal_decompose and auto_seasonality are dedicated to analyzing the seasonal component of the time series. seasonal_decompose performs a seasonal decomposition of the data, while auto_seasonality aids in the automatic detection of seasonality.\nARIMA: The keys acf_pacf_plot, auto_ar, and auto_ma are part of the ARIMA (Autoregressive Integrated Moving Average) model analysis. acf_pacf_plot generates autocorrelation and partial autocorrelation plots, auto_ar determines the order of the autoregressive part of the model, and auto_ma does the same for the moving average part.\n\nTime Series Multivariate params - Visualization: The scatter_plot key is used to create scatter plots for each column in the DataFrame, offering a visual tool to understand the relationship between different variables in the dataset.\n\nCorrelation: The lagged_correlation_heatmap key facilitates the creation of a heatmap, which visually represents the lagged correlation between the target column and the feature columns of a demo dataset. This provides a convenient way to examine the time-delayed correlation between different series.\nCointegration: The engle_granger_coint key sets a threshold for conducting the Engle-Granger cointegration test, which is a statistical method used to identify the long-term correlation between two or more time series.\n\n\nconfig= {\n    \"regression_forecast_plot\": {\n        \"start_date\": '2010-01-01',\n        \"end_date\": '2022-01-01'\n    }\n}\n\n\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model = vm_model_B,\n    models = list_of_models,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/time_series_data_validation_full_suite.html",
    "href": "notebooks/time_series_data_validation_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Demo notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework. Ensuring the quality and an a robust exploratory data analysis of time series data is essential for accurate model predictions and robust decision-making processes.\nIn this demo, we will walk through different data validation suites of tests tailored for time series data, showcasing how these tools can assist you in identifying potential issues and inconsistencies in the data.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\nfrom validmind.datasets.regression import (\n    identify_frequencies, \n    resample_to_common_frequency\n)\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\nWe can find all the test suites and test plans available in the developer framework by calling the following functions:\n\nAll test suites: vm.test_suites.list_suites()\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_suites.list_suites()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_full_suite\n      BinaryClassifierFullSuite\n      Full test suite for binary classification models.\n      tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      binary_classifier_model_validation\n      BinaryClassifierModelValidation\n      Test suite for binary classification models.\n      binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      tabular_dataset\n      TabularDataset\n      Test suite for tabular datasets.\n      tabular_dataset_description, tabular_data_quality\n    \n    \n      time_series_dataset\n      TimeSeriesDataset\n      Test suite for time series datasets.\n      time_series_data_quality, time_series_univariate, time_series_multivariate\n    \n    \n      time_series_model_validation\n      TimeSeriesModelValidation\n      Test suite for time series model validation.\n      regression_model_performance, regression_models_comparison, time_series_forecast\n    \n  \n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\n\n\n\nConigure your use case.\n\n# from validmind.datasets.classification import lending_club as demo_dataset\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\n# Split the dataset into test and training \ndf = demo_dataset.load_data()\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   FEDFUNDS      825 non-null    float64\n 2   GS10          841 non-null    float64\n 3   UNRATE        903 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\n\nUsers can input the configuration to a test suite using config, allowing fine-tuning the suite according to their specific data requirements.\nTime Series Data Quality params - time_series_outliers is set to identify outliers using a specific Z-score threshold - time_series_missing_values defines a minimum threshold to identify missing data points.\nTime Series Univariate params - Visualization: time_series_line_plot and time_series_histogram are designed to generate line and histogram plots respectively for each column in a DataFrame.\n\nSeasonality: seasonal_decompose and auto_seasonality are dedicated to analyzing the seasonal component of the time series. seasonal_decompose performs a seasonal decomposition of the data, while auto_seasonality aids in the automatic detection of seasonality.\nStationarity: window_size determines the number of consecutive data points used for calculating the rolling mean and standard deviation.\nARIMA: acf_pacf_plot, auto_ar, and auto_ma are part of the ARIMA (Autoregressive Integrated Moving Average) model analysis. acf_pacf_plot generates autocorrelation and partial autocorrelation plots, auto_ar determines the order of the autoregressive part of the model, and auto_ma does the same for the moving average part.\n\nTime Series Multivariate params - Visualization: scatter_plot is used to create scatter plots for each column in the DataFrame, offering a visual tool to understand the relationship between different variables in the dataset.\n\nCorrelation: lagged_correlation_heatmap facilitates the creation of a heatmap, which visually represents the lagged correlation between the target column and the feature columns of a demo dataset. This provides a convenient way to examine the time-delayed correlation between different series.\nCointegration: engle_granger_coint sets a threshold for conducting the Engle-Granger cointegration test, which is a statistical method used to identify the long-term correlation between two or more time series.\n\n\nconfig={\n    \n    # TIME SERIES DATA QUALITY PARAMS\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    },\n    \n    # TIME SERIES UNIVARIATE PARAMS \n    \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n     \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive'\n    },\n     \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 4\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS \n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\n\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nNo frequency could be inferred for variable 'MORTGAGE30US'. Skipping seasonal decomposition and plots for this variable.\n\n\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nShow the frequencies of each variable in the raw dataset.\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      FEDFUNDS\n      MS\n    \n    \n      2\n      GS10\n      MS\n    \n    \n      3\n      UNRATE\n      MS\n    \n  \n\n\n\n\nHandle frequencies by resampling all variables to a common frequency.\n\npreprocessed_df = resample_to_common_frequency(df, common_frequency=demo_dataset.frequency)\nfrequencies = identify_frequencies(preprocessed_df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      MS\n    \n    \n      1\n      FEDFUNDS\n      MS\n    \n    \n      2\n      GS10\n      MS\n    \n    \n      3\n      UNRATE\n      MS\n    \n  \n\n\n\n\n\n\nRun the same suite again after handling frequencies.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nHandle the missing values by droping all the nan values.\n\npreprocessed_df = preprocessed_df.dropna()\n\n\n\nRun the same test suite to check there are no missing values and frequencies of all variables are the same.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nHandle stationarity by taking the first difference.\n\npreprocessed_df = preprocessed_df.diff().fillna(method='bfill')\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead."
  }
]