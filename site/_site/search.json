[
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html",
    "href": "notebooks/tutorials/intro_for_model_developers.html",
    "title": "ValidMind Introduction for Model Developers",
    "section": "",
    "text": "As a model developer, learn how the end-to-end documentation process works based on common scenarios you encounter in model development settings.\nAs a prerequisite, a model documentation template must be available on the platform. You can view the available templates to see what has been defined on the platform.\nThis notebook uses a binary classification model as an example, but the same principles shown here apply to other model types."
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#overview-of-the-notebook",
    "href": "notebooks/tutorials/intro_for_model_developers.html#overview-of-the-notebook",
    "title": "ValidMind Introduction for Model Developers",
    "section": "Overview of the notebook",
    "text": "Overview of the notebook\n1. Initializing the ValidMind Developer Framework\nValidMind’s developer framework provides a rich collection of documentation tools and test suites, from documenting descriptions of datasets to validation and testing of models using a variety of open-source testing frameworks.\n2. Start the model development process with raw data, run out-of-the box tests, and add evidence to model documentation\nLearn how to access ValidMind’s test repository of individual tests that you will use as building blocks to ensure a model is being built appropriately. The goal is to show how to run tests, investigate results, and add tests results or evidence to the documentation.\nFor a full list of out-of-the-box tests, see Test descriptions or try the interactive Test sandbox.\n3. Implementing custom tests\nUsually, model developers have their own custom tests and it is important to include this within the model documentation. We will show you how to include custom tests and then how they can be added to documentation as additional evidence.\n4. Finalize testing and documentation\nLearn how you can ensure that model documentation includes custom tests and how to make test configuration changes that apply to all tests in the model documentation template. At the end of this section you should have a fully documented model ready for review."
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#contents",
    "href": "notebooks/tutorials/intro_for_model_developers.html#contents",
    "title": "ValidMind Introduction for Model Developers",
    "section": "Contents",
    "text": "Contents\n\nOverview of the notebook\nAbout ValidMind\n\nBefore you begin\nNew to ValidMind?\nKey concepts\n\n1. Initializing the ValidMind Developer Framework\n\nInstall the client library\nRegister a new model in ValidMind UI and initialize the client library\nVerify & preview the documentation template\n\n2. Start the model development process with raw data, run out-of-the box tests, and add evidence to model documentation\n\nInitialize the ValidMind datasets\nRun some tabular data tests\nUtilize test output\nDocumenting the results based on two datasets\n\nRun run_documentation_tests() using vm_raw_dataset_new as input\nLog the individual result high correlation test using vm_raw_dataset (no data cleanup)\nLog the individual result high correlation test using vm_raw_dataset_new (balanced classes and reduced features)\n\nAdd individual test results to model documentation\nModel Testing\nInitialize model evaluation objects and assigning predictions\nRun the model evaluation tests\n\n3. Implementing custom tests\n\nCreate a confusion matrix plot\nAdd parameters to custom metrics\nPass parameters to custom metrics\nLog the confusion matrix results\nUsing external test providers\n\nCreate a folder of custom tests from existing inline tests\nSave an inline test to a file\nDefine and register a LocalTestProvider that points to that folder\n\nInitializing a local test provider\n\nRun test provider tests\nAdd the test results to your documentation\n\n\n4. Finalize testing and documentation\n\nUse run_documentation_tests() to ensure custom test results are included in your documentation\nViewing and updating the configuration for the entire model documentation template\n\nUpdate the config\n\n\nWhere to go from here\n\nUse cases\nMore how-to guides and code samples\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#about-validmind",
    "href": "notebooks/tutorials/intro_for_model_developers.html#about-validmind",
    "title": "ValidMind Introduction for Model Developers",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models. You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#initializing-the-validmind-developer-framework",
    "href": "notebooks/tutorials/intro_for_model_developers.html#initializing-the-validmind-developer-framework",
    "title": "ValidMind Introduction for Model Developers",
    "section": "1. Initializing the ValidMind Developer Framework",
    "text": "1. Initializing the ValidMind Developer Framework\n\n\nInstall the client library\nPlease note the following recommended Python versions to use:\n\nPython 3.7 &gt; x &lt;= 3.11\n\nThe client library provides Python support for the ValidMind Developer Framework. To install it run:\n\n%pip install -q validmind\n\n\n\n\nRegister a new model in ValidMind UI and initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\n\n\nVerify & preview the documentation template\nLet’s verify that you have connected to ValidMind and that the appropriate template is selected. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results for this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\nBefore learning how to run tests, let’s explore the list of all available tests in the ValidMind Developer Framework. You can see that the documentation template for this model has references to some of the test IDs listed below.\n\nvm.tests.list_tests()"
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#start-the-model-development-process-with-raw-data-run-out-of-the-box-tests-and-add-evidence-to-model-documentation",
    "href": "notebooks/tutorials/intro_for_model_developers.html#start-the-model-development-process-with-raw-data-run-out-of-the-box-tests-and-add-evidence-to-model-documentation",
    "title": "ValidMind Introduction for Model Developers",
    "section": "2. Start the model development process with raw data, run out-of-the box tests, and add evidence to model documentation",
    "text": "2. Start the model development process with raw data, run out-of-the box tests, and add evidence to model documentation\nIn this section you learn how to explore the individual tests available in ValidMind and how to run them and change parameters as necessary. You will use a public dataset from Kaggle that models a bank customer churn prediction use case. The target column, Exited has a value of 1 when a customer has churned and 0 otherwise.\nYou can find more information about this dataset here.\nThe ValidMind Developer Framework provides a wrapper to automatically load the dataset as a Pandas DataFrame object.\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n)\n\nraw_df = demo_dataset.load_data()\nraw_df.head()\n\nLet’s do some data quality assessments by running a few individual tests related to data assessment. You will use the vm.tests.list_tests() function introduced above in combination with vm.tests.list_tags() and vm.tests.list_task_types() to find which prebuilt tests are relevant for data quality assessment.\n\n# Get the list of available tags\nsorted(vm.tests.list_tags())\n\n\n# Get the list of available task types\nsorted(vm.tests.list_task_types())\n\nYou can pass tags and task_types as parameters to the vm.tests.list_tests() function to filter the tests based on the tags and task types. For example, to find tests related to tabular data quality for classification models, you can call list_tests() like this:\n\nvm.tests.list_tests(task=\"classification\", tags=[\"tabular_data\", \"data_quality\"])\n\n\n\nInitialize the ValidMind datasets\nNow, assume we have identified some tests we want to run with regards to the data we are intending to use. The next step is to connect your data with a ValidMind Dataset object. This step is always necessary every time you want to connect a dataset to documentation and produce test results through ValidMind. You only need to do it one time per dataset.\nYou can initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\n\n\n# vm_raw_dataset is now a VMDataset object that you can pass to any ValidMind test\nvm_raw_dataset = vm.init_dataset(\n    dataset=raw_df,\n    input_id=\"raw_dataset\",\n    target_column=\"Exited\",\n)\n\n\n\n\nRun some tabular data tests\nIndividual tests can be easily run by calling the run_test function provided by the validmind.tests module. The function takes the following arguments:\n\ntest_id: The ID of the test to run. To find a particular test and get its ID, refer to the explore_tests notebook. Look above for example after running ‘vm.test_suites.describe_suite’ as column ‘Test ID’ will contain the id.\nparams: A dictionary of parameters for the test. These will override any default_params set in the test definition. Refer to the explore_tests notebook to find the default parameters for a test. See below for examples.\n\nThe inputs expected by a test can also be found in the test definition. Let’s take validmind.data_validation.DescriptiveStatistics as an example. Note that the output of the describe_test() function below shows that this test expects a dataset as input:\n\nvm.tests.describe_test(\"validmind.data_validation.DescriptiveStatistics\")\n\nNow, let’s run a few tests to assess the quality of the dataset.\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.DescriptiveStatistics\",\n    inputs={\"dataset\": vm_raw_dataset},\n)\n\n\ntest2 = vm.tests.run_test(\n    test_id=\"validmind.data_validation.ClassImbalance\",\n    inputs={\"dataset\": vm_raw_dataset},\n    params={\"min_percent_threshold\": 30},\n)\n\nYou can see that the class imbalance test did not pass according to the value of min_percent_threshold we have set. Here is how you can re-run the test on some processed data to address this data quality issue. In this case we apply a very simple rebalancing technique to the dataset.\n\nimport pandas as pd\n\nraw_copy_df = raw_df.sample(frac=1)  # Create a copy of the raw dataset\n\n# Create a balanced dataset with the same number of exited and not exited customers\nexited_df = raw_copy_df.loc[raw_copy_df[\"Exited\"] == 1]\nnot_exited_df = raw_copy_df.loc[raw_copy_df[\"Exited\"] == 0].sample(n=exited_df.shape[0])\n\nbalanced_raw_df = pd.concat([exited_df, not_exited_df])\nbalanced_raw_df = balanced_raw_df.sample(frac=1, random_state=42)\n\nWith this new raw dataset, you can re-run the individual test to see if it passes the class imbalance test requirement. Remember to register new VM Dataset object since that is the type of input required by run_test():\n\n# Register new data and now 'balanced_raw_dataset' is the new dataset object of interest\nvm_balanced_raw_dataset = vm.init_dataset(\n    dataset=balanced_raw_df,\n    input_id=\"balanced_raw_dataset\",\n    target_column=\"Exited\",\n)\n\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.ClassImbalance\",\n    inputs={\"dataset\": vm_balanced_raw_dataset},\n    params={\"min_percent_threshold\": 30},\n)\n\n\n\n\nUtilize test Output\nHere is an example for how you can utilize the output from a ValidMind test for futher use, for example, if you want to remove highly correlated features. The example below shows how you can get the list of features with the highest correlation coefficients and use them to reduce the final list of features for modeling.\n\ncorr_results = vm.tests.run_test(\n    test_id=\"validmind.data_validation.HighPearsonCorrelation\",\n    params={\"max_threshold\": 0.3},\n    inputs={\"dataset\": vm_balanced_raw_dataset},\n)\n\nLet’s assume we want to remove highly correlated features from the dataset. corr_results is an object of type ThresholdTestResult and we can inspects its individual results to get access to the features that failed the test. In general, all ValidMind tests can return two different types of results:\n\nMetricResult: most metrics return this type of result\nThresholdTestResult: metrics that compare a metric to a threshold return this type of result\n\n\nprint(corr_results.test_results)\nprint(\"test_name: \", corr_results.test_results.test_name)\nprint(\"params: \", corr_results.test_results.params)\nprint(\"passed: \", corr_results.test_results.passed)\nprint(\"results: \", corr_results.test_results.results)\n\nLet’s inspect the results and extract a list of features that failed the test:\n\ncorr_results.test_results.results\n\nRemove the highly correlated features and create a new VM dataset object. Note the use of different input_ids. This allows tracking the inputs used when running each individual test.\n\nhigh_correlation_features = [\n    result.column\n    for result in corr_results.test_results.results\n    if result.passed == False\n]\nhigh_correlation_features\n\n\n# Remove the highly correlated features from the dataset\nbalanced_raw_no_age_df = balanced_raw_df.drop(columns=high_correlation_features)\n\n# Re-initialize the dataset object\nvm_raw_dataset_preprocessed = vm.init_dataset(\n    dataset=balanced_raw_no_age_df,\n    input_id=\"raw_dataset_preprocessed\",\n    target_column=\"Exited\",\n)\n\nRe-running the test with the reduced feature set should pass the test. You can also plot the correlation matrix to visualize the new correlation between features:\n\ncorr_results = vm.tests.run_test(\n    test_id=\"validmind.data_validation.HighPearsonCorrelation\",\n    params={\"max_threshold\": 0.3},\n    inputs={\"dataset\": vm_raw_dataset_preprocessed},\n)\n\n\ncorr_results = vm.tests.run_test(\n    test_id=\"validmind.data_validation.PearsonCorrelationMatrix\",\n    inputs={\"dataset\": vm_raw_dataset_preprocessed},\n)\n\n\n\n\nDocumenting the results based on two datasets\nWe have now done some analysis on two different datasets and we should able to document why certain things were done to the raw data with testing to support it. Every test result returned by the run_test() function has a .log() method that can be used to log the test results to ValidMind. When logging individual results to ValidMind you need to manually add those results in a specific section of the model documentation.\nWhen using run_documentation_tests(), it’s possible to automatically populate a section with the results of all tests that were registered in the documentation template.\nTo show how to add individual results to any documentation section, we’re going to populate the entire data_preparation section of the documentation using the clean vm_raw_dataset_preprocessed dataset as input, and then we’re going to document an additional result for the highly correlated dataset vm_balanced_raw_dataset. The following two steps will accomplish this:\n\nRun run_documentation_tests() using vm_raw_dataset_preprocessed as input. This populates the entire data preparation section for every test that is already part of the documentation template.\nLog the individual result of the high correlation test that used vm_balanced_raw_dataset (that had a highly correlated Age column) as input\n\nAfter adding the result of step #2 to the documentation you will be able to explain the changes made to the raw data by editing the default description of the test result on the UI.\n\n\nRun run_documentation_tests() using vm_raw_dataset_preprocessed as input\nrun_documentation_tests() allows you to run multiple tests at once and log the results to the documentation. The function takes the following arguments:\n\ninputs: any inputs to be passed to the tests\nconfig: a dictionary &lt;test_id&gt;:&lt;test_config&gt; that allows configuring each test individually. Each test config has the following form:\n\nparams: individual test parameters\ninputs: individual test inputs. When passed, this overrides any inputs passed from the run_documentation_tests() function\n\n\n\ntest_config = {\n    \"validmind.data_validation.ClassImbalance\": {\n        \"params\": {\"min_percent_threshold\": 30},\n    },\n    \"validmind.data_validation.HighPearsonCorrelation\": {\n        \"params\": {\"max_threshold\": 0.3},\n    },\n}\n\ntests_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_raw_dataset_preprocessed,\n    },\n    config=test_config,\n    section=[\"data_preparation\"],\n)\n\n\n\n\nLog the individual result of the high correlation test that used vm_balanced_raw_dataset (that had a highly correlated Age column) as input\nHere you can use a custom result_id to tag the individual result with a unique identifier. This result_id can be appended to test_id with a : separator. The balanced_raw_dataset result identifier will correspond to the balanced_raw_dataset input, the dataset that still has the Age column.\n\nresult = vm.tests.run_test(\n    test_id=\"validmind.data_validation.HighPearsonCorrelation:balanced_raw_dataset\",\n    params={\"max_threshold\": 0.3},\n    inputs={\"dataset\": vm_balanced_raw_dataset},\n)\nresult.log()\n\n\n\n\n\nAdd individual test results to model documentation\nYou can now visit the documentation page for the model you connected to at the beginning of this notebook and add a new content block in the relevant section.\nTo do this, go to the documentation page of your model and navigate to the Data Preparation -&gt; Correlations and Interactions section. Then hover after the “Pearson Correlation Matrix” content block to reveal the + button as shown in the screenshot below.\n\n\n\nscreenshot showing insert button for test-driven blocks\n\n\nClick on the + button and select Test-Driven Block. This will open a dialog where you can select Threshold Test as the type of the test-driven content block, and then select the High Pearson Correlation Vm Raw Dataset Test metric. This will show a preview of the result and it should match the results shown above.\n\n\n\nscreenshot showing the selected test result in the dialog\n\n\nFinally, click on the Insert block button to add the test result to the documentation. You’ll now see two individual results for the high correlation test in the Correlations and Interactions section of the documentation. To finalize the documentation, you can edit the test result’s description block to explain the changes made to the raw data and the reasons behind them as we can see in the screenshot below.\n\n\n\nscreenshot showing the high pearson correlation block\n\n\n\n\n\nModel Testing\nWe have focused so far on the data assessment and pre-processing that usually occurs prior to any models being built. Now we are going to assume we have built a model and we want to incorporate some model results in our documentation.\nLet’s train a simple logistic regression model on the dataset and evaluate its performance. You will use the LogisticRegression class from the sklearn.linear_model and use ValidMind tests to evaluate the model’s performance.\nBefore training the model, we need to encode the categorical features in the dataset. You will use the OneHotEncoder class from the sklearn.preprocessing module to encode the categorical features. The categorical features in the dataset are Geography and Gender.\n\nbalanced_raw_no_age_df.head()\n\n\nbalanced_raw_no_age_df = pd.get_dummies(\n    balanced_raw_no_age_df, columns=[\"Geography\", \"Gender\"], drop_first=True\n)\nbalanced_raw_no_age_df.head()\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Split the input and target variables\nX = balanced_raw_no_age_df.drop(\"Exited\", axis=1)\ny = balanced_raw_no_age_df[\"Exited\"]\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=42,\n)\n\n# Logistic Regression grid params\nlog_reg_params = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    \"solver\": [\"liblinear\"],\n}\n\n# Grid search for Logistic Regression\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n\n# Logistic Regression best estimator\nlog_reg = grid_log_reg.best_estimator_\n\n\n\n\nInitialize model evaluation objects and assigning predictions\nThe last step for evaluating the model’s performance is to initialize the ValidMind Dataset and Model objects and assign model predictions to each dataset. You will use the init_dataset, init_model and assign_predictions functions to initialize these objects.\n\ntrain_df = X_train\ntrain_df[\"Exited\"] = y_train\ntest_df = X_test\ntest_df[\"Exited\"] = y_test\n\nvm_train_ds = vm.init_dataset(\n    input_id=\"train_dataset_final\",\n    dataset=train_df,\n    target_column=\"Exited\",\n)\n\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset_final\",\n    dataset=test_df,\n    target_column=\"Exited\",\n)\n\n# Register the model\nvm_model = vm.init_model(log_reg, input_id=\"log_reg_model_v1\")\n\nOnce the model has been registered you can assign model predictions to the training and test datasets. The assign_predictions() method from the Dataset object can link existing predictions to any number of models. If no prediction values are passed, the method will compute predictions automatically:\n\nvm_train_ds.assign_predictions(model=vm_model)\nvm_test_ds.assign_predictions(model=vm_model)\n\n\n\n\nRun the model evaluation tests\nIn this part, we focus on running the tests within the model development section of the model documentation. Only tests associated with this section will be executed, and the corresponding results will be updated in the model documentation. In the example below, you will focus on only running tests for the model development section of the document.\nNote the additional config that is passed to run_documentation_tests(). This allows you to override inputs or params in certain tests. In our case, we want to explicitly use the vm_train_ds for the validmind.model_validation.sklearn.ClassifierPerformance:in_sample test, since it’s supposed to run on the training dataset and not the test dataset.\n\ntest_config = {\n    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n        \"inputs\": {\n            \"dataset\": vm_train_ds,\n            \"model\": vm_model,\n        },\n    }\n}\nresults = vm.run_documentation_tests(\n    section=[\"model_development\"],\n    inputs={\n        \"dataset\": vm_test_ds,  # Any test that requires a single dataset will use vm_test_ds\n        \"model\": vm_model,\n        \"datasets\": (\n            vm_train_ds,\n            vm_test_ds,\n        ),  # Any test that requires multiple datasets will use vm_train_ds and vm_test_ds\n    },\n    config=test_config,\n)"
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#implementing-custom-tests",
    "href": "notebooks/tutorials/intro_for_model_developers.html#implementing-custom-tests",
    "title": "ValidMind Introduction for Model Developers",
    "section": "3. Implementing custom tests",
    "text": "3. Implementing custom tests\nThis section assumes that model developers already have a repository of custom made tests that they consider critical to include in the documentation. Here we provide details on how to easily integrate custom tests with ValidMind.\nFor a more in-depth introduction to custom metrics, refer to this notebook.\nA custom metric is any function that takes a set of inputs and parameters as arguments and returns one or more outputs. The function can be as simple or as complex as you need it to be. It can use external libraries, make API calls, or do anything else that you can do in Python. The only requirement is that the function signature and return values can be “understood” and handled by the ValidMind Developer Framework. As such, custom metrics offer added flexibility by extending the default metrics provided by ValidMind, enabling you to document any type of model or use case.\nIn the following example, you will learn how to implement a custom inline metric/test that calculates the confusion matrix for a binary classification model. You will see that the custom metric function is just a regular Python function that can include and require any Python library as you see fit.\nNOTE: in the context of Jupyter notebooks, we will use the word inline to refer to functions (or code) defined in the same notebook where they are used (this one) and not in a separate file, as we will see later with test providers.\n\n\nCreate a confusion matrix plot\nTo understand how to create a custom metric from anything, let’s first create a confusion matrix plot using the confusion_matrix function from the sklearn.metrics module.\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\n# Get the predicted classes\ny_pred = log_reg.predict(vm_test_ds.x)\n\nconfusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n\ncm_display = metrics.ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix, display_labels=[False, True]\n)\ncm_display.plot()\n\nWe will now create a @vm.metric wrapper that will allow you to create a reusable metric. Note the following changes in the code below:\n\nThe function confusion_matrix takes two arguments dataset and model. This is a VMDataset and VMModel object respectively.\n\nVMDataset objects allow you to access the dataset’s true (target) values by accessing the .y attribute.\nVMDataset objects allow you to access the predictions for a given model by accessing the .y_pred() method.\n\nThe function docstring provides a description of what the metric does. This will be displayed along with the result in this notebook as well as in the ValidMind platform.\nThe function body calculates the confusion matrix using the sklearn.metrics.confusion_matrix function as we just did above.\nThe function then returns the ConfusionMatrixDisplay.figure_ object - this is important as the ValidMind framework expects the output of the custom metric to be a plot or a table.\nThe @vm.metric decorator is doing the work of creating a wrapper around the function that will allow it to be run by the ValidMind framework. It also registers the metric so it can be found by the ID my_custom_metrics.ConfusionMatrix (see the section below on how test IDs work in ValidMind and why this format is important)\n\n\n@vm.metric(\"my_custom_metrics.ConfusionMatrix\")\ndef confusion_matrix(dataset, model):\n    \"\"\"The confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known.\n\n    The confusion matrix is a 2x2 table that contains 4 values:\n\n    - True Positive (TP): the number of correct positive predictions\n    - True Negative (TN): the number of correct negative predictions\n    - False Positive (FP): the number of incorrect positive predictions\n    - False Negative (FN): the number of incorrect negative predictions\n\n    The confusion matrix can be used to assess the holistic performance of a classification model by showing the accuracy, precision, recall, and F1 score of the model on a single figure.\n    \"\"\"\n    y_true = dataset.y\n    y_pred = dataset.y_pred(model=model)\n\n    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n\n    cm_display = metrics.ConfusionMatrixDisplay(\n        confusion_matrix=confusion_matrix, display_labels=[False, True]\n    )\n    cm_display.plot()\n\n    plt.close()  # close the plot to avoid displaying it\n\n    return cm_display.figure_  # return the figure object itself\n\nYou can now run the newly created custom metric on both the training and test datasets using the run_test() function:\n\n# Training dataset\nresult = vm.tests.run_test(\n    \"my_custom_metrics.ConfusionMatrix:training_dataset\",\n    inputs={\"model\": vm_model, \"dataset\": vm_train_ds},\n)\n\n\n# Test dataset\nresult = vm.tests.run_test(\n    \"my_custom_metrics.ConfusionMatrix:test_dataset\",\n    inputs={\"model\": vm_model, \"dataset\": vm_test_ds},\n)\n\n\n\n\nAdd parameters to custom metrics\nCustom metrics can take parameters just like any other function. Let’s modify the confusion_matrix function to take an additional parameter normalize that will allow you to normalize the confusion matrix.\n\n@vm.metric(\"my_custom_metrics.ConfusionMatrix\")\ndef confusion_matrix(dataset, model, normalize=False):\n    \"\"\"The confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known.\n\n    The confusion matrix is a 2x2 table that contains 4 values:\n\n    - True Positive (TP): the number of correct positive predictions\n    - True Negative (TN): the number of correct negative predictions\n    - False Positive (FP): the number of incorrect positive predictions\n    - False Negative (FN): the number of incorrect negative predictions\n\n    The confusion matrix can be used to assess the holistic performance of a classification model by showing the accuracy, precision, recall, and F1 score of the model on a single figure.\n    \"\"\"\n    y_true = dataset.y\n    y_pred = dataset.y_pred(model=model)\n\n    if normalize:\n        confusion_matrix = metrics.confusion_matrix(y_true, y_pred, normalize=\"all\")\n    else:\n        confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n\n    cm_display = metrics.ConfusionMatrixDisplay(\n        confusion_matrix=confusion_matrix, display_labels=[False, True]\n    )\n    cm_display.plot()\n\n    plt.close()  # close the plot to avoid displaying it\n\n    return cm_display.figure_  # return the figure object itself\n\n\n\n\nPass parameters to custom metrics\nYou can pass parameters to custom metrics by providing a dictionary of parameters to the run_test() function. The parameters will override any default parameters set in the custom metric definition. Note that dataset and model are still passed as inputs. Since these are VMDataset or VMModel inputs, they have a special meaning. When declaring a dataset, model, datasets or models argument in a custom metric function, the Developer Framework will expect these get passed as inputs to run_test() (or run_documentation_tests() instead).\nRe-running the confusion matrix with normalize=True looks like this:\n\n# Test dataset with normalize=True\nresult = vm.tests.run_test(\n    \"my_custom_metrics.ConfusionMatrix:test_dataset_normalized\",\n    inputs={\"model\": vm_model, \"dataset\": vm_test_ds},\n    params={\"normalize\": True},\n)\n\n\n\n\nLog the confusion matrix results\nAs you saw in the pearson correlation example, you can log any result to the ValidMind platform with the .log() method of the result object. This will allow you to add the result to the documentation.\nYou can now do the same for the confusion matrix results.\n\nresult.log()\n\n\n\n\nUsing external test providers\nCreating inline custom tests with a function is a great way to customize your model documentation. However, sometimes you may want to reuse the same set of tests across multiple models and share them with developers in your organization. In this case, you can create a custom test provider that will allow you to load custom tests from a local folder or a git repository.\nIn this section you will learn how to declare a local filesystem test provider that allows loading tests from a local folder following these high level steps:\n\nCreate a folder of custom tests from existing, inline tests (tests that exists in your active Jupyter notebook)\nSave an inline test to a file\nDefine and register a LocalTestProvider that points to that folder\nRun test provider tests\nAdd the test results to your documentation\n\n\n\nCreate a folder of custom tests from existing inline tests\nHere you will create a new folder that will contain reusable, custom tests. The following code snippet will create a new my_tests directory in the current working directory if it doesn’t exist.\n\ntests_folder = \"my_tests\"\n\nimport os\n\n# create tests folder\nos.makedirs(tests_folder, exist_ok=True)\n\n# remove existing tests\nfor f in os.listdir(tests_folder):\n    # remove files and pycache\n    if f.endswith(\".py\") or f == \"__pycache__\":\n        os.system(f\"rm -rf {tests_folder}/{f}\")\n\nAfter running the command above, you should see a new directory next to this notebook file:\n\n\n\nscreenshot showing my_tests directory\n\n\n\n\n\nSave an inline test to a file\nThe @vm.metric decorator that was used above to register these as one-off custom tests also adds a convenience method to the function object that allows you to simply call &lt;func_name&gt;.save() to save it to a file. This will save the function to a Python file to a path you specify. In this case, you can pass the variable tests_folder to save it to the custom tests folder we created.\nNormally, this will get you started by creating the file and saving the function code with the correct name. But it won’t automatically add any import or other functions/variables outside of the function that are needed for the test to run. The save() method allows you to pass an optional imports argument that will ensure the necessary imports are added to the file.\nFor the confusion_matrix metric, note the imports that are required for the function to run properly:\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nYou can pass these imports to the save() method to ensure they are included in the file with the following command:\n\nconfusion_matrix.save(\n    tests_folder,\n    imports=[\"import matplotlib.pyplot as plt\", \"from sklearn import metrics\"],\n)\n\n\nWhat happened?\nThe save() method saved the confusion_matrix function to a file named ConfusionMatrix.py in the my_tests folder. Note that the new file provides some context on the origin of the test, which is useful for traceability.\n# Saved from __main__.confusion_matrix\n# Original Test ID: my_custom_metrics.ConfusionMatrix\n# New Test ID: &lt;test_provider_namespace&gt;.ConfusionMatrix\nAdditionally, the new test function has been stripped off its decorator, as it now resides in a file that will be loaded by the test provider:\ndef ConfusionMatrix(dataset, model, normalize=False):\n\n\n\n\nDefine and register a LocalTestProvider that points to that folder\nWith the my_tests folder now having a sample custom test, you can now initialize a test provider that will tell the Developer Framework where to find these tests. ValidMind offers out-of-the-box test providers for local tests (i.e. tests in a folder) or a Github provider for tests in a Github repository. You can also create your own test provider by creating a class that has a load_test method that takes a test ID and returns the test function matching that ID.\nThe most important attribute for a test provider is its namespace. This is a string that will be used to prefix test IDs in model documentation. This allows you to have multiple test providers with tests that can even share the same ID, but are distinguished by their namespace.\nAn extended introduction to test providers can be found in this notebook.\n\n\n\n\nInitializing a local test provider\nFor most use-cases, the local test provider should be sufficient. This test provider allows you load custom tests from a designated directory. Let’s go ahead and see how we can do this with our custom tests.\n\nfrom validmind.tests import LocalTestProvider\n\n# initialize the test provider with the tests folder we created earlier\nmy_test_provider = LocalTestProvider(tests_folder)\n\nvm.tests.register_test_provider(\n    namespace=\"my_test_provider\",\n    test_provider=my_test_provider,\n)\n# `my_test_provider.load_test()` will be called for any test ID that starts with `my_test_provider`\n# e.g. `my_test_provider.ConfusionMatrix` will look for a function named `ConfusionMatrix` in `my_tests/ConfusionMatrix.py` file\n\n\n\nRun test provider tests\nNow that you have set up the test provider, you can run any test that’s located in the tests folder by using the run_test() method as with any other test. For tests that reside in a test provider directory, the test ID will be the namespace specified when registering the provider, followed by the path to the test file relative to the tests folder. For example, the Confusion Matrix test we created earlier will have the test ID my_test_provider.ConfusionMatrix. You could organize the tests in subfolders, say classification and regression, and the test ID for the Confusion Matrix test would then be my_test_provider.classification.ConfusionMatrix.\nLet’s go ahead and re-run the confusion matrix test by using the test ID my_test_provider.ConfusionMatrix. This should load the test from the test provider and run it as before.\n\nresult = vm.tests.run_test(\n    \"my_test_provider.ConfusionMatrix\",\n    inputs={\"model\": vm_model, \"dataset\": vm_test_ds},\n    params={\"normalize\": True},\n)\n\nresult.log()\n\n\n\n\nAdd the test results to your documentation\nYou have already seen how to add individual results to the model documentation using the platform. Let’s repeat the process and add the confusion matrix to the Model Development -&gt; Model Evaluation section of the documentation. The “add test driven block” dialog should now show the new test result coming from the test provider:\n\n\n\nscreenshot showing confusion matrix result"
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#finalize-testing-and-documentation",
    "href": "notebooks/tutorials/intro_for_model_developers.html#finalize-testing-and-documentation",
    "title": "ValidMind Introduction for Model Developers",
    "section": "4. Finalize testing and documentation",
    "text": "4. Finalize testing and documentation\nIn this section we cover how to finalize the testing and documentation of your model by focusing on:\n\nUsing run_documentation_tests() to ensure custom test results are included in your documentation\nViewing and updating the configuration for the entire model documentation template\n\n\n\nUse run_documentation_tests() to ensure custom test results are included in your documentation\nAfter adding test driven blocks to your model documentation, changes should persist and become available every time you call vm.preview_template(). However, you need to reload the connection to the ValidMind platform if you have added test driven blocks when the connection was already established.\n\nvm.reload()\n\nNow, run preview_template() and verify that the new confusion matrix test you added is included in the proper section.\n\nvm.preview_template()\n\nSince the test ID is now registered in the document you can now run tests for an entire section and all additional custom tests should be loaded without issues. Let’s run all tests in the model_evaluation section of the documentation. Note that we have been running the sample custom confusion matrix with normalize=True to demonstrate the ability to provide custom parameters.\nIn the Run the model evaluation tests section above you learned how to assign inputs to individual tests with run_documentation_tests(). Assigning parametesr is similar, you only need to provide assign a params dictionary to a given test ID, my_test_provider.ConfusionMatrix in this case.\n\ntest_config = {\n    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n        \"inputs\": {\n            \"dataset\": vm_train_ds,\n            \"model\": vm_model,\n        },\n    },\n    \"my_test_provider.ConfusionMatrix\": {\n        \"params\": {\"normalize\": True},\n    },\n}\nresults = vm.run_documentation_tests(\n    section=[\"model_evaluation\"],\n    inputs={\n        \"dataset\": vm_test_ds,  # Any test that requires a single dataset will use vm_test_ds\n        \"model\": vm_model,\n        \"datasets\": (\n            vm_train_ds,\n            vm_test_ds,\n        ),  # Any test that requires multiple datasets will use vm_train_ds and vm_test_ds\n    },\n    config=test_config,\n)\n\n\n\n\nViewing and updating the configuration for the entire model documentation template\nThe Developer Framework provides a utility function called vm.get_test_suite().get_default_config() that allows you to render the default configuration for the entire documentation template. This configuration will contain all the test IDs and their default parameters. You can then modify this configuration as needed and pass it to run_documentation_tests() to run all tests in the documentation template if needed. You also have the option to continue running tests for one section at a time, get_default_config() still provides a useful reference for providing default parametes to every test.\n\nimport json\n\nproject_test_suite = vm.get_test_suite()\nconfig = project_test_suite.get_default_config()\nprint(\"Suite Config: \\n\", json.dumps(config, indent=2))\n\n\n\nUpdate the config\nNote that the default config does not assign any inputs to a test, this is expected. You can assign inputs to individual tests as needed, depending on the datasets and models you want to pass to individual tests. The config dictionary, as a mapping of test IDs to test configurations, allows you to do this.\nFor this particular documentation template (binary classification), the Developer Framework provides a sample configuration that can be used to populate the entire model documentation using the following inputs as placeholders:\n\nA raw_dataset raw dataset\nA train_dataset training dataset\nA test_dataset test dataset\nA trained model instance\n\nAs part of updating the config you will need to ensure the correct input_ids are used in the final config passed to run_documentation_tests().\n\nfrom validmind.datasets.classification import customer_churn\nfrom validmind.utils import preview_test_config\n\ntest_config = customer_churn.get_demo_test_config()\npreview_test_config(test_config)\n\nUsing this sample configuration, let’s finish populating model documentation by running all tests for the model_development section of the documentation. Recall that the training and test datasets in our exercise have the following input_id values:\n\ntrain_dataset_final for the training dataset\ntest_dataset_final for the test dataset\n\n\nconfig = {\n    \"validmind.model_validation.ModelMetadata\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\"},\n    },\n    \"validmind.data_validation.DatasetSplit\": {\n        \"inputs\": {\"datasets\": [\"train_dataset_final\", \"test_dataset_final\"]},\n    },\n    \"validmind.model_validation.sklearn.PopulationStabilityIndex\": {\n        \"inputs\": {\n            \"model\": \"log_reg_model_v1\",\n            \"datasets\": [\"train_dataset_final\", \"test_dataset_final\"],\n        },\n        \"params\": {\"num_bins\": 10, \"mode\": \"fixed\"},\n    },\n    \"validmind.model_validation.sklearn.ConfusionMatrix\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n    },\n    \"my_test_provider.ConfusionMatrix\": {\n        \"inputs\": {\"dataset\": \"test_dataset_final\", \"model\": \"log_reg_model_v1\"},\n    },\n    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"train_dataset_final\"}\n    },\n    \"validmind.model_validation.sklearn.ClassifierPerformance:out_of_sample\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"}\n    },\n    \"validmind.model_validation.sklearn.PrecisionRecallCurve\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n    },\n    \"validmind.model_validation.sklearn.ROCCurve\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n    },\n    \"validmind.model_validation.sklearn.TrainingTestDegradation\": {\n        \"inputs\": {\n            \"model\": \"log_reg_model_v1\",\n            \"datasets\": [\"train_dataset_final\", \"test_dataset_final\"],\n        },\n        \"params\": {\n            \"metrics\": [\"accuracy\", \"precision\", \"recall\", \"f1\"],\n            \"max_threshold\": 0.1,\n        },\n    },\n    \"validmind.model_validation.sklearn.MinimumAccuracy\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n        \"params\": {\"min_threshold\": 0.7},\n    },\n    \"validmind.model_validation.sklearn.MinimumF1Score\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n        \"params\": {\"min_threshold\": 0.5},\n    },\n    \"validmind.model_validation.sklearn.MinimumROCAUCScore\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n        \"params\": {\"min_threshold\": 0.5},\n    },\n    \"validmind.model_validation.sklearn.PermutationFeatureImportance\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n    },\n    \"validmind.model_validation.sklearn.SHAPGlobalImportance\": {\n        \"inputs\": {\"model\": \"log_reg_model_v1\", \"dataset\": \"test_dataset_final\"},\n        \"params\": {\"kernel_explainer_samples\": 10},\n    },\n    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n        \"inputs\": {\n            \"model\": \"log_reg_model_v1\",\n            \"datasets\": [\"train_dataset_final\", \"test_dataset_final\"],\n        },\n        \"params\": {\n            \"thresholds\": {\"accuracy\": 0.75, \"precision\": 0.5, \"recall\": 0.5, \"f1\": 0.7}\n        },\n    },\n    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n        \"inputs\": {\n            \"model\": \"log_reg_model_v1\",\n            \"datasets\": [\"train_dataset_final\", \"test_dataset_final\"],\n        },\n        \"params\": {\"cut_off_percentage\": 4},\n    },\n    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n        \"inputs\": {\n            \"model\": \"log_reg_model_v1\",\n            \"datasets\": [\"train_dataset_final\", \"test_dataset_final\"],\n        },\n        \"params\": {\n            \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n            \"accuracy_decay_threshold\": 4,\n        },\n    },\n}\n\n\nfull_suite = vm.run_documentation_tests(\n    section=\"model_development\",\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/tutorials/intro_for_model_developers.html#where-to-go-from-here",
    "href": "notebooks/tutorials/intro_for_model_developers.html#where-to-go-from-here",
    "title": "ValidMind Introduction for Model Developers",
    "section": "Where to go from here",
    "text": "Where to go from here\nIn this notebook you have learned the end-to-end process to document a model with the ValidMind Developer Framework, running through some very common scenarios in a typical model development setting:\n\nRunning out-of-the-box tests\nDocumenting your model by adding evidence to model documentation\nExtending the capabilities of the Developer Framework by implementing custom tests\nEnsuring that the documentation is complete by running all tests in the documentation template\n\nAs a next step, you can explore the following notebooks to get a deeper understanding on how the developer framework allows you generate model documentation for any use case:\n\n\nUse cases\n\nApplication scorecard demo\nLinear regression documentation demo\nLLM model documentation demo\n\n\n\n\nMore how-to guides and code samples\n\nExplore available tests in detail\nIn-depth guide for implementing custom tests\nIn-depth guide to external test providers\nConfiguring dataset features\nIntroduction to unit and composite metrics\n\n\n\n\nDiscover more learning resources\nAll notebook samples can be found in the following directories of the Developer Framework GitHub repository:\n\nCode samples\nHow-to guides"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html",
    "title": "Document multiple results for the same test",
    "section": "",
    "text": "Documentation templates facilitate the presentation of multiple unique metric results for a single metric.\nConsider various scenarios where you may intend to showcase results of the same metric with diverse inputs:\nThis interactive notebook guides you through the process of documenting a model with the ValidMind Developer Framework. It uses the Bank Customer Churn Prediction sample dataset from Kaggle to train a simple classification model. As part of the notebook, you will learn how to render more than one unique metric result for the same metric while exploring how the documentation process works:"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#contents",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#contents",
    "title": "Document multiple results for the same test",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nUpdate the customer churn demo template\n\nInitialize the Python environment\n\nPreview the documentation template\n\n\nLoad the sample dataset\n\nInitialize a ValidMind dataset object\n\n\nDocument the model\n\nPrepare datasets\n\nInitialize the training and test datasets\n\nRun documentation tests\n\nRun the individual tests using the run_test\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#about-validmind",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#about-validmind",
    "title": "Document multiple results for the same test",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#install-the-client-library",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#install-the-client-library",
    "title": "Document multiple results for the same test",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#initialize-the-client-library",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#initialize-the-client-library",
    "title": "Document multiple results for the same test",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Binary classification as the template and Marketing/Sales - Attrition/Churn Management as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"...\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#update-the-customer-churn-demo-template",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#update-the-customer-churn-demo-template",
    "title": "Document multiple results for the same test",
    "section": "Update the customer churn demo template",
    "text": "Update the customer churn demo template\nBefore you initialize the client library by running the notebook, edit the Binary classification template to make a copy of a metric of interest and update it with different result_id fields for each entry:\n\nGo to Settings &gt; Templates and click on the Binary classification template. Let’s say we want to show Skewness results for training and test datasets.\n\nTo do this we replace\n- content_type: test\n  content_id: validmind.data_validation.Skewness\nwith\n- content_type: test\n  content_id: validmind.data_validation.Skewness:training_data\n- content_type: test\n  content_id: validmind.data_validation.Skewness:test_data\nThis way, we can show two results of the same test in the model document. Here, the training_data and test_data could be any string. However, they should be unique for the same test.\n\nClick on Prepare new version, provide some version notes and click on Save new version to save a new version of this template.\nNext, we need to swap our model documentation to use this new version of the template. Follow the steps on this guide to swap the template of our customer churn model: https://docs.validmind.ai/guide/swap-documentation-templates.html.\n\nIn the following sections we provide more context on how these content_id fields mentioned earlier get mapped to the actual tests."
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#initialize-the-python-environment",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#initialize-the-python-environment",
    "title": "Document multiple results for the same test",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections. You will see two blocks with different result IDs for skewness.\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#load-the-sample-dataset",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#load-the-sample-dataset",
    "title": "Document multiple results for the same test",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library, along with a second, different dataset (taiwan_credit) you can try as well.\nTo be able to use either sample dataset, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nInitialize a ValidMind dataset object\nBefore you can run a test suite, which are a collection of tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to analyze\ntarget_column — the name of the target column in the dataset\nclass_labels — the list of class labels used for classification model training\n\n\nvm_dataset = vm.init_dataset(\n    input_id=\"raw_dataset\",\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#document-the-model",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#document-the-model",
    "title": "Document multiple results for the same test",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\n\nPrepare datasets\nDataFrame (df) preprocessing is simplified by employing demo_dataset.preprocess to partition it into distinct datasets (train_df, validation_df, and test_df)\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\n\n\nInitialize the training and test datasets\nWith the datasets ready, you can now initialize the training and test datasets (train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    input_id=\"train_dataset\",\n    dataset=train_df,\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset\",\n    dataset=test_df,\n    target_column=demo_dataset.target_column\n)\n\n\n\n\nRun documentation tests\nNow specify inputs and params for individual tests using config parameter. The results for the both the datasets will be visible in the documentation. The inputs in the config get priority over global inputs in the run_documentation_tests.\n\nconfig = {\n    \"validmind.data_validation.Skewness:training_data\": {\n        \"params\": {\"max_threshold\": 1},\n        \"inputs\": {\"dataset\": vm_train_ds}\n    },\n    \"validmind.data_validation.Skewness:test_data\": {\n        \"params\": {\"max_threshold\": 1.5},\n        \"inputs\": {\"dataset\": vm_test_ds}\n    },\n}\n\ntests_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_dataset,\n    },\n    config=config,\n    section=[\"data_preparation\"]\n)\n\n\n\n\nRun the individual tests using the run_test\nNow run the Skewness tests for training and test datasets. The results for the both the datasets will be visible in the documentation.\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.Skewness:training_data\",\n    params={\n        \"max_threshold\": 1\n    },\n    inputs={\n        \"dataset\": vm_train_ds\n    }\n)\ntest.log()\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.Skewness:test_data\",\n    params={\n        \"max_threshold\": 1.5\n    },\n    inputs={\n        \"dataset\": vm_test_ds,\n    }\n)\ntest.log()"
  },
  {
    "objectID": "notebooks/how_to/document_multiple_results_for_the_same_test.html#next-steps",
    "href": "notebooks/how_to/document_multiple_results_for_the_same_test.html#next-steps",
    "title": "Document multiple results for the same test",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\nExpand the 2. Data Preparation section and take a look around.\nYou can now see the skewness tests results of training and test datasets in the Data Preparation section.\n\nFrom here, you can also make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html",
    "href": "notebooks/how_to/explore_test_suites.html",
    "title": "Explore test suites",
    "section": "",
    "text": "Explore the the test suites and tests that are available in the ValidMind Developer Framework, and retrieve detailed information for each.\nThis notebook shows how you can learn more about the test suites and tests that are available in the ValidMind Developer Framework. The step-by-step instructions provide all the code required to retrieve a list of available test suites, details for a specific test suite, details for a specific test within a suite, a verbose details view of a test suite and its tests, and a list of all tests."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#contents",
    "href": "notebooks/how_to/explore_test_suites.html#contents",
    "title": "Explore test suites",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nGet a list of available test suites\n\nGet details for a test suite\n\nGet details for a test\n\nGet a verbose details view of a test suite and its tests\n\n\nList all tests\n\nNext steps\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#about-validmind",
    "href": "notebooks/how_to/explore_test_suites.html#about-validmind",
    "title": "Explore test suites",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#install-the-client-library",
    "href": "notebooks/how_to/explore_test_suites.html#install-the-client-library",
    "title": "Explore test suites",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install -q validmind\n\nWARNING: You are using pip version 22.0.3; however, version 24.0 is available.\nYou should consider upgrading via the '/Users/andres/code/validmind-sdk/.venv/bin/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#get-a-list-of-available-test-suites",
    "href": "notebooks/how_to/explore_test_suites.html#get-a-list-of-available-test-suites",
    "title": "Explore test suites",
    "section": "Get a list of available test suites",
    "text": "Get a list of available test suites\nTo get the list of all test suites available in the ValidMind Developer Framework:\n\nimport validmind as vm\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTests\n\n\n\n\nclassifier_model_diagnosis\nClassifierDiagnosis\nTest suite for sklearn classifier model diagnosis tests\nvalidmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nclassifier_full_suite\nClassifierFullSuite\nFull test suite for binary classification models.\nvalidmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierPerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nclassifier_metrics\nClassifierMetrics\nTest suite for sklearn classifier metrics\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierPerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nclassifier_model_validation\nClassifierModelValidation\nTest suite for binary classification models.\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierPerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nclassifier_validation\nClassifierPerformance\nTest suite for sklearn classifier models\nvalidmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\ncluster_full_suite\nClusterFullSuite\nFull test suite for clustering models.\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.HomogeneityScore, validmind.model_validation.sklearn.CompletenessScore, validmind.model_validation.sklearn.VMeasure, validmind.model_validation.sklearn.AdjustedRandIndex, validmind.model_validation.sklearn.AdjustedMutualInformation, validmind.model_validation.sklearn.FowlkesMallowsScore, validmind.model_validation.sklearn.ClusterPerformanceMetrics, validmind.model_validation.sklearn.ClusterCosineSimilarity, validmind.model_validation.sklearn.SilhouettePlot, validmind.model_validation.ClusterSizeDistribution, validmind.model_validation.sklearn.HyperParametersTuning, validmind.model_validation.sklearn.KMeansClustersOptimization\n\n\ncluster_metrics\nClusterMetrics\nTest suite for sklearn clustering metrics\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.HomogeneityScore, validmind.model_validation.sklearn.CompletenessScore, validmind.model_validation.sklearn.VMeasure, validmind.model_validation.sklearn.AdjustedRandIndex, validmind.model_validation.sklearn.AdjustedMutualInformation, validmind.model_validation.sklearn.FowlkesMallowsScore, validmind.model_validation.sklearn.ClusterPerformanceMetrics, validmind.model_validation.sklearn.ClusterCosineSimilarity, validmind.model_validation.sklearn.SilhouettePlot\n\n\ncluster_performance\nClusterPerformance\nTest suite for sklearn cluster performance\nvalidmind.model_validation.ClusterSizeDistribution\n\n\nembeddings_full_suite\nEmbeddingsFullSuite\nFull test suite for embeddings models.\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.embeddings.DescriptiveAnalytics, validmind.model_validation.embeddings.CosineSimilarityDistribution, validmind.model_validation.embeddings.ClusterDistribution, validmind.model_validation.embeddings.EmbeddingsVisualization2D, validmind.model_validation.embeddings.StabilityAnalysisRandomNoise, validmind.model_validation.embeddings.StabilityAnalysisSynonyms, validmind.model_validation.embeddings.StabilityAnalysisKeyword, validmind.model_validation.embeddings.StabilityAnalysisTranslation\n\n\nembeddings_metrics\nEmbeddingsMetrics\nTest suite for embeddings metrics\nvalidmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.embeddings.DescriptiveAnalytics, validmind.model_validation.embeddings.CosineSimilarityDistribution, validmind.model_validation.embeddings.ClusterDistribution, validmind.model_validation.embeddings.EmbeddingsVisualization2D\n\n\nembeddings_model_performance\nEmbeddingsPerformance\nTest suite for embeddings model performance\nvalidmind.model_validation.embeddings.StabilityAnalysisRandomNoise, validmind.model_validation.embeddings.StabilityAnalysisSynonyms, validmind.model_validation.embeddings.StabilityAnalysisKeyword, validmind.model_validation.embeddings.StabilityAnalysisTranslation\n\n\nhyper_parameters_optimization\nKmeansParametersOptimization\nTest suite for sklearn hyperparameters optimization\nvalidmind.model_validation.sklearn.HyperParametersTuning, validmind.model_validation.sklearn.KMeansClustersOptimization\n\n\nllm_classifier_full_suite\nLLMClassifierFullSuite\nFull test suite for LLM classification models.\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.nlp.StopWords, validmind.data_validation.nlp.Punctuations, validmind.data_validation.nlp.CommonWords, validmind.data_validation.nlp.TextDescription, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierPerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis, validmind.prompt_validation.Bias, validmind.prompt_validation.Clarity, validmind.prompt_validation.Conciseness, validmind.prompt_validation.Delimitation, validmind.prompt_validation.NegativeInstruction, validmind.prompt_validation.Robustness, validmind.prompt_validation.Specificity\n\n\nprompt_validation\nPromptValidation\nTest suite for prompt validation\nvalidmind.prompt_validation.Bias, validmind.prompt_validation.Clarity, validmind.prompt_validation.Conciseness, validmind.prompt_validation.Delimitation, validmind.prompt_validation.NegativeInstruction, validmind.prompt_validation.Robustness, validmind.prompt_validation.Specificity\n\n\nnlp_classifier_full_suite\nNLPClassifierFullSuite\nFull test suite for NLP classification models.\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.nlp.StopWords, validmind.data_validation.nlp.Punctuations, validmind.data_validation.nlp.CommonWords, validmind.data_validation.nlp.TextDescription, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierPerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nregression_metrics\nRegressionMetrics\nTest suite for performance metrics of regression metrics\nvalidmind.data_validation.DatasetSplit, validmind.model_validation.ModelMetadata, validmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nregression_model_description\nRegressionModelDescription\nTest suite for performance metric of regression model of statsmodels library\nvalidmind.data_validation.DatasetSplit, validmind.model_validation.ModelMetadata\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest suite for metrics comparison of regression model of statsmodels library\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs, validmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nregression_models_comparison\nRegressionModelsComparison\nTest suite for regression models performance comparison\nvalidmind.model_validation.sklearn.RegressionModelsPerformanceComparison\n\n\nregression_full_suite\nRegressionFullSuite\nFull test suite for regression models.\nvalidmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues, validmind.data_validation.DatasetSplit, validmind.model_validation.ModelMetadata, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.RegressionErrors, validmind.model_validation.sklearn.RegressionR2Square, validmind.model_validation.sklearn.RegressionModelsPerformanceComparison\n\n\nregression_performance\nRegressionPerformance\nTest suite for regression model performance\nvalidmind.model_validation.sklearn.RegressionErrors, validmind.model_validation.sklearn.RegressionR2Square\n\n\nsummarization_metrics\nSummarizationMetrics\nTest suite for Summarization metrics\nvalidmind.model_validation.RougeMetrics, validmind.model_validation.TokenDisparity, validmind.model_validation.BleuScore, validmind.model_validation.BertScore, validmind.model_validation.ContextualRecall\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\nvalidmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest suite to extract metadata and descriptive statistics from a tabular dataset\nvalidmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix\n\n\ntabular_data_quality\nTabularDataQuality\nTest suite for data quality on tabular datasets\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues\n\n\ntext_data_quality\nTextDataQuality\nTest suite for data quality on text data\nvalidmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.nlp.StopWords, validmind.data_validation.nlp.Punctuations, validmind.data_validation.nlp.CommonWords, validmind.data_validation.nlp.TextDescription\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest suite for data quality on time series datasets\nvalidmind.data_validation.TimeSeriesOutliers, validmind.data_validation.TimeSeriesMissingValues, validmind.data_validation.TimeSeriesFrequency\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\nvalidmind.data_validation.TimeSeriesOutliers, validmind.data_validation.TimeSeriesMissingValues, validmind.data_validation.TimeSeriesFrequency, validmind.data_validation.TimeSeriesLinePlot, validmind.data_validation.TimeSeriesHistogram, validmind.data_validation.ACFandPACFPlot, validmind.data_validation.SeasonalDecompose, validmind.data_validation.AutoSeasonality, validmind.data_validation.AutoStationarity, validmind.data_validation.RollingStatsPlot, validmind.data_validation.AutoAR, validmind.data_validation.AutoMA, validmind.data_validation.ScatterPlot, validmind.data_validation.LaggedCorrelationHeatmap, validmind.data_validation.EngleGrangerCoint, validmind.data_validation.SpreadPlot\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nvalidmind.data_validation.DatasetSplit, validmind.model_validation.ModelMetadata, validmind.model_validation.statsmodels.RegressionModelsCoeffs, validmind.model_validation.statsmodels.RegressionModelsPerformance, validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels, validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\ntime_series_multivariate\nTimeSeriesMultivariate\nThis test suite provides a preliminary understanding of the features and relationship in multivariate dataset. It presents various multivariate visualizations that can help identify patterns, trends, and relationships between pairs of variables. The visualizations are designed to explore the relationships between multiple features simultaneously. They allow you to quickly identify any patterns or trends in the data, as well as any potential outliers or anomalies. The individual feature distribution can also be explored to provide insight into the range and frequency of values observed in the data. This multivariate analysis test suite aims to provide an overview of the data structure and guide further exploration and modeling.\nvalidmind.data_validation.ScatterPlot, validmind.data_validation.LaggedCorrelationHeatmap, validmind.data_validation.EngleGrangerCoint, validmind.data_validation.SpreadPlot\n\n\ntime_series_sensitivity\nTimeSeriesSensitivity\nThis test suite performs sensitivity analysis on a statsmodels OLS linear regression model by applying distinct shocks to each input variable individually and then computing the model's predictions. The aim of this test suite is to investigate the model's responsiveness to variations in its inputs. By juxtaposing the model's predictions under baseline and shocked conditions, users can visually evaluate the sensitivity of the model to changes in each variable. This kind of analysis can also shed light on potential model limitations, including over-reliance on specific variables or insufficient responsiveness to changes in inputs. As a result, this test suite can provide insights that may be beneficial for refining the model structure, improving its robustness, and ensuring a more reliable prediction performance.\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nThis test suite provides a preliminary understanding of the target variable(s) used in the time series dataset. It visualizations that present the raw time series data and a histogram of the target variable(s). The raw time series data provides a visual inspection of the target variable's behavior over time. This helps to identify any patterns or trends in the data, as well as any potential outliers or anomalies. The histogram of the target variable displays the distribution of values, providing insight into the range and frequency of values observed in the data.\nvalidmind.data_validation.TimeSeriesLinePlot, validmind.data_validation.TimeSeriesHistogram, validmind.data_validation.ACFandPACFPlot, validmind.data_validation.SeasonalDecompose, validmind.data_validation.AutoSeasonality, validmind.data_validation.AutoStationarity, validmind.data_validation.RollingStatsPlot, validmind.data_validation.AutoAR, validmind.data_validation.AutoMA"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#get-details-for-a-test-suite",
    "href": "notebooks/how_to/explore_test_suites.html#get-details-for-a-test-suite",
    "title": "Explore test suites",
    "section": "Get details for a test suite",
    "text": "Get details for a test suite\nTo get the list of tests available in a given test suite:\n\nvm.test_suites.describe_suite(\"classifier_full_suite\")\n\n\n\n\n\n\nID\nName\nDescription\nTests\n\n\n\n\nclassifier_full_suite\nClassifierFullSuite\nFull test suite for binary classification models.\nvalidmind.data_validation.DatasetDescription, validmind.data_validation.DescriptiveStatistics, validmind.data_validation.PearsonCorrelationMatrix, validmind.data_validation.ClassImbalance, validmind.data_validation.Duplicates, validmind.data_validation.HighCardinality, validmind.data_validation.HighPearsonCorrelation, validmind.data_validation.MissingValues, validmind.data_validation.Skewness, validmind.data_validation.UniqueRows, validmind.data_validation.TooManyZeroValues, validmind.model_validation.ModelMetadata, validmind.data_validation.DatasetSplit, validmind.model_validation.sklearn.ConfusionMatrix, validmind.model_validation.sklearn.ClassifierPerformance, validmind.model_validation.sklearn.PermutationFeatureImportance, validmind.model_validation.sklearn.PrecisionRecallCurve, validmind.model_validation.sklearn.ROCCurve, validmind.model_validation.sklearn.PopulationStabilityIndex, validmind.model_validation.sklearn.SHAPGlobalImportance, validmind.model_validation.sklearn.MinimumAccuracy, validmind.model_validation.sklearn.MinimumF1Score, validmind.model_validation.sklearn.MinimumROCAUCScore, validmind.model_validation.sklearn.TrainingTestDegradation, validmind.model_validation.sklearn.ModelsPerformanceComparison, validmind.model_validation.sklearn.OverfitDiagnosis, validmind.model_validation.sklearn.WeakspotsDiagnosis, validmind.model_validation.sklearn.RobustnessDiagnosis\n\n\n\n\n\n\n\nGet details for a test\nTo get the details for a given test:\n\nvm.tests.describe_test(\"validmind.data_validation.DescriptiveStatistics\")\n\n\n\n\n\n\n\nGet a verbose details view of a test suite and its tests\nTo get more comprehensive details for test suites and tests:\n\nvm.test_suites.describe_suite(\"classifier_full_suite\", verbose=True)\n\n\n\n\n\n\nTest Suite ID\nTest Suite Name\nTest Suite Section\nTest ID\nTest Name\nTest Type\n\n\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.DatasetDescription\nDataset Description\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.DescriptiveStatistics\nDescriptive Statistics\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_dataset_description\nvalidmind.data_validation.PearsonCorrelationMatrix\nPearson Correlation Matrix\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.ClassImbalance\nClass Imbalance\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.Duplicates\nDuplicates\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.HighCardinality\nHigh Cardinality\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.HighPearsonCorrelation\nHigh Pearson Correlation\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.MissingValues\nMissing Values\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.Skewness\nSkewness\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.UniqueRows\nUnique Rows\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\ntabular_data_quality\nvalidmind.data_validation.TooManyZeroValues\nToo Many Zero Values\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.ModelMetadata\nModel Metadata\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.data_validation.DatasetSplit\nDataset Split\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ConfusionMatrix\nConfusion Matrix\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ClassifierPerformance\nClassifier Performance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\nPermutation Feature Importance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\nPrecision Recall Curve\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.ROCCurve\nROC Curve\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\nPopulation Stability Index\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_metrics\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\nSHAP Global Importance\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.MinimumAccuracy\nMinimum Accuracy\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.MinimumF1Score\nMinimum Score\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\nMinimum ROCAUC Score\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.TrainingTestDegradation\nTraining Test Degradation\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_validation\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\nModels Performance Comparison\nMetric\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_model_diagnosis\nvalidmind.model_validation.sklearn.OverfitDiagnosis\nOverfit Diagnosis\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_model_diagnosis\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\nWeakspots Diagnosis\nThresholdTest\n\n\nclassifier_full_suite\nClassifierFullSuite\nclassifier_model_diagnosis\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\nRobustness Diagnosis\nThresholdTest"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#list-all-tests",
    "href": "notebooks/how_to/explore_test_suites.html#list-all-tests",
    "title": "Explore test suites",
    "section": "List all tests",
    "text": "List all tests\nTo get the list of tests and their purpose:\n\nvm.tests.list_tests()\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nThresholdTest\nBias\nEvaluates bias in a Large Language Model based on the order and distribution of exemplars in a prompt....\nvalidmind.prompt_validation.Bias\n\n\nThresholdTest\nClarity\nEvaluates and scores the clarity of prompts in a Large Language Model based on specified guidelines....\nvalidmind.prompt_validation.Clarity\n\n\nThresholdTest\nSpecificity\nEvaluates and scores the specificity of prompts provided to a Large Language Model (LLM), based on clarity,...\nvalidmind.prompt_validation.Specificity\n\n\nThresholdTest\nRobustness\nAssesses the robustness of prompts provided to a Large Language Model under varying conditions and contexts....\nvalidmind.prompt_validation.Robustness\n\n\nThresholdTest\nNegative Instruction\nEvaluates and grades the use of affirmative, proactive language over negative instructions in LLM prompts....\nvalidmind.prompt_validation.NegativeInstruction\n\n\nThresholdTest\nConciseness\nAnalyzes and grades the conciseness of prompts provided to a Large Language Model....\nvalidmind.prompt_validation.Conciseness\n\n\nThresholdTest\nDelimitation\nEvaluates the proper use of delimiters in prompts provided to Large Language Models....\nvalidmind.prompt_validation.Delimitation\n\n\nMetric\nBert Score\nEvaluates text generation models' performance by calculating precision, recall, and F1 score based on BERT...\nvalidmind.model_validation.BertScore\n\n\nMetric\nRegard Score\n**Purpose:**...\nvalidmind.model_validation.RegardScore\n\n\nMetric\nBleu Score\nAssesses translation quality by comparing machine-translated sentences with human-translated ones using BLEU score....\nvalidmind.model_validation.BleuScore\n\n\nMetric\nContextual Recall\nEvaluates a Natural Language Generation model's ability to generate contextually relevant and factually correct...\nvalidmind.model_validation.ContextualRecall\n\n\nMetric\nRegard Histogram\n**Purpose:**...\nvalidmind.model_validation.RegardHistogram\n\n\nMetric\nToxicity Histogram\n**Purpose:**...\nvalidmind.model_validation.ToxicityHistogram\n\n\nMetric\nRouge Metrics\nEvaluates the quality of machine-generated text using various ROUGE metrics, and visualizes the results....\nvalidmind.model_validation.RougeMetrics\n\n\nMetric\nModel Metadata\nExtracts and summarizes critical metadata from a machine learning model instance for comprehensive analysis....\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nBert Score Aggregate\nEvaluates the aggregate performance of text generation models by computing the average precision, recall,...\nvalidmind.model_validation.BertScoreAggregate\n\n\nMetric\nCluster Size Distribution\nCompares and visualizes the distribution of cluster sizes in model predictions and actual data for assessing...\nvalidmind.model_validation.ClusterSizeDistribution\n\n\nMetric\nToken Disparity\nAssess and visualize token count disparity between model's predicted and actual dataset....\nvalidmind.model_validation.TokenDisparity\n\n\nMetric\nToxicity Score\n**Purpose:**...\nvalidmind.model_validation.ToxicityScore\n\n\nMetric\nRouge Metrics Aggregate\nEvaluates the average quality of machine-generated text using various ROUGE metrics and visualizes the aggregated results....\nvalidmind.model_validation.RougeMetricsAggregate\n\n\nMetric\nEmbeddings Visualization D\nVisualizes 2D representation of text embeddings generated by a model using t-SNE technique....\nvalidmind.model_validation.embeddings.EmbeddingsVisualization2D\n\n\nThresholdTest\nStability Analysis Random Noise\nEvaluate robustness of embeddings models to random noise introduced by using...\nvalidmind.model_validation.embeddings.StabilityAnalysisRandomNoise\n\n\nMetric\nCosine Similarity Distribution\nAssesses the similarity between predicted text embeddings from a model using a Cosine Similarity distribution...\nvalidmind.model_validation.embeddings.CosineSimilarityDistribution\n\n\nThresholdTest\nStability Analysis Translation\nEvaluate robustness of embeddings models to noise introduced by translating...\nvalidmind.model_validation.embeddings.StabilityAnalysisTranslation\n\n\nMetric\nCluster Distribution\nAssesses the distribution of text embeddings across clusters produced by a model using KMeans clustering....\nvalidmind.model_validation.embeddings.ClusterDistribution\n\n\nThresholdTest\nStability Analysis\nBase class for embeddings stability analysis tests\nvalidmind.model_validation.embeddings.StabilityAnalysis\n\n\nThresholdTest\nStability Analysis Keyword\nEvaluate robustness of embeddings models to keyword swaps on the test dataset...\nvalidmind.model_validation.embeddings.StabilityAnalysisKeyword\n\n\nThresholdTest\nStability Analysis Synonyms\nEvaluates the stability of text embeddings models when words in test data are replaced by their synonyms randomly....\nvalidmind.model_validation.embeddings.StabilityAnalysisSynonyms\n\n\nMetric\nDescriptive Analytics\nEvaluates statistical properties of text embeddings in an ML model via mean, median, and standard deviation...\nvalidmind.model_validation.embeddings.DescriptiveAnalytics\n\n\nMetric\nRegression Models Performance Comparison\nCompares and evaluates the performance of multiple regression models using five different metrics: MAE, MSE, RMSE,...\nvalidmind.model_validation.sklearn.RegressionModelsPerformanceComparison\n\n\nMetric\nAdjusted Mutual Information\nEvaluates clustering model performance by measuring mutual information between true and predicted labels, adjusting...\nvalidmind.model_validation.sklearn.AdjustedMutualInformation\n\n\nMetric\nSilhouette Plot\nCalculates and visualizes Silhouette Score, assessing degree of data point suitability to its cluster in ML models....\nvalidmind.model_validation.sklearn.SilhouettePlot\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nAdjusted Rand Index\nMeasures the similarity between two data clusters using the Adjusted Rand Index (ARI) metric in clustering machine...\nvalidmind.model_validation.sklearn.AdjustedRandIndex\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nHomogeneity Score\nAssesses clustering homogeneity by comparing true and predicted labels, scoring from 0 (heterogeneous) to 1...\nvalidmind.model_validation.sklearn.HomogeneityScore\n\n\nMetric\nCompleteness Score\nEvaluates a clustering model's capacity to categorize instances from a single class into the same cluster....\nvalidmind.model_validation.sklearn.CompletenessScore\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nCluster Performance Metrics\nEvaluates the performance of clustering machine learning models using multiple established metrics....\nvalidmind.model_validation.sklearn.ClusterPerformanceMetrics\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nMetric\nFowlkes Mallows Score\nEvaluates the similarity between predicted and actual cluster assignments in a model using the Fowlkes-Mallows...\nvalidmind.model_validation.sklearn.FowlkesMallowsScore\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nCluster Cosine Similarity\nMeasures the intra-cluster similarity of a clustering model using cosine similarity....\nvalidmind.model_validation.sklearn.ClusterCosineSimilarity\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nMetric\nV Measure\nEvaluates homogeneity and completeness of a clustering model using the V Measure Score....\nvalidmind.model_validation.sklearn.VMeasure\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nMetric\nRegression Square\n**Purpose**: The purpose of the RegressionR2Square Metric test is to measure the overall goodness-of-fit of a...\nvalidmind.model_validation.sklearn.RegressionR2Square\n\n\nMetric\nRegression Errors\n**Purpose**: This metric is used to measure the performance of a regression model. It gauges the model's accuracy...\nvalidmind.model_validation.sklearn.RegressionErrors\n\n\nMetric\nCluster Performance\nEvaluates and compares a clustering model's performance on training and testing datasets using multiple defined...\nvalidmind.model_validation.sklearn.ClusterPerformance\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nHyper Parameters Tuning\nExerts exhaustive grid search to identify optimal hyperparameters for the model, improving performance....\nvalidmind.model_validation.sklearn.HyperParametersTuning\n\n\nMetric\nK Means Clusters Optimization\nOptimizes the number of clusters in K-means models using Elbow and Silhouette methods....\nvalidmind.model_validation.sklearn.KMeansClustersOptimization\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegression Models Coeffs\nCompares feature importance by evaluating and contrasting coefficients of different regression models....\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs\n\n\nMetric\nBox Pierce\nDetects autocorrelation in time-series data through the Box-Pierce test to validate model performance....\nvalidmind.model_validation.statsmodels.BoxPierce\n\n\nMetric\nRegression Coeffs Plot\nVisualizes regression coefficients with 95% confidence intervals to assess predictor variables' impact on response...\nvalidmind.model_validation.statsmodels.RegressionCoeffsPlot\n\n\nMetric\nRegression Model Sensitivity Plot\nTests the sensitivity of a regression model to variations in independent variables by applying shocks and...\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\nMetric\nRegression Models Performance\nEvaluates and compares regression models' performance using R-squared, Adjusted R-squared, and MSE metrics....\nvalidmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nMetric\nZivot Andrews Arch\nEvaluates the order of integration and stationarity of time series data using Zivot-Andrews unit root test....\nvalidmind.model_validation.statsmodels.ZivotAndrewsArch\n\n\nMetric\nRegression Model Outsample Comparison\nComputes MSE and RMSE for multiple regression models using out-of-sample test to assess model's prediction accuracy...\nvalidmind.model_validation.statsmodels.RegressionModelOutsampleComparison\n\n\nMetric\nRegression Model Forecast Plot Levels\nCompares and visualizes forecasted and actual values of regression models on both raw and transformed datasets....\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nPD Rating Class Plot\nAssesses and visualizes credit risk distribution across different rating classes within a dataset via default...\nvalidmind.model_validation.statsmodels.PDRatingClassPlot\n\n\nMetric\nScorecard Histogram\nCreates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model....\nvalidmind.model_validation.statsmodels.ScorecardHistogram\n\n\nMetric\nFeature Importance And Significance\nEvaluates and visualizes the statistical significance and feature importance using regression and decision tree...\nvalidmind.model_validation.statsmodels.FeatureImportanceAndSignificance\n\n\nMetric\nL Jung Box\nAssesses autocorrelations in dataset features by performing a Ljung-Box test on each feature....\nvalidmind.model_validation.statsmodels.LJungBox\n\n\nMetric\nLogistic Reg Prediction Histogram\nGenerates and visualizes histograms of the Probability of Default predictions for both positive and negative...\nvalidmind.model_validation.statsmodels.LogisticRegPredictionHistogram\n\n\nMetric\nJarque Bera\nAssesses normality of dataset features in an ML model using the Jarque-Bera test....\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nPhillips Perron Arch\nExecutes Phillips-Perron test to assess the stationarity of time series data in each ML model feature....\nvalidmind.model_validation.statsmodels.PhillipsPerronArch\n\n\nMetric\nKolmogorov Smirnov\nExecutes a feature-wise Kolmogorov-Smirnov test to evaluate alignment with normal distribution in datasets....\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nResiduals Visual Inspection\nProvides a comprehensive visual analysis of residuals for regression models utilizing various plot types....\nvalidmind.model_validation.statsmodels.ResidualsVisualInspection\n\n\nMetric\nShapiro Wilk\nEvaluates feature-wise normality of training data using the Shapiro-Wilk test....\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nScorecard Bucket Histogram\nEvaluates and visualizes distribution of risk categories in a classification model's scores, useful in credit risk...\nvalidmind.model_validation.statsmodels.ScorecardBucketHistogram\n\n\nMetric\nRegression Model Insample Comparison\nEvaluates and compares in-sample performance of multiple regression models using R-Squared, Adjusted R-Squared,...\nvalidmind.model_validation.statsmodels.RegressionModelInsampleComparison\n\n\nMetric\nRegression Feature Significance\nAssesses and visualizes the statistical significance of features in a set of regression models....\nvalidmind.model_validation.statsmodels.RegressionFeatureSignificance\n\n\nMetric\nRegression Model Summary\nEvaluates regression model performance using metrics including R-Squared, Adjusted R-Squared, MSE, and RMSE....\nvalidmind.model_validation.statsmodels.RegressionModelSummary\n\n\nMetric\nKPSS\nExecutes KPSS unit root test to validate stationarity of time-series data in machine learning model....\nvalidmind.model_validation.statsmodels.KPSS\n\n\nMetric\nLilliefors\nAssesses the normality of feature distributions in an ML model's training dataset using the Lilliefors test....\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nLogistic Reg Cumulative Prob\nVisualizes cumulative probabilities of positive and negative classes for both training and testing in logistic...\nvalidmind.model_validation.statsmodels.LogisticRegCumulativeProb\n\n\nMetric\nRuns Test\nExecutes Runs Test on ML model to detect non-random patterns in output data sequence....\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nScorecard Probabilities Histogram\nEvaluates risk classification of a model by visualizing the distribution of default probability across score...\nvalidmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram\n\n\nMetric\nDFGLS Arch\nExecutes Dickey-Fuller GLS metric to determine order of integration and check stationarity in time series data....\nvalidmind.model_validation.statsmodels.DFGLSArch\n\n\nMetric\nAuto ARIMA\nEvaluates ARIMA models for time-series forecasting, ranking them using Bayesian and Akaike Information Criteria....\nvalidmind.model_validation.statsmodels.AutoARIMA\n\n\nThresholdTest\nADF Test\nAssesses the stationarity of time series data using the Augmented Dickey-Fuller (ADF) test....\nvalidmind.model_validation.statsmodels.ADFTest\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\nMetric\nRegression Model Forecast Plot\nGenerates plots to visually compare the forecasted outcomes of one or more regression models against actual...\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlot\n\n\nMetric\nADF\nAssesses the stationarity of a time series dataset using the Augmented Dickey-Fuller (ADF) test....\nvalidmind.model_validation.statsmodels.ADF\n\n\nMetric\nDurbin Watson Test\nAssesses autocorrelation in time series data features using the Durbin-Watson statistic....\nvalidmind.model_validation.statsmodels.DurbinWatsonTest\n\n\nMetric\nMissing Values Risk\nAssesses and quantifies the risk related to missing values in a dataset used for training an ML model....\nvalidmind.data_validation.MissingValuesRisk\n\n\nMetric\nIQR Outliers Table\nDetermines and summarizes outliers in numerical features using Interquartile Range method....\nvalidmind.data_validation.IQROutliersTable\n\n\nMetric\nBivariate Features Bar Plots\nGenerates visual bar plots to analyze the relationship between paired features within categorical data in the model....\nvalidmind.data_validation.BivariateFeaturesBarPlots\n\n\nThresholdTest\nSkewness\nEvaluates the skewness of numerical data in a machine learning model and checks if it falls below a set maximum...\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nTests dataset for duplicate entries, ensuring model reliability via data quality verification....\nvalidmind.data_validation.Duplicates\n\n\nMetric\nMissing Values Bar Plot\nCreates a bar plot showcasing the percentage of missing values in each column of the dataset with risk...\nvalidmind.data_validation.MissingValuesBarPlot\n\n\nMetric\nDataset Description\nProvides comprehensive analysis and statistical summaries of each field in a machine learning model's dataset....\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatter Plot\nCreates a scatter plot matrix to visually analyze feature relationships, patterns, and outliers in a dataset....\nvalidmind.data_validation.ScatterPlot\n\n\nThresholdTest\nTime Series Outliers\nIdentifies and visualizes outliers in time-series data using z-score method....\nvalidmind.data_validation.TimeSeriesOutliers\n\n\nMetric\nTabular Categorical Bar Plots\nGenerates and visualizes bar plots for each category in categorical features to evaluate dataset's composition....\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nAuto Stationarity\nAutomates Augmented Dickey-Fuller test to assess stationarity across multiple time series in a DataFrame....\nvalidmind.data_validation.AutoStationarity\n\n\nMetric\nDescriptive Statistics\nPerforms a detailed descriptive statistical analysis of both numerical and categorical data within a model's...\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nANOVA One Way Table\nApplies one-way ANOVA (Analysis of Variance) to identify statistically significant numerical features in the...\nvalidmind.data_validation.ANOVAOneWayTable\n\n\nMetric\nTarget Rate Bar Plots\nGenerates bar plots visualizing the default rates of categorical features for a classification machine learning...\nvalidmind.data_validation.TargetRateBarPlots\n\n\nMetric\nPearson Correlation Matrix\nEvaluates linear dependency between numerical variables in a dataset via a Pearson Correlation coefficient heat map....\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nFeature Target Correlation Plot\nVisualizes the correlation between input features and model's target output in a color-coded horizontal bar plot....\nvalidmind.data_validation.FeatureTargetCorrelationPlot\n\n\nMetric\nTabular Numerical Histograms\nGenerates histograms for each numerical feature in a dataset to provide visual insights into data distribution and...\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nMetric\nIsolation Forest Outliers\nDetects outliers in a dataset using the Isolation Forest algorithm and visualizes results through scatter plots....\nvalidmind.data_validation.IsolationForestOutliers\n\n\nMetric\nChi Squared Features Table\nExecutes Chi-Squared test for each categorical feature against a target column to assess significant association....\nvalidmind.data_validation.ChiSquaredFeaturesTable\n\n\nThresholdTest\nHigh Cardinality\nAssesses the number of unique values in categorical columns to detect high cardinality and potential overfitting....\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissing Values\nEvaluates dataset quality by ensuring missing value ratio across all features does not exceed a set threshold....\nvalidmind.data_validation.MissingValues\n\n\nMetric\nDefault Ratesby Risk Band Plot\nGenerates a bar plot showcasing the distribution of default rates across different risk bands in a dataset....\nvalidmind.data_validation.DefaultRatesbyRiskBandPlot\n\n\nMetric\nRolling Stats Plot\nThis test evaluates the stationarity of time series data by plotting its rolling mean and standard deviation....\nvalidmind.data_validation.RollingStatsPlot\n\n\nMetric\nTabular Description Tables\nSummarizes key descriptive statistics for numerical, categorical, and datetime variables in a dataset....\nvalidmind.data_validation.TabularDescriptionTables\n\n\nMetric\nAuto MA\nAutomatically selects the optimal Moving Average (MA) order for each variable in a time series dataset based on...\nvalidmind.data_validation.AutoMA\n\n\nThresholdTest\nUnique Rows\nVerifies the diversity of the dataset by ensuring that the count of unique rows exceeds a prescribed threshold....\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nToo Many Zero Values\nIdentifies numerical columns in a dataset that contain an excessive number of zero values, defined by a threshold...\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHigh Pearson Correlation\nIdentifies highly correlated feature pairs in a dataset suggesting feature redundancy or multicollinearity....\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nAC Fand PACF Plot\nAnalyzes time series data using Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to...\nvalidmind.data_validation.ACFandPACFPlot\n\n\nMetric\nBivariate Histograms\nGenerates bivariate histograms for paired features, aiding in visual inspection of categorical variables'...\nvalidmind.data_validation.BivariateHistograms\n\n\nMetric\nWOE Bin Table\nCalculates and assesses the Weight of Evidence (WoE) and Information Value (IV) of each feature in a ML model....\nvalidmind.data_validation.WOEBinTable\n\n\nMetric\nHeatmap Feature Correlations\nCreates a heatmap to visually represent correlation patterns between pairs of numerical features in a dataset....\nvalidmind.data_validation.HeatmapFeatureCorrelations\n\n\nThresholdTest\nTime Series Frequency\nEvaluates consistency of time series data frequency and generates a frequency plot....\nvalidmind.data_validation.TimeSeriesFrequency\n\n\nMetric\nDataset Split\nEvaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML...\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nSpread Plot\nVisualizes the spread relationship between pairs of time-series variables in a dataset, thereby aiding in...\nvalidmind.data_validation.SpreadPlot\n\n\nMetric\nTime Series Line Plot\nGenerates and analyses time-series data through line plots revealing trends, patterns, anomalies over time....\nvalidmind.data_validation.TimeSeriesLinePlot\n\n\nMetric\nPi T Credit Scores Histogram\nGenerates a histogram visualization for observed and predicted credit default scores....\nvalidmind.data_validation.PiTCreditScoresHistogram\n\n\nMetric\nAuto Seasonality\nAutomatically identifies and quantifies optimal seasonality in time series data to improve forecasting model...\nvalidmind.data_validation.AutoSeasonality\n\n\nMetric\nBivariate Scatter Plots\nGenerates bivariate scatterplots to visually inspect relationships between pairs of predictor variables in machine...\nvalidmind.data_validation.BivariateScatterPlots\n\n\nMetric\nEngle Granger Coint\nValidates co-integration in pairs of time series data using the Engle-Granger test and classifies them as...\nvalidmind.data_validation.EngleGrangerCoint\n\n\nThresholdTest\nTime Series Missing Values\nValidates time-series data quality by confirming the count of missing values is below a certain threshold....\nvalidmind.data_validation.TimeSeriesMissingValues\n\n\nMetric\nTime Series Histogram\nVisualizes distribution of time-series data using histograms and Kernel Density Estimation (KDE) lines....\nvalidmind.data_validation.TimeSeriesHistogram\n\n\nMetric\nLagged Correlation Heatmap\nAssesses and visualizes correlation between target variable and lagged independent variables in a time-series...\nvalidmind.data_validation.LaggedCorrelationHeatmap\n\n\nMetric\nSeasonal Decompose\nDecomposes dataset features into observed, trend, seasonal, and residual components to identify patterns and...\nvalidmind.data_validation.SeasonalDecompose\n\n\nMetric\nWOE Bin Plots\nGenerates visualizations of Weight of Evidence (WoE) and Information Value (IV) for understanding predictive power...\nvalidmind.data_validation.WOEBinPlots\n\n\nThresholdTest\nClass Imbalance\nEvaluates and quantifies class distribution imbalance in a dataset used by a machine learning model....\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nIQR Outliers Bar Plot\nVisualizes outlier distribution across percentiles in numerical data using Interquartile Range (IQR) method....\nvalidmind.data_validation.IQROutliersBarPlot\n\n\nMetric\nPi TPD Histogram\nAssesses credit risk prediction accuracy of a model by comparing actual and predicted defaults at a chosen point in...\nvalidmind.data_validation.PiTPDHistogram\n\n\nMetric\nAuto AR\nAutomatically identifies the optimal Autoregressive (AR) order for a time series using BIC and AIC criteria....\nvalidmind.data_validation.AutoAR\n\n\nMetric\nTabular Date Time Histograms\nGenerates histograms to provide graphical insight into the distribution of time intervals in model's datetime data....\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\nMetric\nPunctuations\nAnalyzes and visualizes the frequency distribution of punctuation usage in a given text dataset....\nvalidmind.data_validation.nlp.Punctuations\n\n\nMetric\nCommon Words\nIdentifies and visualizes the 40 most frequent non-stopwords in a specified text column within a dataset....\nvalidmind.data_validation.nlp.CommonWords\n\n\nThresholdTest\nHashtags\nAssesses hashtag frequency in a text column, highlighting usage trends and potential dataset bias or spam....\nvalidmind.data_validation.nlp.Hashtags\n\n\nThresholdTest\nMentions\nCalculates and visualizes frequencies of '@' prefixed mentions in a text-based dataset for NLP model analysis....\nvalidmind.data_validation.nlp.Mentions\n\n\nMetric\nText Description\nPerforms comprehensive textual analysis on a dataset using NLTK, evaluating various parameters and generating...\nvalidmind.data_validation.nlp.TextDescription\n\n\nThresholdTest\nStop Words\nEvaluates and visualizes the frequency of English stop words in a text dataset against a defined threshold....\nvalidmind.data_validation.nlp.StopWords"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites.html#next-steps",
    "href": "notebooks/how_to/explore_test_suites.html#next-steps",
    "title": "Explore test suites",
    "section": "Next steps",
    "text": "Next steps\nBy harnessing the functionalities presented in this guide, you should be able to easily list and filter through all of ValidMind’s available tests and find those you are interested in running against your model and/or dataset.\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html",
    "href": "notebooks/how_to/configure_dataset_features.html",
    "title": "Configure dataset features",
    "section": "",
    "text": "When initializing a ValidMind dataset object, you can pass in a list of features to use instead of utilizing all dataset columns when running tests.\nThis notebook shows how to use custom feature columns with init_dataset. The default behavior of init_dataset is to utilize all dataset columns when running tests. It is also possible to pass in a list of features to use and thus restrict computations to only those features."
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#contents",
    "href": "notebooks/how_to/configure_dataset_features.html#contents",
    "title": "Configure dataset features",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nBefore you begin\n\nNew to ValidMind?\n\n\nInstall the client library\n\nInitialize the client library\n\nLoad the sample dataset\n\nInitialize the training and test datasets\n\nDefining custom features\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#about-validmind",
    "href": "notebooks/how_to/configure_dataset_features.html#about-validmind",
    "title": "Configure dataset features",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#before-you-begin-1",
    "href": "notebooks/how_to/configure_dataset_features.html#before-you-begin-1",
    "title": "Configure dataset features",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\n\n\nNew to ValidMind?\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now!\n\n\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#install-the-client-library",
    "href": "notebooks/how_to/configure_dataset_features.html#install-the-client-library",
    "title": "Configure dataset features",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#initialize-the-client-library",
    "href": "notebooks/how_to/configure_dataset_features.html#initialize-the-client-library",
    "title": "Configure dataset features",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Binary classification as the template and Marketing/Sales - Attrition/Churn Management as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#load-the-sample-dataset",
    "href": "notebooks/how_to/configure_dataset_features.html#load-the-sample-dataset",
    "title": "Configure dataset features",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\n\n%matplotlib inline\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\n# You can also try a different dataset with:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nInitialize the training and test datasets\nBefore you can run a test suite, which are just a collection of tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to analyze\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — the name of the target column in the dataset\nfeature_columns - the names of the feature columns in the dataset\n\n\nfeature_columns = ['CreditScore', 'Age', 'Tenure', 'Balance',\n                   'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    input_id=\"raw_dataset\",\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns\n)\n\n\n\n\nDefining custom features\nThis section shows how we can define a subset of features to use when computing dataset metrics. Any feature that is not included in the feature_columns argument is omitted from the computation of the DescriptiveStatistics metric in the examples below.\nIn the following example we use the metric DescriptiveStatistics to show how the output changes when customizing features.\n\nRunning metric with all the features.\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    input_id=\"raw_dataset_all_features\",\n    target_column=demo_dataset.target_column\n)\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.DescriptiveStatistics\",\n    inputs={\"dataset\": vm_dataset}\n)\n\n\nRunning metric with a subset of features.\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    input_id=\"raw_dataset_subset\",\n    target_column=demo_dataset.target_column,\n    feature_columns=['CreditScore', 'Age', 'Balance', 'Geography']\n)\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.DescriptiveStatistics\",\n    inputs={\"dataset\": vm_dataset}\n)"
  },
  {
    "objectID": "notebooks/how_to/configure_dataset_features.html#next-steps",
    "href": "notebooks/how_to/configure_dataset_features.html#next-steps",
    "title": "Configure dataset features",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html",
    "href": "notebooks/how_to/load_datasets_predictions.html",
    "title": "Load dataset predictions",
    "section": "",
    "text": "To enable tests to make use of predictions, you can load predictions in ValidMind dataset objects in multiple different ways.\nThis interactive notebook includes the code required to load the demo dataset, preprocess the raw dataset and train a model for testing, and initialize ValidMind objects. Additionally, it offers options for loading predictions using the assign_predictions() function, such as loading predictions from a file, linking an existing prediction column in the dataset with a model, or allowing the developer framework to run and link predictions to a model."
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#contents",
    "href": "notebooks/how_to/load_datasets_predictions.html#contents",
    "title": "Load dataset predictions",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nPreview the documentation template\n\n\nLoad the sample dataset\n\nPrepocess the raw dataset\n\nTrain models for testing\n\nInitialize ValidMind objects\n\nInitialize the ValidMind models\n\nInitialize the ValidMind datasets\n\n\nOptions to load predictions using the developer frameworks\n\nLoad predictions from a file\n\nPredictions calculated outside of VM\n\nAssign predictions to the training dataset\n\nRun an example test\n\nLink an existing prediction column in the dataset with a model\n\nLink prediction column to a specific model\n\n\nLink an existing prediction column in the dataset with a model\n\nPass &lt;vm_model&gt; in dataset interface\n\nThrough assign_predictions interface\n\n\nRun an example test\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#about-validmind",
    "href": "notebooks/how_to/load_datasets_predictions.html#about-validmind",
    "title": "Load dataset predictions",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#install-the-client-library",
    "href": "notebooks/how_to/load_datasets_predictions.html#install-the-client-library",
    "title": "Load dataset predictions",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#initialize-the-client-library",
    "href": "notebooks/how_to/load_datasets_predictions.html#initialize-the-client-library",
    "title": "Load dataset predictions",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#load-the-sample-dataset",
    "href": "notebooks/how_to/load_datasets_predictions.html#load-the-sample-dataset",
    "title": "Load dataset predictions",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n)\n\nraw_df = demo_dataset.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#prepocess-the-raw-dataset",
    "href": "notebooks/how_to/load_datasets_predictions.html#prepocess-the-raw-dataset",
    "title": "Load dataset predictions",
    "section": "Prepocess the raw dataset",
    "text": "Prepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#train-models-for-testing",
    "href": "notebooks/how_to/load_datasets_predictions.html#train-models-for-testing",
    "title": "Load dataset predictions",
    "section": "Train models for testing",
    "text": "Train models for testing\n\nInitialize XGBoost and Logistic Regression Classifiers\n\n\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost\n\n%matplotlib inline\n\nxgb = xgboost.XGBClassifier(early_stopping_rounds=10)\nxgb.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nxgb.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nlr = LogisticRegression(random_state=0)\nlr.fit(\n    x_train,\n    y_train,\n)"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#initialize-validmind-objects",
    "href": "notebooks/how_to/load_datasets_predictions.html#initialize-validmind-objects",
    "title": "Load dataset predictions",
    "section": "Initialize ValidMind objects",
    "text": "Initialize ValidMind objects\n\n\nInitialize the ValidMind models\n\nvm_model_xgb = vm.init_model(\n    xgb,\n    input_id=\"xgb\",\n)\nvm_model_lr = vm.init_model(\n    lr,\n    input_id=\"lr\",\n)\n\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\nclass_labels — an optional value to map predicted classes to class labels\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_raw_ds = vm.init_dataset(\n    input_id=\"raw_dataset\",\n    dataset=raw_df,\n    target_column=demo_dataset.target_column,\n)\n\nvm_train_ds = vm.init_dataset(\n    input_id=\"train_dataset\",\n    dataset=train_df,\n    target_column=demo_dataset.target_column,\n)\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset\", dataset=test_df, target_column=demo_dataset.target_column\n)"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#options-to-load-predictions-using-the-developer-frameworks",
    "href": "notebooks/how_to/load_datasets_predictions.html#options-to-load-predictions-using-the-developer-frameworks",
    "title": "Load dataset predictions",
    "section": "Options to load predictions using the developer frameworks",
    "text": "Options to load predictions using the developer frameworks\n\n\nLoad predictions from a file\nThis creates a new column called &lt;model_id&gt;_prediction in the dataset and assigns metadata to track that the &lt;model_id&gt;_prediction column is linked to the model &lt;model_id&gt;\n\n\n\nPredictions calculated outside of VM\n\nimport pandas as pd\n\ntrain_xgb_prediction = pd.DataFrame(xgb.predict(x_train), columns=[\"xgb_prediction\"])\ntest__xgb_prediction = pd.DataFrame(xgb.predict(x_val), columns=[\"xgb_prediction\"])\n\ntrain_lr_prediction = pd.DataFrame(lr.predict(x_train), columns=[\"lr_prediction\"])\ntest_lr_prediction = pd.DataFrame(lr.predict(x_val), columns=[\"lr_prediction\"])\n\n\n\n\nAssign predictions to the training dataset\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model:\n\nvm_train_ds.assign_predictions(\n    model=vm_model_xgb, prediction_values=train_xgb_prediction.xgb_prediction.values\n)\nvm_train_ds.assign_predictions(\n    model=vm_model_lr, prediction_values=train_lr_prediction.lr_prediction.values\n)\n\n\n\n\nRun an example test\nNow, let’s run an example test such as MinimumAccuracy twice to show how we’re able to load the correct model predictions by using the model input parameter, even though we’re passing the same train_ds dataset instance to the test:\n\nfull_suite = vm.tests.run_test(\n    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_xgb},\n)\n\n\nfull_suite = vm.tests.run_test(\n    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"model\": vm_model_lr,\n    },\n)\n\n\n\n\nLink an existing prediction column in the dataset with a model\nThis approach allows loading datasets that already have prediction columns in addition to feature and target columns. The developer framework assigns metadata to track the predictions column that are linked to a given &lt;vm_model&gt; model.\n\ntrain_df2 = train_df.copy()\ntrain_df2[\"xgb_prediction\"] = train_xgb_prediction.xgb_prediction.values\ntrain_df2[\"lr_prediction\"] = train_lr_prediction.lr_prediction.values\ntrain_df2.head(5)\n\n\nfeature_columns = [\n    \"CreditScore\",\n    \"Gender\",\n    \"Age\",\n    \"Tenure\",\n    \"Balance\",\n    \"NumOfProducts\",\n    \"HasCrCard\",\n    \"IsActiveMember\",\n    \"EstimatedSalary\",\n    \"Geography_France\",\n    \"Geography_Germany\",\n    \"Geography_Spain\",\n]\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df2,\n    input_id=\"train_dataset\",\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns,\n)\n\n\n\nLink prediction column to a specific model\nThe prediction_column parameter informs the Dataset object about the model that should be linked to that column.\n\nvm_train_ds.assign_predictions(model=vm_model_xgb, prediction_column=\"xgb_prediction\")\nvm_train_ds.assign_predictions(model=vm_model_lr, prediction_column=\"lr_prediction\")\n\n\nfull_suite = vm.tests.run_test(\n    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_xgb},\n)\n\n\nfull_suite = vm.tests.run_test(\n    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_lr},\n)\n\n\n\n\n\nLink an existing prediction column in the dataset with a model\nThis lets the developer framework run model predictions, creates a new column called &lt;model_id&gt;_prediction, and assign metadata to track that the &lt;model_id&gt;_prediction column is linked to the &lt;vm_model&gt; model.\nThere are two ways run and assign model predictions with the developer framework:\n\nWhen initializing a Dataset with init_dataset(). This is the most straightforward method to assign predictions for a single model.\nUsing dataset.assign_predictions(). This allows assigning predictions to a dataset for one or more models.\n\n\n\nPass &lt;vm_model&gt; in dataset interface\n\nfeature_columns = [\n    \"CreditScore\",\n    \"Gender\",\n    \"Age\",\n    \"Tenure\",\n    \"Balance\",\n    \"NumOfProducts\",\n    \"HasCrCard\",\n    \"IsActiveMember\",\n    \"EstimatedSalary\",\n    \"Geography_France\",\n    \"Geography_Germany\",\n    \"Geography_Spain\",\n]\n\nvm_train_ds = vm.init_dataset(\n    model=vm_model_xgb,\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns,\n)\n\n\n\n\nThrough assign_predictions interface\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns,\n)\n\n\nPerform predictions using the same assign_predictions interface\n\nvm_train_ds.assign_predictions(model=vm_model_xgb)\nvm_train_ds.assign_predictions(model=vm_model_lr)\n\n\n\n\n\n\nRun an example test\nNow, let’s run an example test such as MinimumAccuracy twice to show how we’re able to load the correct model predictions by using the model input parameter, even though we’re passing the same train_ds dataset instance to the test:\n\nfull_suite = vm.tests.run_test(\n    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n    inputs={\"dataset\": vm_train_ds, \"model\": vm_model_xgb},\n)\n\n\nfull_suite = vm.tests.run_test(\n    \"validmind.model_validation.sklearn.MinimumAccuracy\",\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"model\": vm_model_lr,\n    },\n)"
  },
  {
    "objectID": "notebooks/how_to/load_datasets_predictions.html#next-steps",
    "href": "notebooks/how_to/load_datasets_predictions.html#next-steps",
    "title": "Load dataset predictions",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html",
    "href": "notebooks/how_to/run_unit_metrics.html",
    "title": "Run unit metrics",
    "section": "",
    "text": "To turn complex evidence into actionable insights, you can run a unit metric as a single-value measure to quantify and monitor risks throughout a model’s lifecycle.\nIn this interactive notebook, we introduce the concept of unit metric and provide a step-by-step guide on how to define, execute and extract results from these measures. As an example, we use data from a customer churn use case to fit a binary classification model. To illustrate the application of these measures, we show you how to run sklearn classification metrics as unit metrics, demonstrating their utility in quantifying model performance and risk.\nIn Model Risk Management (MRM), the primary objective is to identify, assess, and mitigate the risks associated with the development, implementation, and ongoing use of quantitative models. The process of measuring risk involves the understanding and assessment of evidence generated throw multiple tests across all the model development lifecycle stages, from data collection and data quality to model performance and explainability.\nEvidence versus risk\nThe distinction between evidence and quantifiable risk measures is a critical aspect of MRM. Evidence, in this context, refers to the outputs from various tests conducted throughout the model lifecycle. For instance, a table displaying the number of missing values per feature in a dataset is a form of evidence. It shows where data might be incomplete, which can affect the model’s performance and reliability. Similarly, a Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is evidence of the model’s classification performance.\nHowever, these pieces of evidence do not offer a direct measure of risk. To quantify risk, one must derive metrics from this evidence that reflect the potential impact on the model’s performance and the decisions it informs. For example, the missing data rate, calculated as the percentage of missing values in the dataset, is a quantifiable risk measure that indicates the risk associated with data quality. Similarly, the accuracy score, which measures the proportion of correctly classified labels, acts as an indicator of performance risk in a classification model.\nUnit metric\nA Unit metric is a single value measure that is used to identify and monitor risks arising from the development of Machine Learning or AI models. This metric simplifies evidence into a single actionable number, that can be monitored and compared over time or across different models or datasets.\nProperties:\nIncorporating unit metrics into your ML workflow streamlines risk assessment, turning complex analyses into clear, actionable insights."
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#contents",
    "href": "notebooks/how_to/run_unit_metrics.html#contents",
    "title": "Run unit metrics",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInitialize the client library\n\nNotebook setup\n\nLoad the demo dataset\n\nTrain a model for testing\n\nCompute predictions\n\nInitialize ValidMind objects\n\nAssign predictions\n\nRunning unit metrics\n\nCompute F1 score\n\nPass parameters\n\nLoad the last computed value\n\nUnit metrics for model performance\n\nRun multiple unit metrics as a single test\n\nAdding composite metrics to the documentation template\n\nReconnect to the documentation project\n\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#about-validmind",
    "href": "notebooks/how_to/run_unit_metrics.html#about-validmind",
    "title": "Run unit metrics",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_unit_metrics.html#initialize-the-client-library",
    "title": "Run unit metrics",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"...\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#notebook-setup",
    "href": "notebooks/how_to/run_unit_metrics.html#notebook-setup",
    "title": "Run unit metrics",
    "section": "Notebook setup",
    "text": "Notebook setup\n\nimport xgboost as xgb\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_unit_metrics.html#load-the-demo-dataset",
    "title": "Run unit metrics",
    "section": "Load the demo dataset",
    "text": "Load the demo dataset\nIn this example, we load a demo dataset to fit a customer churn model.\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n)\n\nraw_df = demo_dataset.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_unit_metrics.html#train-a-model-for-testing",
    "title": "Run unit metrics",
    "section": "Train a model for testing",
    "text": "Train a model for testing\nWe train a simple customer churn model for our test.\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\nfeature_columns = [col for col in test_df.columns if col != demo_dataset.target_column]\nfeature_columns"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#compute-predictions",
    "href": "notebooks/how_to/run_unit_metrics.html#compute-predictions",
    "title": "Run unit metrics",
    "section": "Compute predictions",
    "text": "Compute predictions\nAfter the model is fitted, we compute model predictions and predictive probabilities, then add them to the customer churn dataset:\n\n# Compute predictive probabilities for the test dataset\n# Here, we only use the probabilities for the positive class (class 1)\npredictive_probabilities = model.predict_proba(\n    test_df.drop(demo_dataset.target_column, axis=1)\n)[:, 1]\n\n# Add the predictive probabilities as a new column to the test dataframe\ntest_df[\"PredictiveProbabilities\"] = predictive_probabilities\n\n# Add the predictions from the predictive probabilities as a new column to the test dataframe\ntest_df[\"Predictions\"] = (predictive_probabilities &gt; 0.5).astype(int)\n\n# Display the first few rows of the updated dataframe to verify\ntest_df.head()"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#initialize-validmind-objects",
    "href": "notebooks/how_to/run_unit_metrics.html#initialize-validmind-objects",
    "title": "Run unit metrics",
    "section": "Initialize ValidMind objects",
    "text": "Initialize ValidMind objects\nOnce the datasets and model are prepared for validation, we initialize ValidMind dataset and model, specifying features and targets columns. The property input_id allows users to uniquely identify each dataset and model. This allows for the creation of multiple versions of datasets and models, enabling us to compute metrics by specifying which versions we want to use as inputs.\n\nimport validmind as vm\n\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset\",\n    dataset=test_df,\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns,\n)\n\nvm_model = vm.init_model(model=model, input_id=\"my_model\")"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#assign-predictions",
    "href": "notebooks/how_to/run_unit_metrics.html#assign-predictions",
    "title": "Run unit metrics",
    "section": "Assign predictions",
    "text": "Assign predictions\nAssigning Pre-computed Predictions\nWe use vm_test to incorporate a column named ‘Predictions’, which consists of pre-computed predictions associated with vm_model. The assign_predictions method facilitates the addition of multiple prediction columns as necessary. By linking these precomputed predictions to a specific model through this method, we establish a clear reference system, allowing for precise identification of the predictions needed for various computational tasks.\n\nvm_test_ds.assign_predictions(\n    model=vm_model,\n    prediction_column=\"Predictions\",\n    probability_column=\"PredictiveProbabilities\",\n)\n\n\nvm_test_ds.extra_columns"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#running-unit-metrics",
    "href": "notebooks/how_to/run_unit_metrics.html#running-unit-metrics",
    "title": "Run unit metrics",
    "section": "Running unit metrics",
    "text": "Running unit metrics\n\n\nCompute F1 score\nThe following snippet shows how to set up and execute a unit metric implementation of the F1 score from sklearn. In this example, our objective is to compute F1 for the test dataset. Therefore, we specify vm_test_ds as the dataset in the inputs along with the metric_id.\nDataset to Metric Input Mapping\nTo accurately compute the F1 score, it’s essential to ensure that these columns are correctly aligned and contain the relevant data. The F1 score requires two inputs:\n\nthe predictions y_pred and\nthe true labels y_true\n\nSince vm_test_ds has the capability to include multiple prediction columns, each linked to a different model. Therefore, it’s essential to specify both the dataset for extracting the target column and the correct prediction column, as well as the model to ensure the selection of the appropriate prediction column for that specific model, referred to as vm_model.\nWhen calculating the F1 score, it’s essential to use the correct prediction column associated with vm_model within vm_test_ds. This prediction column is dynamically identified based on the model id, specified in input_id.\n\nfrom validmind.tests import run_test\n\ninputs = {\"model\": vm_model, \"dataset\": vm_test_ds}\n\n\nrun_test(\"validmind.unit_metrics.classification.sklearn.F1\", inputs=inputs)\n\n\n\n\nPass parameters\nWhen using the unit metric implementation of the F1 score from sklearn, it’s important to note that this implementation supports all parameters of the original sklearn.metrics.f1_score function. This flexibility allows you to tailor the metric computation to your specific needs and scenarios.\nBelow, we provide a brief description the parameters you can pass to customize the F1 score calculation:\n\naverage: Specifies the averaging method for the F1 score. Common options include ‘micro’, ‘macro’, ‘samples’, ‘weighted’, or None.\nsample_weight: Allows for weighting of samples. By default, it is None, but it can be an array of weights that are applied to the samples, useful for cases where some classes are more important than others.\nzero_division: Defines the behavior when there is a division by zero during F1 calculation. Options are ‘warn’, ‘raise’, or a numeric value like 0 or 1, indicating what value to set when encountering division by zero.\n\n\nrun_test(\n    \"validmind.unit_metrics.classification.sklearn.F1\",\n    inputs=inputs,\n    params={\n        \"average\": \"micro\",\n        \"sample_weight\": None,\n        \"zero_division\": \"warn\",\n    },\n)\n\n\n\n\nLoad the last computed value\nUnit metrics are designed to optimize performance and efficiency by caching results of metric computations. When you execute a metric with the same signature —a unique combination of the metric ID, model, inputs, and parameters- a second time, validmind retrieves the result from its last computed value instead of recalculating it. This feature ensures faster access to metrics you’ve previously run and conserves computational resources.\nFirst computation of precision metric\nIn this first example, the precision metric is computed for the first time with a specific dataset. The result of this computation is stored in the cache.\n\nrun_test(\"validmind.unit_metrics.classification.sklearn.Precision\", inputs=inputs)\n\nSecond computation with the same signature\nIn this second example, the same precision metric computation is requested again with the identical inputs. Since the signature (metric ID and inputs) matches the previous run, validmind loads the result directly from the cache instead of recomputing it.\n\nrun_test(\"validmind.unit_metrics.classification.sklearn.Precision\", inputs=inputs)\n\nComputation with a changed signature\nIn this example, the signature is modified by adding parameters. This change prompts validmind to compute the metric anew, as the new signature does not match any stored result. The outcome is then cached, ready for any future requests with the same signature.\n\n\nrun_test(\n    \"validmind.unit_metrics.classification.sklearn.Precision\",\n    inputs=inputs,\n    params={\n        \"average\": \"micro\",\n        \"sample_weight\": None,\n        \"zero_division\": \"warn\",\n    }\n)\n\n\n\n\nUnit metrics for model performance\n\nrun_test(\"validmind.unit_metrics.classification.sklearn.Accuracy\", inputs=inputs)\n\n\nrun_test(\"validmind.unit_metrics.classification.sklearn.Recall\", inputs=inputs)\n\n\nrun_test(\"validmind.unit_metrics.classification.sklearn.ROC_AUC\", inputs=inputs)"
  },
  {
    "objectID": "notebooks/how_to/run_unit_metrics.html#next-steps",
    "href": "notebooks/how_to/run_unit_metrics.html#next-steps",
    "title": "Run unit metrics",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html",
    "title": "Document an application scorecard model",
    "section": "",
    "text": "Build and document an application scorecard model with the ValidMind Developer Framework by using Kaggle’s Lending Club sample dataset to build a simple application scorecard.\nAn application scorecard model is a type of statistical model used in credit scoring to evaluate the creditworthiness of potential borrowers by generating a score based on various characteristics of an applicant — such as credit history, income, employment status, and other relevant financial data.\nThis interactive notebook provides a step-by-step guide for loading a demo dataset, preprocessing the raw data, training a model for testing, setting up test inputs, initializing the required ValidMind objects, running the test, and then logging the results to ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#contents",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#contents",
    "title": "Document an application scorecard model",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nInitialize the Python environment\n\nPreview the documentation template\n\n\nLoad the sample dataset\n\nPrepocess the dataset\n\nFeature engineering\n\n\nTrain the model\n\nCompute probabilities\n\nCompute binary predictions\n\nCompute scores\n\n\nDocument the model\n\nInitialize the ValidMind datasets\n\nInitialize a model object\n\nAssign prediction values and probabilities to the datasets\n\nAssign scores to the datasets\n\nData validation\n\nRun tests for raw data tests\n\nRun tests for preprocessed data\n\nRun tests for WoE analysis\n\n\nModel validation\n\nRun tests for model training\n\nRun tests for model evaluation\n\nRun tests for model explainability\n\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#about-validmind",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#about-validmind",
    "title": "Document an application scorecard model",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: The classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#install-the-client-library",
    "title": "Document an application scorecard model",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#initialize-the-client-library",
    "title": "Document an application scorecard model",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Credit Risk Scorecard\nUse case: Risk Management/CECL\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"...\",\n  api_secret = \"...\",\n  project = \"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#initialize-the-python-environment",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#initialize-the-python-environment",
    "title": "Document an application scorecard model",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nimport statsmodels.api as sm\n\n%matplotlib inline\n\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou’ll upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#load-the-sample-dataset",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#load-the-sample-dataset",
    "title": "Document an application scorecard model",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you’ll need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.credit_risk import lending_club\n\ndf = lending_club.load_data(source=\"offline\")\n\ndf.head()\n\n\n\nPrepocess the dataset\nIn the preprocessing step we perform a number of operations to get ready for building our application scorecard.\nWe use the lending_club.preprocess to simplify preprocessing. This function performs the following operations: - Filters the dataset to include only loans for debt consolidation or credit card purposes - Removes loans classified under the riskier grades “F” and “G” - Excludes uncommon home ownership types and standardizes employment length and loan terms into numerical formats - Discards unnecessary fields and any entries with missing information to maintain a clean and robust dataset for modeling\n\npreprocess_df = lending_club.preprocess(df)\npreprocess_df.head()\n\n\n\n\nFeature engineering\nIn the feature engineering phase, we apply specific transformations to optimize the dataset for predictive modeling in our application scorecard.\nUsing the ending_club.feature_engineering() function, we conduct the following operations: - WoE encoding: Converts both numerical and categorical features into Weight of Evidence (WoE) values. WoE is a statistical measure used in scorecard modeling that quantifies the relationship between a predictor variable and the binary target variable. It calculates the ratio of the distribution of good outcomes to the distribution of bad outcomes for each category or bin of a feature. This transformation helps to ensure that the features are predictive and consistent in their contribution to the model. - Integration of WoE bins: Ensures that the WoE transformed values are integrated throughout the dataset, replacing the original feature values while excluding the target variable from this transformation. This transformation is used to maintain a consistent scale and impact of each variable within the model, which helps make the predictions more stable and accurate.\n\nfe_df = lending_club.feature_engineering(preprocess_df)\nfe_df.head()"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#train-the-model",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#train-the-model",
    "title": "Document an application scorecard model",
    "section": "Train the model",
    "text": "Train the model\nIn this section, we focus on constructing and refining our predictive model. - We begin by dividing our data, which is based on Weight of Evidence (WoE) features, into training and testing sets (train_df, test_df). - With lending_club.split, we employ a simple random split, randomly allocating data points to each set to ensure a mix of examples in both. - Additionally, by setting add_constant=True, we include an intercept term in our model.\n\ntrain_df, test_df = lending_club.split(fe_df, add_constant=True)\n\nx_train = train_df.drop(lending_club.target_column, axis=1)\ny_train = train_df[lending_club.target_column]\nx_test = test_df.drop(lending_club.target_column, axis=1)\ny_test = test_df[lending_club.target_column]\n\n# Define the model\nmodel = sm.GLM(\n    y_train, \n    x_train, \n    family=sm.families.Binomial())\n\n# Fit the model\nmodel = model.fit()\nmodel.summary()\n\n\n\nCompute probabilities\n\ntrain_probabilities = model.predict(x_train)\ntest_probabilities = model.predict(x_test)\n\n\n\n\nCompute binary predictions\n\ncut_off_threshold = 0.5\ntrain_binary_predictions = (train_probabilities &gt; cut_off_threshold).astype(int)\ntest_binary_predictions = (test_probabilities &gt; cut_off_threshold).astype(int)\n\n\n\n\nCompute scores\nIn this phase, we translate model predictions into actionable scores using probability estimates generated by our trained model.\n\n# Compute scores from the probabilities \ntrain_scores = lending_club.compute_scores(train_probabilities)\ntest_scores = lending_club.compute_scores(test_probabilities)"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#document-the-model",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#document-the-model",
    "title": "Document an application scorecard model",
    "section": "Document the model",
    "text": "Document the model\nTo document the model with the ValidMind Developer Framework, you’ll need to: 1. Preprocess the raw dataset 2. Initialize some training and test datasets 3. Initialize a model object you can use for testing 4. Run the full suite of tests\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset: The dataset that you want to provide as input to tests.\ninput_id: A unique identifier that allows tracking what inputs are used when running each individual test.\ntarget_column: A required argument if tests require access to true values. This is the name of the target column in the dataset.\n\nWith all datasets ready, you can now initialize the raw, processed, training and test datasets (raw_df, preprocessed_df, fe_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_raw_dataset = vm.init_dataset(\n    dataset=df,\n    input_id=\"raw_dataset\",\n    target_column=lending_club.target_column,\n)\n\nvm_preprocess_dataset = vm.init_dataset(\n    dataset=preprocess_df,\n    input_id=\"preprocess_dataset\",\n    target_column=lending_club.target_column,\n)\n\nvm_fe_dataset = vm.init_dataset(\n    dataset=fe_df,\n    input_id=\"fe_dataset\",\n    target_column=lending_club.target_column,\n)\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=lending_club.target_column,\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df, \n    input_id=\"test_dataset\", \n    target_column=lending_club.target_column\n)\n\n\n\n\nInitialize a model object\nYou will also need to initialize a ValidMind model object (vm_model) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    input_id=\"glm_model\",\n)\n\n\n\n\nAssign prediction values and probabilities to the datasets\nWith our model now trained, we’ll move on to assigning both the predictive probabilities coming directly from the model’s predictions, and the binary prediction after applying the cutoff threshold described in the previous steps. - These tasks are achieved through the use of the assign_predictions() method associated with the VM dataset object. - This method links the model’s class prediction values and probabilities to our VM train and test datasets.\n\nvm_train_ds.assign_predictions(\n    model=vm_model,\n    prediction_values=train_binary_predictions,\n    prediction_probabilities = train_probabilities,\n)\n\nvm_test_ds.assign_predictions(\n    model=vm_model,\n    prediction_values=test_binary_predictions,\n    prediction_probabilities = test_probabilities,\n)\n\n\n\n\nAssign scores to the datasets\nCredit scorecards revolve around scores computed from model predictions rather than raw predictions, so let’s compute some scores!\n\nTo make this process auditable and ensure scores are properly integrated with our datasets, we use the add_extra_column() method from the VM dataset object.\nThis approach allows us to append scores directly to our data, maintaining a streamlined and coherent dataset ready for analysis.\n\n\nvm_train_ds.add_extra_column(\"glm_scores\", train_scores)\nvm_test_ds.add_extra_column(\"glm_scores\", test_scores)\n\n\n\n\nData validation\nDuring data validation, we ensure that the data we’re working with is accurate, consistent, and ready for empirical analysis. We’re looking for any anomalies that might skew our models or results.\nIn this section, we use tests to collect evidence across raw, preprocessed, and feature-engineered datasets.\n\n\nRun tests for raw data tests\nWe perform initial validation of our raw data to understand its structure and detect any anomalies that could affect the modeling process. At this stage, we use a couple of tests that help us understand the data:\n\nTabular description tables: Statistical summary for each variable\nMissing values bar plot: Plots presence of any missing data\n\nTo ensure we can run this test again against a different dataset, we’ll label these test results with &lt;test_id&gt;:raw_dataset. This will make the results identifiable in the database, allowing us to include them in the UI.\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.TabularDescriptionTables:raw_dataset\",\n    inputs = {\n        \"dataset\": vm_raw_dataset\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.MissingValuesBarPlot:raw_dataset\",\n    inputs = {\n        \"dataset\": vm_raw_dataset\n    }\n)\ntest.log()\n\n\n\n\nRun tests for preprocessed data\nBefore the modeling process begins, the data undergoes a final quality validation. To ensure the data is ready for the next data preparation step, we conduct the following tests:\n\nTabular description tables: Checking structural integrity after preprocessing\nIQR outliers table: Searching for statistical anomalies that may impact results\nClass imbalance: Investigating the proportionality of outcome classes\nTabular numerical histograms: Visualizing distributions for numerical variables\nTabular categorical bar plots: Analyzing frequencies of categorical variables\nTarget rate bar plots: Examining the distribution of the target variable across categories\nPearson correlation matrix: Identifying linear relationships between variables\n\nSimilar to the previous step, we now want to run the same description tables against the preprocessed dataset to identify improvements and observe the effects of our changes on the data. This time, we’ll do this by adding the &lt;test_id&gt;:preprocess_dataset&gt; label to our results.\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.TabularDescriptionTables:preprocess_dataset\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.IQROutliersTable\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.ClassImbalance\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.TabularNumericalHistograms\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.TabularCategoricalBarPlots\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.TargetRateBarPlots\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    },\n    params = {\n        \"default_column\": lending_club.target_column,\n        \"columns\": None\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.PearsonCorrelationMatrix\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.HighPearsonCorrelation\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    }\n)\ntest.log()\n\n\n\n\nRun tests for WoE analysis\nTo ensure data segmentation aligns with predictive value, we perform detailed tests on the following:\n\nWOE bin table: This test checks the bins and their corresponding Weight of Evidence values, ensuring accuracy and relevance.\nWOE bin plots: These plots display the distribution of WoE across the data spectrum, providing a visual assessment of alignment and consistency.\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.WOEBinTable\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    },\n    params = {\n        \"breaks_adj\": lending_club.breaks_adj\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.WOEBinPlots\",\n    inputs = {\n        \"dataset\": vm_preprocess_dataset\n    },\n    params = {\n        \"breaks_adj\": lending_club.breaks_adj\n    }\n)\ntest.log()\n\n\n\n\n\nModel validation\nThis phase verifies the model’s predictive power and generalizability. Generalizability is the ability to accurately make future predictions based on past observations — how is this model going to perform when working with real data?\nTests are applied to both training and evaluation stages to ensure robustness and accuracy.\n\n\nRun tests for model training\nAfter training our model, we’ll want to inspect the model coefficients and some of their statistical properties, such as probability values. A model coefficient is value that represents the weight or effect of an independent variable in a model. This step is crucial as it informs us about the relevance of the features used to train the model. We can also begin making decisions regarding the quality of our model.\nThese tests assist us with this inspection:\n\nDataset split: Assesses the distribution of data across training and testing sets\nRegression coeffs plot: Visual inspection of the model’s coefficients\nRegression models coeffs: Detailed evaluation of the regression coefficients’ values\n\nNote that we use the models interface and pass a model as a list. This is because these tests support multiple models — if you have candidate models, you can use these tests to examine the differences in coefficients across multiple models.\n\ntest= vm.tests.run_test(\n    \"validmind.data_validation.DatasetSplit\",\n    inputs = {\n        \"datasets\": [vm_train_ds, vm_test_ds]\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.RegressionCoeffsPlot\",\n    inputs = {\n        \"models\": [vm_model]\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.RegressionModelsCoeffs\",\n    inputs = {\n        \"models\": [vm_model]\n    }\n)\ntest.log()\n\n\n\n\nRun tests for model evaluation\nOnce we are satisfied with the trained model after inspecting the coefficients, we elevate our evaluation by examining more detailed performance metrics — such as the confusion matrix, Gini coefficient, and various visualizations. Here we also begin to examine the credit scores, comparing them across the test and training datasets to see how effectively they distinguish between defaulted and non-defaulted customers.\nThese tests will help us to check whether the calibration of the scorecard is fit for purpose. Below is a list of tests we use to perform a thorough evaluation of both the binary classification model and the scorecard:\n\nClassifier performance: Summarizing logistic regression metrics\nGINI table: Assessing the discriminatory power of the model\nConfusion matrix: Understanding classification accuracy\nROC curve: Plotting the trade-off between sensitivity and specificity\nPrediction probabilities histogram: Distributing predictions to visualize outcomes\nCumulative prediction probabilities: Cumulative probability analysis for predictions\nScorecard histogram: Evaluating the distribution of scorecard points\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.sklearn.ClassifierPerformance:train_dataset\",\n    inputs = {\n        \"dataset\": vm_train_ds,\n        \"model\": vm_model,\n    }\n)\ntest.log()\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.sklearn.ClassifierPerformance:test_dataset\",\n    inputs = {\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.GINITable\",\n    inputs = {\n        \"datasets\": [vm_train_ds, vm_test_ds],\n        \"model\": vm_model,\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.sklearn.ConfusionMatrix\",\n    inputs = {\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model\n    },\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.sklearn.ROCCurve\",\n    inputs = {\n        \"model\": vm_model,\n        \"dataset\": vm_test_ds\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.PredictionProbabilitiesHistogram\",\n    inputs = {\n        \"model\": vm_model,\n        \"datasets\": [vm_train_ds, vm_test_ds]\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.CumulativePredictionProbabilities\",\n    inputs = {\n        \"model\": vm_model,\n        \"datasets\": [vm_train_ds, vm_test_ds]\n    }\n)\ntest.log()\n\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.ScorecardHistogram\",\n    inputs = {\n        \"model\": vm_model,\n        \"datasets\": [vm_train_ds, vm_test_ds]\n    },\n    params = {\n        \"score_column\": \"glm_scores\"\n    }\n)\ntest.log()\n\n\n\n\nRun tests for model explainability\nFollowing the detailed evaluation of the model through the performance metrics above, we then focus on model explainability to further understand the contributions of individual features. Model explainability is the ability to understand and interpret the decisions made by a machine learning model. This analysis is crucial for ensuring that our credit scoring model is not only effective but also interpretable in practical scenarios.\nHere are the key tests we deploy to analyze the model’s explainability after evaluating its overall performance:\n\nRegression permutation feature importance: Identifies which features most significantly affect the model’s predictions by observing changes in performance when feature values are shuffled\nFeatures AUC: Determines the discriminative power of each feature, showcasing how well each can independently predict the outcome\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.statsmodels.RegressionPermutationFeatureImportance\",\n    inputs = {\n        \"model\": vm_model,\n        \"dataset\": vm_test_ds\n    }\n)\ntest.log()\n\n\ntest= vm.tests.run_test(\n    \"validmind.model_validation.FeaturesAUC\",\n    inputs = {\n        \"model\": vm_model,\n        \"dataset\": vm_test_ds\n    }\n)\ntest.log()"
  },
  {
    "objectID": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#next-steps",
    "href": "notebooks/code_samples/credit_risk/application_scorecard_demo.html#next-steps",
    "title": "Document an application scorecard model",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand the following sections and take a look around:\n\n2. Data Preparation\n3. Model Development\n\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation (hint: some of the tests in 2.3. Feature Selection and Engineering look like they need some attention), view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready.\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html",
    "title": "Document a time series forecasting model",
    "section": "",
    "text": "Automatically document time series forecasting models by running the test suite for time series datasets.\nThis interactive notebook shows you how to use the ValidMind Developer Framework to import and prepare data and before running a data validation test suite, followed by loading a pre-trained model and running a model validation test suite.\nAs part of the notebook, you will learn how to:"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#contents",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#contents",
    "title": "Document a time series forecasting model",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nBefore you begin\n\nNew to ValidMind?\n\n\nInstall the client library\n\nInitialize the client library\n\nExplore available test suites\n\n\nImport raw data\n\nImport FRED dataset\n\n\nRun data validation test suite on raw data\n\nExplore the time series dataset test suites\n\nInitialize the dataset\n\nRun time series dataset test suite on raw dataset\n\n\nPreprocess data\n\nHandle frequencies, missing values and stationairty\n\n\nRun data validation test suite on processed data\n\nLoad pre-trained models\n\nLoad pre-trained models\n\nInitialize Validmind models\n\n\nRun model validation test suite on models\n\nExplore the time series model validation test suite\n\nRun model validation test suite on a list of models\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#about-validmind",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#about-validmind",
    "title": "Document a time series forecasting model",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#before-you-begin-1",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#before-you-begin-1",
    "title": "Document a time series forecasting model",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#install-the-client-library",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#install-the-client-library",
    "title": "Document a time series forecasting model",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#initialize-the-client-library",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#initialize-the-client-library",
    "title": "Document a time series forecasting model",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Time Series Forecasting as the template and Credit Risk - Underwriting - Loan as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"...\",\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\n\nExplore available test suites\nIn this notebook we will run a collection of test suites that are available in the ValidMind Developer Framework. Test suites group together a collection of tests that are relevant for a specific use case. In our case, we will run test different test suites for time series forecasting models. Once a test suite runs successfully, its results will be automatically uploaded to the ValidMind platform.\n\nvm.test_suites.list_suites()\n\nFor our example use case we will run the following test suites:\n\ntime_series_dataset\ntime_series_model_validation"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#import-raw-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#import-raw-data",
    "title": "Document a time series forecasting model",
    "section": "Import raw data",
    "text": "Import raw data\n\n\nImport FRED dataset\nFederal Reserve Economic Data, or FRED, is a comprehensive database maintained by the Federal Reserve Bank of St. Louis. It offers a wide array of economic data from various sources, including U.S. government agencies and international organizations. The dataset encompasses numerous economic indicators across various categories such as employment, consumer price indices, money supply, and gross domestic product, among others.\nFRED provides a valuable resource for researchers, policymakers, and anyone interested in understanding economic trends and conducting economic analysis. The platform also includes tools for data visualization, which can help users interpret complex economic data and identify trends over time.\nThe following code snippet imports a sample FRED dataset into a Pandas dataframe:\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\ndf = demo_dataset.load_data()\ndf.tail(10)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#run-data-validation-test-suite-on-raw-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#run-data-validation-test-suite-on-raw-data",
    "title": "Document a time series forecasting model",
    "section": "Run data validation test suite on raw data",
    "text": "Run data validation test suite on raw data\n\n\nExplore the time series dataset test suites\nLet’s see what tests are included on each test suite:\n\nvm.test_suites.describe_suite(\"time_series_data_quality\")\n\n\nvm.test_suites.describe_suite(\"time_series_univariate\")\n\n\n\n\nInitialize the dataset\nUse the ValidMind Developer Framework to initialize the dataset object:\n\nvm_dataset = vm.init_dataset(\n    input_id=\"raw_dataset\",\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\n\n\n\nRun time series dataset test suite on raw dataset\nNext, use the ValidMind Developer Framework to run the test suite for time series datasets:\n\nconfig = {\n    # TIME SERIES DATA QUALITY PARAMS\n    \"validmind.data_validation.TimeSeriesOutliers\": {\n        \"params\": {\n            \"zscore_threshold\": 3\n        }\n    },\n    \"validmind.data_validation.TimeSeriesMissingValues\": {\n        \"params\": {\n            \"min_threshold\": 2\n        }\n    },\n\n    # TIME SERIES UNIVARIATE PARAMS\n    \"validmind.data_validation.RollingStatsPlot\": {\n        \"params\": {\n            \"window_size\": 12\n        }\n    },\n    \"validmind.data_validation.SeasonalDecompose\": {\n        \"params\": {\n            \"seasonal_model\": \"additive\"\n        }\n    },\n    \"validmind.data_validation.AutoSeasonality\": {\n        \"params\": {\n            \"min_period\": 1,\n            \"max_period\": 3\n        }\n    },\n    \"validmind.data_validation.AutoStationarity\": {\n        \"params\": {\n            \"max_order\": 3,\n            \"threshold\": 0.05\n        }\n    },\n    \"validmind.data_validation.AutoAR\": {\n        \"params\": {\n            \"max_ar_order\": 2\n        }\n    },\n    \"validmind.data_validation.AutoMA\": {\n        \"params\": {\n            \"max_ma_order\": 2\n        }\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS\n    \"validmind.data_validation.LaggedCorrelationHeatmap\": {\n        \"params\": {\n            \"target_col\": demo_dataset.target_column,\n            \"independent_vars\": demo_dataset.feature_columns\n        }\n    },\n    \"validmind.data_validation.EngleGrangerCoint\": {\n        \"params\": {\n            \"threshold\": 0.05\n        }\n    },\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    inputs={\n        \"dataset\": vm_dataset,\n    },\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#preprocess-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#preprocess-data",
    "title": "Document a time series forecasting model",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nHandle frequencies, missing values and stationairty\n\n# Sample frequencies to Monthly\nresampled_df = df.resample(\"MS\").last()\n\n#  Remove all missing values\nnona_df = resampled_df.dropna()\n\n#  Take the first different across all variables\npreprocessed_df = nona_df.diff().dropna()"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#run-data-validation-test-suite-on-processed-data",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#run-data-validation-test-suite-on-processed-data",
    "title": "Document a time series forecasting model",
    "section": "Run data validation test suite on processed data",
    "text": "Run data validation test suite on processed data\n\nvm_dataset = vm.init_dataset(\n    input_id=\"preprocess_dataset\",\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    inputs={\"dataset\": vm_dataset},\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#load-pre-trained-models",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#load-pre-trained-models",
    "title": "Document a time series forecasting model",
    "section": "Load pre-trained models",
    "text": "Load pre-trained models\n\n\nLoad pre-trained models\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\n\n\n\n\nInitialize Validmind models\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(\n    input_id=\"train_a_dataset\",\n    dataset=train_df_A,\n    target_column=demo_dataset.target_column\n)\nvm_test_ds_A = vm.init_dataset(\n    input_id=\"test_a_dataset\",\n    dataset=test_df_A,\n    target_column=demo_dataset.target_column\n)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(\n    input_id=\"train_b_dataset\",\n    dataset=train_df_B,\n    target_column=demo_dataset.target_column\n)\nvm_test_ds_B = vm.init_dataset(\n    input_id=\"test_b_dataset\",\n    dataset=test_df_B,\n    target_column=demo_dataset.target_column\n)\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    input_id=\"model_a\",\n    model=model_A,\n\n)\n\nvm_train_ds_A.assign_predictions(model=vm_model_A)\nvm_test_ds_A.assign_predictions(model=vm_model_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    input_id=\"model_b\",\n    model=model_B,\n)\nvm_train_ds_B.assign_predictions(model=vm_model_B)\nvm_test_ds_B.assign_predictions(model=vm_model_B)\n\nmodels = [vm_model_A, vm_model_B]"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#run-model-validation-test-suite-on-models",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#run-model-validation-test-suite-on-models",
    "title": "Document a time series forecasting model",
    "section": "Run model validation test suite on models",
    "text": "Run model validation test suite on models\n\n\nExplore the time series model validation test suite\n\nvm.test_suites.describe_test_suite(\"time_series_model_validation\")\n\n\n\n\nRun model validation test suite on a list of models\n\nconfig = {\n    \"validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\": {\n        \"params\": {\n            \"transformation\": \"integrate\",\n        }\n    },\n    \"validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\": {\n        \"params\": {\n            \"transformation\": \"integrate\",\n            \"shocks\": [0.3],\n        }\n    },\n    \"validmind.model_validation.statsmodels.RegressionModelsPerformance\": {\n        \"inputs\": {\n            \"in_sample_datasets\": (vm_train_ds_A, vm_train_ds_B),\n            \"out_of_sample_datasets\": (vm_test_ds_A, vm_test_ds_B),\n            \"models\": models,\n        },\n        \"params\": {\n            \"transformation\": \"integrate\",\n            \"shocks\": [0.3],\n        }\n    },\n    \"validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\": {\n        \"inputs\": {\n            \"datasets\": (vm_train_ds_A, vm_test_ds_A),\n            \"models\": [vm_model_A],\n        },\n    },\n    \"validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\": {\n        \"inputs\": {\n            \"datasets\": (vm_train_ds_A, vm_test_ds_A),\n            \"models\": [vm_model_A],\n        },\n    }\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    inputs={\n        \"dataset\": vm_train_ds_A,\n        \"datasets\": (vm_train_ds_A, vm_test_ds_A),\n        \"model\": vm_model_A,\n        \"models\": models,\n    },\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#next-steps",
    "href": "notebooks/code_samples/time_series/tutorial_time_series_forecasting.html#next-steps",
    "title": "Document a time series forecasting model",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/rag_documentation_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/rag_documentation_demo.html",
    "title": "RAG Model Documentation Demo",
    "section": "",
    "text": "In this notebook, we are going to implement a simple RAG Model for automating the process of answering RFP questions using GenAI. We will see how we can initialize an embedding model, a retrieval model and a generator model with LangChain components and use them within the ValidMind developer framework to run tests against them. Finally, we will see how we can put them together in a Pipeline and run that to get e2e results and run tests against that."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/rag_documentation_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/rag_documentation_demo.html#about-validmind",
    "title": "RAG Model Documentation Demo",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\n\nFunctionModels: ValidMind offers support for creating VMModel instances from Python functions. This enables us to support any “model” by simply using the provided function as the model’s predict method.\nPipelineModels: ValidMind models (VMModel instances) of any type can be piped together to create a model pipeline. This allows model components to be created and tested/documented independently, and then combined into a single model for end-to-end testing and documentation. We use the | operator to pipe models together.\nRAG: RAG stands for Retrieval Augmented Generation and refers to a wide range of GenAI applications where some form of retrieval is used to add context to the prompt so that the LLM that generates content can refer to it when creating its output. In this notebook, we are going to implement a simple RAG setup using LangChain components."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "",
    "text": "Document a large language model (LLM) specialized in sentiment analysis for financial news using the ValidMind Developer Framework.\nThis interactive notebook shows you how to set up the ValidMind Developer Framework, initializes the client library, and uses a specific prompt template for analyzing the sentiment of sentences in a dataset. The notebook also includes example data to test the model’s ability to correctly identify sentiment as positive, negative, or neutral."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#about-validmind",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#before-you-begin",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#before-you-begin",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nThis notebook requires an OpenAI API secret key to run. If you don’t have one, visit API keys on OpenAI’s site to create a new key for yourself. Note that API usage charges may apply.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#install-the-client-library",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#initialize-the-client-library",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select LLM-based Text Classification as the template and Marketing/Sales - Analytics as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nGet ready to run the analysis\nImport the ValidMind FoundationModel and Prompt classes needed for the sentiment analysis later on:\n\nfrom validmind.models import FoundationModel, Prompt\n\nCheck your access to the OpenAI API:\n\nimport os\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nfrom openai import OpenAI\n\nmodel = OpenAI()\n\n\ndef call_model(prompt):\n    return (\n        model.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        .choices[0]\n        .message.content\n    )\n\nSet the prompt guidelines for the sentiment analysis:\n\nprompt_template = \"\"\"\nYou are an AI with expertise in sentiment analysis, particularly in the context of financial news.\nYour task is to analyze the sentiment of a specific sentence provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the sentence.\n\nSentence to Analyze:\n```\n{Sentence}\n```\n\nPlease respond with the sentiment of the sentence denoted by one of either 'positive', 'negative', or 'neutral'.\nPlease respond only with the sentiment enum value. Do not include any other text in your response.\n\nNote: Ensure that your analysis is based on the content of the sentence and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"Sentence\"]\n\n\n\nGet your sample dataset ready for analysis\nTo perform the sentiment analysis for financial news we’re going to load a local copy of this dataset: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\nThis dataset contains two columns, Sentiment and Sentence. The sentiment can be negative, neutral or positive.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./datasets/sentiments_with_predictions.csv\")"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#run-the-model-documentation-tests",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#run-the-model-documentation-tests",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "Run the model documentation tests",
    "text": "Run the model documentation tests\nFirst, use the ValidMind Developer Framework to initialize the dataset and model objects necessary for documentation. The ValidMind predict_fn function allows the model to be tested and evaluated in a standardized manner:\n\nvm_test_ds = vm.init_dataset(\n    dataset=df,\n    input_id=\"test_dataset\",\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\nvm_model = vm.init_model(\n    FoundationModel(\n        predict_fn=call_model,\n        prompt=Prompt(\n            template=prompt_template,\n            variables=prompt_variables,\n        ),\n    ),\n    input_id=\"gpt_35_model\",\n)\n\n# Assign model predictions to the test dataset\nvm_test_ds.assign_predictions(vm_model, prediction_column=\"gpt_35_prediction\")\n\nNext, use the ValidMind Developer Framework to run validation tests on the model. The vm.run_documentation_tests function analyzes the current project’s documentation template and collects all the tests associated with it into a test suite.\nThe function then runs the test suite, logs the results to the ValidMind API and displays them to you.\n\ntest_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    }\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#next-steps",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_integration_demo.html#next-steps",
    "title": "Sentiment analysis of financial data using a large language model (LLM)",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation in the ValidMind Platform UI:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your model documentation that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at Get started with the ValidMind Developer Framework."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html",
    "title": "Automate news summarization using LLMs",
    "section": "",
    "text": "Document a LLM-based text summarization model of news using the CNN DailyMail sample dataset from HuggingFace with the ValidMind Developer Framework.\nAs part of the notebook, you will learn how to develop an text summarization model while exploring how the documentation process works:"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#use-case",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#use-case",
    "title": "Automate news summarization using LLMs",
    "section": "Use case",
    "text": "Use case\nThe purpose of this notebook is to showcase how to document an automated news summarization system using a Large Language Model (LLM). This AI system leverages a large language model (LLM) to process and condense web-based news articles into concise summaries.\nData Sources\nThe CNN/DailyMail Dataset is a collection tailored for text summarization, containing over 300,000 news articles from two significant English-speaking regions, the US and the UK. Each row comprises an article, a highlight section, and a unique ID. The highlights in the dataset are summaries that have been written by the original journalists. The CNN articles were written between April 2007 and April 2015. The Daily Mail articles were written between June 2010 and April 2015.\nThe original dataset includes pre-divided splits: train, validation, and test. In this demo, as we are not training a LLM in the traditional machine learning sense but rather using prompt engineering to guide the LLM to function as a text summarizer, we do not adhere to the conventional distinction between training and test datasets. Therefore, we exclusively utilize the test dataset, applying it as a validation or “gold” standard to evaluate the effectiveness of our summarization through prompt engineering.\nWorkflow\nThe workflow comprises four primary stages, starting with article selection, where articles from the test dataset are chosen. This is followed by prompt engineering, where a prompt is crafted to communicate the summarization task to the LLM. In the summarization stage, the prompt is input into the LLM, which then produces summaries based on the article content. The final stage involves LLM response evaluation, where the summaries generated by the LLM are measured against the original journalist-authored highlights to evaluate the summarization quality."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#about-validmind",
    "title": "Automate news summarization using LLMs",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, you can make use of the following resources alongside this notebook:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#before-you-begin",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#before-you-begin",
    "title": "Automate news summarization using LLMs",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#install-the-client-library",
    "title": "Automate news summarization using LLMs",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#initialize-the-client-library",
    "title": "Automate news summarization using LLMs",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: LLM-based Text Summarization\nUse case: Marketing/Sales - Sales/Prospecting\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#initialize-the-python-environment",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#initialize-the-python-environment",
    "title": "Automate news summarization using LLMs",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\n# Install the `datasets` library from huggingface\n%pip install -q datasets\n%matplotlib inline\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#load-the-sample-dataset",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#load-the-sample-dataset",
    "title": "Automate news summarization using LLMs",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\nfrom validmind.datasets.nlp import cnn_dailymail\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{cnn_dailymail.target_column}' \"\n    f\"\\n\\t• Input text column: {cnn_dailymail.text_column} \"\n    f\"\\n\\t• Prediction columns: '{cnn_dailymail.t5_prediction}', '{cnn_dailymail.gpt_35_prediction_column}'\"\n)\n\n\ntrain_df, test_df = cnn_dailymail.load_data(source=\"offline\", dataset_size=\"100\")\n\n# Display the first few rows of the dataframe to check the loaded data.\ncnn_dailymail.display_nice(train_df.head())"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#document-the-model",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#document-the-model",
    "title": "Automate news summarization using LLMs",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\nSetup the Large Language Model (LLM)\nThis section prepares our environment to use OpenAI’s Large Language Model by setting up the API key and defining a function to call the model.\n\nimport os\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nfrom openai import OpenAI\n\nmodel = OpenAI()\n\n\ndef call_model(prompt):\n    return (\n        model.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        .choices[0]\n        .message.content\n    )\n\n\n\nSetup up the Prompt\nIn this section, we construct a structured prompt template designed to guide the AI in summarizing the CNN Daily news. The template emphasizes the AI’s role as an expert in parsing and condensing news information. It instructs the AI to focus on the article’s core content, avoiding assumptions or external data.\n\nprompt_template = \"\"\"\nYou are an AI with expertise in summarizing financial news.\nYour task is to provide a concise summary of the specific news article provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the article.\n\nArticle to Summarize:\n\n```\n{article}\n```\n\nPlease respond with a concise summary of the article's main points.\nEnsure that your summary is based on the content of the article and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"article\"]\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\n\nWith all datasets ready, you can now initialize training and test datasets (train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nfrom validmind.models import FoundationModel, Prompt\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    input_id=\"test_dataset\",\n    text_column=\"article\",\n    target_column=\"highlights\",\n)\n\nvm_model = vm.init_model(\n    FoundationModel(\n        predict_fn=call_model,\n        prompt=Prompt(\n            template=prompt_template,\n            variables=prompt_variables,\n        ),\n    ),\n    input_id=\"gpt_35\",\n)\n\n\n\nAssign predictions to the datasets\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n\n# Assign pre-computed model predictions to the test dataset\nvm_test_ds.assign_predictions(vm_model, prediction_column=\"gpt_35_prediction\")\n\nprint(vm_test_ds)\n\n\n\nData validation\nThis section focuses on performing a series of data description tests to gain insights into the basic characteristics of our text data. The goal of data description in this use case is verifying that the data meets certain standards and criteria before it is used for text summarization tasks. We conduct the follwoing NLP data quality tests:\n\nText Description: Assess the general context and provide a summary of the dataset.\nCommon Words: Determine the most frequently occurring words that could indicate key themes.\nPunctuations: Analyze punctuation patterns to understand sentence structures and emphases.\nStop Words: Identify and remove common stopwords to clarify the significant textual elements.\n\n\ntest = vm.tests.run_test(\n    \"validmind.data_validation.nlp.TextDescription\",\n    inputs={\n        \"dataset\": vm_test_ds,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.data_validation.nlp.CommonWords\",\n    inputs={\n        \"dataset\": vm_test_ds,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.data_validation.nlp.Punctuations\",\n    inputs={\n        \"dataset\": vm_test_ds,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.data_validation.nlp.StopWords\",\n    inputs={\n        \"dataset\": vm_test_ds,\n    },\n)\ntest.log()\n\n\n\nPrompt Validation\nThis section conducts a critical analysis of prompts to ensure their effectiveness when interacting with AI models. It involves systematic checks across several dimensions to enhance the quality of the interaction between the user and the AI:\n\nBias: Evaluate prompts for impartiality.\nClarity: Confirm the prompts are clearly understood.\nConciseness: Verify that the prompts are brief and concise.\nDelimitation: Check the boundaries and extent of prompts.\nNegative Instruction: Review prompts for any negative phrasing that could be misconstrued.\nSpecificity: Assess prompts for detailed and precise instructions.\n\n\ntest = vm.tests.run_test(\n    \"validmind.prompt_validation.Bias\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.prompt_validation.Clarity\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.prompt_validation.Conciseness\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.prompt_validation.Delimitation\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.prompt_validation.NegativeInstruction\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.prompt_validation.Specificity\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\n\nModel Validation\nThis section is dedicated to the assessment of the AI model’s understanding and processing of language data. It involves validating the model through various embedding and performance tests, ensuring the model’s output is as expected and reliable.\n\nRun embeddings tests\nThis subsection involves conducting tests to examine the semantic space where words or phrases from the vocabulary are mapped. We check for:\n\nCosine Similarity Distribution: Analyze the degree of similarity between vectors.\nCluster Distribution: Observe how embeddings group together, potentially indicating similarities in meaning.\nDescriptive Analytics: Provide statistical descriptions of the embedding space.\nStability Analysis Keyword: Test embeddings stability against keyword variations.\nStability Analysis Random Noise: Assess how random noise affects the stability of embeddings.\nStability Analysis Synonyms: Evaluate the consistency of embeddings for synonymous words.\n\n\nfrom transformers import pipeline\n\nembedding_model = pipeline(\n    \"feature-extraction\",\n    model=\"bert-base-uncased\",\n    tokenizer=\"bert-base-uncased\",\n    truncation=True,\n)\n\nvm_embedding_model = vm.init_model(\n    model=embedding_model,\n    input_id=\"bert_embedding_model\",\n)\n\n\nvm_test_ds.assign_predictions(\n    model=vm_embedding_model,\n)\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.embeddings.CosineSimilarityDistribution\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_embedding_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.embeddings.ClusterDistribution\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_embedding_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.embeddings.DescriptiveAnalytics\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_embedding_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.embeddings.StabilityAnalysisKeyword\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_embedding_model,\n    },\n    params={\n        \"text_column\": \"article\",\n        \"keyword_dict\": {\"finance\": \"financial\"},\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_embedding_model,\n    },\n    params={\n        \"text_column\": \"article\",\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.embeddings.StabilityAnalysisSynonyms\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_embedding_model,\n    },\n    params={\n        \"text_column\": \"article\",\n        \"probability:\": 0.1,\n    },\n)\ntest.log()\n\n\n\nRun model performance tests\nHere we measure the model’s linguistic performance across various metrics, including:\n\nToken Disparity: Examine the distribution of token usage.\nRouge Metrics: Use Recall-Oriented Understudy for Gisting Evaluation to assess the summary quality.\nBert Score: Implement BERT-based evaluations of token similarity.\nContextual Recall: Test the model’s ability to recall contextual information.\nBleu Score: Evaluate the quality of machine translation.\nMeteor Score: Measure translation hypothesis against reference translations.\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.TokenDisparity\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.RougeScore\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n    params={\n        \"metric\": \"rouge-1\",\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.BertScore\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.ContextualRecall\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.BleuScore\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.MeteorScore\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\n\nRun bias and toxicity tests\nThe focus of this subsection is on identifying any potential bias or toxicity in the model’s language processing. We conduct:\n\nToxicity Score: Quantify the degree of toxicity in content generated by the model.\nToxicity Histogram: Visualize the distribution of toxicity scores.\nRegard Score: Assess the model’s language for indications of respect or disrespect.\nRegard Histogram: Plot the frequencies of different levels of regard to identify patterns.\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.ToxicityScore\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()\n\n\ntest = vm.tests.run_test(\n    \"validmind.model_validation.RegardScore\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\ntest.log()"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#next-steps",
    "href": "notebooks/code_samples/NLP_and_LLM/llm_summarization_demo.html#next-steps",
    "title": "Automate news summarization using LLMs",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation right in the ValidMind Platform UI:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your model documentation that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at How do I use the framework?."
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html",
    "title": "Customize metric outputs using output templates",
    "section": "",
    "text": "Run individual tests and metrics for experimentation and when exploring and building output templates to create custom results.\nThe ValidMind Developer Framework provides a suite of tests and metrics to help you evaluate the performance of your machine learning models. The out-of-the-box results are designed to be informative and easy to understand, but you may want to customize the look and feel of the results to better suit your needs. This might include things like removing or adding columns from the results, changing the formatting or structure of a table, or adding entirely new tables to the results. Output templates allow you to do all of these things and more.\nOutput templates currently can create and cutomize tables. They are written in HTML and use the Jinja2 templating language.\nAs part of the notebook, you will build on the simple quickstart_customer_churn notebook and learn how to:\nThis interactive notebook uses the Bank Customer Churn Prediction sample dataset from Kaggle to train a simple classification model."
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#contents",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#contents",
    "title": "Customize metric outputs using output templates",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey Concepts\n\nHow documentation template work with output templates\n\nExample documentation template\n\n\n\nInstall the client library\n\nInitialize the client library\n\nInitialize the Python environment\n\nLoad the sample dataset\n\nDocument the model\n\nPrepocess the raw dataset\n\nInitialize the ValidMind datasets\n\nInitialize a model object\n\nAssign predictions to the datasets\n\nRun individual tests and customize the results\n\nRun the full suite of tests with output templates\n\nAdd the output template to the documentation template\n\nRun the full suite of tests\n\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#about-validmind",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#about-validmind",
    "title": "Customize metric outputs using output templates",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey Concepts\n\nOutput Templates: Customizable HTML templates that define the look and feel of the results produced by the ValidMind tests. They are written in HTML and use the Jinja2 templating language.\nJinja2 Templating Language: A powerful templating language for Python that allows you to embed expressions and control structures in your HTML templates.\nCustomizing Tables: Output templates allow you to customize the look and feel of the tables produced by the ValidMind tests. This includes things like adding or removing columns, changing the formatting or structure of the table, and adding entirely new tables to the results.\nDocumentation Templates: Documentation templates are covered in the quickstart notebook and are the base for all model documentation. They are written in YAML and define the entire structure and content of a model’s documentation. Output templates are not part of the documentation template, but they are defined in and shared via a field in the documentation template.\n\n\n\n\nHow documentation template work with output templates\nBelow is a section of the standard Binary Classification Documentation Template that comes pre-installed with the ValidMind Developer Framework.\nThe top-level model_evaluation section contains a list of sub sections which contain content blocks. These blocks can be either editable text blocks or test-driven blocks where the content_id identifies the threshold test or metric within the Developer Framework whose results will be displayed in that block. Now the key thing here is that each of these tests produces a specific output that is not directly editable from the ValidMind platform. This is where output templates come in, both figuraively and literally. They can be added as an optional field in the content block and will use the raw test output data in an HTML template to produce a custom table that can be displayed in the documentation.\n\n\nExample documentation template\n- id: model_development\n  title: Model Development\n  index_only: true\n  sections:\n    - id: model_training\n      title: Model Training\n      guidelines:\n        - Describe the model training process, including the algorithm used, any\n          hyperparameters or settings, and the optimization techniques employed\n          to minimize the loss function or maximize the objective function.\n        - ... (additional guidelines)\n      contents:\n        - content_type: metric\n          content_id: validmind.model_validation.ModelMetadata\n        - ... (additional content blocks)\n      parent_section: model_development\n    - id: model_evaluation\n      title: Model Evaluation\n      guidelines:\n        - Describe the process used to evaluate the model's performance on a\n          test or validation dataset that was not used during training, to\n          assess its generalizability and robustness.\n        - ... (additional guidelines)\n      contents:\n        - content_type: metric\n          content_id: validmind.model_validation.sklearn.ConfusionMatrix\n        - content_type: metric\n          content_id: validmind.model_validation.sklearn.ClassifierPerformance\n        - ... (additional content blocks)\n      parent_section: model_development\nIn the above example, the validmind.model_validation.sklearn.ClassifierPerformance produces two tables like this:\n\n\n\nAlt text\n\n\nBut with output templates, you can customize the look and feel of the output to produce a much simpler/clearer version like this:\n\n\n\nAlt text\n\n\nHow this is accomplished is with the following output template:\n- content_type: metric\n  content_id: validmind.model_validation.sklearn.ClassifierPerformance:with_template\n  output_template: |\n    &lt;table&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Accuracy&lt;/th&gt;\n                &lt;th&gt;Precision&lt;/th&gt;\n                &lt;th&gt;Recall&lt;/th&gt;\n                &lt;th&gt;F1 Score&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;{{ value[\"accuracy\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ value[\"weighted avg\"][\"precision\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ value[\"weighted avg\"][\"recall\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ value[\"weighted avg\"][\"f1-score\"] }}&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/tbody&gt;\n    &lt;/table&gt;\nAs you can see, the output template is a simple HTML table that uses the Jinja2 templating language to embed expressions that reference the raw test output data. The {{ value[\"accuracy\"] }} expression, for example, references the accuracy key in the raw test output data. This is how you can customize the look and feel of the results produced by the ValidMind tests.\nNow that you understand the basics of output templates, the following sections will guide you through the process of creating and using them in your code."
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#install-the-client-library",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#install-the-client-library",
    "title": "Customize metric outputs using output templates",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#initialize-the-client-library",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#initialize-the-client-library",
    "title": "Customize metric outputs using output templates",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)"
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#initialize-the-python-environment",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#initialize-the-python-environment",
    "title": "Customize metric outputs using output templates",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nimport xgboost as xgb\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#load-the-sample-dataset",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#load-the-sample-dataset",
    "title": "Customize metric outputs using output templates",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n)\n\nraw_df = demo_dataset.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#document-the-model",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#document-the-model",
    "title": "Customize metric outputs using output templates",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\n\nPrepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\nInitialize XGBoost classifier: Creates an XGBClassifier object with early stopping rounds set to 10.\nSet evaluation metrics: Specifies metrics for model evaluation as “error,” “logloss,” and “auc.”\nFit the model: Trains the model on x_train and y_train using the validation set (x_val, y_val). Verbose output is disabled.\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\nclass_labels — an optional value to map predicted classes to class labels\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nimport validmind as vm\n\nvm_raw_dataset = vm.init_dataset(\n    dataset=raw_df,\n    input_id=\"raw_dataset\",\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels,\n)\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df, input_id=\"train_dataset\", target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df, input_id=\"test_dataset\", target_column=demo_dataset.target_column\n)\n\n\n\n\nInitialize a model object\nAdditionally, you need to initialize a ValidMind model object (vm_model) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    input_id=\"model\",\n)\n\n\n\n\nAssign predictions to the datasets\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n\nvm_train_ds.assign_predictions(model=vm_model)\nvm_test_ds.assign_predictions(model=vm_model)\n\n\n\n\nRun individual tests and customize the results\nInstead of running the full suite of tests, you can run individual tests and metrics. This is useful for experimentation and when exploring and building output templates to create custom results. Lets go ahead and run a single test, the ClassifierInSamplePerformance metric, and see how we can create fully customized results from the output using output templates.\n\nfrom validmind.tests import run_test\n\nFirst, let’s run the test as normal and see the standard output:\n\nresult = run_test(\n    test_id=\"validmind.model_validation.sklearn.ClassifierPerformance\",\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"model\": vm_model,\n    },\n)\n\nLet’s also take a look at the result object that is returned when running the test and see how we can grab the raw metric value from it to start developing our output template:\n\nimport json\n\nprint(\"In Sample Performance Raw Value:\")\nprint(json.dumps(result.metric.value, indent=2))\n\nThis is the raw value object that will get passed into the output template and accessible just with the value variable name. Now let’s go ahead and create a simple output template like in the example and then see how we can test it directly against the result object:\n\noutput_template = \"\"\"\n&lt;table&gt;\n    &lt;thead&gt;\n        &lt;tr&gt;\n            &lt;th&gt;Accuracy&lt;/th&gt;\n            &lt;th&gt;Precision&lt;/th&gt;\n            &lt;th&gt;Recall&lt;/th&gt;\n            &lt;th&gt;F1 Score&lt;/th&gt;\n        &lt;/tr&gt;\n    &lt;/thead&gt;\n    &lt;tbody&gt;\n        &lt;tr&gt;\n            &lt;td&gt;{{ value[\"accuracy\"] }}&lt;/td&gt;\n            &lt;td&gt;{{ value[\"weighted avg\"][\"precision\"] }}&lt;/td&gt;\n            &lt;td&gt;{{ value[\"weighted avg\"][\"recall\"] | number }}&lt;/td&gt;\n            &lt;td&gt;{{ value[\"weighted avg\"][\"f1-score\"] | number }}&lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/tbody&gt;\n&lt;/table&gt;\n\"\"\"\n# specifically notice how the values from the result are being accessed inside the template\n# also notice that we can use filters to format the values e.g. `| number` to format the number to 4 decimal places\n\n\n# we can immediately re-render while trying different output templates\nresult.render(output_template=output_template)\n\nAnd, there you go, you have successfully created and used a custom output template. Try making some changes to the template html and see how it affects the output. You can also add more complex logic and control structures to the template using the Jinja2 templating language here.\nNow that you have a working output template, it can also be passed right into the run_test function to produce the same results as before:\n\nresult = run_test(\n    test_id=\"validmind.model_validation.sklearn.ClassifierPerformance\",\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"model\": vm_model,\n    },\n    output_template=output_template,\n)\n\nAwesome! So you have seen how to create and use output templates when running individual tests. In a real-world scenario though, you would want to add the output template to the documentation template so that it can live there as a permanent customization. This is what we will cover next.\n\n\n\nRun the full suite of tests with output templates\nNow that you’ve seen how to run an individual test and customize the output, let’s see how you can apply that concept to model documentation by adding the output template to the documentation template and running the full suite of tests.\n\n\nAdd the output template to the documentation template\nFirst, go to your project in the ValidMind UI and go to Settings &gt; Templates. Find the Binary Classification Template that is used by the Customer Churn project. Click on the Edit button to bring up the template editor. Then, add the following content block below the existing validmind.model_validation.sklearn.ClassifierPerformance metric:\n- content_type: metric\n  content_id: validmind.model_validation.sklearn.ClassifierPerformance:with_template\n  output_template: |\n    &lt;table&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Accuracy&lt;/th&gt;\n                &lt;th&gt;Precision&lt;/th&gt;\n                &lt;th&gt;Recall&lt;/th&gt;\n                &lt;th&gt;F1 Score&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;{{ value[\"accuracy\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ value[\"weighted avg\"][\"precision\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ value[\"weighted avg\"][\"recall\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ value[\"weighted avg\"][\"f1-score\"] }}&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/tbody&gt;\n    &lt;/table&gt;\nThis will add a second version of the ClassifierPerformance metric so we can compare the standard output with the custom output.\n\n\n\nRun the full suite of tests\nNow that you’ve added the output template to the documentation template, you can run the following code cells to initialize the client which retrieves the template. Then you can run the full suite of tests to see the custom output in the documentation and on the ValidMind UI.\n\nfull_suite = vm.run_documentation_tests(\n    section=[\"model_development\"],\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"datasets\": (vm_train_ds, vm_test_ds),\n        \"model\": vm_model,\n    },\n)"
  },
  {
    "objectID": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#next-steps",
    "href": "notebooks/code_samples/customization/customizing_metrics_with_output_templates.html#next-steps",
    "title": "Customize metric outputs using output templates",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html",
    "title": "Implement custom tests",
    "section": "",
    "text": "Custom tests or metrics extend the functionality of ValidMind, allowing you to document any model or use case with added flexibility.\nValidMind provides a comprehensive set of metrics out-of-the-box to evaluate and document your models and datasets. We recognize there will be cases where the default metrics do not support a model or dataset, or specific documentation is needed. In these cases, you can create and use your own custom code to accomplish what you need. To streamline custom code integration, we support the creation of custom metric functions.\nThis interactive notebook provides a step-by-step guide for implementing and registering custom metrics with ValidMind, running them individually, viewing the results on the ValidMind platform, and incorporating them into your model documentation template."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#contents",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#contents",
    "title": "Implement custom tests",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nBefore you begin\n\nNew to ValidMind?\n\n\nInstall the client library\n\nInitialize the client library\n\nImplement a Custom Metric\n\nRun the Custom Metric\n\nSetup the Model and Dataset\n\nRun the Custom Metric\n\n\nAdding Custom Metrics to Model Documentation\n\nSome More Custom Metrics\n\nCustom Metric: Table of Model Hyperparameters\n\nCustom Metric: External API Call\n\nCustom Metric: Passing Parameters\n\nCustom Metric: Multiple Tables and Plots in a Single Metric\n\nCustom Metric: Images\n\n\nConclusion\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#about-validmind",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#about-validmind",
    "title": "Implement custom tests",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#before-you-begin-1",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#before-you-begin-1",
    "title": "Implement custom tests",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\n\n\nNew to ValidMind?\nTo access the ValidMind Platform UI, you’ll need an account.\nSigning up is FREE — Create your account.\n\n\n\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#install-the-client-library",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#install-the-client-library",
    "title": "Implement custom tests",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#initialize-the-client-library",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#initialize-the-client-library",
    "title": "Implement custom tests",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Binary classification as the template and Marketing/Sales - Attrition/Churn Management as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"...\",\n  api_key = \"...\",\n  api_secret = \"...\",\n  project = \"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#implement-a-custom-metric",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#implement-a-custom-metric",
    "title": "Implement custom tests",
    "section": "Implement a Custom Metric",
    "text": "Implement a Custom Metric\nLet’s start off by creating a simple custom metric that creates a Confusion Matrix for a binary classification model. We will use the sklearn.metrics.confusion_matrix function to calculate the confusion matrix and then display it as a heatmap using plotly. (This is already a built-in metric in ValidMind, but we will use it as an example to demonstrate how to create custom metrics.)\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\n\n@vm.metric(\"my_custom_metrics.ConfusionMatrix\")\ndef confusion_matrix(dataset, model):\n    \"\"\"The confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known.\n\n    The confusion matrix is a 2x2 table that contains 4 values:\n\n    - True Positive (TP): the number of correct positive predictions\n    - True Negative (TN): the number of correct negative predictions\n    - False Positive (FP): the number of incorrect positive predictions\n    - False Negative (FN): the number of incorrect negative predictions\n\n    The confusion matrix can be used to assess the holistic performance of a classification model by showing the accuracy, precision, recall, and F1 score of the model on a single figure.\n    \"\"\"\n    y_true = dataset.y\n    y_pred = dataset.y_pred(model)\n\n    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n\n    cm_display = metrics.ConfusionMatrixDisplay(\n        confusion_matrix=confusion_matrix,\n        display_labels=[False, True]\n    )\n    cm_display.plot()\n\n    plt.close()  # close the plot to avoid displaying it\n    \n    return cm_display.figure_  # return the figure object itself\n\nThats our custom metric defined and ready to go… Let’s take a look at whats going on here:\n\nThe function confusion_matrix takes two arguments dataset and model. This is a VMDataset and VMModel object respectively.\nThe function docstring provides a description of what the metric does. This will be displayed along with the result in this notebook as well as in the ValidMind platform.\nThe function body calculates the confusion matrix using the sklearn.metrics.confusion_matrix function and then plots it using sklearn.metric.ConfusionMatrixDisplay.\nThe function then returns the ConfusionMatrixDisplay.figure_ object - this is important as the ValidMind framework expects the output of the custom metric to be a plot or a table.\nThe @vm.metric decorator is doing the work of creating a wrapper around the function that will allow it to be run by the ValidMind framework. It also registers the metric so it can be found by the ID my_custom_metrics.ConfusionMatrix (see the section below on how test IDs work in ValidMind and why this format is important)"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#run-the-custom-metric",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#run-the-custom-metric",
    "title": "Implement custom tests",
    "section": "Run the Custom Metric",
    "text": "Run the Custom Metric\nNow that we have defined and registered our custom metric, lets see how we can run it and properly use it in the ValidMind platform.\n\n\nSetup the Model and Dataset\nFirst let’s setup a an example model and dataset to run our custom metic against. Since this is a Confusion Matrix, we will use the Customer Churn dataset that ValidMind provides and train a simple XGBoost model.\n\nimport xgboost as xgb\nfrom validmind.datasets.classification import customer_churn\n\nraw_df = customer_churn.load_data()\ntrain_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n\nx_train = train_df.drop(customer_churn.target_column, axis=1)\ny_train = train_df[customer_churn.target_column]\nx_val = validation_df.drop(customer_churn.target_column, axis=1)\ny_val = validation_df[customer_churn.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nEasy enough! Now we have a model and dataset setup and trained. One last thing to do is bring the dataset and model into the ValidMind framework:\n\n# for now, we'll just use the test dataset\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    target_column=customer_churn.target_column,\n    input_id=\"test_dataset\",\n)\n\nvm_model = vm.init_model(model, input_id=\"model\")\n\n# link the model to the dataset\nvm_test_ds.assign_predictions(model=vm_model)\n\n\n\n\nRun the Custom Metric\nNow that we have our model and dataset setup, we have everything we need to run our custom metric. We can do this by importing the run_test function from the validmind.tests module and passing in the test ID of our custom metric along with the model and dataset we want to run it against.\n\nNotice how the inputs dictionary is used to map an input_id which we set above to the model and dataset keys that are expected by our custom metric function. This is how the ValidMind framework knows which inputs to pass to different metrics and is key when using many different datasets and models.\n\n\nfrom validmind.tests import run_test\n\nresult = run_test(\"my_custom_metrics.ConfusionMatrix\", inputs={\"model\": \"model\", \"dataset\": \"test_dataset\"})\n\nYou’ll notice that the docstring becomes a markdown description of the test. The figure is then displayed as the test result. What you see above is how it will look in the ValidMind platform as well. Let’s go ahead and log the result to see how that works.\n\nresult.log()"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#adding-custom-metrics-to-model-documentation",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#adding-custom-metrics-to-model-documentation",
    "title": "Implement custom tests",
    "section": "Adding Custom Metrics to Model Documentation",
    "text": "Adding Custom Metrics to Model Documentation\nTo do this, go to the documentation page of the model you registered above and navigate to the Model Development -&gt; Model Evaluation section. Then hover between any existing content block to reveal the + button as shown in the screenshot below.\n\n\n\nscreenshot showing insert button for test-driven blocks\n\n\nNow click on the + button and select the Test-Driven Block option. This will open a dialog where you can select Metric as the type of test and the My Custom Metrics Confusion Matrix from the list of available metrics. You can preview the result and then click Insert Block to add it to the documentation.\n\n\n\nscreenshot showing how to insert a test-driven block\n\n\nThe test should match the result you see above. It is now part of your documentation and will now be run everytime you run vm.run_documentation_tests() for your model. Let’s do that now.\n\nvm.reload()\n\nIf you preview the template, it should show the custom metric in the Model Development-&gt;Model Evaluation section:\n\nvm.preview_template()\n\nJust so we can run all of the tests in the template, let’s initialize the train and raw dataset.\n(see the quickstart_customer_churn_full_suite.ipynb notebook and the ValidMind docs for more information on what we are doing here)\n\nvm_raw_dataset = vm.init_dataset(\n    dataset=raw_df,\n    input_id=\"raw_dataset\",\n    target_column=customer_churn.target_column,\n    class_labels=customer_churn.class_labels,\n)\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=customer_churn.target_column,\n)\nvm_train_ds.assign_predictions(model=vm_model)\n\nTo run all the tests in the template, you can use the vm.run_documentation_tests() and pass the inputs we initialized above and the demo config from our customer_churn module. We will have to add a section to the config for our new test to tell it which inputs it should receive. This is done by simply adding a new element in the config dictionary where the key is the ID of the test and the value is a dictionary with the following structure:\n{\n    \"inputs\": {\n        \"model\": \"test_dataset\",\n        \"dataset\": \"model\",\n    }\n}\n\nfrom validmind.utils import preview_test_config\n\ntest_config = customer_churn.get_demo_test_config()\ntest_config[\"my_custom_metrics.ConfusionMatrix\"] = {\n    \"inputs\": {\n        \"dataset\": \"test_dataset\",\n        \"model\": \"model\",\n    }\n}\npreview_test_config(test_config)\n\n\nfull_suite = vm.run_documentation_tests(config=test_config)"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#some-more-custom-metrics",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#some-more-custom-metrics",
    "title": "Implement custom tests",
    "section": "Some More Custom Metrics",
    "text": "Some More Custom Metrics\nNow that you understand the entire process of creating custom metrics and using them in your documentation, let’s create a few more to see different ways you can utilize custom metrics.\n\n\nCustom Metric: Table of Model Hyperparameters\nThis custom metric will display a table of the hyperparameters used in the model:\n\n@vm.metric(\"my_custom_metrics.Hyperparameters\")\ndef hyperparameters(model):\n    \"\"\"The hyperparameters of a machine learning model are the settings that control the learning process.\n    These settings are specified before the learning process begins and can have a significant impact on the\n    performance of the model.\n\n    The hyperparameters of a model can be used to tune the model to achieve the best possible performance\n    on a given dataset. By examining the hyperparameters of a model, you can gain insight into how the model\n    was trained and how it might be improved.\n    \"\"\"\n    hyperparameters = model.model.get_xgb_params() # dictionary of hyperparameters\n\n    # turn the dictionary into a table where each row contains a hyperparameter and its value\n    return [{\"Hyperparam\": k, \"Value\": v} for k, v in hyperparameters.items() if v]\n\n\nresult = run_test(\"my_custom_metrics.Hyperparameters\", inputs={\"model\": \"model\"})\nresult.log()\n\nSince the metric has been run and logged, you can add it to your documentation using the same process as above. It should look like this:\n\n\n\nscreenshot showing hyperparameters metric\n\n\nFor our simple toy model, there are aren’t really any proper hyperparameters but you can see how this could be useful for more complex models that have gone through hyperparameter tuning.\n\n\n\nCustom Metric: External API Call\nThis custom metric will make an external API call to get the current BTC price and display it as a table. This demonstrates how you might integrate external data sources into your model documentation in a programmatic way. You could, for instance, setup a pipeline that runs a metric like this every day to keep your model documentation in sync with an external system.\n\nimport requests\n\n\n@vm.metric(\"my_custom_metrics.ExternalAPI\")\ndef external_api():\n    \"\"\"This metric calls an external API to get the current BTC price. It then creates\n    a table with the relevant data so it can be displayed in the documentation.\n\n    The purpose of this metric is to demonstrate how to call an external API and use the\n    data in a metric. A metric like this could even be setup to run in a scheduled\n    pipeline to keep your documentation in-sync with an external data source.\n    \"\"\"\n    url = \"https://api.coindesk.com/v1/bpi/currentprice.json\"\n    response = requests.get(url)\n    data = response.json()\n\n    # extract the time and the current BTC price in USD\n    return [\n        {\n            \"Time\": data[\"time\"][\"updated\"],\n            \"Price (USD)\": data[\"bpi\"][\"USD\"][\"rate\"],\n        }\n    ]\n\n\nresult = run_test(\"my_custom_metrics.ExternalAPI\")\nresult.log()\n\nAgain, you can add this to your documentation to see how it looks:\n\n\n\nscreenshot showing BTC price metric\n\n\n\n\n\nCustom Metric: Passing Parameters\nCustom metric functions, as stated earlier, can take both inputs and params. When you define your function there is no need to distinguish between the two, the ValidMind framework will handle that for you. You simply need to add both to the function as arguments and the framework will pass in the correct values.\nSo for instance, if you wanted to parameterize the first custom metric we created, the confusion matrix, you could do so like this:\ndef confusion_matrix(dataset: VMDataset, model: VMModel, my_param: str = \"Default Value\"):\n    pass\nAnd then when you run the test, you can pass in the parameter like this:\nvm.run_test(\n    \"my_custom_metrics.ConfusionMatrix\",\n    inputs={\"model\": \"model\", \"dataset\": \"test_dataset\"},\n    params={\"my_param\": \"My Value\"},\n)\nOr if you are running the entire documentation template, you would update the config like this:\ntest_config[\"my_custom_metrics.ConfusionMatrix\"] = {\n    \"inputs\": {\n        \"dataset\": \"test_dataset\",\n        \"model\": \"model\",\n    },\n    \"params\": {\n        \"my_param\": \"My Value\",\n    },\n}\nLet’s go ahead and create a toy metric that takes a parameter and uses it in the result:\n\nimport plotly_express as px\n\n\n@vm.metric(\"my_custom_metrics.ParameterExample\")\ndef parameter_example(plot_title = \"Default Plot Title\", x_col=\"sepal_width\", y_col=\"sepal_length\"):\n    \"\"\"This metric takes two parameters and creates a scatter plot based on them.\n\n    The purpose of this metric is to demonstrate how to create a metric that takes\n    parameters and uses them to generate a plot. This can be useful for creating\n    metrics that are more flexible and can be used in a variety of scenarios.\n    \"\"\"\n    # return px.scatter(px.data.iris(), x=x_col, y=y_col, color=\"species\")\n    return px.scatter(px.data.iris(), x=x_col, y=y_col, color=\"species\", title=plot_title)\n\n\nresult = run_test(\n    \"my_custom_metrics.ParameterExample\",\n    params={\n        \"plot_title\": \"My Cool Plot\",\n        \"x_col\": \"sepal_width\",\n        \"y_col\": \"sepal_length\",\n    },\n)\nresult.log()\n\nPlay around with this and see how you can use parameters, default values and other features to make your custom metrics more flexible and useful.\nHere’s how this one looks in the documentation: \n\n\n\nCustom Metric: Multiple Tables and Plots in a Single Metric\nCustom metric functions, as stated earlier, can return more than just one table or plot. In fact, any number of tables and plots can be returned. Let’s see an example of this:\n\nimport numpy as np\nimport plotly_express as px\n\n@vm.metric(\"my_custom_metrics.ComplexOutput\")\ndef complex_output():\n    \"\"\"This metric demonstrates how to return many tables and figures in a single metric\"\"\"\n    # create a couple tables\n    table = [{\"A\": 1, \"B\": 2}, {\"A\": 3, \"B\": 4}]\n    table2 = [{\"C\": 5, \"D\": 6}, {\"C\": 7, \"D\": 8}]\n\n    # create a few figures showing some random data\n    fig1 = px.line(x=np.arange(10), y=np.random.rand(10), title=\"Random Line Plot\")\n    fig2 = px.bar(x=[\"A\", \"B\", \"C\"], y=np.random.rand(3), title=\"Random Bar Plot\")\n    fig3 = px.scatter(x=np.random.rand(10), y=np.random.rand(10), title=\"Random Scatter Plot\")\n\n    return {\n        \"My Cool Table\": table,\n        \"Another Table\": table2,\n    }, fig1, fig2, fig3\n\n\nresult = run_test(\"my_custom_metrics.ComplexOutput\")\nresult.log()\n\nNotice how you can return the tables as a dictionary where the key is the title of the table and the value is the table itself. You could also just return the tables by themselves but this way you can give them a title to more easily identify them in the result.\n\n\n\nscreenshot showing multiple tables and plots\n\n\n\n\n\nCustom Metric: Images\nIf you are using a plotting library that isn’t supported by ValidMind (i.e. not matplotlib or plotly), you can still return the image directly as a bytes-like object. This could also be used to bring any type of image into your documentation in a programmatic way. For instance, you may want to include a diagram of your model architecture or a screenshot of a dashboard that your model is integrated with. As long as you can produce the image with Python or open it from a file, you can include it in your documentation.\n\nimport io\nimport matplotlib.pyplot as plt\n\n\n@vm.metric(\"my_custom_metrics.Image\")\ndef image():\n    \"\"\"This metric demonstrates how to return an image in a metric\"\"\"\n\n    # create a simple plot\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3, 4])\n    ax.set_title(\"Simple Line Plot\")\n\n    # save the plot as a PNG image (in-memory buffer)\n    img_data = io.BytesIO()\n    fig.savefig(img_data, format=\"png\")\n    img_data.seek(0)\n\n    plt.close()  # close the plot to avoid displaying it\n\n    return img_data.read()\n\n\nresult = run_test(\"my_custom_metrics.Image\")\nresult.log()\n\nAdding this custom metric to your documentation will display the image:\n\n\n\nscreenshot showing image custom metric"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#conclusion",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#conclusion",
    "title": "Implement custom tests",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we have demonstrated how to create custom metrics in ValidMind. We have shown how to define custom metric functions, register them with the ValidMind framework, run them against models and datasets, and add them to model documentation templates. We have also shown how to return tables and plots from custom metrics and how to use them in the ValidMind platform. We hope this tutorial has been helpful in understanding how to create and use custom metrics in ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/implement_custom_tests.html#next-steps",
    "href": "notebooks/code_samples/custom_tests/implement_custom_tests.html#next-steps",
    "title": "Implement custom tests",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The guide to elevating your AI model risk workflow",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Documentation\n                            The guide to elevating your AI model risk workflow\n                            Need help? Find all the information you need to use our platform for model risk management (AI model risk).\n                            \n                            \n                                \n                                    \n                                        \n                                            \n                                        \n                                        \n                                                                \n                                        \n                                    \n                                \n                            \n                            QuickStart\n                        \n                    \n                \n                \n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating key aspects of the model risk management process, ValidMind is an AI model risk solution designed for the unique documentation and validation needs of model developers and validators.\n                    Model Documentation Automation\n                    AI Model Risk & Lifecycle Management\n                    Communication & TrackingUser Guides\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Automate your model documentation and testing tasks with the ValidMind Developer Framework.Get started\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and documentation to ensure they comply with organizational & regulatory requirements.Get started\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "about/glossary/glossary.html",
    "href": "about/glossary/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This glossary of terms provides short definitions for technical terms you find commonly used in our product documentation grouped by terms related to:"
  },
  {
    "objectID": "about/glossary/glossary.html#validmind",
    "href": "about/glossary/glossary.html#validmind",
    "title": "Glossary",
    "section": "ValidMind",
    "text": "ValidMind\n\nValidMind AI risk platform\nThese two features are intertwined and work in tandem to help streamline your model lifecycle.\n\nValidMind Developer Framework (developer framework) \n\nAn open-source suite of documentation tools and test suites designed to document models, test models for weaknesses, and identify overfit areas. Enables automating the generation of model documentation by uploading documentation, metrics, and test results to the ValidMind AI risk platform.\n\nValidMind Platform UI (platform UI) \n\nA hosted multi-tenant architecture that includes the ValidMind cloud-based web interface, APIs, databases, documentation and validation engine, and various internal services.\n\n\n\n\nValidMind core features\n\nclient library, Python client library\n\nEnables the interaction of your development environment with ValidMind as part of the ValidMind Developer Framework.\n\ndocumentation automation\n\nA core benefit of ValidMind that allows for the automatic creation of model documentation using predefined templates and test suites.\n\nmodel inventory \n\nA feature of the ValidMind platform where you can track, manage, and oversee the lifecycle of models. Covers the full model lifecycle, including customizable documentation and approval workflows for different user roles, status and activity tracking, and periodic revalidation.\n\ntemplate, documentation template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n\n\nValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results. When rendered, produces a document that model developers can use for model validation.\n\ntest\n\nA function contained in the developer framework, designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind platform to generate the model documentation according to the template that is associated with the documentation.\n\n\nTests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n\ntest suite\n\nA collection of tests which are run together to generate model documentation end-to-end for specific use cases.\n\n\nFor example, the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use cases."
  },
  {
    "objectID": "about/glossary/glossary.html#models-and-model-risk-management",
    "href": "about/glossary/glossary.html#models-and-model-risk-management",
    "title": "Glossary",
    "section": "Models and model risk management",
    "text": "Models and model risk management\n\nModels\n\nmodel\n\nSR 11-7 defines a model as a “quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates.”\n\nmodel development\n\nAn iterative process in which many models are derived, tested, and built upon until a model fitting the desired criteria is achieved.\n\nmodel documentation\n\nA structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses.\n\n\nWithin the realm of model risk management, this documentation serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n\nmodel inventory\n\nA systematic and organized record of all quantitative and qualitative models used within an organization. This inventory facilitates oversight, tracking, and assessment by listing each model’s purpose, characteristics, owners, validation status, and associated risks.\n\n\nAlso see: ValidMind model inventory\n\nmodel lifecycle\n\nSubset of stages defining the lifecycle of a model; encompasses all steps for operating, governing, and maintaining a model until it is decommissioned (model development, model validation, model approval, model implementation, retirement).\n\nmodel risk\n\nThe potential for financial loss, incorrect decisions, or unintended consequences resulting from errors or inaccuracies in AI or machine learning models. Model risk typically arises from incorrect or inappropriate use of models, inaccurate assumptions, or limitations in data quality.\n\n\nFor example, consequences of unmitigated model risk can include adverse outcomes such as financial loss, damage to reputation, and regulatory penalties.\n\nmodel risk management (MRM)\n\nA structured approach to identifying, assessing, mitigating, and monitoring risks arising from the use of quantitative and qualitative models within an organization. Ensures that models are developed, validated, and used appropriately, with robust controls in place. Encompasses practices such as maintaining a model inventory, conducting periodic validations, and ensuring proper documentation.\n\n\n\n\nModel risk management\n\n1st line of defense\n\nBusiness unit(s) responsible for model development, initial validation, and implementation during the model lifecycle. As the 1st line of defense, model developers must document and test models to ensure that they are accurate, robust, and fit for purpose.\n\n2nd line of defense\n\nAn independent oversight function that provides a governance framework for the model lifecycle. As the 2nd line of defense, model validators must independently validate and challenge models created by model developers to ensure that model risk management principles are followed.\n\n3rd line of defense\n\nTypically an internal audit function responsible for providing an independent and comprehensive review of the risk management processes and controls that the first two lines have implemented.\n\nmodel developer\n\nResponsible for the design, implementation, and maintenance of models to ensure they are fit-for-purpose, accurate, and aligned with business requirements. As subject matter experts, they collaborate with model validators and other business units, ensuring the models are conceptually sound and robust.\n\nmodel governance\n\nA framework of policies, procedures, and standards established to oversee the lifecycle of models within an organization. Ensures that models are developed, validated, implemented, and retired in a controlled and consistent manner, promoting accountability, transparency, and adherence to regulatory requirements.\n\nmodel implementation\n\nA collaborative effort among model developers and model owners. Model implementation includes a formalized implementation plan and associated procedures, a review of results, and a record of model change procedures.\n\nmodel owner\n\nResponsible for coordinating model development, model implementation, ongoing model monitoring and maintaining the model’s administration, such as model documentation and model risk reporting.\n\nmodel user\n\nThose who rely on the model’s outputs to inform business decisions.\n\nmodel validation\n\nA systematic process to evaluate and verify that a model is performing as intended, accurately represents the phenomena it is designed to capture, and is appropriate for its specified purpose. This assessment encompasses a review of the model’s conceptual soundness, data integrity, calibration, and performance outcomes, as well as testing against out-of-sample datasets.\n\n\nWithin model risk management, model validation ensures that potential risks associated with model errors, misuse, or misunderstanding are identified and mitigated.\n\nmodel validator\n\nResponsible for conducting independent assessments of models to ensure their accuracy, reliability, and appropriateness for intended purposes. The role involves evaluating a model’s conceptual soundness, data integrity, calibration methods, and overall performance, typically using out-of-sample datasets.\n\n\nModel validators identify potential risks and weaknesses, ensuring that models within an organization meet established standards and regulatory requirements, and provide recommendations to model developers for improvements or modifications.\n\nthree lines of defense\n\nA structured approach to model risk management, consisting of three independent functions:\n\n\n\nThe first line consists of business units responsible for model development, validation, and implementation. They ensure that models are accurate, robust, and fit for purpose.\nThe second line is an independent model risk oversight function that provides a governance framework and guidance for model risk management.\nThe third line is the internal or external audit function, which assesses the effectiveness of model risk management practices and controls.\n\n\nvalidation report\n\nA formal document produced after a model validation process, outlining the findings, assessments, and recommendations related to a specific model’s performance, appropriateness, and limitations. Provides a comprehensive review of the model’s conceptual framework, data sources and integrity, calibration methods, and performance outcomes.\n\n\nWithin model risk management, the validation report is crucial for ensuring transparency, demonstrating regulatory compliance, and offering actionable insights for model refinement or adjustments."
  },
  {
    "objectID": "about/glossary/glossary.html#model-documentation",
    "href": "about/glossary/glossary.html#model-documentation",
    "title": "Glossary",
    "section": "Model documentation",
    "text": "Model documentation\n\nEach section of your model documentation should address critical aspects of the model’s lifecycle, from conceptualization and data preparation through development and ongoing management. This comprehensive documentation approach is essential for ensuring the model’s reliability, relevance, and compliance with business and regulatory standards.\n\nconceptual soundness\n\nEstablishes the foundation of the model, covering the model overview, intended use and business use case, regulatory requirements, model limitations, and the rationale behind the model selection. It emphasizes the model’s purpose, scope, and constraints, which are crucial for stakeholders to understand the model’s applicability and limitations.\n\ndata preparation\n\nDetails the data description, including dataset summary, data quality tests, descriptive statistics, correlations and interactions, and feature selection and engineering. It provides transparency into the data used for model training, ensuring that the model is built on a solid and relevant dataset.\n\nmodel development\n\nDiscusses the model training, evaluation, explainability, interpretability, and diagnosis, including model weak spots, overfit regions, and robustness. This section is vital for understanding how the model was developed, how it performs, and its areas of strength and weakness.\n\nmonitoring and governance\n\nFocuses on the model’s ongoing monitoring plan, implementation, and governance plan. It outlines strategies for maintaining the model’s performance over time and ensuring that it remains compliant with regulatory requirements and ethical standards."
  },
  {
    "objectID": "about/glossary/glossary.html#validation-reports",
    "href": "about/glossary/glossary.html#validation-reports",
    "title": "Glossary",
    "section": "Validation reports",
    "text": "Validation reports\n\nA validation report is a comprehensive review that evaluates a model’s accuracy, performance, and suitability for its intended purpose. It encompasses the process of model risk assessment, identifying areas of potential error or risk within the model’s components, such as data inputs and algorithms. The report follows established validation guidelines to ensure consistency and adherence to internal and regulatory standards.\n\nactions\n\nRecommended steps or measures to address findings from model validation or risk assessments.\n\nevidence\n\nMaterial provided by the developer and reviewed by the validator, such as model documentation, source code, datasets, monitoring reports or previous validation reports.\n\nfindings\n\nObservations or issues identified during model validation, including any deviations from expected performance or standards.\n\nmodel risk assessment\n\nThe process of identifying and evaluating risks associated with the use and potential errors in a financial model.\n\nmodel risk areas\n\nSpecific components or aspects of a model where risk might be present, such as data inputs, algorithms, or implementation.\n\nongoing monitoring report\n\nA periodic report assessing the model’s performance and compliance over time, ensuring it remains valid under changing conditions.\n\nvalidation guidelines\n\nEstablished standards or procedures for conducting thorough and consistent model validations, usually aligned with principles within specific models or AI risk frameworks."
  },
  {
    "objectID": "about/glossary/glossary.html#developer-tools",
    "href": "about/glossary/glossary.html#developer-tools",
    "title": "Glossary",
    "section": "Developer tools",
    "text": "Developer tools\n\ndecorator, Python decorator\n\nA design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure.\n\n\nDecorators are a simpler way for users to run their own code as a ValidMind test.\n\ninputs\n\nObjects to be evaluated and documented in the developer framework. They can be any of the following:\n\n\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model(). See the Model Documentation or the for more information.\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset(). See the Dataset Documentation for more information.\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\n\noutputs\n\nCustom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n\nparameters\n\nAdditional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\n\npip\n\nA package manager for Python, used to install and manage software packages written in the Python programming language.\n\n\nValidMind uses the pip command to install the Python client library that is part of the ValidMind Developer Framework so that model developers can make use of its features.\n\nJupyterHub\n\nA multi-user server provides a platform for users to interactively work with data science and scientific computing tools in a collaborative environment.\n\n\nValidMind uses JupyterHub to share live code, how-to instructions, and visualizations via notebooks as part of our getting started experience for new users.\n\nJupyter notebook\n\nAllows users to create and share documents containing live code, data visualizations, and narrative text. Supports various programming languages, most notably Python, and is widely used for data analysis, machine learning, scientific research, and educational purposes.\n\n\nValidMind uses notebooks to share sample code and how-to instructions with users that you can adapt to your own use case.\n\nmetrics, custom metrics\n\nMetrics are a subset of tests that do not have thresholds. Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\n\n\nIn the context of ValidMind’s Jupyter notebooks, metrics and tests can be thought of as interchangeable concepts.\n\nGitHub\n\nA cloud-based platform that provides hosting for software development and version control using Git. GitHub offers collaboration tools such as bug tracking, feature requests, task management, and continuous integration pipelines.\n\n\nValidMind uses GitHub to share open-source software with you."
  },
  {
    "objectID": "about/glossary/glossary.html#artificial-intelligence",
    "href": "about/glossary/glossary.html#artificial-intelligence",
    "title": "Glossary",
    "section": "Artificial intelligence",
    "text": "Artificial intelligence\nSee IBM’s series on artificial intelligence for more in-depth resources.\n\nartificial intelligence (AI)\n\nArtificial intelligence is a broad term used to classify machines that mimic human intelligence and human cognitive functions like problem-solving and learning.\n\ndeep-learning\n\nA subset of machine learning that uses multi-layered neural networks (deep neural networks) to simulate the complex decision-making power of the human brain.\n\ngenerative AI (GenAI)\n\nGenerative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.\n\nlarge language model (LLM)\n\nAdvanced types of artificial intelligence models designed to understand, generate, and interact with human language at a sophisticated level, such as ChatGPT.\n\nmachine learning\n\nMachine learning is a subset of artificial intelligence that allows for optimization. It helps make predictions that minimize the errors that arise from merely guessing.\n\ntraditional statistical models\n\nMathematical frameworks used to analyze and make inferences from data. These models are foundational in statistics and serve to explain relationships, predict outcomes, and guide decision-making across various fields, such as economics, biology, engineering, and social sciences."
  },
  {
    "objectID": "about/style-guide.html",
    "href": "about/style-guide.html",
    "title": "ValidMind style guide",
    "section": "",
    "text": "A style guide helps create distinct yet unified communication across all areas of a product experience, from in-app interactions to technical documentation and blog posts."
  },
  {
    "objectID": "about/style-guide.html#goals",
    "href": "about/style-guide.html#goals",
    "title": "ValidMind style guide",
    "section": "Goals",
    "text": "Goals\nAt ValidMind, we value transparency and accessibility—we aim to speak simply and effectively. We also believe in creating community, by presenting information in a manner that encourages collaboration and feedback from users old and new.\nOur processes reflect a holistic journey—accommodating the needs of users begins in the design phase, documentation supplements instead of replaces intuitive or engaging and guided user experiences, and publications should help our audience understand value and maximize their potential with ValidMind.\n\n\n\n\n\n\nThe following guidelines are meant to reflect these above principles, and ensure that all of our communications adhere to our vision."
  },
  {
    "objectID": "about/style-guide.html#concise-but-thorough",
    "href": "about/style-guide.html#concise-but-thorough",
    "title": "ValidMind style guide",
    "section": "Concise, but thorough",
    "text": "Concise, but thorough\nBe clear and concise. Avoid unnecessary jargon or convoluted language. But when in doubt, clarify; define complex phrases and provide context for product terminology.\n\nUse active voice\nThe quickest way to accomplish clarity is through use of the active voice. Active voice follows natural speaking patterns, is easy to understand, and reduces ambiguity about responsibilities especially when describing tasks.\n\nActive voice examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nYou’ll need to review the generated content.\nThe generated content will be reviewed by you.\n\n\n\n\n\nDon’t be obtuse\n\nUse exoteric (common) language whenever possible, and provide proper explanations or appropriate resources for technical terms. (You see what we did there? Parentheses—so handy.)\nAcronyms often require domain knowledge that a reader might not have. Avoid acronyms unless you define them first.\n\n\nCommon language and defining acronyms examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nModel risk management aims to ensure the accuracy and reliability of models, reduce operational risks, and comply with regulatory requirements such as the Comprehensive Capital Analysis and Review (CCAR) and the Dodd-Frank Act Stress Tests (DFAST).\nMRM aims to corroborate the accuracy and reliability of models, abate OpRisk, and induce observation with regulatory requirements such as CCAR and DFAS.\n\n\n\n\n\nBreak it down\n\nDivide complex ideas into simple, digestible parts.\nBulleted lists are your new best friend!\nTry incorporating different mediums (such as visual aids like images, charts, etc.) to help fortify understanding, and to provide the reader respite from walls of text."
  },
  {
    "objectID": "about/style-guide.html#everybodys-welcome",
    "href": "about/style-guide.html#everybodys-welcome",
    "title": "ValidMind style guide",
    "section": "Everybody’s welcome",
    "text": "Everybody’s welcome\nEngage with your audience in a friendly, open, and encouraging manner. Every reader is here to learn, and at ValidMind we believe that experts are always enthusiastic to share their knowledge.\n\nAppeal to humans\nBehind every page, there’s a person. In every word, lies an opportunity to win your audience over.\n\nShow empathy. Acknowledge user frustrations and celebrate successes.\nInject humor when appropriate. Sometimes, a touch of levity can make technical subjects more engaging. However, keep in mind context and be judicious when applying humor.\n\n\nEmpathy and humor examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nUser acknowledgement: Documenting findings can be difficult and tedious for even the most seasoned of validators.\nUser dismissal: For experienced validators, documenting findings is a breeze.\n\n\nSuccess toast: Nice work—you’ve successfully registered your first model!\nInappropriate humor: We lost your model documentation, oops! Here, have a pony! (e.g. error message for serious issue)\n\n\n\n\n\nBe positive\nEnable value for your audience, by empowering them.\n\nFocus on solutions. Instead of highlighting what’s wrong, guide the reader to the right path. When possible, offer workarounds.\nEmphasize benefits. Clearly communicate the positive impacts and how they help the user.\n\n\nSolutions and benefits examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nWhile the layout of these columns on this page is fixed, you can export the data which will allow you to rearrange the information as desired.\nYou can’t edit this page, or change the layout of these columns.\n\n\n\n\n\nMake it accessible\n\nBe inclusive. Knowledge is for everyone, and asks for no qualifications other than curiosity. Use bias-free communication to break down barriers and acknowledge the diversity of our user base.\nNot everyone uses technology the same way. To support as wide an audience as possible, provide alternative means to learn about important concepts. This can include alt tags for images, table headers, and alternative formats such as transcripts for videos.\n\n\nBias-free communication and inclusivity examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nThe findings are then submitted to the chair of the MRMG (model risk management group), who will then review the recommendations and provide their status assessments.\nThe findings are then submitted to the chairman of the MRMG, who will then review the recommendations and provide his status assessments.\n\n\n![](link-evidence.png){fig-alt=\"A screenshot of the validation report section 2.1.1. that shows a compliance assessment with the option to link to evidence.\"}\n![](link-evidence.png)"
  },
  {
    "objectID": "about/style-guide.html#lets-chat",
    "href": "about/style-guide.html#lets-chat",
    "title": "ValidMind style guide",
    "section": "Let’s chat",
    "text": "Let’s chat\nBe professional, but keep it casual and warm. Write like you would speak—imagine explaining the concept to a curious friend.\n\nMake it personal\nAddress the reader directly by using the second person.\n\nSecond person creates a sense of connection and reinforces a user’s agency when it comes to performing tasks.\nIt is also an easy way to remove any assumptions we may have about a user’s role, and reinforces bias-free communication.\n\n\n2nd person examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nAfter completing this QuickStart, you will be able to view your test results as part of your model documentation right in the ValidMind Platform UI.\nAfter completing this QuickStart, the model developer will be able to view the test results as part of the model documentation right in the ValidMind Platform UI.\n\n\n\n\n\nAvoid stiff formality\n\nMake use of contractions where appropriate.\nStay away from overly formal or stilted language.\nIf there’s a simpler way to say it: say it!\n\n\nInformal language examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nOnce you’ve registered the model, you can then grab the unique code snippet that will have been generated for you to use in the next step.\nFirst, you must register the model as this will generate a unique code snippet that needs to be copied. Then, you need to retrieve the code snippet so that you can make use of it in the following step.\n\n\n\n\n\nFocus on teamwork\nKeep your audience involved in the process. Engage the reader with questions, and appeal to their interests.\n\nAudience engagement examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nWhat if you were able to more easily, more effortlessly discern uncertainty, unleashing the full potential of your model outputs? That’s the power of ValidMind.\nValidMind easily and effortlessly discerns uncertainty, unleashing the full potential of model outputs."
  },
  {
    "objectID": "about/style-guide.html#comprehensive-assistance",
    "href": "about/style-guide.html#comprehensive-assistance",
    "title": "ValidMind style guide",
    "section": "Comprehensive assistance",
    "text": "Comprehensive assistance\nConveying knowledge and empowering users starts in the product interface. While external documentation can be helpful for supporting advanced functionality, users should not be lost on the basics without further reading.\n\nMake it interactive. Embedded user tutorials should exist whenever possible, and be intuitive and not obstructive.\nShow only what’s necessary when necessary. Progressive disclosure can help guide workflows as well as minimize visual clutter."
  },
  {
    "objectID": "about/style-guide.html#american-english",
    "href": "about/style-guide.html#american-english",
    "title": "ValidMind style guide",
    "section": "American English",
    "text": "American English\nWhile the ValidMind community spans far and wide, its heart finds its home in Palo Alto, California. When writing for ValidMind, keep things consistent by using American English spelling and grammar conventions.\n\nAmerican English examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nAt the center of this page, you will see the elements organized from most recent to least recent by default.\nAt the centre of this page, you will see the elements organised from most recent to least recent by default."
  },
  {
    "objectID": "about/style-guide.html#formatting",
    "href": "about/style-guide.html#formatting",
    "title": "ValidMind style guide",
    "section": "Formatting",
    "text": "Formatting\n\nSentence case\nIn general, follow sentence-style capitalization to minimize the confusion of when to capitalize and when not to capitalize.\n\nSentence case examples\n\n\nCorrect\nIncorrect\n\n\n\n\nGet started with ValidMind\nGet Started with ValidMind\n\n\n\n\n\nHeadings\n\nMake them imperative! Individual task headings are a call to action. Gerunds (“-ing”) are acceptable when introducing a set of instructions with individual tasks.\n\nAvoid numbered headings. Most headers do not require numbering, as order can be discerned from context. Numbering headers can make it difficult to shift information around when information changes.\n\nDon’t use terminal punctuation. While headings should be in sentence case, they are not sentences.\n\n\nHeading examples\n\n\nCorrect\nIncorrect\n\n\n\n\nLogin to ValidMind\n1. Logging into ValidMind.\n\n\n\n\n\nEmphasis\nUse emphatic styling sparingly, in order not to overwhelm the reader with visual distractions.\n\nBolding: Some light bolding can be helpful to draw attention to core concepts. Bolding is also used to highlight UI elements that the user can interact with.\nItalics: Italics should not be used for emphasis, only for first uses of terms on the page to set the stage.\nQuotation marks: Quotation marks should generally only be employed for quoting speech.\n\n\nEmphasis examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nAt ValidMind, we value transparency and accessibility—we aim to speak simply and effectively. (e.g. Highlighting the important concept within a sentence.)\nAt ValidMind, we value transparency and accessibility—we aim to speak simply and effectively. (e.g. Highlighting the entire sentence.)\n\n\nIn ValidMind Platform UI, click Model Inventory on the left sidebar.\nIn ValidMind Platform UI, click “Model Inventory” on the left sidebar.\n\n\nUncertainty can be summed up as the difference between reality and the outputs from the model selected to approximate reality.\n“Uncertainty” can be summed up as the difference between reality and the outputs from the model selected to approximate reality.\n\n\n“ValidMind is the only platform today that is purpose-built for model risk management professionals in the banking industry,” Jacobi says.\nAt ValidMind, we value “transparency and accessibility”—we aim to speak simply and effectively.\n\n\n\n\n\nContent types\n\nCode\n\nFormat code in its own code block.\nCode language is required.\n\n\n\n\n\n\n\nWithin a Jupyter Notebook\n\n\n\nYou can simply use a code cell rather than a markdown cell.\n\n\n\nCorrect:\n%pip install -q validmind\n\nIncorrect:\n      Using Python, call %pip install -q validmind.\n\n\nMathematics\nMathematical formulas should be rendered using LaTeX formatting. On our WordPress blog posts, this is taken care of by the WP Quick LaTeX plugin.\n\nMathematical formula examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\n\\(likes \\sim Binomial(n_{feedbacks},p_{like})\\)\n$likes \\sim Binomial(n_{feedbacks},p_{like})$"
  },
  {
    "objectID": "about/style-guide.html#proper-nouns",
    "href": "about/style-guide.html#proper-nouns",
    "title": "ValidMind style guide",
    "section": "Proper nouns",
    "text": "Proper nouns\nIn the context of model risk management, proper nouns include specific models, laws, or regulations, such as “Basel IV” or “SR 11-7.” These refer to specific frameworks or guidelines and you spell them with initial capital letters or exactly as indicated by official sources.\n\nTerms that are not proper nouns include general concepts such as “model validation,” “stress testing,” “risk assessment,” and “backtesting.”\nThese are common terms in the field and are not capitalized unless starting a sentence.\n\n\nProper noun examples\n\n\n\n\n\n\nCorrect\nIncorrect\n\n\n\n\nSS1/23 – Model risk management principles for banks\nModel Validation\n\n\nvalidation report\nbasel 4\n\n\nmachine learning\nFinancial Services industry\n\n\n\n\nProduct names\nWithin our documentation, you are able to reference constants such as the ValidMind Developer Framework and ValidMind Platform UI via variables.\n\nUse the variables shown on the table below instead of writing out the phrases to enable consistency between guides everywhere except for image alt text or Mermaid charts.\nIf product names need to be updated, simply amend the _variables.yml file to see changes reflected throughout all guides.\nPlease note that variables will not work within any of the Jupyter Notebook code samples as these are technically standalone files.\n\n\nValidMind product variable keys\n\n\n\n\n\n\n\nProduct Name\nVariable Key\nDescription\n\n\n\n\nValidMind AI risk platform\n{{&lt; var vm_risk &gt;}}\nRisk platform with a developer framework for documenting and testing models, alongside a platform UI hosting cloud-based tools, APIs, databases, and validation engines.\n\n\nValidMind Developer Framework\n{{&lt; var vm_framework &gt;}}\nOpen-source suite of of tools that connects to the platform UI.\n\n\nValidMind Platform UI\n{{&lt; var vm_platform &gt;}}\nHosted multi-tenant architecture that includes a cloud-based web interface.\n\n\ndeveloper framework\n{{&lt; var vm_dev &gt;}}\nShort form of ValidMind Developer Framework.\n\n\nplatform UI\n{{&lt; var vm_ui &gt;}}\nShort form of ValidMind Platform UI.\n\n\nhttps://app.prod.validmind.ai\n{{&lt; var vm_url &gt;}}\nURL of the ValidMind Platform UI.\n\n\n\n\n\n\n\n\n\nSee also the glossary for extended information on product names."
  },
  {
    "objectID": "about/style-guide.html#additional-style-guides",
    "href": "about/style-guide.html#additional-style-guides",
    "title": "ValidMind style guide",
    "section": "Additional style guides",
    "text": "Additional style guides\nThis style guide is meant to help get you familiar with speaking in the ValidMind voice, but not be an exhaustive list of conventions or rules you should be following when writing.\nIf it’s specified in our guide, follow those instructions—otherwise, you can check out the guides below for more inspiration:\n\nGoogle Style Guides\nMicrosoft Writing Style Guide\nApple Style Guide"
  },
  {
    "objectID": "about/validmind-community.html",
    "href": "about/validmind-community.html",
    "title": "ValidMind community",
    "section": "",
    "text": "Work with financial models, in model risk management (MRM), or are simply enthusiastic about artificial intelligence (AI) and machine learning and how these tools are actively shaping our futures within the finance industry and beyond? Congratulations — you’re already part of the ValidMind community! Come learn and play with us."
  },
  {
    "objectID": "about/validmind-community.html#interact-with-validmind",
    "href": "about/validmind-community.html#interact-with-validmind",
    "title": "ValidMind community",
    "section": "Interact with ValidMind",
    "text": "Interact with ValidMind\n\nJoin our Slack community: Be part of building a space for AI risk practitioners and those looking to break into the industry. Get technical support, share ideas, chat about model risk, AI, large language models, and much more!\nSign up for our newsletter: Receive exciting updates from ValidMind and curated material surrounding AI risk management.\nKeep up with the latest news in AI and MRM, as well as announcements and original content from ValidMind on our blog.\nWant to connect with other inquiring minds? Find out what events ValidMind is hosting and attending — we’d be delighted to see you!"
  },
  {
    "objectID": "about/validmind-community.html#build-with-validmind",
    "href": "about/validmind-community.html#build-with-validmind",
    "title": "ValidMind community",
    "section": "Build with ValidMind",
    "text": "Build with ValidMind\n\n\n\n\n\n\nValidMind style guide\n\n\n\nThe first step towards producing material for ValidMind is to familarize yourself with our style guide.\nLearn about our company vision, get to know our brand’s voice and preferred formatting conventions, and find additional resources to strengthen your writing skills.\n\n\nHere at ValidMind, we embrace an open source ideology. This means that we think expertise is inclusive, and is also open to evolving — there’s always an opportunity to learn from each other, and help each other improve.\nAs a member of the ValidMind community, we invite you to be part of our process. From our public documentation to the code behind our developer framework, we’ve exposed the wiring for inspection in hopes that great minds think differently and dare to spark change.\n\nContribution ideas\nPlease note that all community contributions are subject to review by the ValidMind team. See our software license agreement for more details.\n\nCode samples: Have a Jupyter Notebook that works perfectly with ValidMind, or covers a sample use case not already provided? Send it to us!\nValidMind Developer Framework: Live by the developer framework, and have an idea that will streamline the experience for others? Let us know!\nProduct documentation: Spot a typo, or have identified a gap in instruction you think would be beneficial for the community to fill? Submit an edit!"
  },
  {
    "objectID": "about/overview.html",
    "href": "about/overview.html",
    "title": "About ValidMind",
    "section": "",
    "text": "The ValidMind platform is a suite of tools helping developers, data scientists and risk & compliance stakeholders identify potential risks in their AI and large language models, and generate robust, high-quality model documentation that meets regulatory requirements.\nThe platform is adept at handling many use cases, including models compatible with the Hugging Face Transformers API, and GPT 3.5, GPT 4, and hosted LLama2 and Falcon-based models (focused on text classification and text summarization use cases).\nIn addition to LLMs, ValidMind can also handle testing and documentation generation for a wide variety of models, including:\nWhat sets ValidMind apart is its focus on simplifying complex tasks for both model developers and validators. By automating critical and often tedious aspects of the model lifecycle, such as documentation, validation, and testing, we enable model developers to concentrate on building better models.\nWe do all of this while making it easy to align with regulatory guidelines on model risk management in the United States, the United Kingdom, and Canada. These regulations include the Federal Reserve’s SR 11-7, the UK’s SS1/23 and CP6/22), and Canada’s Guideline E-23."
  },
  {
    "objectID": "about/overview.html#automated-model-testing-documentation",
    "href": "about/overview.html#automated-model-testing-documentation",
    "title": "About ValidMind",
    "section": "Automated model testing & documentation",
    "text": "Automated model testing & documentation\nOur developer framework streamlines the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence or machine learning models, and large language models (LLMs).\nIts main purpose is to automate the documentation process, ensuring that it aligns with regulatory and compliance standards.\nRead more …"
  },
  {
    "objectID": "about/overview.html#model-risk-governance-management",
    "href": "about/overview.html#model-risk-governance-management",
    "title": "About ValidMind",
    "section": "Model risk governance management",
    "text": "Model risk governance management\nOur ValidMind model risk management platform offers an integrated platform to manage validation reports, track findings, and report on model risk compliance across your model portfolio.\nIts main purpose is to enable your organization to monitor and manage models effectively, focusing on mitigating risks, maintaining governance, and ensuring compliance throughout the entire enterprise.\nRead more …"
  },
  {
    "objectID": "about/overview.html#ready-to-try-out-validmind",
    "href": "about/overview.html#ready-to-try-out-validmind",
    "title": "About ValidMind",
    "section": "Ready to try out ValidMind?",
    "text": "Ready to try out ValidMind?\nOur QuickStart is the quickest and easiest way to try out our product features."
  },
  {
    "objectID": "tests/data_validation/IsolationForestOutliers.html",
    "href": "tests/data_validation/IsolationForestOutliers.html",
    "title": "IsolationForestOutliers",
    "section": "",
    "text": "IsolationForestOutliers\nDetects outliers in a dataset using the Isolation Forest algorithm and visualizes results through scatter plots.\nPurpose: The IsolationForestOutliers test is designed to identify anomalies or outliers in the model’s dataset using the isolation forest algorithm. This algorithm assumes that anomalous data points can be isolated more quickly due to their distinctive properties. By creating isolation trees and identifying instances with shorter average path lengths, the test is able to pick out data points that differ from the majority.\nTest Mechanism: The test uses the isolation forest algorithm, which builds an ensemble of isolation trees by randomly selecting features and splitting the data based on random thresholds. It isolates anomalies rather than focusing on normal data points. For each pair of variables, a scatter plot is generated which distinguishes the identified outliers from the inliers. The results of the test can be visualized using these scatter plots, illustrating the distinction between outliers and inliers.\nSigns of High Risk: - The presence of high contamination, indicating a large number of anomalies - Inability to detect clusters of anomalies that are close in the feature space - Misclassifying normal instances as anomalies - Failure to detect actual anomalies\nStrengths: - Ability to handle large, high-dimensional datasets - Efficiency in isolating anomalies instead of normal instances - Insensitivity to the underlying distribution of data - Ability to recognize anomalies even when they are not separated from the main data cloud through identifying distinctive properties - Visually presents the test results for better understanding and interpretability\nLimitations: - Difficult to detect anomalies that are close to each other or prevalent in datasets - Dependency on the contamination parameter which may need fine-tuning to be effective - Potential failure in detecting collective anomalies if they behave similarly to normal data - Potential lack of precision in identifying which features contribute most to the anomalous behavior"
  },
  {
    "objectID": "tests/data_validation/HighCardinality.html",
    "href": "tests/data_validation/HighCardinality.html",
    "title": "HighCardinality",
    "section": "",
    "text": "HighCardinality\nAssesses the number of unique values in categorical columns to detect high cardinality and potential overfitting.\nPurpose: The “High Cardinality” test is used to evaluate the number of unique values present in the categorical columns of a dataset. In this context, high cardinality implies the presence of a large number of unique, non-repetitive values in the dataset.\nTest Mechanism: The test first infers the dataset’s type and then calculates an initial numeric threshold based on the test parameters. It only considers columns classified as “Categorical”. For each of these columns, the number of distinct values (n_distinct) and the percentage of distinct values (p_distinct) are calculated. The test will pass if n_distinct is less than the calculated numeric threshold. Lastly, the results, which include details such as column name, number of distinct values, and pass/fail status, are compiled into a table.\nSigns of High Risk: - A large number of distinct values (high cardinality) in one or more categorical columns implies a high risk. - A column failing the test (n_distinct &gt;= num_threshold) is another indicator of high risk.\nStrengths: - The High Cardinality test is effective in early detection of potential overfitting and unwanted noise. - It aids in identifying potential outliers and inconsistencies, thereby improving data quality. - The test can be applied to both, classification and regression task types, demonstrating its versatility.\nLimitations: - The test is restricted to only “Categorical” data types and is thus not suitable for numerical or continuous features, limiting its scope. - The test does not consider the relevance or importance of unique values in categorical features, potentially causing it to overlook critical data points. - The threshold (both number and percent) used for the test is static and may not be optimal for diverse datasets and varied applications. Further mechanisms to adjust and refine this threshold could enhance its effectiveness."
  },
  {
    "objectID": "tests/data_validation/DefaultRatesbyRiskBandPlot.html",
    "href": "tests/data_validation/DefaultRatesbyRiskBandPlot.html",
    "title": "DefaultRatesbyRiskBandPlot",
    "section": "",
    "text": "DefaultRatesbyRiskBandPlot\nGenerates a bar plot showcasing the distribution of default rates across different risk bands in a dataset.\nPurpose: The Default Rates by Risk Band Plot metric aims to quantify and visually represent default rates across varying risk bands within a specific dataset. This information is essential in evaluating the functionality of credit risk models, by providing a comprehensive view of default rates across a range of risk categories.\nTest Mechanism: The applied test approach involves a calculated bar plot. This plot is derived by initially determining the count of accounts in every risk band and then converting these count values into percentages by dividing by the total quantity of accounts. The percentages are then depicted as a bar plot, clearly showcasing the proportion of total accounts associated with each risk band. Hence, the plot delivers a summarized depiction of default risk across various bands. The ‘Dark24’ color sequence is used in the plot to ensure each risk band is easily distinguishable.\nSigns of High Risk: - High risk may be indicated by a significantly large percentage of accounts associated with high-risk bands. - High exposure to potential default risk in the dataset indicates potential weaknesses in the model’s capability to effectively manage or predict credit risk.\nStrengths: - The metric’s primary strengths lie in its simplicity and visual impact. - The graphical display of default rates allows for a clear understanding of the spread of default risk across risk bands. - Using a bar chart simplifies the comparison between various risk bands and can highlight potential spots of high risk. - This approach assists in identifying any numerical imbalances or anomalies, thus facilitating the task of evaluating and contrasting performance across various credit risk models.\nLimitations: - The key constraint of this metric is that it cannot provide any insights as to why certain risk bands might have higher default rates than others. - If there is a large imbalance in the number of accounts across risk bands, the visual representation might not accurately depict the true distribution of risk. - Other factors contributing to credit risk beyond the risk bands are not considered. - The metric’s reliance on a visual format might potentially lead to misinterpretation of results, as graphical depictions can sometimes be misleading."
  },
  {
    "objectID": "tests/data_validation/TargetRateBarPlots.html",
    "href": "tests/data_validation/TargetRateBarPlots.html",
    "title": "TargetRateBarPlots",
    "section": "",
    "text": "TargetRateBarPlots\nGenerates bar plots visualizing the default rates of categorical features for a classification machine learning model.\nPurpose: This test, implemented as a metric, is designed to provide an intuitive, graphical summary of the decision-making patterns exhibited by a categorical classification machine learning model. The model’s performance is evaluated using bar plots depicting the ratio of target rates—meaning the proportion of positive classes—for different categorical inputs. This allows for an easy, at-a-glance understanding of the model’s accuracy.\nTest Mechanism: The test involves creating a pair of bar plots for each categorical feature in the dataset. The first plot depicts the frequency of each category in the dataset, with each category visually distinguished by its unique color. The second plot shows the mean target rate of each category (sourced from the “default_column”). Plotly, a Python library, is used to generate these plots, with distinct plots created for each feature. If no specific columns are selected, the test will generate plots for each categorical column in the dataset.\nSigns of High Risk: - Inconsistent or non-binary values in the “default_column” could complicate or render impossible the calculation of average target rates. - Particularly low or high target rates for a specific category might suggest that the model is misclassifying instances of that category.\nStrengths: - This test offers a visually interpretable breakdown of the model’s decisions, providing an easy way to spot irregularities, inconsistencies, or patterns. - Its flexibility allows for the inspection of one or multiple columns, as needed.\nLimitations: - The test is less useful when dealing with numeric or continuous data, as it’s designed specifically for categorical features. - If the model in question is dealing with a multi-class problem rather than binary classification, the test’s assumption of binary target values (0s and 1s) becomes a significant limitation. - The readability of the bar plots drops as the number of distinct categories increases in the dataset, which can make them harder to understand and less useful."
  },
  {
    "objectID": "tests/data_validation/PearsonCorrelationMatrix.html",
    "href": "tests/data_validation/PearsonCorrelationMatrix.html",
    "title": "PearsonCorrelationMatrix",
    "section": "",
    "text": "PearsonCorrelationMatrix\nEvaluates linear dependency between numerical variables in a dataset via a Pearson Correlation coefficient heat map.\nPurpose: This test is intended to evaluate the extent of linear dependency between all pairs of numerical variables in the given dataset. It provides the Pearson Correlation coefficient, which reveals any high correlations present. The purpose of doing this is to identify potential redundancy, as variables that are highly correlated can often be removed to reduce the dimensionality of the dataset without significantly impacting the model’s performance.\nTest Mechanism: This metric test generates a correlation matrix for all numerical variables in the dataset using the Pearson correlation formula. A heat map is subsequently created to visualize this matrix effectively. The color of each point on the heat map corresponds to the magnitude and direction (positive or negative) of the correlation, with a range from -1 (perfect negative correlation) to 1 (perfect positive correlation). Any correlation coefficients higher than 0.7 (in absolute terms) are indicated in white in the heat map, suggesting a high degree of correlation.\nSigns of High Risk: - A large number of variables in the dataset showing a high degree of correlation (coefficients approaching ±1). This indicates redundancy within the dataset, suggesting that some variables may not be contributing new information to the model. - This could potentially lead to overfitting.\nStrengths: - The primary strength of this metric test is its ability to detect and quantify the linearity of relationships between variables. This allows for the identification of redundant variables, which in turn can help in simplifying models and potentially improving their performance. - The visualization aspect (heatmap) is another strength as it offers an easy-to-understand overview of the correlations, beneficial for those not comfortable navigating numerical matrices.\nLimitations: - The primary limitation of Pearson Correlation is its inability to detect non-linear relationships between variables, which can lead to missed opportunities for dimensionality reduction. - It only measures the degree of linear relationship and not the strength of effect of one variable on the other. - The cutoff value of 0.7 for high correlation is a somewhat arbitrary choice and some valid dependencies might be missed if they have a correlation coefficient less than this value."
  },
  {
    "objectID": "tests/data_validation/TooManyZeroValues.html",
    "href": "tests/data_validation/TooManyZeroValues.html",
    "title": "TooManyZeroValues",
    "section": "",
    "text": "TooManyZeroValues\nIdentifies numerical columns in a dataset that contain an excessive number of zero values, defined by a threshold percentage.\nPurpose: The ‘TooManyZeroValues’ test is utilized to identify numerical columns in the dataset that may present a quantity of zero values considered excessive. The aim is to detect situations where these may implicate data sparsity or a lack of variation, limiting their effectiveness within a machine learning model. The definition of ‘too many’ is quantified as a percentage of total values, with a default set to 3%.\nTest Mechanism: This test is conducted by looping through each column in the dataset and categorizing those that pertain to numerical data. On identifying a numerical column, the function computes the total quantity of zero values and their ratio to the total row count. Should the proportion exceed a pre-set threshold parameter, set by default at 0.03 or 3%, the column is considered to have failed the test. The results for each column are summarised and reported, indicating the count and percentage of zero values for each numerical column, alongside a status indicating whether the column has passed or failed the test.\nSigns of High Risk: - Indicators evidencing a high risk connected with this test would include numerical columns showing a high ratio of zero values when compared to the total count of rows (exceeding a pre-determined threshold). - Columns characterized by zero values across the board suggest a complete lack of data variation, signifying high risk.\nStrengths: - Assists in highlighting columns featuring an excess of zero values that could otherwise go unnoticed within a large dataset. - Provides the flexibility to alter the threshold that determines when the quantity of zero values becomes ‘too many’, thus catering to specific needs of a particular analysis or model. - Offers feedback in the form of both counts and percentages of zero values, which allows a closer inspection of the distribution and proportion of zeros within a column. - Targets specifically numerical data, thereby avoiding inappropriate application to non-numerical columns and mitigating the risk of false test failures.\nLimitations: - Is exclusively designed to check for zero values, and doesn’t assesses the potential impact of other values that could affect the dataset, such as extremely high or low figures, missing values or outliers. - Lacks the ability to detect a repetitive pattern of zeros, which could be significant in time-series or longitudinal data. - Zero values can actually be meaningful in some contexts, therefore tagging them as ‘too many’ could potentially misinterpret the data to some extent. - This test does not take into consideration the context of the dataset, and fails to recognize that within certain columns, a high number of zero values could be quite normal and not necessarily an indicator of poor data quality. - Cannot evaluate non-numerical or categorical columns, which might bring with them different types of concerns or issues."
  },
  {
    "objectID": "tests/data_validation/TabularDescriptionTables.html",
    "href": "tests/data_validation/TabularDescriptionTables.html",
    "title": "TabularDescriptionTables",
    "section": "",
    "text": "TabularDescriptionTables\nSummarizes key descriptive statistics for numerical, categorical, and datetime variables in a dataset.\nPurpose: The main purpose of this metric is to gather and present the descriptive statistics of numerical, categorical, and datetime variables present in a dataset. The attributes it measures include the count, mean, minimum and maximum values, percentage of missing values, data types of fields, and unique values for categorical fields, among others.\nTest Mechanism: The test first segregates the variables in the dataset according to their data types (numerical, categorical, or datetime). Then, it compiles summary statistics for each type of variable. The specifics of these statistics vary depending on the type of variable:\n\nFor numerical variables, the metric extracts descriptors like count, mean, minimum and maximum values, count of missing values, and data types.\nFor categorical variables, it counts the number of unique values, displays unique values, counts missing values, and identifies data types.\nFor datetime variables, it counts the number of unique values, identifies the earliest and latest dates, counts missing values, and identifies data types.\n\nSigns of High Risk: - Masses of missing values in the descriptive statistics results could hint at high risk or failure, indicating potential data collection, integrity, and quality issues. - Detection of inappropriate distributions for numerical variables, like having negative values for variables that are always supposed to be positive. - Identifying inappropriate data types, like a continuous variable being encoded as a categorical type.\nStrengths: - Provides a comprehensive overview of the dataset. - Gives a snapshot into the essence of the numerical, categorical, and datetime fields. - Identifies potential data quality issues such as missing values or inconsistencies crucial for building credible machine learning models. - The metadata, including the data type and missing value information, are vital for anyone including data scientists dealing with the dataset before the modeling process.\nLimitations: - It does not perform any deeper statistical analysis or tests on the data. - It does not handle issues such as outliers, or relationships between variables. - It offers no insights into potential correlations or possible interactions between variables. - It does not investigate the potential impact of missing values on the performance of the machine learning models. - It does not explore potential transformation requirements that may be necessary to enhance the performance of the chosen algorithm."
  },
  {
    "objectID": "tests/data_validation/Duplicates.html",
    "href": "tests/data_validation/Duplicates.html",
    "title": "Duplicates",
    "section": "",
    "text": "Duplicates\nTests dataset for duplicate entries, ensuring model reliability via data quality verification.\nPurpose: The ‘Duplicates’ metric is designed to check for duplicate rows within the dataset provided to the model. It serves as a measure of data quality, ensuring that the model isn’t merely memorizing duplicate entries or being swayed by redundant information. This is an important step in the pre-processing of data for both classification and regression tasks.\nTest Mechanism: This metric operates by checking each row for duplicates in the dataset. If a text column is specified in the dataset, the test is conducted on this column; if not, the test is run on all feature columns. The number and percentage of duplicates are calculated and returned in a DataFrame. Additionally, a test is passed if the total count of duplicates falls below a specified minimum threshold.\nSigns of High Risk: - A high number of duplicate rows in the dataset. This can lead to overfitting where the model performs well on the training data but poorly on unseen data. - A high percentage of duplicate rows in the dataset. A large proportion of duplicate values could indicate that there’s a problem with data collection or processing.\nStrengths: - Assists in improving the reliability of the model’s training process by ensuring the training data is not contaminated with duplicate entries which can distort statistical analyses. - Provides both absolute number and percentage value of duplicate rows, giving a thorough overview of data quality - Highly customizable as it allows for setting a user-defined minimum threshold to determine if the test has been passed.\nLimitations: - This test does not distinguish between benign duplicates (i.e., coincidental identical entries in different rows) and problematic duplicates originating from data collection or processing errors. - Since the test becomes more computationally intensive as the size of the dataset increases, it might not be suitable for very large datasets. - It can only check for exact duplicates and may miss semantically similar information packaged differently."
  },
  {
    "objectID": "tests/data_validation/DatasetDescription.html",
    "href": "tests/data_validation/DatasetDescription.html",
    "title": "DatasetDescription",
    "section": "",
    "text": "DatasetDescription\nProvides comprehensive analysis and statistical summaries of each field in a machine learning model’s dataset.\nPurpose: The test depicted in the script is meant to run a comprehensive analysis on a Machine Learning model’s datasets. The test or metric is implemented to obtain a complete summary of the fields in the dataset, including vital statistics of each field such as count, distinct values, missing values, histograms for numerical, categorical, boolean, and text fields. This summary gives a comprehensive overview of the dataset to better understand the characteristics of the data that the model is trained on or evaluates.\nTest Mechanism: The DatasetDescription class accomplishes the purpose as follows: firstly, the test method “run” infers the data type of each column in the dataset and stores the details (id, column type). For each field, describe_dataset_field” method is invoked to collect statistical information about the field, including count, missing value count and its proportion to the total, unique value count, and its proportion to the total. Depending on the data type of a field, histograms are generated that reflect the distribution of data within the field. Numerical fields use “get_numerical_histograms” method to calculate histogram distribution, whereas for categorical, boolean and text fields, a histogram is computed with frequencies of each unique value in the datasets. For unsupported types, an error is raised. Lastly, a summary table is built to aggregate all the statistical insights and histograms of the fields in a dataset.\nSigns of High Risk: - High ratio of missing values to total values in one or more fields which may impact quality of the predictions. - Unsupported data types in dataset fields. - Large number of unique values in the dataset’s fields which might make it harder for the model to establish patterns. - Extreme skewness or irregular distribution of data as reflected in the histograms.\nStrengths: - Provides a detailed analysis of the dataset with versatile summaries like count, unique values, histograms etc. - Flexibility in handling different types of data: numerical, categorical, boolean, and text. - Useful in detecting problems in the dataset like missing values, unsupported data types, irregular data distribution etc. - The summary gives a comprehensive understanding of dataset features allowing developers to make informed decisions.\nLimitations: - The computation can be expensive from a resource standpoint, particularly for large datasets with numerous fields. - The histograms use arbitrary number of bins which may not be the optimal number of bins for specific data distribution. - Unsupported data types for columns will raise an error which may limit evaluating the dataset. - Fields with all null or missing values are not included in histogram computation. - This test only validates the quality of the dataset but doesn’t address the model’s performance directly."
  },
  {
    "objectID": "tests/data_validation/nlp/Toxicity.html",
    "href": "tests/data_validation/nlp/Toxicity.html",
    "title": "Toxicity",
    "section": "",
    "text": "Toxicity"
  },
  {
    "objectID": "tests/data_validation/nlp/Sentiment.html",
    "href": "tests/data_validation/nlp/Sentiment.html",
    "title": "Sentiment",
    "section": "",
    "text": "Sentiment"
  },
  {
    "objectID": "tests/data_validation/nlp/TextDescription.html",
    "href": "tests/data_validation/nlp/TextDescription.html",
    "title": "TextDescription",
    "section": "",
    "text": "TextDescription\nPerforms comprehensive textual analysis on a dataset using NLTK, evaluating various parameters and generating visualizations.\nPurpose: This test uses the TextDescription metric to conduct a comprehensive textual analysis of a given dataset. Various parameters such as total words, total sentences, average sentence length, total paragraphs, total unique words, most common words, total punctuations, and lexical diversity are evaluated. This metric aids in comprehending the nature of the text and evaluating the potential challenges that machine learning algorithms deployed for textual analysis, language processing, or summarization might face.\nTest Mechanism: The test works by parsing the given dataset and utilizes the NLTK (Natural Language Toolkit) library for tokenizing the text into words, sentences, and paragraphs. Subsequently, it processes the text further by eliminating stopwords declared in ‘unwanted_tokens’ and punctuations. Next, it determines parameters like the total count of words, sentences, paragraphs, punctuations alongside the average sentence length and lexical diversity. Lastly, the result from these calculations is condensed and scatter plots for certain variable combinations (e.g. Total Words vs Total Sentences, Total Words vs Total Unique Words) are produced, providing a visual representation of the text’s structure.\nSigns of High Risk: - Anomalies or an increase in complexity within the lexical diversity results. - Longer sentences and paragraphs. - High uniqueness of words. - Presence of a significant amount of unwanted tokens. - Missing or erroneous visualizations. These signs suggest potential risk in text processing ML models, indicating that the ability of the model to absorb and process text could be compromised.\nStrengths: - An essential pre-processing tool, specifically for textual analysis in machine learning model data. - Provides a comprehensive breakdown of a text dataset, which aids in understanding both structural and vocabulary complexity. - Generates visualizations of correlations between chosen variables to further comprehend the text’s structure and complexity.\nLimitations: - Heavy reliance on the NLTK library, restricting its use to only the languages that NLTK supports. - Limited customization capacity as the undesirable tokens and stop words are predefined. - Lacks the ability to consider semantics or grammatical complexities, which could be crucial aspects in language processing. - Assumes that the document is well-structured (includes sentences and paragraphs); therefore, unstructured or poorly formatted text may distort the results."
  },
  {
    "objectID": "tests/data_validation/nlp/StopWords.html",
    "href": "tests/data_validation/nlp/StopWords.html",
    "title": "StopWords",
    "section": "",
    "text": "StopWords\nEvaluates and visualizes the frequency of English stop words in a text dataset against a defined threshold.\nPurpose: The StopWords threshold test is a tool designed for assessing the quality of text data in an ML model. It focuses on the identification and analysis of “stop words” in a given dataset. Stop words are frequent, common, yet semantically insignificant words (for example: “the”, “and”, “is”) in a language. This test evaluates the proportion of stop words to the total word count in the dataset, in essence, scrutinizing the frequency of stop word usage. The core objective is to highlight the prevalent stop words based on their usage frequency, which can be instrumental in cleaning the data from noise and improving ML model performance.\nTest Mechanism: The StopWords test initiates on receiving an input of a ‘VMDataset’ object. Absence of such an object will trigger an error. The methodology involves inspection of the text column of the VMDataset to create a corpus’ (a collection of written texts). Leveraging the Natural Language Toolkit’s (NLTK) stop word repository, the test screens the corpus for any stop words and documents their frequency. It further calculates the percentage usage of each stop word compared to the total word count in the corpus. This percentage is evaluated against a predefined ‘min_percent_threshold’. If this threshold is breached, the test returns a failed output. Top prevailing stop words along with their usage percentages are returned, facilitated by a bar chart visualization of these stop words and their frequency.\nSigns of High Risk: - A percentage of any stop words exceeding the predefined ‘min_percent_threshold’. - High frequency of stop words in the dataset which may adversely affect the application’s analytical performance due to noise creation.\nStrengths: - The ability to scrutinize and quantify the usage of stop words. - Provides insights into potential noise in the text data due to stop words. This can directly aid in enhancing model training efficiency. - The test includes a bar chart visualization feature to easily interpret and action upon the stop words frequency information.\nLimitations: - The test only supports English stop words, making it less effective with datasets of other languages. - The ‘min_percent_threshold’ parameter may require fine-tuning for different datasets, impacting the overall effectiveness of the test. - Contextual use of the stop words within the dataset is not considered which may lead to overlooking their significance in certain contexts. - The test focuses specifically on the frequency of stop words, not providing direct measures of model performance or predictive accuracy."
  },
  {
    "objectID": "tests/data_validation/nlp/LanguageDetection.html",
    "href": "tests/data_validation/nlp/LanguageDetection.html",
    "title": "LanguageDetection",
    "section": "",
    "text": "LanguageDetection"
  },
  {
    "objectID": "tests/data_validation/IQROutliersTable.html",
    "href": "tests/data_validation/IQROutliersTable.html",
    "title": "IQROutliersTable",
    "section": "",
    "text": "IQROutliersTable\nDetermines and summarizes outliers in numerical features using Interquartile Range method.\nPurpose: The “Interquartile Range Outliers Table” (IQROutliersTable) metric has been designed for identifying and summarizing outliers within numerical features of a dataset using the Interquartile Range (IQR) method. The purpose of this exercise is crucial in the pre-processing of data as outliers can substantially distort the statistical analysis and debilitate the performance of machine learning models.\nTest Mechanism: The IQR, which is the range separating the first quartile (25th percentile) from the third quartile (75th percentile), is calculated for each numerical feature within the dataset. An outlier is defined as a data point falling below the “Q1 - 1.5 * IQR” or above “Q3 + 1.5 * IQR” range. The metric then computes the number of outliers along with their minimum, 25th percentile, median, 75th percentile, and maximum values for each numerical feature. If no specific features are chosen, the metric will apply to all numerical features in the dataset. The default outlier threshold is set to 1.5, following the standard definition of outliers in statistical analysis, although it can be customized by the user.\nSigns of High Risk: - High risk is indicated by a large number of outliers in multiple features. - Outliers that are significantly distanced from the mean value of variables could potentially signal high risk. - Data entry errors or other data quality issues could be manifested through extremely high or low outlier values.\nStrengths: - It yields a comprehensive summary of outliers for each numerical feature within the dataset. This enables the user to pinpoint features with potential quality issues. - The IQR method is not overly affected by extremely high or low outlier values as it is based on quartile calculations. - The versatility of this metric grants the ability to customize the method to work on selected features and set a defined threshold for outliers.\nLimitations: - The metric might cause false positives if the variable of interest veers away from a normal or near-normal distribution, notably in the case of skewed distributions. - It does not extend to provide interpretation or recommendations for tackling outliers and relies on the user or a data scientist to conduct further analysis of the results. - As it only functions on numerical features, it cannot be used for categorical data. - For data that has undergone heavy pre-processing, was manipulated, or inherently possesses a high kurtosis (heavy tails), the pre-set threshold may not be optimal for outlier detection."
  },
  {
    "objectID": "tests/data_validation/Skewness.html",
    "href": "tests/data_validation/Skewness.html",
    "title": "Skewness",
    "section": "",
    "text": "Skewness\nEvaluates the skewness of numerical data in a machine learning model and checks if it falls below a set maximum threshold.\nPurpose: The purpose of the Skewness test is to measure the asymmetry in the distribution of data within a predictive machine learning model. Specifically, it evaluates the divergence of said distribution from a normal distribution. In understanding the level of skewness, we can potentially identify issues with data quality, an essential component for optimizing the performance of traditional machine learning models in both classification and regression settings.\nTest Mechanism: This test calculates skewness of numerical columns in a dataset, which is extracted from the DataFrame, specifically focusing on numerical data types. The skewness value is then contrasted against a predetermined maximum threshold, set by default to 1. The skewness value under review is deemed to have passed the test only if it is less than this maximum threshold; otherwise, the test is considered ‘fail’. Subsequently, the test results of each column, together with the skewness value and column name, are cached.\nSigns of High Risk:\n\nThe presence of substantial skewness levels that significantly exceed the maximum threshold is an indication of skewed data distribution and subsequently high model risk.\nPersistent skewness in data could signify that the foundational assumptions of the machine learning model may not be applicable, potentially leading to subpar model performance, erroneous predictions, or biased inferences.\n\nStrengths:\n\nFast and efficient identification of unequal data\ndistributions within a machine learning model is enabled by the skewness test.\nThe maximum threshold parameter can be adjusted to meet the user’s specific needs, enhancing the test’s versatility.\n\nLimitations:\n\nThe test only evaluates numeric columns, which means that data in non-numeric columns could still include bias or problematic skewness that this test does not capture.\nThe test inherently assumes that the data should follow a normal distribution, an expectation which may not always be met in real-world data.\nThe risk grading is largely dependent on a subjective threshold, which may result in excessive strictness or leniency depending upon selection. This factor might require expert input and recurrent iterations for refinement."
  },
  {
    "objectID": "tests/data_validation/AutoStationarity.html",
    "href": "tests/data_validation/AutoStationarity.html",
    "title": "AutoStationarity",
    "section": "",
    "text": "AutoStationarity\nAutomates Augmented Dickey-Fuller test to assess stationarity across multiple time series in a DataFrame.\nPurpose: The AutoStationarity metric is intended to automatically detect and evaluate the stationary nature of each time series in a DataFrame. It incorporates the Augmented Dickey-Fuller (ADF) test, a statistical approach used to assess stationarity. Stationarity is a fundamental property suggesting that statistic features like mean and variance remain unchanged over time. This is necessary for many time-series models.\nTest Mechanism: The mechanism for the AutoStationarity test involves applying the Augmented Dicky-Fuller test to each time series within the given dataframe to assess if they are stationary. Every series in the dataframe is looped, using the ADF test up to a defined maximum order (configurable and by default set to 5). The p-value resulting from the ADF test is compared against a predetermined threshold (also configurable and by default set to 0.05). The time series is deemed stationary at its current differencing order if the p-value is less than the threshold.\nSigns of High Risk: - A significant number of series not achieving stationarity even at the maximum order of differencing can indicate high risk or potential failure in the model. - This could suggest the series may not be appropriately modeled by a stationary process, hence other modeling approaches might be required.\nStrengths: - The key strength in this metric lies in the automation of the ADF test, enabling mass stationarity analysis across various time series and boosting the efficiency and credibility of the analysis. - The utilization of the ADF test, a widely accepted method for testing stationarity, lends authenticity to the results derived. - The introduction of the max order and threshold parameters give users the autonomy to determine their preferred levels of stringency in the tests.\nLimitations: - The Augumented Dicky-Fuller test and the stationarity test are not without their limitations. These tests are premised on the assumption that the series can be modeled by an autoregressive process, which may not always hold true. - The stationarity check is highly sensitive to the choice of threshold for the significance level; an extremely high or low threshold could lead to incorrect results regarding the stationarity properties. - There’s also a risk of over-differencing if the maximum order is set too high, which could induce unnecessary cycles."
  },
  {
    "objectID": "tests/data_validation/ScatterPlot.html",
    "href": "tests/data_validation/ScatterPlot.html",
    "title": "ScatterPlot",
    "section": "",
    "text": "ScatterPlot\nCreates a scatter plot matrix to visually analyze feature relationships, patterns, and outliers in a dataset.\nPurpose: The ScatterPlot metric is designed to offer a visual analysis of a given dataset by constructing a scatter plot matrix encapsulating all the dataset’s features (or columns). Its primary function lies in unearthing relationships, patterns, or outliers across different features, thus providing both quantitative and qualitative insights into the multidimensional relationships within the dataset. This visual assessment aids in understanding the efficacy of the chosen features for model training and their overall suitability.\nTest Mechanism: Using the seaborn library, the ScatterPlot class creates the scatter plot matrix. The process includes retrieving all columns from the dataset, verifying their existence, and subsequently generating a pairplot for these columns. A kernel density estimate (kde) is utilized to present a smoother, univariate distribution along the grid’s diagonal. The final plot is housed in an array of Figure objects, each wrapping a matplotlib figure instance for storage and future usage.\nSigns of High Risk: - The emergence of non-linear or random patterns across different feature pairs. This may suggest intricate relationships unfit for linear presumptions. - A lack of clear patterns or clusters which might point to weak or non-existent correlations among features, thus creating a problem for certain model types. - The occurrence of outliers as visual outliers in your data can adversely influence the model’s performance.\nStrengths: - It offers insight into the multidimensional relationships among multiple features. - It assists in identifying trends, correlations, and outliers which could potentially affect the model’s performance. - As a diagnostic tool, it can validate whether certain assumptions made during the model-creation process, such as linearity, hold true. - The tool’s versatility extends to its application for both regression and classification tasks.\nLimitations: - Scatter plot matrices may become cluttered and hard to decipher as the number of features escalates, resulting in complexity and confusion. - While extremely proficient in revealing pairwise relationships, these matrices may fail to illuminate complex interactions that involve three or more features. - These matrices are primarily visual tools, so the precision of quantitative analysis may be compromised. - If not clearly visible, outliers can be missed, which could negatively affect model performance. - It assumes that the dataset can fit into the computer’s memory, which might not always be valid particularly for extremely large datasets."
  },
  {
    "objectID": "tests/data_validation/WOEBinPlots.html",
    "href": "tests/data_validation/WOEBinPlots.html",
    "title": "WOEBinPlots",
    "section": "",
    "text": "WOEBinPlots\nGenerates visualizations of Weight of Evidence (WoE) and Information Value (IV) for understanding predictive power of categorical variables in a data set.\nPurpose: This test is designed to visualize the Weight of Evidence (WoE) and Information Value (IV) for categorical variables in a provided dataset. By showcasing the data distribution across different categories of each feature, it aids in understanding each variable’s predictive power in the context of a classification-based machine learning model. Commonly used in credit scoring models, WoE and IV are robust statistical methods for evaluating a variable’s predictive power.\nTest Mechanism: The test implementation follows defined steps. Initially, it selects non-numeric columns from the dataset and changes them to string type, paving the way for accurate binning. It then performs an automated WoE binning operation on these selected features, effectively categorizing the potential values of a variable into distinct bins. After the binning process, the function generates two separate visualizations (a scatter chart for WoE values and a bar chart for IV) for each variable. These visual presentations are formed according to the spread of each metric across various categories of each feature.\nSigns of High Risk: - Errors occurring during the binning process. - Challenges in converting non-numeric columns into string data type. - Misbalance in the distribution of WoE and IV, with certain bins overtaking others conspicuously. This could denote that the model is disproportionately dependent on certain variables or categories for predictions, an indication of potential risks to its robustness and generalizability.\nStrengths: - Provides a detailed visual representation of the relationship between feature categories and the target variable. This grants an intuitive understanding of each feature’s contribution to the model. - Allows for easy identification of features with high impact, facilitating feature selection and enhancing comprehension of the model’s decision logic. - WoE conversions are monotonic, upholding the rank ordering of the original data points, which simplifies analysis.\nLimitations: - The method is largely reliant on the binning process, and an inappropriate binning threshold or bin number choice might result in a misrepresentation of the variable’s distribution. - While excellent for categorical data, the encoding of continuous variables into categorical can sometimes lead to information loss. - Extreme or outlier values can dramatically affect the computation of WoE and IV, skewing results. - The method requires a sufficient number of events per bin to generate a reliable information value and weight of evidence."
  },
  {
    "objectID": "tests/data_validation/TimeSeriesHistogram.html",
    "href": "tests/data_validation/TimeSeriesHistogram.html",
    "title": "TimeSeriesHistogram",
    "section": "",
    "text": "TimeSeriesHistogram\nVisualizes distribution of time-series data using histograms and Kernel Density Estimation (KDE) lines.\nPurpose: The purpose of this metric is to perform a histogram analysis on time-series data. It primarily assesses the distribution of values within a dataset over a period of time, typically used for regression tasks. The types of data that this metric can be applicable to are diverse, ranging from internet traffic and stock prices to weather data. This analysis provides valuable insights into the probability distribution, skewness, and peakness (kurtosis) underlying the data.\nTest Mechanism: This test operates on a specific column within the dataset that is required to have a datetime type index. It goes through each column in the given dataset, creating a histogram with Seaborn’s histplot function. In cases where the dataset includes more than one time-series (i.e., more than one column with a datetime type index), a distinct histogram is plotted for each series. Additionally, a kernel density estimate (KDE) line is drawn for each histogram, providing a visualization of the data’s underlying probability distribution. The x and y-axis labels are purposely hidden to concentrate solely on the data distribution.\nSigns of High Risk: - The dataset lacks a column with a datetime type index. - The specified columns do not exist within the dataset. - The data distribution within the histogram demonstrates high degrees of skewness or kurtosis, which could bias the model. - Outliers that differ significantly from the primary data distribution are present.\nStrengths: - It serves as a visual diagnostic tool, offering an ideal starting point for understanding the overall behavior and distribution trends within the dataset. - It is effective for both single and multiple time-series data analysis. - The Kernel Density Estimation (KDE) line provides a smooth estimate of the overall trend in data distribution.\nLimitations: - The metric only presents a high-level view of data distribution and does not offer specific numeric measures such as skewness or kurtosis. - The histogram does not display precise data values; due to the data grouping into bins, some detail is inevitably lost, marking a trade-off between precision and general overview. - The histogram cannot handle non-numeric data columns. - The histogram’s shape may be sensitive to the number of bins used."
  },
  {
    "objectID": "tests/data_validation/SeasonalDecompose.html",
    "href": "tests/data_validation/SeasonalDecompose.html",
    "title": "SeasonalDecompose",
    "section": "",
    "text": "SeasonalDecompose\nDecomposes dataset features into observed, trend, seasonal, and residual components to identify patterns and validate dataset.\nPurpose: This test utilizes the Seasonal Decomposition of Time Series by Loess (STL) method to decompose a dataset into its fundamental components: observed, trend, seasonal, and residuals. The purpose is to identify implicit patterns, majorly any seasonality, in the dataset’s features which aid in developing a more comprehensive understanding and effectively validating the dataset.\nTest Mechanism: The testing process exploits the seasonal_decompose function from the statsmodels.tsa.seasonal library to evaluate each feature in the dataset. It isolates each feature into four components: observed, trend, seasonal, and residuals, and generates essentially six subplot graphs per feature for visual interpretation of the results. Prior to the seasonal decomposition, non-finite values are scrutinized and removed thus, ensuring reliability in the analysis.\nSigns of High Risk: - Non-Finiteness: If a dataset carries too many non-finite values it might flag high risk as these values are omitted before conducting the seasonal decomposition. - Frequent Warnings: The test could be at risk if it chronically fails to infer frequency for a scrutinized feature. - High Seasonality: A high seasonal component could potentially render forecasts unreliable due to overwhelming seasonal variation.\nStrengths: - Seasonality Detection: The code aptly discerns hidden seasonality patterns in the features of datasets. - Visualization: The test facilitates interpretation and comprehension via graphical representations. - Unrestricted Usage: The code is not confined to any specific regression model, thereby promoting wide-ranging applicability.\nLimitations: - Dependence on Assumptions: The test presumes that features in the dataset are periodically distributed. If no frequency could be inferred for a variable, that feature is excluded from the test. - Handling Non-finite Values: The test disregards non-finite values during the analysis which could potentially result in incomplete understanding of the dataset. - Unreliability with Noisy Datasets: The test tends to produce unreliable results when used with heavy noise present in the dataset."
  },
  {
    "objectID": "tests/data_validation/ClassImbalance.html",
    "href": "tests/data_validation/ClassImbalance.html",
    "title": "ClassImbalance",
    "section": "",
    "text": "ClassImbalance\nEvaluates and quantifies class distribution imbalance in a dataset used by a machine learning model.\nPurpose: The ClassImbalance test is designed to evaluate the distribution of target classes in a dataset that’s utilized by a machine learning model. Specifically, it aims to ensure that the classes aren’t overly skewed, which could lead to bias in the model’s predictions. It’s crucial to have a balanced training dataset to avoid creating a model that’s biased with high accuracy for the majority class and low accuracy for the minority class.\nTest Mechanism: This ClassImbalance test operates by calculating the frequency (expressed as a percentage) of each class in the target column of the dataset. It then checks whether each class appears in at least a set minimum percentage of the total records. This minimum percentage is a modifiable parameter, but the default value is set to 10%.\nSigns of High Risk:\n\nAny class that represents less than the pre-set minimum percentage threshold is marked as high risk, implying a potential class imbalance.\nThe function provides a pass/fail outcome for each class based on this criterion.\nFundamentally, if any class fails this test, it’s highly likely that the dataset possesses imbalanced class distribution.\n\nStrengths:\n\nThe test can spot under-represented classes that could affect the efficiency of a machine learning model.\nThe calculation is straightforward and swift.\nThe test is highly informative because it not only spots imbalance, but it also quantifies the degree of imbalance.\nThe adjustable threshold enables flexibility and adaptation to differing use-cases or domain-specific needs.\nThe test creates a visually insightful plot showing the classes and their corresponding proportions, enhancing interpretability and comprehension of the data.\n\nLimitations:\n\nThe test might struggle to perform well or provide vital insights for datasets with a high number of classes. In such cases, the imbalance could be inevitable due to the inherent class distribution.\nSensitivity to the threshold value might result in faulty detection of imbalance if the threshold is set excessively high.\nRegardless of the percentage threshold, it doesn’t account for varying costs or impacts of misclassifying different classes, which might fluctuate based on specific applications or domains.\nWhile it can identify imbalances in class distribution, it doesn’t provide direct methods to address or correct these imbalances.\nThe test is only applicable for classification opearations and unsuitable for regression or clustering tasks."
  },
  {
    "objectID": "tests/data_validation/AutoAR.html",
    "href": "tests/data_validation/AutoAR.html",
    "title": "AutoAR",
    "section": "",
    "text": "AutoAR\nAutomatically identifies the optimal Autoregressive (AR) order for a time series using BIC and AIC criteria.\nPurpose:\nThe AutoAR test is intended to automatically identify the Autoregressive (AR) order of a time series by utilizing the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC). AR order is crucial in forecasting tasks as it dictates the quantity of prior terms in the sequence to use for predicting the current term. The objective is to select the most fitting AR model that encapsulates the trend and seasonality in the time series data.\nTest Mechanism:\nThe test mechanism operates by iterating through a possible range of AR orders up to a defined maximum. An AR model is fitted for each order, and the corresponding BIC and AIC are computed. BIC and AIC statistical measures are designed to penalize models for complexity, preferring simpler models that fit the data proficiently. To verify the stationarity of the time series, the Augmented Dickey-Fuller test is executed. The AR order, BIC, and AIC findings, are compiled into a dataframe for effortless comparison. Then, the AR order with the smallest BIC is established as the desirable order for each variable.\nSigns of High Risk:\n\nAn augmented Dickey Fuller test p-value &gt; 0.05, indicating the time series isn’t stationary, may lead to inaccurate results.\nProblems with the model fitting procedure, such as computational or convergence issues.\nContinuous selection of the maximum specified AR order may suggest insufficient set limit.\n\nStrengths:\n\nThe test independently pinpoints the optimal AR order, thereby reducing potential human bias.\nIt strikes a balance between model simplicity and goodness-of-fit to avoid overfitting.\nHas the capability to account for stationarity in a time series, an essential aspect for dependable AR modelling.\nThe results are aggregated into an comprehensive table, enabling an easy interpretation.\n\nLimitations:\n\nThe tests need a stationary time series input.\nThey presume a linear relationship between the series and its lags.\nThe search for the best model is constrained by the maximum AR order supplied in the parameters. Therefore, a low max_ar_order could result in subpar outcomes.\nAIC and BIC may not always agree on the selection of the best model. This potentially requires the user to juggle interpretational choices."
  },
  {
    "objectID": "tests/data_validation/SpreadPlot.html",
    "href": "tests/data_validation/SpreadPlot.html",
    "title": "SpreadPlot",
    "section": "",
    "text": "SpreadPlot\nVisualizes the spread relationship between pairs of time-series variables in a dataset, thereby aiding in identification of potential correlations.\nPurpose: The SpreadPlot metric is intended to graphically illustrate and analyse the relationships between pairs of time series variables within a given dataset. This facilitated understanding helps in identifying and assessing potential time series correlations, like cointegration, between the variables.\nTest Mechanism: The SpreadPlot metric operates by computing and representing the spread between each pair of time series variables in the dataset. In particular, the difference between two variables is calculated and presented as a line graph. This method is iterated for each unique pair of variables in the dataset.\nSigns of High Risk: Potential indicators of high risk related to the SpreadPlot metric might include:\n\nLarge fluctuations in the spread over a given timespan\nUnexpected patterns or trends that may signal a potential risk in the underlying correlations between the variables\nPresence of significant missing data or extreme outlier values, which could potentially skew the spread and indicate high risk\n\nStrengths: The SpreadPlot metric provides several key advantages:\n\nIt allows for thorough visual examination and interpretation of the correlations between time-series pairs\nIt aids in revealing complex relationships like cointegration\nIt enhances interpretability by visualising the relationships, thereby helping in spotting outliers and trends\nIt is capable of handling numerous variable pairs from the dataset through a versatile and adaptable process\n\nLimitations: Despite its advantages, the SpreadPlot metric does have certain drawbacks:\n\nIt primarily serves as a visualisation tool and does not offer quantitative measurements or statistics to objectively determine relationships\nIt heavily relies on the quality and granularity of the data - missing data or outliers can notably disturb the interpretation of the relationships\nIt can become inefficient or difficult to interpret with a high number of variables due to the profuse number of plots\nIt might not completely capture intricate non-linear relationships between the variables"
  },
  {
    "objectID": "tests/data_validation/ACFandPACFPlot.html",
    "href": "tests/data_validation/ACFandPACFPlot.html",
    "title": "ACFandPACFPlot",
    "section": "",
    "text": "ACFandPACFPlot\nAnalyzes time series data using Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to reveal trends and correlations.\nPurpose: The ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plot test is employed to analyze time series data in machine learning models. It illuminates the correlation of the data over time by plotting the correlation of the series with its own lags (ACF), and the correlations after removing effects already accounted for by earlier lags (PACF). This information can identify trends, such as seasonality, degrees of autocorrelation, and inform the selection of order parameters for AutoRegressive Integrated Moving Average (ARIMA) models.\nTest Mechanism: The ACFandPACFPlot test accepts a dataset with a time-based index. It first confirms the index is of a datetime type, then handles any NaN values. The test subsequently generates ACF and PACF plots for each column in the dataset, producing a subplot for each. If the dataset doesn’t include key columns, an error is returned.\nSigns of High Risk:\n\nSudden drops in the correlation at a specific lag might signal a model at high risk.\nConsistent high correlation across multiple lags could also indicate non-stationarity in the data, which may suggest that a model estimated on this data won’t generalize well to future, unknown data.\n\nStrengths:\n\nACF and PACF plots offer clear graphical representations of the correlations in time series data.\nThese plots are effective at revealing important data characteristics such as seasonality, trends, and correlation patterns.\nThe insights from these plots aid in better model configuration, particularly in the selection of ARIMA model parameters.\n\nLimitations:\n\nACF and PACF plots are exclusively for time series data and hence, can’t be applied to all ML models.\nThese plots require large, consistent datasets as gaps could lead to misleading results.\nThe plots can only represent linear correlations and fail to capture any non-linear relationships within the data.\nThe plots might be difficult for non-experts to interpret and should not replace more advanced analyses."
  },
  {
    "objectID": "tests/data_validation/WOEBinTable.html",
    "href": "tests/data_validation/WOEBinTable.html",
    "title": "WOEBinTable",
    "section": "",
    "text": "WOEBinTable\nCalculates and assesses the Weight of Evidence (WoE) and Information Value (IV) of each feature in a ML model.\nPurpose: The Weight of Evidence (WoE) and Information Value (IV) test is intended to evaluate the predictive power of each feature in the machine learning model. The test generates binned groups of values from each feature in a dataset, computes the WoE value and the IV value for each bin. These values provide insights on the relationship between each feature and the target variable and their contribution towards the predictive output of the model.\nTest Mechanism: The metric leverages the scorecardpy.woebin method to perform WoE-based automatic binning on the dataset. Depending on the parameter breaks_adj, the method adjusts the cut-off points for binning numeric variables. The bins are then used to calculate the WoE and IV. The metric requires a dataset with the target variable defined. The metric outputs a dataframe that comprises the bin boundaries, WoE, and IV values for each feature.\nSigns of High Risk: - High IV values, which denote variables with too much predictive power which might lead to overfitting - Errors during the binning process, which might be due to inappropriate data types or poorly defined bins\nStrengths: - The WoE and IV test is highly effective for feature selection in binary classification problems, as it quantifies how much predictive information is packed within each feature regarding the binary outcome - The WoE transformation creates a monotonic relationship between the target and independent variables\nLimitations: - Mainly designed for binary classification tasks, therefore it might not be applicable or reliable for multi-class classification or regression tasks - If the dataset has many features or the features are not binnable or they are non-numeric, this process might encounter difficulties - This metric doesn’t help in identifying if the predictive factor being observed is a coincidence or a real phenomenon due to data randomness"
  },
  {
    "objectID": "tests/data_validation/TimeSeriesFrequency.html",
    "href": "tests/data_validation/TimeSeriesFrequency.html",
    "title": "TimeSeriesFrequency",
    "section": "",
    "text": "TimeSeriesFrequency\nEvaluates consistency of time series data frequency and generates a frequency plot.\nPurpose: The purpose of the TimeSeriesFrequency test is to evaluate the consistency in the frequency of data points in a time-series dataset. This test inspects the intervals or duration between each data point to determine if a fixed pattern (such as daily, weekly, or monthly) exists. The identification of such patterns is crucial to time-series analysis as any irregularities could lead to erroneous results and hinder the model’s capacity for identifying trends and patterns.\nTest Mechanism: Initially, the test checks if the dataframe index is in datetime format. Subsequently, it utilizes pandas’ infer_freq method to identify the frequency of each data series within the dataframe. The infer_freq method attempts to establish the frequency of a time series and returns both the frequency string and a dictionary relating these strings to their respective labels. The test compares the frequencies of all datasets. If they share a common frequency, the test passes, but it fails if they do not. Additionally, Plotly is used to create a frequency plot, offering a visual depiction of the time differences between consecutive entries in the dataframe index.\nSigns of High Risk: - The test fails, indicating multiple unique frequencies within the dataset. This failure could suggest irregular intervals between observations, potentially interrupting pattern recognition or trend analysis. - The presence of missing or null frequencies could be an indication of inconsistencies in data or gaps within the data collection process.\nStrengths: - This test uses a systematic approach to checking the consistency of data frequency within a time-series dataset. - It increases the model’s reliability by asserting the consistency of observations over time, an essential factor in time-series analysis. - The test generates a visual plot, providing an intuitive representation of the dataset’s frequency distribution, which caters to visual learners and aids in interpretation and explanation.\nLimitations: - This test is only applicable to time-series datasets and hence not suitable for other types of datasets. - The infer_freq method might not always correctly infer frequency when faced with missing or irregular data points. - Depending on context or the model under development, mixed frequencies might sometimes be acceptable, but this test considers them a failing condition."
  },
  {
    "objectID": "tests/data_validation/PiTCreditScoresHistogram.html",
    "href": "tests/data_validation/PiTCreditScoresHistogram.html",
    "title": "PiTCreditScoresHistogram",
    "section": "",
    "text": "PiTCreditScoresHistogram\nGenerates a histogram visualization for observed and predicted credit default scores.\nPurpose: The PiT (Point in Time) Credit Scores Histogram metric is used to evaluate the predictive performance of a credit risk assessment model. This metric provides a visual representation of observed versus predicted default scores and enables quick and intuitive comparison for model assessment.\nTest Mechanism: This metric generates histograms for both observed and predicted score distributions of defaults and non-defaults. The simultaneous representation of both the observed and predicted scores sheds light on the model’s ability to accurately predict credit risk.\nSigns of High Risk: - Significant discrepancies between the observed and predicted histograms, suggesting that the model may not be adequately addressing certain risk factors. - Concentration of predicted defaults towards one end of the graph, or uneven distribution in comparison to observed scores, indicating potential issues in the model’s interpretation of the data or outcome prediction.\nStrengths: - Provides an intuitive visual representation of model performance that’s easy to comprehend, even for individuals without a technical background. - Useful for understanding the model’s ability to distinguish between defaulting and non-defaulting entities. - Specifically tailored for assessing credit risk models. The Point in Time (PiT) factor considers the evolution of credit risk over time.\nLimitations: - As the information is visual, precise and quantitative results for detailed statistical analyses may not be obtained. - The method relies on manual inspection and comparison, introducing subjectivity and potential bias. - Subtle discrepancies might go unnoticed and it could be less reliable for identifying such cues. - Performance may degrade when score distributions overlap significantly or when too many scores are plotted, resulting in cluttered or hard-to-decipher graphs."
  },
  {
    "objectID": "tests/data_validation/EngleGrangerCoint.html",
    "href": "tests/data_validation/EngleGrangerCoint.html",
    "title": "EngleGrangerCoint",
    "section": "",
    "text": "EngleGrangerCoint\nValidates co-integration in pairs of time series data using the Engle-Granger test and classifies them as Cointegrated’ or ‘Not cointegrated’.\nPurpose: The intent of this Engle-Granger cointegration test is to explore and quantify the degree of co-movement between pairs of time series variables in a dataset. This is particularly useful in enhancing the accuracy of predictive regressions whenever the underlying variables are co-integrated, i.e., they move together over time.\nTest Mechanism: The test first drops any non-applicable values from the input dataset and then iterates over each pair of variables to apply the Engle-Granger cointegration test. The test generates a ‘p’ value, which is then compared against a pre-specified threshold (0.05 by default). The pair is labeled as ‘Cointegrated’ if the ‘p value is less than or equal to the threshold or ’Not cointegrated’ otherwise. A summary table is returned by the metric showing cointegration results for each variable pair.\nSigns of High Risk: - A high risk might be indicated if a significant number of variables that were hypothesized to be cointegrated do not pass the test. - Another sign of high risk is if a considerable number of ‘p’ values are close to the threshold. This is a risk because minor fluctuations in the data can switch the decision between ‘Cointegrated’ and ‘Not cointegrated’.\nStrengths: - The Engle-Granger cointegration test provides an effective way to analyze relationships between time series, particularly in contexts where it’s essential to check if variables are moving together in a statistically significant manner. - It is useful in various domains, especially finance or economics. Here, predictive models often hinge on understanding how different variables move together over time.\nLimitations: - The Engle-Granger cointegration test assumes that the time series are integrated of the same order, which isn’t always true in multivariate time series datasets. - The presence of non-stationary characteristics in the series or structural breaks can result in falsely positive or negative cointegration results. - The test may not perform well for small sample sizes due to lack of statistical power. Therefore, it should be used with caution, and whenever possible, supplemented with other predictive indicators for a more robust model evaluation."
  },
  {
    "objectID": "tests/data_validation/TimeSeriesLinePlot.html",
    "href": "tests/data_validation/TimeSeriesLinePlot.html",
    "title": "TimeSeriesLinePlot",
    "section": "",
    "text": "TimeSeriesLinePlot\nGenerates and analyses time-series data through line plots revealing trends, patterns, anomalies over time.\nPurpose: The TimeSeriesLinePlot metric is designed to generate and analyze time series data through the creation of line plots. This assists in the initial inspection of the data by providing a visual representation of patterns, trends, seasonality, irregularity, and anomalies that may be present in the dataset over a period of time.\nTest Mechanism: The mechanism for this Python class involves extracting the column names from the provided dataset and subsequently generates line plots for each column using the Plotly Python library. For every column in the dataset, a time-series line plot is created where the values are plotted against the dataset’s datetime index. It is important to note that indexes that are not of datetime type will result in a ValueError.\nSigns of High Risk: - Presence of time-series data that does not have datetime indices. - Provided columns do not exist in the provided dataset. - The detection of anomalous patterns or irregularities in the time-series plots, indicating potential high model instability or probable predictive error.\nStrengths: - The visual representation of complex time series data, which simplifies understanding and helps in recognizing temporal trends, patterns, and anomalies. - The adaptability of the metric, which allows it to effectively work with multiple time series within the same dataset. - Enables the identification of anomalies and irregular patterns through visual inspection, assisting in spotting potential data or model performance problems.\nLimitations: - The effectiveness of the metric is heavily reliant on the quality and patterns of the provided time series data. - Exclusively a visual tool, it lacks the capability to provide quantitative measurements, making it less effective for comparing and ranking multiple models or when specific numerical diagnostics are needed. - The metric necessitates that the time-specific data has been transformed into a datetime index, with the data formatted correctly. - The metric has an inherent limitation in that it cannot extract deeper statistical insights from the time series data, which can limit its efficacy with complex data structures and phenomena."
  },
  {
    "objectID": "tests/model_validation/statsmodels/LJungBox.html",
    "href": "tests/model_validation/statsmodels/LJungBox.html",
    "title": "LJungBox",
    "section": "",
    "text": "LJungBox\nAssesses autocorrelations in dataset features by performing a Ljung-Box test on each feature.\nPurpose: The Ljung-Box test is a type of statistical test utilized to ascertain whether there are autocorrelations within a given dataset that differ significantly from zero. In the context of a machine learning model, this test is primarily used to evaluate data utilized in regression tasks, especially those involving time series and forecasting.\nTest Mechanism: The test operates by iterating over each feature within the training dataset and applying the acorr_ljungbox function from the statsmodels.stats.diagnostic library. This function calculates the Ljung-Box statistic and p-value for each feature. These results are then stored in a dictionary where the keys are the feature names and the values are dictionaries containing the statistic and p-value respectively. Generally, a lower p-value indicates a higher likelihood of significant autocorrelations within the feature.\nSigns of High Risk: - A high risk or failure in the model’s performance relating to this test might be indicated by high Ljung-Box statistic values or low p-values. - These outcomes suggest the presence of significant autocorrelations in the respective features. If not properly consider or handle in the machine learning model, these can negatively affect model performance or bias.\nStrengths: - The Ljung-Box test is a powerful tool for detecting autocorrelations within datasets, especially in time series data. - It provides quantitative measures (statistic and p-value) that allow for precise evaluation of autocorrelation. - This test can be instrumental in avoiding issues related to autoregressive residuals and other challenges in regression models.\nLimitations: - The Ljung-Box test cannot detect all types of non-linearity or complex interrelationships among variables. - Testing individual features may not fully encapsulate the dynamics of the data if features interact with each other. - It is designed more for traditional statistical models and may not be fully compatible with certain types of complex machine learning models."
  },
  {
    "objectID": "tests/model_validation/statsmodels/ShapiroWilk.html",
    "href": "tests/model_validation/statsmodels/ShapiroWilk.html",
    "title": "ShapiroWilk",
    "section": "",
    "text": "ShapiroWilk\nEvaluates feature-wise normality of training data using the Shapiro-Wilk test.\nPurpose: The Shapiro-Wilk test is utilized to investigate whether a particular dataset conforms to the standard normal distribution. This analysis is crucial in machine learning modeling because the normality of the data can profoundly impact the performance of the model. This metric is especially useful in evaluating various features of the dataset in both classification and regression tasks.\nTest Mechanism: The Shapiro-Wilk test is conducted on each feature column of the training dataset to determine if the data contained fall within the normal distribution. The test presents a statistic and a p-value, with the p-value serving to validate or repudiate the null hypothesis, which is that the tested data is normally distributed.\nSigns of High Risk: - A p-value that falls below 0.05 signifies a high risk as it discards the null hypothesis, indicating that the data does not adhere to the normal distribution. - For machine learning models built on the presumption of data normality, such an outcome could result in subpar performance or incorrect predictions.\nStrengths: - The Shapiro-Wilk test is esteemed for its level of accuracy, thereby making it particularly well-suited to datasets of small to moderate sizes. - It proves its versatility through its efficient functioning in both classification and regression tasks. - By separately testing each feature column, the Shapiro-Wilk test can raise an alarm if a specific feature does not comply with the normality.\nLimitations: - The Shapiro-Wilk test’s sensitivity can be a disadvantage as it often rejects the null hypothesis (i.e., data is normally distributed), even for minor deviations, especially in large datasets. This may lead to unwarranted ‘false alarms’ of high risk by deeming the data as not normally distributed even if it approximates normal distribution. - Exceptional care must be taken in managing missing data or outliers prior to testing as these can greatly skew the results. - Lastly, the Shapiro-Wilk test is not optimally suited for processing data with pronounced skewness or kurtosis."
  },
  {
    "objectID": "tests/model_validation/statsmodels/ResidualsVisualInspection.html",
    "href": "tests/model_validation/statsmodels/ResidualsVisualInspection.html",
    "title": "ResidualsVisualInspection",
    "section": "",
    "text": "ResidualsVisualInspection\nProvides a comprehensive visual analysis of residuals for regression models utilizing various plot types.\nPurpose: The main purpose of this metric is to visualize and analyze the residuals (the differences between the observed and predicted values) of a regression problem. It allows for a graphical exploration of the model’s errors, helping to identify statistical patterns or anomalies that may indicate a systematic bias in the model’s predictions. By inspecting the residuals, we can check how well the model fits the data and meets the assumptions of the model.\nTest Mechanism: The metric generates four common types of residual plots which are: a histogram with kernel density estimation, a quantile-quantile (Q-Q) plot, a residuals series dot plot, and an autocorrelation function (ACF) plot.\n\nThe residuals histogram with kernel density estimation visualizes the distribution of residuals and allows to check if they are normally distributed.\nQ-Q plot compares the observed quantiles of the data to the quantiles of a standard normal distribution, helping to assess the normality of residuals.\nA residuals dot plot indicates the variation in residuals over time, which helps in identifying any time-related pattern in residuals.\nACF plot visualizes the correlation of an observation with its previous observations, helping to pinpoint any seasonality effect within residuals.\n\nSigns of High Risk:\n\nSkewness or asymmetry in the histogram or a significant deviation from the straight line in the Q-Q plot, which indicates that the residuals aren’t normally distributed.\nLarge spikes in the ACF plot, indicating that the residuals are correlated, in violation of the assumption that they are independent.\nNon-random patterns in the dot plot of residuals, indicating potential model misspecification.\n\nStrengths:\n\nVisual analysis of residuals is a powerful yet simple way to understand a model’s behavior across the data set and to identify problems with the model’s assumptions or its fit to the data.\nThe test is applicable to any regression model, irrespective of complexity.\nBy exploring residuals, we might uncover relationships that were not captured by the model, revealing opportunities for model improvement.\n\nLimitations:\n\nVisual tests are largely subjective and can be open to interpretation. Clear-cut decisions about the model based solely on these plots may not be possible.\nThe metrics from the test do not directly infer the action based on the results; domain-specific knowledge and expert judgement is often required to interpret the results.\nThese plots can indicate a problem with the model but they do not necessarily reveal the nature or cause of the problem.\nThe test assumes that the error terms are identically distributed, which might not always be the case in real-world scenarios."
  },
  {
    "objectID": "tests/model_validation/statsmodels/BoxPierce.html",
    "href": "tests/model_validation/statsmodels/BoxPierce.html",
    "title": "BoxPierce",
    "section": "",
    "text": "BoxPierce\nDetects autocorrelation in time-series data through the Box-Pierce test to validate model performance.\nPurpose: The Box-Pierce test is utilized to detect the presence of autocorrelation in a time-series dataset. Autocorrelation, or serial correlation, refers to the degree of similarity between observations based on the temporal spacing between them. This test is essential for affirming the quality of a time-series model by ensuring that the error terms in the model are random and do not adhere to a specific pattern.\nTest Mechanism: The implementation of the Box-Pierce test involves calculating a test statistic along with a corresponding p-value derived from the dataset features. These quantities are used to test the null hypothesis that posits the data to be independently distributed. This is achieved by iterating over every feature column in the time-series data and applying the acorr_ljungbox function of the statsmodels library. The function yields the Box-Pierce test statistic as well as the respective p-value, all of which are cached as test results.\nSigns of High Risk:\n\nA low p-value, typically under 0.05 as per statistical convention, throws the null hypothesis of independence into question. This implies that the dataset potentially houses autocorrelations, thus indicating a high-risk scenario concerning model performance.\nLarge Box-Pierce test statistic values may indicate the presence of autocorrelation.\n\nStrengths:\n\nDetects patterns in data that are supposed to be random, thereby ensuring no underlying autocorrelation.\nCan be computed efficiently given its low computational complexity.\nCan be widely applied to most regression problems, making it very versatile.\n\nLimitations:\n\nAssumes homoscedasticity (constant variance) and normality of residuals, which may not always be the case in real-world datasets.\nMay exhibit reduced power for detecting complex autocorrelation schemes such as higher-order or negative correlations.\nIt only provides a general indication of the existence of autocorrelation, without providing specific insights into the nature or patterns of the detected autocorrelation.\nIn the presence of exhibits trends or seasonal patterns, the Box-Pierce test may yield misleading results.\nApplicability is limited to time-series data, which limits its overall utility."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelSensitivityPlot.html",
    "href": "tests/model_validation/statsmodels/RegressionModelSensitivityPlot.html",
    "title": "RegressionModelSensitivityPlot",
    "section": "",
    "text": "RegressionModelSensitivityPlot\nTests the sensitivity of a regression model to variations in independent variables by applying shocks and visualizing the effects.\nPurpose: The Regression Sensitivity Plot metric is designed to perform sensitivity analysis on regression models. This metric aims to measure the impact of slight changes (shocks) applied to individual variables on the system’s outcome while keeping all other variables constant. By doing so, it analyzes the effects of each independent variable on the dependent variable within the regression model and helps identify significant risk factors that could substantially influence the model’s output.\nTest Mechanism: This metric operates by initially applying shocks of varying magnitudes, defined by specific parameters, to each of the model’s features, one at a time. With all other variables held constant, a new prediction is made for each dataset subjected to shocks. Any changes in the model’s predictions are directly attributed to the shocks applied. In the event that the transformation parameter is set to “integrate”, initial predictions and target values undergo transformation via an integration function before being plotted. Lastly, a plot demonstrating observed values against predicted values for each model is generated, showcasing a distinct line graph illustrating predictions for each shock.\nSigns of High Risk: - If the plot exhibits drastic alterations in model predictions consequent to minor shocks to an individual variable, it may indicate high risk. This underscores potentially high model sensitivity to changes in that variable, suggesting over-dependence on that variable for predictions. - Unusually high or unpredictable shifts in response to shocks may also denote potential model instability or overfitting.\nStrengths: - The metric allows identification of variables strongly influencing the model outcomes, paving the way for understanding feature importance. - It generates visual plots which make the results easily interpretable even to non-technical stakeholders. - Beneficial in identifying overfitting and detecting unstable models that over-react to minor changes in variables.\nLimitations: - The metric operates on the assumption that all other variables remain unchanged during the application of a shock. However, real-world situations where variables may possess intricate interdependencies may not always reflect this. - It is best compatible with linear models and may not effectively evaluate the sensitivity of non-linear model configurations. - The metric does not provide a numerical risk measure. It offers only a visual representation, which may invite subjectivity in interpretation."
  },
  {
    "objectID": "tests/model_validation/statsmodels/ZivotAndrewsArch.html",
    "href": "tests/model_validation/statsmodels/ZivotAndrewsArch.html",
    "title": "ZivotAndrewsArch",
    "section": "",
    "text": "ZivotAndrewsArch\nEvaluates the order of integration and stationarity of time series data using Zivot-Andrews unit root test.\nPurpose: The Zivot-Andrews Arch metric is used to evaluate the order of integration for a time series data in a machine learning model. It’s designed to test for stationarity, a crucial aspect in time series analysis where data points are not dependent on time. Stationarity means that the statistical properties such as mean, variance and autocorrelation are all constant over time.\nTest Mechanism: The Zivot-Andrews unit root test is performed on each feature in the dataset using the ZivotAndrews function from the arch.unitroot module. This function returns the Zivot-Andrews metric for each feature, which includes the statistical value, p-value (probability value), the number of used lags, and the number of observations. The p-value is later used to decide on the null hypothesis (the time series has a unit root and is non-stationary) based on a chosen level of significance.\nSigns of High Risk: - A high p-value can suggest high risk. This might indicate that there’s insufficient evidence to reject the null hypothesis, which would mean the time series has a unit root and is therefore non-stationary. - Non-stationary time series data can lead to misleading statistics and unreliable machine learning models.\nStrengths: - The Zivot-Andrews Arch metric dynamically tests for stationarity against structural breaks in time series data, offering robust evaluation of stationarity in features. - This metric is especially beneficial with financial, economic, or other time-series data where data observations lack a consistent pattern and structural breaks may occur.\nLimitations: - The Zivot-Andrews Arch metric assumes that data is derived from a single-equation, autoregressive model. It may, therefore, not be appropriate for multivariate time series data or data which does not align with the autoregressive model assumption. - It might not take into account unexpected shocks or changes in the series trend which can both have a significant impact on the stationarity of the data."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelsCoeffs.html",
    "href": "tests/model_validation/statsmodels/RegressionModelsCoeffs.html",
    "title": "RegressionModelsCoeffs",
    "section": "",
    "text": "RegressionModelsCoeffs\nCompares feature importance by evaluating and contrasting coefficients of different regression models.\nPurpose: The ‘RegressionModelsCoeffs’ metric is utilized to evaluate and compare coefficients of different regression models trained on the same dataset. By examining how each model weighted the importance of different features during training, this metric provides key insights into which factors have the most impact on the model’s predictions and how these patterns differ across models.\nTest Mechanism: The test operates by extracting the coefficients of each regression model using the regression_coefficients()’ method. These coefficients are then consolidated into a dataframe, with each row representing a model and columns corresponding to each feature’s coefficient. It must be noted that this test is exclusive to ‘statsmodels’ and ‘R’ models, other models will result in a ‘SkipTestError’.\nSigns of High Risk: - Discrepancies in how different models weight the same features - Unexpectedly high or low coefficients - The test is inapplicable to certain models because they are not from ‘statsmodels’ or ‘R’ libraries\nStrengths: - Enables insight into the training process of different models - Allows comparison of feature importance across models - Through the review of feature coefficients, the test provides a more transparent evaluation of the model and highlights significant weights and biases in the training procedure\nLimitations: - The test is only compatible with ‘statsmodels’ and ‘R’ regression models - While the test provides contrast in feature weightings among models, it does not establish the most appropriate or accurate weighting, thus remaining subject to interpretation - It does not account for potential overfitting or underfitting of models - The computed coefficients might not lead to effective performance on unseen data"
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelForecastPlotLevels.html",
    "href": "tests/model_validation/statsmodels/RegressionModelForecastPlotLevels.html",
    "title": "RegressionModelForecastPlotLevels",
    "section": "",
    "text": "RegressionModelForecastPlotLevels\nCompares and visualizes forecasted and actual values of regression models on both raw and transformed datasets.\nPurpose: The RegressionModelForecastPlotLevels metric is designed to visually assess a series of regression models performance. It achieves this by contrasting the models’ forecasts with the observed data from the respective training and test datasets. The gauge of accuracy here involves determining the extent of closeness between forecasted and actual values. Accordingly, if any transformations are specified, the metric will handle transforming the data before making this comparison.\nTest Mechanism: The RegressionModelForecastPlotLevels class in Python initiates with a transformation parameter, which default aggregates to None. Initially, the class checks for the presence of model objects and raises a ValueError if none are found. Each model is then processed, creating predictive forecasts for both its training and testing datasets. These forecasts are then contrasted with the actual values and plotted. In situations where a specified transformation, like “integrate,” is specified, the class navigates the transformation steps (performing cumulative sums to generate a novel series, for instance). Finally, plots are produced that compare observed and forecasted values for both the raw and transformed datasets.\nSigns of High Risk: Indications of high risk or failure in the model’s performance can be derived from checking the generated plots. When the forecasted values dramatically deviate from the observed values in either the training or testing datasets, it suggests a high risk situation. A significant deviation could be a symptom of either overfitting or underfitting, both scenarios are worrying. Such discrepancies could inhibit the model’s ability to create precise, generalized results.\nStrengths:\n\nVisual Evaluations: The metric provides a visual and comparative way of assessing multiple regression models at once. This allows easier interpretation and evaluation of their forecasting accuracy.\nTransformation Handling: This metric can handle transformations like “integrate,” enhancing its breadth and flexibility in evaluating different models.\nDetailed Perspective: By looking at the performance on both datasets (training and testing), the metric may give a detailed overview of the model.\n\nLimitations:\n\nSubjectivity: Relying heavily on visual interpretations; assessments may differ from person to person.\nLimited Transformation Capability: Currently, only the “integrate” transformation is supported, implying complex transformations might go unchecked or unhandled.\nOverhead: The plotting mechanism may become computationally costly when applying to extensive datasets, increasing runtime.\nNumerical Measurement: Although visualization is instrumental, a corresponding numerical measure would further reinforce the observations. However, this metric does not provide numerical measures."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionPermutationFeatureImportance.html",
    "href": "tests/model_validation/statsmodels/RegressionPermutationFeatureImportance.html",
    "title": "RegressionPermutationFeatureImportance",
    "section": "",
    "text": "RegressionPermutationFeatureImportance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature values are randomly rearranged. Specifically designed for use with statsmodels, this metric offers insight into the importance of features based on the decrease in model’s predictive accuracy, typically R².\nPurpose: The primary purpose of this metric is to determine which features significantly impact the performance of a regression model developed using statsmodels. The metric measures how much the prediction accuracy deteriorates when each feature’s values are permuted.\nTest Mechanism: This metric shuffles the values of each feature one at a time in the dataset, computes the model’s performance after each permutation, and compares it to the baseline performance. A significant decrease in performance indicates the importance of the feature.\nSigns of High Risk: - Significant reliance on a feature that when permuted leads to a substantial decrease in performance, suggesting overfitting or high model dependency on that feature. - Features identified as unimportant despite known impacts from domain knowledge, suggesting potential issues in model training or data preprocessing.\nStrengths: - Directly assesses the impact of each feature on model performance, providing clear insights into model dependencies. - Model-agnostic within the scope of statsmodels, applicable to any regression model that outputs predictions.\nLimitations: - The metric is specific to statsmodels and cannot be used with other types of models without adaptation. - It does not capture interactions between features, which can lead to underestimating the importance of correlated features. - Assumes independence of features when calculating importance, which might not always hold true."
  },
  {
    "objectID": "tests/model_validation/statsmodels/GINITable.html",
    "href": "tests/model_validation/statsmodels/GINITable.html",
    "title": "GINITable",
    "section": "",
    "text": "GINITable\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets.\nPurpose: The ‘GINITable’ metric is designed to evaluate the performance of a classification model by emphasizing its discriminatory power. Specifically, it calculates and presents three important metrics - the Area under the ROC Curve (AUC), the GINI coefficient, and the Kolmogov-Smirnov (KS) statistic - for both training and test datasets.\nTest Mechanism: Using a dictionary for storing performance metrics for both the training and test datasets, the GINITable’ metric calculates each of these metrics sequentially. The Area under the ROC Curve (AUC) is calculated via the roc_auc_score function from the Scikit-Learn library. The GINI coefficient, a measure of statistical dispersion, is then computed by doubling the AUC and subtracting 1. Finally, the Kolmogov-Smirnov (KS) statistic is calculated via the roc_curve function from Scikit-Learn, with the False Positive Rate (FPR) subtracted from the True Positive Rate (TPR) and the maximum value taken from the resulting data. These metrics are then stored in a pandas DataFrame for convenient visualization.\nSigns of High Risk: - Low values for performance metrics may suggest a reduction in model performance, particularly a low AUC which indicates poor classification performance, or a low GINI coefficient, which could suggest a decreased ability to discriminate different classes. - A high KS value may be an indicator of potential overfitting, as this generally signifies a substantial divergence between positive and negative distributions. - Significant discrepancies between the performance on the training dataset and the test dataset may present another signal of high risk.\nStrengths: - Offers three key performance metrics (AUC, GINI, and KS) in one test, providing a more comprehensive evaluation of the model. - Provides a direct comparison between the model’s performance on training and testing datasets, which aids in identifying potential underfitting or overfitting. - The applied metrics are class-distribution invariant, thereby remaining effective for evaluating model performance even when dealing with imbalanced datasets. - Presents the metrics in a user-friendly table format for easy comprehension and analysis.\nLimitations: - The GINI coefficient and KS statistic are both dependent on the AUC value. Therefore, any errors in the calculation of the latter will adversely impact the former metrics too. - Mainly suited for binary classification models and may require modifications for effective application in multi-class scenarios. - The metrics used are threshold-dependent and may exhibit high variability based on the chosen cut-off points. - The test does not incorporate a method to efficiently handle missing or inefficiently processed data, which could lead to inaccuracies in the metrics if the data is not appropriately preprocessed."
  },
  {
    "objectID": "tests/model_validation/statsmodels/ADF.html",
    "href": "tests/model_validation/statsmodels/ADF.html",
    "title": "ADF",
    "section": "",
    "text": "ADF\nAssesses the stationarity of a time series dataset using the Augmented Dickey-Fuller (ADF) test.\nPurpose: The Augmented Dickey-Fuller (ADF) test metric is used here to determine the order of integration, i.e., the stationarity of a given time series data. The stationary property of data is pivotal in many machine learning models as it impacts the reliability and effectiveness of predictions and forecasts.\nTest Mechanism: The ADF test starts by executing the ADF function from the statsmodels library on every feature of the dataset. Multiple outputs are generated for each run, including the ADF test statistic and p-value, count of lags used, the number of observations factored into the test, critical values at various confidence levels, and the maximized information criterion. These results are stored for each feature for subsequent analysis.\nSigns of High Risk: - An inflated ADF statistic and high p-value (generally above 0.05) insinuate a high risk to the model’s performance due to the presence of a unit root indicating non-stationarity. - Such non-stationarity might result in untrustworthy or insufficient forecasts.\nStrengths: - The ADF test is robust to more sophisticated correlation within the data, which empowers it to be deployed in settings where data might display complex stochastic behavior. - The ADF test provides explicit outputs like test statistics, critical values, and information criterion, thereby enhancing our understanding and transparency of the model validation process.\nLimitations: - The ADF test might demonstrate low statistical power, making it challenging to differentiate between a unit root and near-unit-root processes causing false negatives. - The test assumes the data follows an autoregressive process, which might not be the case all the time. - The ADF test finds it demanding to manage time series data with structural breaks."
  },
  {
    "objectID": "tests/model_validation/statsmodels/PredictionProbabilitiesHistogram.html",
    "href": "tests/model_validation/statsmodels/PredictionProbabilitiesHistogram.html",
    "title": "PredictionProbabilitiesHistogram",
    "section": "",
    "text": "PredictionProbabilitiesHistogram\nGenerates and visualizes histograms of the Probability of Default predictions for both positive and negative classes in training and testing datasets.\nPurpose: This code is designed to generate histograms that display the Probability of Default (PD) predictions for positive and negative classes in both the training and testing datasets. By doing so, it evaluates the performance of a logistic regression model, particularly in the context of credit risk prediction.\nTest Mechanism: The metric executes these steps to run the test: - Firstly, it extracts the target column from both the train and test datasets. - The model’s predict function is then used to calculate probabilities. - These probabilities are added as a new column to the training and testing dataframes. - Histograms are generated for each class (0 or 1 in binary classification scenarios) within the training and testing datasets. - To enhance visualization, the histograms are set to have different opacities. - The four histograms (two for training data and two for testing) are overlaid on two different subplot frames (one for training and one for testing data). - The test returns a plotly graph object displaying the visualization.\nSigns of High Risk: Several indicators could suggest a high risk or failure in the model’s performance. These include: - Significant discrepancies observed between the histograms of training and testing data. - Large disparities between the histograms for the positive and negative classes. - These issues could signal potential overfitting or bias in the model. - Unevenly distributed probabilities may also indicate that the model does not accurately predict outcomes.\nStrengths: This metric and test offer several benefits, including: - The visual representation of the PD predictions made by the model, which aids in understanding the model’s behaviour. - The ability to assess both the training and testing datasets, adding depth to the validation of the model. - Highlighting disparities between multiple classes, providing potential insights into class imbalance or data skewness issues. - Particularly beneficial for credit risk prediction, it effectively visualizes the spread of risk across different classes.\nLimitations: Despite its strengths, the test has several limitations: - It is specifically tailored for binary classification scenarios, where the target variable only has two classes; as such, it isn’t suited for multi-class classification tasks. - This metric is mainly applicable for logistic regression models. It might not be effective or accurate when used on other model types. - While the test provides a robust visual representation of the model’s PD predictions, it does not provide a quantifiable measure or score to assess model performance."
  },
  {
    "objectID": "tests/model_validation/statsmodels/AutoARIMA.html",
    "href": "tests/model_validation/statsmodels/AutoARIMA.html",
    "title": "AutoARIMA",
    "section": "",
    "text": "AutoARIMA\nEvaluates ARIMA models for time-series forecasting, ranking them using Bayesian and Akaike Information Criteria.\nPurpose: The AutoARIMA validation test is designed to evaluate and rank AutoRegressive Integrated Moving Average (ARIMA) models. These models are primarily used for forecasting time-series data. The validation test automatically fits multiple ARIMA models, with varying parameters, to every variable within the given dataset. The models are then ranked based on their Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) values, which provide a basis for the efficient model selection process.\nTest Mechanism: This metric proceeds by generating an array of feasible combinations of ARIMA model parameters which are within a prescribed limit. These limits include max_p, max_d, max_q; they represent the autoregressive, differencing, and moving average components respectively. Upon applying these sets of parameters, the validation test fits each ARIMA model to the time-series data provided. For each model, it subsequently proceeds to calculate and record both the BIC and AIC values, which serve as performance indicators for the model fit. Prior to this parameter fitting process, the Augmented Dickey-Fuller test for data stationarity is conducted on the data series. If a series is found to be non-stationary, a warning message is sent out, given that ARIMA models necessitate input series to be stationary.\nSigns of High Risk: * If the p-value of the Augmented Dickey-Fuller test for a variable exceeds 0.05, a warning is logged. This warning indicates that the series might not be stationary, leading to potentially inaccurate results. * Consistent failure in fitting ARIMA models (as made evident through logged errors) might disclose issues with either the data or model stability.\nStrengths: * The AutoARIMA validation test simplifies the often complex task of selecting the most suitable ARIMA model based on BIC and AIC criteria. * The mechanism incorporates a check for non-stationarity within the data, which is a critical prerequisite for ARIMA models. * The exhaustive search through all possible combinations of model parameters enhances the likelihood of identifying the best-fit model.\nLimitations: * This validation test can be computationally costly as it involves creating and fitting multiple ARIMA models for every variable. * Although the test checks for non-stationarity and logs warnings where present, it does not apply any transformations to the data to establish stationarity. * The selection of models leans solely on BIC and AIC criteria, which may not yield the best predictive model in all scenarios. * The test is only applicable to regression tasks involving time-series data, and may not work effectively for other types of machine learning tasks."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionFeatureSignificance.html",
    "href": "tests/model_validation/statsmodels/RegressionFeatureSignificance.html",
    "title": "RegressionFeatureSignificance",
    "section": "",
    "text": "RegressionFeatureSignificance\nAssesses and visualizes the statistical significance of features in a set of regression models.\nPurpose: The Regression Feature Significance metric assesses the significance of each feature in a given set of regression models. It creates a visualization displaying p-values for every feature of each model, assisting model developers in understanding which features are most influential in their models.\nTest Mechanism: The test mechanism involves going through each fitted regression model in a given list, extracting the model coefficients and p-values for each feature, and then plotting these values. The x-axis on the plot contains the p-values while the y-axis denotes the coefficients of each feature. A vertical red line is drawn at the threshold for p-value significance, which is 0.05 by default. Any features with p-values to the left of this line are considered statistically significant at the chosen level.\nSigns of High Risk: - Any feature with a high p-value (greater than the threshold) is considered a potential high risk, as it suggests the feature is not statistically significant and may not be reliably contributing to the model’s predictions. - A high number of such features may indicate problems with the model validation, variable selection, and overall reliability of the model predictions.\nStrengths: - Helps identify the features that significantly contribute to a model’s prediction, providing insights into the feature importance. - Provides tangible, easy-to-understand visualizations to interpret the feature significance. - Facilitates comparison of feature importance across multiple models.\nLimitations: - This metric assumes model features are independent, which may not always be the case. Multicollinearity (high correlation amongst predictors) can cause high variance and unreliable statistical tests of significance. - The p-value strategy for feature selection doesn’t take into account the magnitude of the effect, focusing solely on whether the feature is likely non-zero. - This test is specific to regression models and wouldn’t be suitable for other types of ML models. - P-value thresholds are somewhat arbitrary and do not always indicate practical significance, only statistical significance."
  },
  {
    "objectID": "tests/model_validation/statsmodels/CumulativePredictionProbabilities.html",
    "href": "tests/model_validation/statsmodels/CumulativePredictionProbabilities.html",
    "title": "CumulativePredictionProbabilities",
    "section": "",
    "text": "CumulativePredictionProbabilities\nVisualizes cumulative probabilities of positive and negative classes for both training and testing in logistic regression models.\nPurpose: This metric is utilized to evaluate the distribution of predicted probabilities for positive and negative classes in a logistic regression model. It’s not solely intended to measure the model’s performance but also provides a visual assessment of the model’s behavior by plotting the cumulative probabilities for positive and negative classes across both the training and test datasets.\nTest Mechanism: The logistic regression model is evaluated by first computing the predicted probabilities for each instance in both the training and test datasets, which are then added as a new column in these sets. The cumulative probabilities for positive and negative classes are subsequently calculated and sorted in ascending order. Cumulative distributions of these probabilities are created for both positive and negative classes across both training and test datasets. These cumulative probabilities are represented visually in a plot, containing two subplots - one for the training data and the other for the test data, with lines representing cumulative distributions of positive and negative classes.\nSigns of High Risk: - Imbalanced distribution of probabilities for either positive or negative classes. - Notable discrepancies or significant differences between the cumulative probability distributions for the training data versus the test data. - Marked discrepancies or large differences between the cumulative probability distributions for positive and negative classes.\nStrengths: - It offers not only numerical probabilities but also provides a visual illustration of data, which enhances the ease of understanding and interpreting the model’s behavior. - Allows for the comparison of model’s behavior across training and testing datasets, providing insights about how well the model is generalized. - It differentiates between positive and negative classes and their respective distribution patterns, which can aid in problem diagnosis.\nLimitations: - Exclusive to classification tasks and specifically to logistic regression models. - Graphical results necessitate human interpretation and may not be directly applicable for automated risk detection. - The method does not give a solitary quantifiable measure of model risk, rather it offers a visual representation and broad distributional information. - If the training and test datasets are not representative of the overall data distribution, the metric could provide misleading results."
  },
  {
    "objectID": "tests/model_validation/statsmodels/KPSS.html",
    "href": "tests/model_validation/statsmodels/KPSS.html",
    "title": "KPSS",
    "section": "",
    "text": "KPSS\nExecutes KPSS unit root test to validate stationarity of time-series data in machine learning model.\nPurpose: The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test is utilized to ensure the stationarity of data within the machine learning model. It specifically works on time-series data to establish the order of integration, which is a prime requirement for accurate forecasting, given the fundamental condition for any time series model is that the series should be stationary.\nTest Mechanism: This metric evaluates the KPSS score for every feature present in the dataset. Within the KPSS score, there are various components, namely: a statistic, a p-value, a used lag, and critical values. The core scheme behind the KPSS score is to test the hypothesis that an observable time series is stationary around a deterministic trend. If the computed statistic surpasses the critical value, the null hypothesis is dismissed, inferring the series is non-stationary.\nSigns of High Risk: - High KPSS score represents a considerable risk, particularly if the calculated statistic is higher than the critical value. - If the null hypothesis is rejected and the series is recognized as non-stationary, it heavily influences the model’s forecasting capability rendering it less effective.\nStrengths: - The KPSS test directly measures the stationarity of a series, allowing it to fulfill a key prerequisite for many time-series models, making it a valuable tool for model validation. - The logics underpinning the test are intuitive and simple, making it understandable and accessible for developers and risk management teams.\nLimitations: - The KPSS test presumes the absence of a unit root in the series and does not differentiate between series that are stationary and those border-lining stationarity. - The test might show restricted power against specific alternatives. - The reliability of the test is contingent on the number of lags selected, which introduces potential bias in the measurement."
  },
  {
    "objectID": "tests/model_validation/RougeScore.html",
    "href": "tests/model_validation/RougeScore.html",
    "title": "RougeScore",
    "section": "",
    "text": "RougeScore"
  },
  {
    "objectID": "tests/model_validation/MeteorScore.html",
    "href": "tests/model_validation/MeteorScore.html",
    "title": "MeteorScore",
    "section": "",
    "text": "MeteorScore"
  },
  {
    "objectID": "tests/model_validation/BleuScore.html",
    "href": "tests/model_validation/BleuScore.html",
    "title": "BleuScore",
    "section": "",
    "text": "BleuScore"
  },
  {
    "objectID": "tests/model_validation/BertScore.html",
    "href": "tests/model_validation/BertScore.html",
    "title": "BertScore",
    "section": "",
    "text": "BertScore"
  },
  {
    "objectID": "tests/model_validation/sklearn/FowlkesMallowsScore.html",
    "href": "tests/model_validation/sklearn/FowlkesMallowsScore.html",
    "title": "FowlkesMallowsScore",
    "section": "",
    "text": "FowlkesMallowsScore\nEvaluates the similarity between predicted and actual cluster assignments in a model using the Fowlkes-Mallows score.\nPurpose:\nThe FowlkesMallowsScore is a performance metric used to validate clustering algorithms within machine learning models. The score intends to evaluate the matching grade between two clusters. It measures the similarity between the predicted and actual cluster assignments, thus gauging the accuracy of the model’s clustering capability.\nTest Mechanism:\nThe FowlkesMallowsScore method applies the fowlkes_mallows_score function from the sklearn library to evaluate the model’s accuracy in clustering different types of data. The test fetches the datasets from the model’s training and testing datasets as inputs then compares the resulting clusters against the previously known clusters to obtain a score. A high score indicates a better clustering performance by the model.\nSigns of High Risk:\n\nA low Fowlkes-Mallows score (near zero): This indicates that the model’s clustering capability is poor and the algorithm isn’t properly grouping data.\nInconsistently low scores across different datasets: this may indicate that the model’s clustering performance is not robust and the model may fail when applied to unseen data.\n\nStrengths:\n\nThe Fowlkes-Mallows score is a simple and effective method for evaluating the performance of clustering algorithms.\nThis metric takes into account both precision and recall in its calculation, therefore providing a balanced and comprehensive measure of model performance.\nThe Fowlkes-Mallows score is non-biased meaning it treats False Positives and False Negatives equally.\n\nLimitations:\n\nAs a pairwise-based method, this score can be computationally intensive for large datasets and can become unfeasible as the size of the dataset increases.\nThe Fowlkes-Mallows score works best with balanced distribution of samples across clusters. If this condition is not met, the score can be skewed.\nIt does not handle mismatching numbers of clusters between the true and predicted labels. As such, it may return misleading results if the predicted labels suggest a different number of clusters than what is in the true labels."
  },
  {
    "objectID": "tests/model_validation/sklearn/ClassifierPerformance.html",
    "href": "tests/model_validation/sklearn/ClassifierPerformance.html",
    "title": "ClassifierPerformance",
    "section": "",
    "text": "ClassifierPerformance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy, and ROC AUC scores.\nPurpose: The supplied script is designed to evaluate the performance of Machine Learning classification models. It accomplishes this by computing precision, recall, F1-Score, and accuracy, as well as the ROC AUC (Receiver operating characteristic - Area under the curve) scores, thereby providing a comprehensive analytic view of the models’ performance. The test is adaptable, handling binary and multiclass models equally effectively.\nTest Mechanism: The script produces a report that includes precision, recall, F1-Score, and accuracy, by leveraging the classification_report from the scikit-learn’s metrics module. For multiclass models, macro and weighted averages for these scores are also calculated. Additionally, the ROC AUC scores are calculated and included in the report using the script’s unique multiclass_roc_auc_score function. The outcome of the test (report format) differs based on whether the model is binary or multiclass.\nSigns of High Risk: - Low values for precision, recall, F1-Score, accuracy, and ROC AUC, indicating poor performance. - Imbalance in precision and recall scores. Precision highlights correct positive class predictions, while recall indicates the accurate identification of actual positive cases. Imbalance may indicate flawed model performance. - A low ROC AUC score, especially scores close to 0.5 or lower, strongly suggests a failing model.\nStrengths: - The script is versatile, capable of assessing both binary and multiclass models. - It uses a variety of commonly employed performance metrics, offering a comprehensive view of a model’s performance. - The use of ROC-AUC as a metric aids in determining the most optimal threshold for classification, especially beneficial when evaluation datasets are unbalanced.\nLimitations: - The test assumes correctly identified labels for binary classification models and raises an exception if the positive class is not labeled as “1”. However, this setup may not align with all practical applications. - This script is specifically designed for classification models and is not suited to evaluate regression models. - The metrics computed may provide limited insights in cases where the test dataset does not adequately represent the data the model will encounter in real-world scenarios."
  },
  {
    "objectID": "tests/model_validation/sklearn/ClusterCosineSimilarity.html",
    "href": "tests/model_validation/sklearn/ClusterCosineSimilarity.html",
    "title": "ClusterCosineSimilarity",
    "section": "",
    "text": "ClusterCosineSimilarity\nMeasures the intra-cluster similarity of a clustering model using cosine similarity.\n1. Purpose: The purpose of this metric is to measure how similar the data points within each cluster of a clustering model are. This is done using cosine similarity, which compares the multi-dimensional direction (but not magnitude) of data vectors. From a Model Risk Management perspective, this metric is used to quantitatively validate that clusters formed by a model have high intra-cluster similarity.\n2. Test Mechanism: This test works by first extracting the true and predicted clusters of the model’s training data. Then, it computes the centroid (average data point) of each cluster. Next, it calculates the cosine similarity between each data point within a cluster and its respective centroid. Finally, it outputs the mean cosine similarity of each cluster, highlighting how similar, on average, data points in a cluster are to the cluster’s centroid.\n3. Signs of High Risk:\n\nLow mean cosine similarity for one or more clusters: If the mean cosine similarity is low, the data points within the respective cluster have high variance in their directions. This can be indicative of poor clustering, suggesting that the model might not be suitably separating the data into distinct patterns.\nHigh disparity between mean cosine similarity values across clusters: If there’s a significant difference in mean cosine similarity across different clusters, this could indicate imbalance in how the model forms clusters.\n\n4. Strengths:\n\nCosine similarity operates in a multi-dimensional space, making it effective for measuring similarity in high dimensional datasets, typical for many machine learning problems.\nIt provides an agnostic view of the cluster performance by only considering the direction (and not the magnitude) of each vector.\nThis metric is not dependent on the scale of the variables, making it equally effective on different scales.\n\n5. Limitations:\n\nCosine similarity does not consider magnitudes (i.e. lengths) of vectors, only their direction. This means it may overlook instances where clusters have been adequately separated in terms of magnitude.\nThis method summarily assumes that centroids represent the average behavior of data points in each cluster. This might not always be true, especially in clusters with high amounts of variance or non-spherical shapes.\nIt primarily works with continuous variables and is not suitable for binary or categorical variables.\nLastly, although rare, perfect perpendicular vectors (cosine similarity = 0) could be within the same cluster, which may give an inaccurate representation of a ‘bad’ cluster due to low cosine similarity score."
  },
  {
    "objectID": "tests/model_validation/sklearn/ClusterPerformanceMetrics.html",
    "href": "tests/model_validation/sklearn/ClusterPerformanceMetrics.html",
    "title": "ClusterPerformanceMetrics",
    "section": "",
    "text": "ClusterPerformanceMetrics\nEvaluates the performance of clustering machine learning models using multiple established metrics.\nPurpose:\nThe ClusterPerformanceMetrics test is used to assess the performance and validity of clustering machine learning models. It evaluates homogeneity, completeness, V measure score, the Adjusted Rand Index, the Adjusted Mutual Information, and the Fowlkes-Mallows score of the model. These metrics provide a holistic understanding of the model’s ability to accurately form clusters of the given dataset.\nTest Mechanism:\nThe ClusterPerformanceMetrics test runs a clustering ML model over a given dataset and then calculates six metrics using the Scikit-learn metrics computation functions: Homogeneity Score, Completeness Score, V Measure, Adjusted Rand Index (ARI), Adjusted Mutual Information (AMI), and Fowlkes-Mallows Score. It then returns the result as a summary, presenting the metric values for both training and testing datasets.\nSigns of High Risk:\n\nLow Homogeneity Score: This indicates that the clusters formed contain a variety of classes, resulting in less pure clusters.\nLow Completeness Score: This suggests that class instances are scattered across multiple clusters rather than being gathered in a single cluster.\nLow V Measure: This would report a low overall clustering performance.\nARI close to 0 or Negative: This implies that clustering results are random or disagree with the true labels.\nAMI close to 0: It means that clustering labels are random compared with the true labels.\nLow Fowlkes-Mallows score: Signifies less precise and poor clustering performance in terms of precision and recall.\n\nStrengths:\n\nProvides a comprehensive view of clustering model performance by examining multiple clustering metrics.\nUses established and widely accepted metrics from scikit-learn, providing reliability in the results.\nAble to provide performance metrics for both training and testing datasets.\nClearly defined and human-readable descriptions of each score make it easy to understand what each score represents.\n\nLimitations:\n\nIt only applies to clustering models; not suitable for other types of machine learning models.\nDoes not test for overfitting or underfitting in the clustering model.\nAll the scores rely on ground truth labels, the absence or inaccuracy of which can lead to misleading results.\nDoes not consider aspects like computational efficiency of the model or its capability to handle high dimensional data."
  },
  {
    "objectID": "tests/model_validation/sklearn/MinimumF1Score.html",
    "href": "tests/model_validation/sklearn/MinimumF1Score.html",
    "title": "MinimumF1Score",
    "section": "",
    "text": "MinimumF1Score\nEvaluates if the model’s F1 score on the validation set meets a predefined minimum threshold.\nPurpose: The main objective of this test is to ensure that the F1 score, a balanced measure of precision and recall, of the model meets or surpasses a predefined threshold on the validation dataset. The F1 score is highly useful for gauging model performance in classification tasks, especially in cases where the distribution of positive and negative classes is skewed.\nTest Mechanism: The F1 score for the validation dataset is computed through the scikit-learn’s metrics in Python. The scoring mechanism differs based on the classification problem: for multi-class problems, macro averaging is used (metrics are calculated separately and their unweighted mean is found), and for binary classification, the built-in f1_score calculation is used. The obtained F1 score is then assessed against the predefined minimum F1 score that is expected from the model.\nSigns of High Risk:\n\nIf a model returns an F1 score that is less than the established threshold, it is regarded as high risk.\nA low F1 score might suggest that the model is not finding an optimal balance between precision and recall, see: it isn’t successfully identifying positive classes while minimizing false positives.\n\nStrengths:\n\nThis metric gives a balanced measure of a model’s performance by accounting for both false positives and false negatives.\nIt has a particular advantage in scenarios with imbalanced class distribution, where an accuracy measure can be misleading.\nThe flexibility of setting the threshold value allows for tailoring the minimum acceptable performance.\n\nLimitations:\n\nThe testing method may not be suitable for all types of models and machine learning tasks.\nAlthough the F1 score gives a balanced view of a model’s performance, it presupposes an equal cost for false positives and false negatives, which may not always be true in certain real-world scenarios. As a consequence, practitioners might have to rely on other metrics such as precision, recall, or the ROC-AUC score that align more closely with their specific requirements."
  },
  {
    "objectID": "tests/model_validation/sklearn/RegressionModelsPerformanceComparison.html",
    "href": "tests/model_validation/sklearn/RegressionModelsPerformanceComparison.html",
    "title": "RegressionModelsPerformanceComparison",
    "section": "",
    "text": "RegressionModelsPerformanceComparison\nCompares and evaluates the performance of multiple regression models using five different metrics: MAE, MSE, RMSE, MAPE, and MBD.\n1. Purpose: The Regression Models Performance Comparison metric is used to measure and compare the performance of regression models. It calculates multiple evaluation metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Bias Deviation (MBD), thereby enabling a comprehensive view of model performance.\n2. Test Mechanism: The test starts by sourcing the true and predicted values from the models. It then computes the MAE, MSE, RMSE, MAPE, and MBD. These calculations encapsulate both the direction and the magnitude of error in predictions, thereby providing a multi-faceted view of model accuracy. It captures these results in a dictionary and compares the performance of all models using these metrics. The results are then appended to a table for presenting a comparative summary.\n3. Signs of High Risk:\n\nHigh values of MAE, MSE, RMSE, and MAPE, which indicate a high error rate and imply a larger departure of the model’s predictions from the true values.\nA large value of MBD, which shows a consistent bias in the model’s predictions.\nIf the test returns an error citing that no models were provided for comparison, it implies a risk in the evaluation process itself.\n\n4. Strengths:\n\nThe metric evaluates models on five different metrics offering a comprehensive analysis of model performance.\nIt compares multiple models simultaneously, aiding in the selection of the best-performing models.\nIt is designed to handle regression tasks and can be seamlessly integrated with libraries like sklearn.\n\n5. Limitations:\n\nThe metric only evaluates regression models and does not evaluate classification models.\nThe test assumes that the models have been trained and tested appropriately prior to evaluation. It does not handle pre-processing, feature selection, or other stages in the model lifecycle.\nIt may fail to run if it doesn’t receive valid models as inputs. The models are passed externally and the test doesn’t have an internal mechanism to verify their validity.\nThe test could exhibit performance limitations if a large number of models is input for comparison."
  },
  {
    "objectID": "tests/model_validation/sklearn/SilhouettePlot.html",
    "href": "tests/model_validation/sklearn/SilhouettePlot.html",
    "title": "SilhouettePlot",
    "section": "",
    "text": "SilhouettePlot\nCalculates and visualizes Silhouette Score, assessing degree of data point suitability to its cluster in ML models.\nPurpose: This test calculates the Silhouette Score, which is a model performance metric used in clustering applications. Primarily, the Silhouette Score evaluates how similar an object (data point) is to its own cluster compared to other clusters. The metric ranges between -1 and 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. Thus, the goal is to achieve a high Silhouette Score, implying well-separated clusters.\nTest Mechanism: The test first extracts the true and predicted labels from the model’s training data. The test runs the Silhouette Score function, which takes as input the training dataset features and the predicted labels, subsequently calculating the average score. This average Silhouette Score is printed for reference. The script then calculates the silhouette coefficients for each data point, helping to form the Silhouette Plot. Each cluster is represented in this plot, with color distinguishing between different clusters. A red dashed line indicates the average Silhouette Score. The Silhouette Scores are also collected into a structured table, facilitating model performance analysis and comparison.\nSigns of High Risk: - A low Silhouette Score, potentially indicating that the clusters are not well separated and that data points may not be fitting well to their respective clusters. - A Silhouette Plot displaying overlapping clusters or the absence of clear distinctions between clusters visually also suggests poor clustering performance.\nStrengths: - The Silhouette Score provides a clear and quantitative measure of how well data points have been grouped into clusters, offering insights into model performance. - The Silhouette Plot provides an intuitive, graphical representation of the clustering mechanism, aiding visual assessments of model performance. - It does not require ground truth labels, so it’s useful when true cluster assignments are not known.\nLimitations: - The Silhouette Score may be susceptible to the influence of outliers, which could impact its accuracy and reliability. - It assumes the clusters are convex and isotropic, which might not be the case with complex datasets. - Due to the average nature of the Silhouette Score, the metric does not account for individual data point assignment nuances, so potentially relevant details may be omitted. - Computationally expensive for large datasets, as it requires pairwise distance computations."
  },
  {
    "objectID": "tests/model_validation/sklearn/AdjustedRandIndex.html",
    "href": "tests/model_validation/sklearn/AdjustedRandIndex.html",
    "title": "AdjustedRandIndex",
    "section": "",
    "text": "AdjustedRandIndex\nMeasures the similarity between two data clusters using the Adjusted Rand Index (ARI) metric in clustering machine learning models.\n1. Purpose: The Adjusted Rand Index (ARI) metric is intended to measure the similarity between two data clusters. This metric is specifically being used for clustering machine learning models to validly quantify how well the model is clustering and producing data groups. It involves comparing the model’s produced clusters against the actual (true) clusters found in the dataset.\n2. Test Mechanism: The Adjusted Rand Index (ARI) is calculated by using the adjusted_rand_score method from the sklearn metrics in Python. The test requires inputs including the model itself and the model’s training and test datasets. The model’s computed clusters and the true clusters are compared, and the similarities are measured to compute the ARI.\n3. Signs of High Risk: - If the ARI is close to zero, it signifies that the model’s cluster assignments are random and don’t match the actual dataset clusters, indicating a high risk. - An ARI of less than zero indicates that the model’s clustering performance is worse than random.\n4. Strengths: - ARI is normalized and it hence gives a consistent metric between -1 and +1, irrespective of raw cluster sizes or dataset size variations. - It doesn’t require a ground truth for computation which makes it ideal for unsupervised learning model evaluations. - It penalizes for false positives and false negatives, providing a robust measure of clustering quality.\n5. Limitations: - In real-world situations, true clustering is often unknown, which can hinder the practical application of the ARI. - The ARI requires all individual data instances to be independent, which may not always hold true. - It may be difficult to interpret the implications of an ARI score without a context or a benchmark, as it is heavily dependent on the characteristics of the dataset used."
  },
  {
    "objectID": "tests/model_validation/sklearn/ConfusionMatrix.html",
    "href": "tests/model_validation/sklearn/ConfusionMatrix.html",
    "title": "ConfusionMatrix",
    "section": "",
    "text": "ConfusionMatrix\nEvaluates and visually represents the classification ML model’s predictive performance using a Confusion Matrix heatmap.\nPurpose: The Confusion Matrix tester is designed to assess the performance of a classification Machine Learning model. This performance is evaluated based on how well the model is able to correctly classify True Positives, True Negatives, False Positives, and False Negatives - fundamental aspects of model accuracy.\nTest Mechanism: The mechanism used involves taking the predicted results (y_test_predict) from the classification model and comparing them against the actual values (y_test_true). A confusion matrix is built using the unique labels extracted from y_test_true, employing scikit-learn’s metrics. The matrix is then visually rendered with the help of Plotly’s create_annotated_heatmap function. A heatmap is created which provides a two-dimensional graphical representation of the model’s performance, showcasing distributions of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\nSigns of High Risk: Indicators of high risk related to the model include: - High numbers of False Positives (FP) and False Negatives (FN), depicting that the model is not effectively classifying the values. - Low numbers of True Positives (TP) and True Negatives (TN), implying that the model is struggling with correctly identifying class labels.\nStrengths: The Confusion Matrix tester brings numerous strengths: - It provides a simplified yet comprehensive visual snapshot of the classification model’s predictive performance. - It distinctly brings out True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), thus, making it easier to focus on potential areas of improvement. - The matrix is beneficial in dealing with multi-class classification problems as it can provide a simple view of complex model performances. - It aids in understanding the different types of errors that the model could potentially make, as it provides in-depth insights into Type-I and Type-II errors.\nLimitations: Despite its various strengths, the Confusion Matrix tester does exhibit some limitations: - In cases of unbalanced classes, the effectiveness of the confusion matrix might be lessened. It may wrongly interpret the accuracy of a model that is essentially just predicting the majority class. - It does not provide a single unified statistic that could evaluate the overall performance of the model. Different aspects of the model’s performance are evaluated separately instead. - It mainly serves as a descriptive tool and does not offer the capability for statistical hypothesis testing. - Risks of misinterpretation exist because the matrix doesn’t directly provide precision, recall, or F1-score data. These metrics have to be computed separately."
  },
  {
    "objectID": "tests/model_validation/sklearn/CompletenessScore.html",
    "href": "tests/model_validation/sklearn/CompletenessScore.html",
    "title": "CompletenessScore",
    "section": "",
    "text": "CompletenessScore\nEvaluates a clustering model’s capacity to categorize instances from a single class into the same cluster.\nPurpose: The Completeness Score metric is used to assess the performance of clustering models. It measures the extent to which all the data points that are members of a given class are elements of the same cluster. The aim is to determine the capability of the model to categorize all instances from a single class into the same cluster.\nTest Mechanism: This test takes three inputs, a model and its associated training and testing datasets. It invokes the completeness_score function from the sklearn library on the labels predicted by the model. High scores indicate that data points from the same class generally appear in the same cluster, while low scores suggest the opposite.\nSigns of High Risk: - Low completeness score: This suggests that the model struggles to group instances from the same class into one cluster, indicating poor clustering performance.\nStrengths: - The Completeness Score provides an effective method for assessing the performance of a clustering model, specifically its ability to group class instances together. - This test metric conveniently relies on the capabilities provided by the sklearn library, ensuring consistent and reliable test results.\nLimitations: - This metric only evaluates a specific aspect of clustering, meaning it may not provide a holistic or complete view of the model’s performance. - It cannot assess the effectiveness of the model in differentiating between separate classes, as it is solely focused on how well data points from the same class are grouped. - The Completeness Score only applies to clustering models; it cannot be used for other types of machine learning models."
  },
  {
    "objectID": "tests/model_validation/sklearn/WeakspotsDiagnosis.html",
    "href": "tests/model_validation/sklearn/WeakspotsDiagnosis.html",
    "title": "WeakspotsDiagnosis",
    "section": "",
    "text": "WeakspotsDiagnosis\nIdentifies and visualizes weak spots in a machine learning model’s performance across various sections of the feature space.\nPurpose: The weak spots test is applied to evaluate the performance of a machine learning model within specific regions of its feature space. This test slices the feature space into various sections, evaluating the model’s outputs within each section against specific performance metrics (e.g., accuracy, precision, recall, and F1 scores). The ultimate aim is to identify areas where the model’s performance falls below the set thresholds, thereby exposing its possible weaknesses and limitations.\nTest Mechanism: The test mechanism adopts an approach of dividing the feature space of the training dataset into numerous bins. The model’s performance metrics (accuracy, precision, recall, F1 scores) are then computed for each bin on both the training and test datasets. A “weak spot” is identified if any of the performance metrics fall below a predetermined threshold for a particular bin on the test dataset. The test results are visually plotted as bar charts for each performance metric, indicating the bins which fail to meet the established threshold.\nSigns of High Risk:\n\nAny performance metric of the model dropping below the set thresholds.\nSignificant disparity in performance between the training and test datasets within a bin could be an indication of overfitting.\nRegions or slices with consistently low performance metrics. Such instances could mean that the model struggles to handle specific types of input data adequately, resulting in potentially inaccurate predictions.\n\nStrengths:\n\nThe test helps pinpoint precise regions of the feature space where the model’s performance is below par, allowing for more targeted improvements to the model.\nThe graphical presentation of the performance metrics offers an intuitive way to understand the model’s performance across different feature areas.\nThe test exhibits flexibility, letting users set different thresholds for various performance metrics according to the specific requirements of the application.\n\nLimitations:\n\nThe binning system utilized for the feature space in the test could over-simplify the model’s behavior within each bin. The granularity of this slicing depends on the chosen ‘bins’ parameter and can sometimes be arbitrary.\nThe effectiveness of this test largely hinges on the selection of thresholds for the performance metrics, which may not hold universally applicable and could be subjected to the specifications of a particular model and application.\nThe test is unable to handle datasets with a text column, limiting its application to numerical or categorical data types only.\nDespite its usefulness in highlighting problematic regions, the test does not offer direct suggestions for model improvement."
  },
  {
    "objectID": "tests/model_validation/sklearn/MinimumAccuracy.html",
    "href": "tests/model_validation/sklearn/MinimumAccuracy.html",
    "title": "MinimumAccuracy",
    "section": "",
    "text": "MinimumAccuracy\nChecks if the model’s prediction accuracy meets or surpasses a specified threshold.\nPurpose: The Minimum Accuracy test’s objective is to verify whether the model’s prediction accuracy on a specific dataset meets or surpasses a predetermined minimum threshold. Accuracy, which is simply the ratio of right predictions to total predictions, is a key metric for evaluating the model’s performance. Considering binary as well as multiclass classifications, accurate labeling becomes indispensable.\nTest Mechanism: The test mechanism involves contrasting the model’s accuracy score with a pre-set minimum threshold value, default value being 0.7. The accuracy score is computed utilizing sklearn’s accuracy_score method, where the true label y_true and predicted label class_pred are compared. If the accuracy score is above the threshold, the test gets a passing mark. The test returns the result along with the accuracy score and threshold used for the test.\nSigns of High Risk: - The risk level for this test surges considerably when the model is unable to achieve or surpass the predefined score threshold. - When the model persistently scores below the threshold, it suggests a high risk of inaccurate predictions, which in turn affects the model’s efficiency and reliability.\nStrengths: - One of the key strengths of this test is its simplicity, presenting a straightforward measure of the holistic model performance across all classes. - This test is particularly advantageous when classes are balanced. - Another advantage of this test is its versatility as it can be implemented on both binary and multiclass classification tasks.\nLimitations: - When analyzing imbalanced datasets, certain limitations of this test emerge. The accuracy score can be misleading when classes in the dataset are skewed considerably. - This can result in favoritism towards the majority class, consequently giving an inaccurate perception of the model performance. - Another limitation is its inability to measure the model’s precision, recall, or capacity to manage false positives or false negatives. - The test majorly focuses on overall correctness and may not be sufficient for all types of model analytics."
  },
  {
    "objectID": "tests/model_validation/sklearn/KMeansClustersOptimization.html",
    "href": "tests/model_validation/sklearn/KMeansClustersOptimization.html",
    "title": "KMeansClustersOptimization",
    "section": "",
    "text": "KMeansClustersOptimization\nOptimizes the number of clusters in K-means models using Elbow and Silhouette methods.\nPurpose: This metric is used to optimize the number of clusters used in K-means clustering models. It intends to measure and evaluate the optimal number of clusters by leveraging two methodologies, namely the Elbow method and the Silhouette method. This is crucial as an inappropriate number of clusters can either overly simplify or overcomplicate the structure of the data, thereby undermining the effectiveness of the model.\nTest Mechanism: The test mechanism involves iterating over a predefined range of cluster numbers and applying both the Elbow method and the Silhouette method. The Elbow method computes the sum of the minimum euclidean distances between data points and their respective cluster centers (distortion). This value decreases as the number of clusters increases; the optimal number is typically at the ‘elbow’ point where the decrease in distortion becomes less pronounced. Meanwhile, the Silhouette method calculates the average silhouette score for each data point in the dataset, providing a measure of how similar each item is to its own cluster compared to other clusters. The optimal number of clusters under this method is the one that maximizes the average silhouette score. The results of both methods are plotted for visual inspection.\nSigns of High Risk: - A high distortion value or a low silhouette average score for the optimal number of clusters. - No clear ‘elbow’ point or plateau observed in the distortion plot, or a uniformly low silhouette average score across different numbers of clusters, suggesting the data is not amenable to clustering. - An optimal cluster number that is unreasonably high or low, suggestive of overfitting or underfitting, respectively.\nStrengths: - Provides both a visual and quantitative method to determine the optimal number of clusters. - Leverages two different methods (Elbow and Silhouette), thereby affording robustness and versatility in assessing the data’s clusterability. - Facilitates improved model performance by allowing for an informed selection of the number of clusters.\nLimitations: - Assumes that a suitable number of clusters exists in the data, which may not always be true, especially for complex or noisy data. - Both methods may fail to provide definitive answers when the data lacks clear cluster structures. - Might not be straightforward to determine the ‘elbow’ point or maximize the silhouette average score, especially in larger and complicated datasets. - Assumes spherical clusters (due to using the Euclidean distance in the Elbow method), which might not align with the actual structure of the data."
  },
  {
    "objectID": "tests/model_validation/sklearn/ROCCurve.html",
    "href": "tests/model_validation/sklearn/ROCCurve.html",
    "title": "ROCCurve",
    "section": "",
    "text": "ROCCurve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic (ROC) curve and calculating the Area Under Curve (AUC) score.\nPurpose: The Receiver Operating Characteristic (ROC) curve is designed to evaluate the performance of binary classification models. This curve illustrates the balance between the True Positive Rate (TPR) and False Positive Rate (FPR) across various threshold levels. In combination with the Area Under the Curve (AUC), the ROC curve aims to measure the model’s discrimination ability between the two defined classes in a binary classification problem (e.g., default vs non-default). Ideally, a higher AUC score signifies superior model performance in accurately distinguishing between the positive and negative classes.\nTest Mechanism: First, this script selects the target model and datasets that require binary classification. It then calculates the predicted probabilities for the test set, and uses this data, along with the true outcomes, to generate and plot the ROC curve. Additionally, it concludes a line signifying randomness (AUC of 0.5). The AUC score for the model’s ROC curve is also computed, presenting a numerical estimation of the model’s performance. If any Infinite values are detected in the ROC threshold, these are effectively eliminated. The resulting ROC curve, AUC score, and thresholds are consequently saved for future reference.\nSigns of High Risk: - A high risk is potentially linked to the model’s performance if the AUC score drops below or nears 0.5. - Another warning sign would be the ROC curve lying closer to the line of randomness, indicating no discriminative ability. - For the model to be deemed competent at its classification tasks, it is crucial that the AUC score is significantly above 0.5.\nStrengths: - This ROC Curve offers an inclusive visual depiction of a model’s discriminative power throughout all conceivable classification thresholds, unlike other metrics that solely disclose model performance at one fixed threshold. - Despite the proportions of the dataset, the AUC Score, which represents the entire ROC curve as a single data point, continues to be consistent, proving to be the ideal choice for such situations.\nLimitations: - The primary limitation is that this test is exclusively structured for binary classification tasks, thus limiting its application towards other model types. - Furthermore, its performance might be subpar with models that output probabilities highly skewed towards 0 or 1. - At the extreme, the ROC curve could reflect high performance even when the majority of classifications are incorrect, provided that the model’s ranking format is retained. This phenomenon is commonly termed the “Class Imbalance Problem”."
  },
  {
    "objectID": "tests/model_validation/sklearn/ClusterPerformance.html",
    "href": "tests/model_validation/sklearn/ClusterPerformance.html",
    "title": "ClusterPerformance",
    "section": "",
    "text": "ClusterPerformance\nEvaluates and compares a clustering model’s performance on training and testing datasets using multiple defined metrics.\nPurpose: This metric, ClusterPerformance, evaluates the performance of a clustering model on both the training and testing datasets. It assesses how well the model defines, forms, and distinguishes clusters of data.\nTest Mechanism: The metric is applied by first predicting the clusters of the training and testing datasets using the clustering model. Next, performance metrics, defined in the method metric_info(), are calculated against the true labels of the datasets. The results for each metric for both datasets are then collated and returned in a summarized table form listing each metric along with its corresponding train and test values.\nSigns of High Risk: - High discrepancy between the performance metric values on the training and testing datasets. This could signify problems such as overfitting or underfitting. - Low performance metric values on the training and testing datasets. There might be a problem with the model itself or the chosen hyperparameters. - If the model’s performance deteriorates consistently across different sets of metrics, this may suggest a broader issue with the model or the dataset.\nStrengths: - Tests the model’s performance on both the training and testing datasets, which helps to identify issues such as overfitting or underfitting. - Allows for a broad range of performance metrics to be used, thus providing a comprehensive evaluation of the model’s clustering capabilities. - Returns a summarized table, which makes it easy to compare the model’s performance across different metrics and datasets.\nLimitations: - The method metric_info() needs to be properly overridden in a subclass for this class to be used, and the metrics to be used must be manually defined. - The performance metrics are calculated on predicted cluster labels, so the metric may not capture the model’s performance well if the clusters are not well separated or if the model has difficulties with certain kinds of clusters. - Doesn’t consider the computational and time complexity of the model. While the model may perform well in terms of the performance metrics, it might be time or resource-intensive. This metric does not account for such scenarios. - Because the comparison is binary (train and test), it might not capture scenarios where the performance changes drastically under different circumstances or categories within the dataset."
  },
  {
    "objectID": "tests/model_validation/ragas/AnswerSimilarity.html",
    "href": "tests/model_validation/ragas/AnswerSimilarity.html",
    "title": "AnswerSimilarity",
    "section": "",
    "text": "AnswerSimilarity"
  },
  {
    "objectID": "tests/model_validation/ragas/ContextEntityRecall.html",
    "href": "tests/model_validation/ragas/ContextEntityRecall.html",
    "title": "ContextEntityRecall",
    "section": "",
    "text": "ContextEntityRecall"
  },
  {
    "objectID": "tests/model_validation/ragas/AspectCritique.html",
    "href": "tests/model_validation/ragas/AspectCritique.html",
    "title": "AspectCritique",
    "section": "",
    "text": "AspectCritique"
  },
  {
    "objectID": "tests/model_validation/ragas/AnswerRelevance.html",
    "href": "tests/model_validation/ragas/AnswerRelevance.html",
    "title": "AnswerRelevance",
    "section": "",
    "text": "AnswerRelevance"
  },
  {
    "objectID": "tests/model_validation/ragas/ContextRelevancy.html",
    "href": "tests/model_validation/ragas/ContextRelevancy.html",
    "title": "ContextRelevancy",
    "section": "",
    "text": "ContextRelevancy"
  },
  {
    "objectID": "tests/model_validation/ToxicityScore.html",
    "href": "tests/model_validation/ToxicityScore.html",
    "title": "ToxicityScore",
    "section": "",
    "text": "ToxicityScore"
  },
  {
    "objectID": "tests/model_validation/embeddings/CosineSimilarityDistribution.html",
    "href": "tests/model_validation/embeddings/CosineSimilarityDistribution.html",
    "title": "CosineSimilarityDistribution",
    "section": "",
    "text": "CosineSimilarityDistribution\nAssesses the similarity between predicted text embeddings from a model using a Cosine Similarity distribution histogram.\nPurpose: This metric is used to assess the degree of similarity between the embeddings produced by a text embedding model using Cosine Similarity. Cosine Similarity is a measure that calculates the cosine of the angle between two vectors. This metric is predominantly used in text analysis — in this case, to determine how closely the predicted text embeddings align with one another.\nTest Mechanism: The implementation starts by computing the cosine similarity between the predicted values of the model’s test dataset. These cosine similarity scores are then plotted on a histogram with 100 bins to visualize the distribution of the scores. The x-axis of the histogram represents the computed Cosine Similarity.\nSigns of High Risk:\n\nIf the cosine similarity scores cluster close to 1 or -1, it may indicate overfitting, as the model’s predictions are almost perfectly aligned. This could suggest that the model is not generalizable.\nA broad spread of cosine similarity scores across the histogram may indicate a potential issue with the model’s ability to generate consistent embeddings.\n\nStrengths:\n\nProvides a visual representation of the model’s performance which is easily interpretable.\nCan help identify patterns, trends, and outliers in the model’s alignment of predicted text embeddings.\nUseful in measuring the similarity between vectors in multi-dimensional space, important in the case of text embeddings.\n\nLimitations:\n\nOnly evaluates the similarity between outputs. It does not provide insight into the model’s ability to correctly classify or predict.\nCosine similarity only considers the angle between vectors and does not consider their magnitude. This can lead to high similarity scores for vectors with vastly different magnitudes but a similar direction.\nThe output is sensitive to the choice of bin number for the histogram. Different bin numbers could give a slightly altered perspective on the distribution of cosine similarity."
  },
  {
    "objectID": "tests/model_validation/embeddings/CosineSimilarityComparison.html",
    "href": "tests/model_validation/embeddings/CosineSimilarityComparison.html",
    "title": "CosineSimilarityComparison",
    "section": "",
    "text": "CosineSimilarityComparison"
  },
  {
    "objectID": "tests/model_validation/embeddings/TSNEComponentsPairwisePlots.html",
    "href": "tests/model_validation/embeddings/TSNEComponentsPairwisePlots.html",
    "title": "TSNEComponentsPairwisePlots",
    "section": "",
    "text": "TSNEComponentsPairwisePlots"
  },
  {
    "objectID": "tests/model_validation/embeddings/ClusterDistribution.html",
    "href": "tests/model_validation/embeddings/ClusterDistribution.html",
    "title": "ClusterDistribution",
    "section": "",
    "text": "ClusterDistribution\nAssesses the distribution of text embeddings across clusters produced by a model using KMeans clustering.\nPurpose: The purpose of this metric is to analyze the distribution of the clusters produced by a text embedding model. By dividing the text embeddings into different clusters, we can understand how the model is grouping or categorizing the text data. This aids in visualizing the organization and segregation of the data and thus gives an understanding of how the model is processing the data.\nTest Mechanism: The metric applies the KMeans clustering algorithm on the predictions made by the model on the testing dataset and divides the text embeddings into a pre-defined number of clusters. By default, this number is set to 5 but can be customized as per requirements. The output of this test is a histogram plot that shows the distribution of embeddings across these clusters.\nSigns of High Risk:\n\nIf the embeddings are skewed towards one or two clusters, that would indicate that the model is not effectively differentiating the various categories in the text data.\nUniform distribution of the embeddings across the clusters might show a lack of proper categorization.\n\nStrengths:\n\nGreat tool to visualize the text data categorization by the model. It provides a way to assess if the model is distinguishing the categories effectively or not.\nIt is flexible with the number of clusters (classes), so can be used on various types of data regardless of the number of categories.\n\nLimitations:\n\nThe success or failure of this test is based on visual interpretation, which might not be enough for making solid conclusions or determining the exact points of failure.\nIt assumes that the division of text embeddings across clusters should ideally be homogeneous, which might not always be the case depending on the nature of the text data.\nIt only applies to text embedding models, reducing its utility across various ML models.\nThis test uses the KMeans clustering algorithm, which assumes that clusters are convex and isotropic. Thus, this test may not work as intended if the true clusters in the data are not of this shape."
  },
  {
    "objectID": "tests/model_validation/embeddings/DescriptiveAnalytics.html",
    "href": "tests/model_validation/embeddings/DescriptiveAnalytics.html",
    "title": "DescriptiveAnalytics",
    "section": "",
    "text": "DescriptiveAnalytics\nEvaluates statistical properties of text embeddings in an ML model via mean, median, and standard deviation histograms.\n1. Purpose: This metric, Descriptive Analytics for Text Embeddings Models, is employed to comprehend the fundamental properties and statistical characteristics of the embeddings in a Machine Learning model. It measures the dimensionality as well as the statistical distributions of embedding values including the mean, median, and standard deviation.\n2. Test Mechanism: The test mechanism involves using the ‘DescriptiveAnalytics’ class provided in the code which includes the ’run function. This function computes three statistical measures - mean, median, and standard deviation of the test predictions from the model. It generates and caches three separate histograms showing the distribution of these measures. Each histogram visualizes the measure’s distribution across the embedding values. Therefore, the method does not utilize a grading scale or threshold; it is fundamentally a visual exploration and data exploration tool.\n3. Signs of High Risk:\n\nAbnormal patterns or values in the distributions of the statistical measures. This may include skewed distributions or a significant amount of outliers.\nVery high standard deviation values which indicate a high degree of variability in the data.\nThe mean and median values are vastly different, suggesting skewed data.\n\n4. Strengths:\n\nProvides a visual and quantifiable understanding of the embeddings’ statistical characteristics, allowing for a comprehensive evaluation.\nFacilitates the identification of irregular patterns and anomalous values that might indicate issues with the machine learning model.\nIt considers three key statistical measures (mean, median, and standard deviation), offering a more well-rounded understanding of the data.\n\n5. Limitations:\n\nThe method does not offer an explicit measure of model performance or accuracy, as it mainly focuses on understanding data properties.\nIt relies heavily on the visual interpretation of histograms. This could be subjective, and important patterns could be overlooked if not carefully reviewed.\nWhile it displays valuable information about the central tendency and spread of data, it does not provide information about correlations between different embedding dimensions."
  },
  {
    "objectID": "tests/model_validation/embeddings/StabilityAnalysisKeyword.html",
    "href": "tests/model_validation/embeddings/StabilityAnalysisKeyword.html",
    "title": "StabilityAnalysisKeyword",
    "section": "",
    "text": "StabilityAnalysisKeyword\nEvaluate robustness of embeddings models to keyword swaps on the test dataset\nThis tests expects a parameter keyword_dict that maps words to other words so that any instances of the key words in the test dataset will be replaced with the corresponding value.\nPurpose: This test metric is used to evaluate the robustness of text embedding machine learning models to keyword swaps. A keyword swap is a scenario where instances of certain specified keywords in the dataset are replaced with other specified words (usually synonyms). The purpose of this metric is to ensure that these models maintain performance stability even when the input data slightly deviates, imitating real-world variability.\nTest Mechanism: The test mechanism involves perturbation of the dataset used in testing the model. Each instance of a specific word found in the dataset is replaced with the corresponding word as specified in a keyword_dict’ mapping. The model is then re-run with the perturbed dataset and the results are compared with the non-perturbed dataset. This comparison quantifies the extent to which keyword swaps impact the model’s performance.\nSigns of High Risk: - A significant drop in model performance after keyword swaps indicates a high risk of model failure in real-world scenarios. - The model results being heavily reliant on specific word choices instead of capturing the context properly.\nStrengths: - This test provides a way to measure model robustness to small changes in input data, which reinforces its applicability and reliability in real-world scenarios. - This test encourages a model to understand the context of a sentence rather than memorizing specific words. - It helps to detect overfitting - a situation where a model performs well on training data but poorly on new or slightly altered data.\nLimitations: - It may not fully address semantic differences that can be introduced through keyword swaps. That is, the replacement words might not preserve the exact semantic meaning of the original words. - It only tests for changes in keywords (word-level alterations) and might not expose model limitations related to structural data changes. - It assumes that the provided ‘keyword_dict’ is an accurate representation of possible real-world variations, which might not always be the case."
  },
  {
    "objectID": "tests/model_validation/embeddings/StabilityAnalysisTranslation.html",
    "href": "tests/model_validation/embeddings/StabilityAnalysisTranslation.html",
    "title": "StabilityAnalysisTranslation",
    "section": "",
    "text": "StabilityAnalysisTranslation\nEvaluate robustness of embeddings models to noise introduced by translating the original text to another language and back.\nPurpose: The purpose of this test is to assess the robustness of text embeddings models under the influence of noise. The noise in this scenario is introduced by translating the original text into another language and then translating it back to the original language. Any significant changes in the model’s output between the original and translated-then-retranslated texts can be indicators of the model’s lack of robustness to noise.\nTest Mechanism: The test mechanism involves several steps:\n\nInitialize the Marian tokenizer and model for both source and target languages.\nTranslate the data from the source language to the target language.\nTranslate the translated data back into the source language.\nCompare the original data with the data that has been translated and back-translated to observe any significant changes.\n\nThe threshold of this test output would then be determined by the tolerance level of the model to these potentially noisy instances.\nSigns of High Risk:\n\nIf there large discrepancies between the original and double-translated text, this could indicate a high level of risk, signifying that the model is not robust to noise.\nIf a translation between languages does not closely maintain the meaning and context of the original language, it may suggest inadequate robustness against this type of noise.\n\nStrengths:\n\nThis metric is an effective way to assess the model’s sensitivity and robustness to language translation noise.\nThe use of translation as a means to introduce noise provides a realistic scenario which the model might encounter in real-world situations.\nThis metric extends beyond simple lexical changes, testing the model’s capacity to maintain semantic meaning under translational perturbations.\n\nLimitations:\n\nRelying solely on translation-related noise for robustness testing can overlook other types of noise not reflected in language translation, such as typographical errors, grammatical mistakes, or random word substitutions.\nPotential inaccuracies or discrepancies in the translation process itself might influence the resultant robustness score, rather than reflect an inherent failing of the model being tested.\nThe test is predominantly language-dependent, hence it might not fully capture the robustness of the model for languages with fewer resources or languages that are highly dissimilar to the source language."
  },
  {
    "objectID": "tests/model_validation/embeddings/EuclideanDistanceComparison.html",
    "href": "tests/model_validation/embeddings/EuclideanDistanceComparison.html",
    "title": "EuclideanDistanceComparison",
    "section": "",
    "text": "EuclideanDistanceComparison"
  },
  {
    "objectID": "tests/prompt_validation/Specificity.html",
    "href": "tests/prompt_validation/Specificity.html",
    "title": "Specificity",
    "section": "",
    "text": "Specificity\nEvaluates and scores the specificity of prompts provided to a Large Language Model (LLM), based on clarity, detail, and relevance.\nPurpose: The Specificity Test evaluates the clarity, precision, and effectiveness of the prompts provided to a Language Learning Model (LLM). It aims to ensure that the instructions embedded in a prompt are indisputably clear and relevant, thereby helping to yank out ambiguity and steer the LLM towards desired outputs. This level of specificity significantly affects the accuracy and relevance of LLM outputs.\nTest Mechanism: The Specificity Test employs an LLM to grade each prompt based on clarity, detail, and relevance parameters within a specificity scale that extends from 1 to 10. On this scale, prompts scoring equal to or more than a predefined threshold (set to 7 by default) pass the evaluation, while those scoring below this threshold fail it. Users can adjust this threshold as per their requirements.\nSigns of High Risk:\n\nPrompts scoring consistently below the established threshold\nVague or ambiguous prompts that do not provide clear direction to the LLM\nOverly verbose prompts that may confuse the LLM instead of providing clear guidance\n\nStrengths:\n\nEnables precise and clear communication with the LLM to achieve desired outputs\nServes as a crucial means to measure the effectiveness of prompts\nHighly customizable, allowing users to set their threshold based on specific use cases\n\nLimitations:\n\nThis test doesn’t consider the content comprehension capability of the LLM\nHigh specificity score doesn’t guarantee a high-quality response from the LLM, as the model’s performance is also dependent on various other factors\nStriking a balance between specificity and verbosity can be challenging, as overly detailed prompts might confuse or mislead the model."
  },
  {
    "objectID": "tests/prompt_validation/Bias.html",
    "href": "tests/prompt_validation/Bias.html",
    "title": "Bias",
    "section": "",
    "text": "Bias\nEvaluates bias in a Large Language Model based on the order and distribution of exemplars in a prompt.\nPurpose: The Bias Evaluation test calculates if and how the order and distribution of exemplars (examples) in a few-shot learning prompt affect the output of a Large Language Model (LLM). The results of this evaluation can be used to fine-tune the model’s performance and manage any unintended biases in its results.\nTest Mechanism: This test uses two checks:\n\nDistribution of Exemplars: The number of positive vs. negative examples in a prompt is varied. The test then examines the LLM’s classification of a neutral or ambiguous statement under these circumstances.\nOrder of Exemplars: The sequence in which positive and negative examples are presented to the model is modified. Their resultant effect on the LLM’s response is studied.\n\nFor each test case, the LLM grades the input prompt on a scale of 1 to 10. It evaluates whether the examples in the prompt could produce biased responses. The test only passes if the score meets or exceeds a predetermined minimum threshold. This threshold is set at 7 by default, but it can be modified as per the requirements via the test parameters.\nSigns of High Risk:\n\nA skewed result favoring either positive or negative responses may suggest potential bias in the model. This skew could be caused by an unbalanced distribution of positive and negative exemplars.\nIf the score given by the model is less than the set minimum threshold, it might indicate a risk of high bias and hence poor performance.\n\nStrengths:\n\nThis test provides a quantitative measure of potential bias, providing clear guidelines for developers about whether their Large Language Model (LLM) contains significant bias.\nIt’s useful in evaluating the impartiality of the model based on the distribution and sequence of examples.\nThe flexibility to adjust the minimum required threshold allows tailoring this test to stricter or more lenient bias standards.\n\nLimitations:\n\nThe test may not pick up on more subtle forms of bias or biases that are not directly related to the distribution or order of exemplars.\nThe test’s effectiveness will decrease if the quality or balance of positive and negative exemplars is not representative of the problem space the model is intended to solve.\nThe use of a grading mechanism to gauge bias may not be entirely accurate in every case, particularly when the difference between threshold and score is narrow."
  },
  {
    "objectID": "tests/prompt_validation/Delimitation.html",
    "href": "tests/prompt_validation/Delimitation.html",
    "title": "Delimitation",
    "section": "",
    "text": "Delimitation\nEvaluates the proper use of delimiters in prompts provided to Large Language Models.\nPurpose: This test, dubbed the “Delimitation Test”, is engineered to assess whether prompts provided to the Language Learning Model (LLM) correctly use delimiters to mark different sections of the input. Well-delimited prompts simplify the interpretation process for LLM, ensuring responses are precise and accurate.\nTest Mechanism: The test employs an LLM to examine prompts for appropriate use of delimiters such as triple quotation marks, XML tags, and section titles. Each prompt is assigned a score from 1 to 10 based on its delimitation integrity. Those with scores equal to or above the preset threshold (which is 7 by default, although it can be adjusted as necessary) pass the test.\nSigns of High Risk:\n\nThe test identifies prompts where a delimiter is missing, improperly placed, or incorrect, which can lead to misinterpretation by the LLM.\nA high-risk scenario may involve complex prompts with multiple tasks or diverse data where correct delimitation is integral to understanding.\nLow scores (below the threshold) are a clear indicator of high risk.\n\nStrengths:\n\nThis test ensures clarity in the demarcation of different components of given prompts.\nIt helps reduce ambiguity in understanding prompts, particularly for complex tasks.\nScoring allows for quantified insight into the appropriateness of delimiter usage, aiding continuous improvement.\n\nLimitations:\n\nThe test only checks for the presence and placement of delimiter, not whether the correct delimiter type is used for the specific data or task.\nIt may not fully reveal the impacts of poor delimitation on LLM’s final performance.\nDepending on the complexity of the tasks and prompts, the preset score threshold may not be refined enough, requiring regular manual adjustment."
  },
  {
    "objectID": "tests/prompt_validation/NegativeInstruction.html",
    "href": "tests/prompt_validation/NegativeInstruction.html",
    "title": "NegativeInstruction",
    "section": "",
    "text": "NegativeInstruction\nEvaluates and grades the use of affirmative, proactive language over negative instructions in LLM prompts.\nPurpose: The Negative Instruction test is utilized to scrutinize the prompts given to a Large Language Model (LLM). The objective is to ensure these prompts are expressed using proactive, affirmative language. The focus is on instructions indicating what needs to be done rather than what needs to be avoided, thereby guiding the LLM more efficiently towards the desired output.\nTest Mechanism: An LLM is employed to evaluate each prompt. The prompt is graded based on its use of positive instructions with scores ranging between 1-10. This grade reflects how effectively the prompt leverages affirmative language while shying away from negative or restrictive instructions. A prompt that attains a grade equal to or above a predetermined threshold (7 by default) is regarded as adhering effectively to the best practices of positive instruction. This threshold can be custom-tailored through the test parameters.\nSigns of High Risk:\n\nLow score obtained from the LLM analysis, indicating heavy reliance on negative instructions in the prompts.\nFailure to surpass the preset minimum threshold.\nThe LLM generates ambiguous or undesirable outputs as a consequence of the negative instructions used in the prompt.\n\nStrengths:\n\nEncourages the usage of affirmative, proactive language in prompts, aiding in more accurate and advantageous model responses.\nThe test result provides a comprehensible score, helping to understand how well a prompt follows the positive instruction best practices.\n\nLimitations:\n\nDespite an adequate score, a prompt could still be misleading or could lead to undesired responses due to factors not covered by this test.\nThe test necessitates an LLM for evaluation, which might not be available or feasible in certain scenarios.\nA numeric scoring system, while straightforward, may oversimplify complex issues related to prompt designing and instruction clarity.\nThe effectiveness of the test hinges significantly on the predetermined threshold level, which can be subjective and may need to be adjusted according to specific use-cases."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#prerequisites",
    "href": "training/training-for-model-risk-governance.html#prerequisites",
    "title": "Training for Model Risk Governance",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo try out this training module, you need to have been onboarded onto the ValidMind training environment.\nLog into the ValidMind Platform UI to check your access:\n\n\n\nLog in\n\n\n\nBe sure to return to this page afterwards."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#model-tracking",
    "href": "training/training-for-model-risk-governance.html#model-tracking",
    "title": "Training for Model Risk Governance",
    "section": "Model tracking",
    "text": "Model tracking\nGet insights into validation findings, highlighting major issues."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#compliance-review",
    "href": "training/training-for-model-risk-governance.html#compliance-review",
    "title": "Training for Model Risk Governance",
    "section": "Compliance review",
    "text": "Compliance review\nEnsure adherence to regulatory requirements and internal policies."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#remediation-tracking",
    "href": "training/training-for-model-risk-governance.html#remediation-tracking",
    "title": "Training for Model Risk Governance",
    "section": "Remediation tracking",
    "text": "Remediation tracking\nMonitor actions taken to address validation findings."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#risk-assessment",
    "href": "training/training-for-model-risk-governance.html#risk-assessment",
    "title": "Training for Model Risk Governance",
    "section": "Risk assessment",
    "text": "Risk assessment\nSummarize risks, including model limitations and needed adjustments."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#executive-summaries",
    "href": "training/training-for-model-risk-governance.html#executive-summaries",
    "title": "Training for Model Risk Governance",
    "section": "Executive summaries",
    "text": "Executive summaries\nGet overviews for validation activities and risk exposure."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#whats-next",
    "href": "training/training-for-model-risk-governance.html#whats-next",
    "title": "Training for Model Risk Governance",
    "section": "What’s next",
    "text": "What’s next\nFind your next learning resource: ValidMind training\n\n\n\nValidMind Academy | Home"
  },
  {
    "objectID": "training/training-overview.html",
    "href": "training/training-overview.html",
    "title": "Welcome to ValidMind Academy",
    "section": "",
    "text": "Gain hands-on experience and explore what ValidMind has to offer with our training environment.\nThe training environment mimics a production environment and includes comprehensive resources such as notebooks with sample code you can run, sample models registered in the model inventory, and draft documentation and validation reports."
  },
  {
    "objectID": "training/training-overview.html#onboarding",
    "href": "training/training-overview.html#onboarding",
    "title": "Welcome to ValidMind Academy",
    "section": "Onboarding",
    "text": "Onboarding\nStep-by-step instructions:\n\nGet your access credentials from ValidMind.\nIf you haven’t reached out to us yet, email info@validmind.ai to enquire about access to the training environment.\n\n\n\nWhen you receive your access credentials, make sure you can log in:\n\nValidMind Jupyter Hub: https://jupyterhub.validmind.ai/\nValidMind Platform UI: https://app.prod.validmind.ai\n\nJoin a kick-off session with ValidMind to get a free instructor-led hands-on demo.\nAfter we have received your request to try out the training environment, we will contact you to set up a kick-off session.\nTry the training environment: Getting started\n\n\n\n\n\n\n\nFor testing and evaluation purposes only\n\n\n\nDo not upload proprietary information — the training environment environment is not to be used for production. If you have questions about testing in the training environment, please provide feedback or ask for help.\n\n\n\nSample models\nSample models registered in the model inventory include:\n\nHousing prices prediction model\nCredit risk scorecard model\nCustomer churn prediction model\nInterest rate time series forecasting model\nLarge language model (LLM) demo application\n\nPlease note that we make updates to the models and datasets available in the training environment from time to time to provide you with our latest features.\n\n\nSample notebooks\nEach interactive notebook includes the sample code needed to automatically document a model:\n\n\n\nNotebook\nInventory model\n\n\n\n\nQuickstart for model documentationGets you started with the basic process of documenting models with ValidMind, from the developer framework to the platform UI.\n[Demo] Customer Churn Model\n\n\nValidMind introduction for model developersAs a model developer, learn how the end-to-end documentation process works based on common scenarios you encounter in model development settings.\n[Demo] Customer Churn Model\n\n\nDocument an application scorecard modelGuides you through building and documenting an application scorecard model using the Lending Club sample dataset from Kaggle.\n[Demo] Credit Risk Model\n\n\nPrompt validation for large language models (LLMs)Run and document prompt validation tests for a large language model (LLM) specialized in sentiment analysis for financial news.\n[Demo] Foundation Model - Text Sentiment Analysis"
  },
  {
    "objectID": "training/training-overview.html#getting-started",
    "href": "training/training-overview.html#getting-started",
    "title": "Welcome to ValidMind Academy",
    "section": "Getting started",
    "text": "Getting started\nWhat you try out first in the training environment depends on your interests:\n\n\nAutomated model testing & documentation\n\nExplore the sample notebooks\nRun tests and test suites\nGenerate model documentation\n\n\n\nValidMind Jupyter Hub\n\n\n\n\n\nModel risk management & governance\n\nExplore the validation report experience\nAssess compliance and link evidence\nView reports to identify risk areas\n\n\n\n\nValidMind Platform UI"
  },
  {
    "objectID": "training/training-overview.html#provide-feedback-or-get-help",
    "href": "training/training-overview.html#provide-feedback-or-get-help",
    "title": "Welcome to ValidMind Academy",
    "section": "Provide feedback or get help",
    "text": "Provide feedback or get help\n\nJoin our Slack community\nHave feedback or questions? We sponsor a Slack community where you can provide feedback or ask questions: Join Our Community Slack.\nThere is a dedicated channel for support: #community-support.\nOur growing Slack community is not just for our products but also aims to foster discussions between AI risk practitioners and those involved in model risk management (MRM). Feel free to take a look around the other channels that are available and stay a while.\n\n\nGet support\nTo get help from a human, send an email to support@validmind.ai."
  },
  {
    "objectID": "training/training-overview.html#related-topics",
    "href": "training/training-overview.html#related-topics",
    "title": "Welcome to ValidMind Academy",
    "section": "Related topics",
    "text": "Related topics\n\nGet started with the ValidMind Developer Framework\nWorking with model documentation\nPreparing validation reports"
  },
  {
    "objectID": "releases/2024-jan-18/highlights.html",
    "href": "releases/2024-jan-18/highlights.html",
    "title": "January 18, 2024",
    "section": "",
    "text": "This release introduces a new dark mode to the ValidMind Platform UI, along with new user and template management features, other enhancements, and bug fixes.\n\n\n\n\n\n\nYou now have the option to set your color theme preference for the ValidMind UI. The theme can be set by toggling Enable Dark Mode in your profile settings.\nHere is how the available themes look side by side:\n\n\n\n\n\n\nLight mode\n\n\n\n\n\n\n\nDark mode\n\n\n\n\nThe selected theme setting is saved to your user profile and applied across all devices where you log in.\n\n\n\n\n\n\n\nWe continue to make easier to set up and administer ValidMind for members in your organization. In this release, we are introducing new user and group management features that are available right in the platform UI.\nThe new user management features are available under Settings &gt; User Directory:\n\nThese features enable you to:\n\nInvite members of your organization by email\nDisplay pending user invitations\nGet notifications for successful or failed user invitations\n\nTry it in the platform UI: User directory\nThe new group management features are available under Settings &gt; Groups:\n\nThese features enable you to:\n\nAssign permissions for multiple users at once\nView existing group members\nAdd or remove group members\n\nTry it in the platform UI: Groups\n\n\n\n\nNew template management features, available under Settings &gt; Templates, enable you to work with templates more efficiently:\n\nYou use these new template management features to:\n\nSee which models use which template, and which version of that template.\nNavigate from templates to models.\nGet direct links for templates that you can share with others.\nNavigate templates more easily by outline sections that you can expand instead of having to parse JSON code.\nEdit templates to create a new template versions with a visual comparison of the differences.\nDuplicate templates and modify them in order to create additional templates for your use cases.\n\nTry it in the ValidMind UI: Templates"
  },
  {
    "objectID": "releases/2024-jan-18/highlights.html#release-highlights",
    "href": "releases/2024-jan-18/highlights.html#release-highlights",
    "title": "January 18, 2024",
    "section": "",
    "text": "This release introduces a new dark mode to the ValidMind Platform UI, along with new user and template management features, other enhancements, and bug fixes.\n\n\n\n\n\n\nYou now have the option to set your color theme preference for the ValidMind UI. The theme can be set by toggling Enable Dark Mode in your profile settings.\nHere is how the available themes look side by side:\n\n\n\n\n\n\nLight mode\n\n\n\n\n\n\n\nDark mode\n\n\n\n\nThe selected theme setting is saved to your user profile and applied across all devices where you log in.\n\n\n\n\n\n\n\nWe continue to make easier to set up and administer ValidMind for members in your organization. In this release, we are introducing new user and group management features that are available right in the platform UI.\nThe new user management features are available under Settings &gt; User Directory:\n\nThese features enable you to:\n\nInvite members of your organization by email\nDisplay pending user invitations\nGet notifications for successful or failed user invitations\n\nTry it in the platform UI: User directory\nThe new group management features are available under Settings &gt; Groups:\n\nThese features enable you to:\n\nAssign permissions for multiple users at once\nView existing group members\nAdd or remove group members\n\nTry it in the platform UI: Groups\n\n\n\n\nNew template management features, available under Settings &gt; Templates, enable you to work with templates more efficiently:\n\nYou use these new template management features to:\n\nSee which models use which template, and which version of that template.\nNavigate from templates to models.\nGet direct links for templates that you can share with others.\nNavigate templates more easily by outline sections that you can expand instead of having to parse JSON code.\nEdit templates to create a new template versions with a visual comparison of the differences.\nDuplicate templates and modify them in order to create additional templates for your use cases.\n\nTry it in the ValidMind UI: Templates"
  },
  {
    "objectID": "releases/2024-jan-18/highlights.html#enhancements",
    "href": "releases/2024-jan-18/highlights.html#enhancements",
    "title": "January 18, 2024",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nStakeholder roles for projects: To simplify the identification of roles that stakeholders hold for model documentation, stakeholders are now categorized by role type. This includes groupings for Owners, Developers, and Validators.\n\n\n\n\n\n\n\n\nUpgraded search experience. We’ve updated the component in our web application responsible for the search command bar interface. The new version brings enhanced search performance, particularly for multi-word queries."
  },
  {
    "objectID": "releases/2024-jan-18/highlights.html#bug-fixes",
    "href": "releases/2024-jan-18/highlights.html#bug-fixes",
    "title": "January 18, 2024",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nText highlighting for comments. We fixed text highlighting associated with comments so that text is more legible in both light and dark themes. Additionally, we fixed the divider background on AI content generation and made some changes to empty text block contents.\n\n\n\nDynamic index display in templates. We fixed an issue where the internal order and index properties were incorrectly exposed in the template editor. These properties have no effect during template editing as they are dynamically generated. The fix now prevents these properties from being saved back to the template.\n\n\n\nActivity links now redirect users to the relevant section. We fixed an issue where clicking on recent activity items related to comments or test descriptions inside a test- driven block would take the user to a non-existent page. This functionality has been corrected by redirecting the user to the section that a content block belongs to.\n\n\n\nSearch experience fixes. We implemented multiple fixes to the search functionality in the platform UI:\n\nLinking from recent activity items has been corrected to ensure accurate navigation.\nRouting to documentation pages when a search result is associated with text in a test-driven block now works as expected.\nSearch now excludes results for content_id that are no longer part of the documentation.\nSearch functionality is now available from project overview pages. Previously, search only worked when invoked from documentation pages.\n\n\n\n\nIssues with the user selection component. We fixed the following UI/UX issues with the user picker interface component:\n\nClicking on X now properly clears the current search input and dismisses the user list.\nClicking on Cancel now properly clears out any unsaved users from the list.\nEntering text now correctly filters users that match the search criteria.\nA new ‘micro search’ utility improves user matching.\n\nAdditionally, we updated the business unit selector component to be a simple dropdown.\n\n\n\nDisappearing status updates on dashboard. We fixed an issue where the Recent activity widget would become unavailable when clicking on Status updates. We now show an empty state element when there is no data to show."
  },
  {
    "objectID": "releases/2024-jan-18/highlights.html#how-to-upgrade",
    "href": "releases/2024-jan-18/highlights.html#how-to-upgrade",
    "title": "January 18, 2024",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html",
    "href": "releases/2024-jan-26/highlights.html",
    "title": "January 26, 2024",
    "section": "",
    "text": "This release includes numerous improvements to the ValidMind Developer Framework, including new features for model and dataset initialization, easier testing, support for additional inputs and the Azure OpenAI API, updated notebooks, bug fixes, deprecations, and much more.\n\n\n\n\n\nWhen initializing a model, you can now pass a dataset with pre-computed model predictions if they are available. By default, if no prediction column is specified when calling init_model, the ValidMind Developer Framework will compute the model predictions on the entire dataset.\nTo illustrate how passing a dataset that includes a prediction column can help, consider the following example without a prediction column:\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\nInternally, this example invokes the predict() method of the model for the training and test datasets when the model is initialized. This approach can be problematic with large datasets: init_model can simply take too long to compute.\nYou can now avoid this issue by providing a dataset with a column containing pre-computed predictions, similar to the following example. If init_model detects this column, it will not generate model predictions at runtime.\n\n\n\nx1\nx2\n…\ntarget_column\nprediction_column\n\n\n\n\n0.1\n0.2\n…\n0\n0\n\n\n0.2\n0.4\n1…\n1\n1\n\n\n\nUsage example with a prediction column:\nvm.init_dataset(\n     dataset=df,\n     feature_columns=[...],\n     target_column= ...,\n     extra_columns={\n         prediction_column: 'NAME-OF-PREDICTION-COLUMN',\n    },\n)\n\n\n\n\nWhen initializing a dataset, the new feature_columns argument lets you specify a list of feature names for prediction to improve efficiency. Internally, the function filters the dataset to retain only these specified features for prediction-related tasks, leaving the remaining dataset available for other purposes, such as segmentation.\nThis improvement replaces the existing behavior of init_dataset, which loaded the entire dataset, incorporating all available features for prediction tasks. While this approach worked well, it could impose limitations when generating segmented metrics and proved somewhat inefficient with large datasets containing numerous features, of which only a small subset were relevant for prediction.\nUsage example:\nfeature_columns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns\n)\nA new notebook illustrates how you can configure these dataset features:\n\nHow to utilize the feature_columns parameter when initizalizing validmind datasets and model objects\nHow feature_columns can be used to report by segment\n\nTry it: Configuring and Using Dataset Features\n\n\n\n\nThe run_documentation_tests() function, used to collect and run all the tests associated with a template, now supports running multiple sections at a time. This means that you no longer need to call the same function twice for two different sections, reducing the potential for errors and enabling you to use a single config object. The previous behavior was to allow running only one section at a time. This change maintains backward compatibility with the existing syntax, requiring no updates to your code.\nExisting example usage: Multiple function calls are needed to run multiple sections\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    section=\"section_1\",\n    config={\n        \"validmind.tests.data_validation.ClassImbalance\": ...\n    } \n)\n\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    section=\"section_2\",\n    config={\n        \"validmind.tests.data_validation.Duplicates\": ...\n    } \n)\nNew example usage: A single function call runs multiple sections\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    section=[\"section_1\", \"section_2\"],\n    config={\n        \"validmind.tests.data_validation.ClassImbalance\": ...,\n        \"validmind.tests.data_validation.Duplicates\": ...\n    } \n)\nTry it: Running Individual Documentation Sections\n\n\n\n\nThe ValidMind Developer Framework now supports passing custom inputs as an inputs dictionary when running individual tests or test suites. This support replaces the standard inputs dataset, model, and models, which are now deprecated.\nNew recommended syntax for passing inputs:\ntest_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_dataset,\n        \"model\": vm_model,\n    },\n)\n\nTo make it easier for you to adopt custom inputs, we have updated our how-to notebooks and code samples to use the new recommended syntax:\n\nHow-to notebooks, including:\n\nIntegrate An External Test Provider\nConfiguring And Using Dataset Features  \n\nCode samples, including:\n\nNLP and LLM models\nRegression models\nTime series models\n\n\nAlso check Standard inputs are deprecated."
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html#release-highlights",
    "href": "releases/2024-jan-26/highlights.html#release-highlights",
    "title": "January 26, 2024",
    "section": "",
    "text": "This release includes numerous improvements to the ValidMind Developer Framework, including new features for model and dataset initialization, easier testing, support for additional inputs and the Azure OpenAI API, updated notebooks, bug fixes, deprecations, and much more.\n\n\n\n\n\nWhen initializing a model, you can now pass a dataset with pre-computed model predictions if they are available. By default, if no prediction column is specified when calling init_model, the ValidMind Developer Framework will compute the model predictions on the entire dataset.\nTo illustrate how passing a dataset that includes a prediction column can help, consider the following example without a prediction column:\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\nInternally, this example invokes the predict() method of the model for the training and test datasets when the model is initialized. This approach can be problematic with large datasets: init_model can simply take too long to compute.\nYou can now avoid this issue by providing a dataset with a column containing pre-computed predictions, similar to the following example. If init_model detects this column, it will not generate model predictions at runtime.\n\n\n\nx1\nx2\n…\ntarget_column\nprediction_column\n\n\n\n\n0.1\n0.2\n…\n0\n0\n\n\n0.2\n0.4\n1…\n1\n1\n\n\n\nUsage example with a prediction column:\nvm.init_dataset(\n     dataset=df,\n     feature_columns=[...],\n     target_column= ...,\n     extra_columns={\n         prediction_column: 'NAME-OF-PREDICTION-COLUMN',\n    },\n)\n\n\n\n\nWhen initializing a dataset, the new feature_columns argument lets you specify a list of feature names for prediction to improve efficiency. Internally, the function filters the dataset to retain only these specified features for prediction-related tasks, leaving the remaining dataset available for other purposes, such as segmentation.\nThis improvement replaces the existing behavior of init_dataset, which loaded the entire dataset, incorporating all available features for prediction tasks. While this approach worked well, it could impose limitations when generating segmented metrics and proved somewhat inefficient with large datasets containing numerous features, of which only a small subset were relevant for prediction.\nUsage example:\nfeature_columns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns\n)\nA new notebook illustrates how you can configure these dataset features:\n\nHow to utilize the feature_columns parameter when initizalizing validmind datasets and model objects\nHow feature_columns can be used to report by segment\n\nTry it: Configuring and Using Dataset Features\n\n\n\n\nThe run_documentation_tests() function, used to collect and run all the tests associated with a template, now supports running multiple sections at a time. This means that you no longer need to call the same function twice for two different sections, reducing the potential for errors and enabling you to use a single config object. The previous behavior was to allow running only one section at a time. This change maintains backward compatibility with the existing syntax, requiring no updates to your code.\nExisting example usage: Multiple function calls are needed to run multiple sections\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    section=\"section_1\",\n    config={\n        \"validmind.tests.data_validation.ClassImbalance\": ...\n    } \n)\n\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    section=\"section_2\",\n    config={\n        \"validmind.tests.data_validation.Duplicates\": ...\n    } \n)\nNew example usage: A single function call runs multiple sections\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    section=[\"section_1\", \"section_2\"],\n    config={\n        \"validmind.tests.data_validation.ClassImbalance\": ...,\n        \"validmind.tests.data_validation.Duplicates\": ...\n    } \n)\nTry it: Running Individual Documentation Sections\n\n\n\n\nThe ValidMind Developer Framework now supports passing custom inputs as an inputs dictionary when running individual tests or test suites. This support replaces the standard inputs dataset, model, and models, which are now deprecated.\nNew recommended syntax for passing inputs:\ntest_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_dataset,\n        \"model\": vm_model,\n    },\n)\n\nTo make it easier for you to adopt custom inputs, we have updated our how-to notebooks and code samples to use the new recommended syntax:\n\nHow-to notebooks, including:\n\nIntegrate An External Test Provider\nConfiguring And Using Dataset Features  \n\nCode samples, including:\n\nNLP and LLM models\nRegression models\nTime series models\n\n\nAlso check Standard inputs are deprecated."
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html#enhancements",
    "href": "releases/2024-jan-26/highlights.html#enhancements",
    "title": "January 26, 2024",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nSupport for Azure OpenAI Service. The ValidMind Developer Framework now supports running LLM-powered tests with the Azure OpenAI Service via API, in addition to the previously supported OpenAI API. To work with Azure OpenAI API endpoints, you need to set the following environment variables before calling vm.init():\n\nAZURE_OPENAI_KEY: API key for authentication\nAZURE_OPENAI_ENDPOINT: API endpoint URL\nAZURE_OPENAI_MODEL: Specifies the language model or service to use\nAZURE_OPENAI_VERSION (optional): Allows specifying a specific version of the service if available\n\n\nTo learn more about configuring Azure OpenAI Service, see Authentication in the official Microsoft documentation."
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html#bug-fixes",
    "href": "releases/2024-jan-26/highlights.html#bug-fixes",
    "title": "January 26, 2024",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nFixed support for OpenAI library &gt;=1.0. We have updated our demonstration notebooks for large language models (LLMs) to provide the correct support for openai &gt;= 1.0.0. Previously, some notebooks were using an older version of the OpenAI client API."
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html#deprecations",
    "href": "releases/2024-jan-26/highlights.html#deprecations",
    "title": "January 26, 2024",
    "section": "Deprecations",
    "text": "Deprecations\n\n\nStandard inputs are deprecated.  The ValidMind Developer Framework now supports passing custom inputs as an inputs dictionary when running individual tests or test suites. As a result, the standard inputs dataset, model, and models are deprecated and might be removed in a future release. If you are a developer, you should update your code to use the new, recommended syntax.\nDeprecated legacy usage for passing inputs:\ntest_suite = vm.run_documentation_tests(\n    dataset=vm_dataset,\n    model=vm_model\n)\nNew recommended usage for passing inputs:\ntest_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_dataset,\n        \"model\": vm_model,\n    },\n)\nAlso check Support for custom inputs.\n\n\n\nRemoved deprecated high-level API methods: The API methods run_template and run_test_plan had been deprecated previously. They have now been removed from the ValidMind Developer Framework.\nIf you are a developer, you should update your code to use the recommended high-level API methods:\n\nrun_template (removed): Use vm.run_documentation_tests\nrun_test_plan (removed) : Use vm.run_test_suite"
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html#user-guide",
    "href": "releases/2024-jan-26/highlights.html#user-guide",
    "title": "January 26, 2024",
    "section": "User guide",
    "text": "User guide\nUpdated Python requirements. We have updated our user guide to clarify the Python versions supported by the ValidMind Developer Framework. We now support Python ≧3.8 and &lt;3.11."
  },
  {
    "objectID": "releases/2024-jan-26/highlights.html#how-to-upgrade",
    "href": "releases/2024-jan-26/highlights.html#how-to-upgrade",
    "title": "January 26, 2024",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2024-feb-14/highlights.html",
    "href": "releases/2024-feb-14/highlights.html",
    "title": "February 14, 2024",
    "section": "",
    "text": "Documentation templates have been updated to support logging each metric run as a unique result, making it possible to run the same test across different datasets or models. To make use of this new feature, you simply add a unique result_id identifier as a suffix to a content_id identifier in the content block definition of a metric or test content type.\nFor example, the following content blocks with the suffixes training_data and test_data enable you to log two individual results for the same test validmind.data_validation.Skewness:\n- content_type: test\n  content_id: validmind.data_validation.Skewness:training_data\n- content_type: metric\n  content_id: validmind.data_validation.Skewness:test_data\nYou can configure each of these unique content_id identifiers by passing the appropriate config and inputs in run_documentation_tests() or run_test(). For example, to configure two separate tests for Skewness using different datasets and parameters:\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.Skewness:training_data\",\n    params={\n        \"max_threshold\": 1\n    },\n    dataset=vm_train_ds,\n)\ntest.log()\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.Skewness:test_data\",\n    params={\n        \"max_threshold\": 1.5\n    },\n    dataset=vm_test_ds\n)\ntest.log()\nTry it yourself: Rendering more than one unique result for the same metric\n\n\n\n\nThe run_documentation_tests() function has been updated to allow passing both test inputs and params via the config parameter.\nPreviously, config could already pass params to each test that you declare. In this example, the test SomeTest receives a custom value for the param min_threshold:\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    config={\n        \"validmind.data_validation.SomeTest\": {\n            \"min_threshold\": 1\n        }\n    }\n)\nWith the updated function, config can now pass both params and inputs to each declared test. For example, to specify what model should be passed to each individual test instance:\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        \"dataset\": vm_dataset,\n        \"model\": xgb_model\n    },\n    config = {\n        \"validmind..model_validation.Accuracy:xgb_model\": {\n            \"params\": { threshold: 0.5 },\n            \"inputs\": { \"model\": xgb_model }\n        },\n        \"validmind..model_validation.Accuracy:lr_model\": {\n            \"params\": { threshold: 0.3 },\n            \"inputs\": { \"model\": lr_model }\n        },\n    }\n)\nHere, the top-level inputs parameter acts as a global inputs parameter, and the individual tests can customize what they see as the input model via their own config parameters.\n\n\n\n\nTo enable model developers to know what task types and tags are available to filter on, we have made some updates to our developer framework:\n\nNew list_task_types() and list_tags() endpoints enable you to list all available task_type and tags across all test classes\nNew list_tasks_and_tags() endpoint enables you to list which tags are associated to which task_type\n\n\n\nTry it: Exploring Tests in the developer framework\n\n\n\n\nWe have added a new feature that tracks which datasets and models are used when running tests. Now, when you initialize datasets or models with vm.init_dataset() and vm.init_model(), we link those inputs with the test results they generate. This makes it clear which inputs were used for each result, improving transparency and making it easier to understand test outcomes. This update does not require any changes to your code and works with existing init methods.\n\n\n\n\n\n\n\nWe are now showing the name of the user who ran the action instead of a generic “Developer Framework” name whenever you generate documentation:\n\n\nBefore\n\n\n\n\nAfter\n\n\n\n\n\n\n\n\nWe simplified the instructions for getting started with the ValidMind Developer Framework in the ValidMind Platform UI. These instructions tell you how to use the code snippet for your model documentation with your own model or with one of our code samples."
  },
  {
    "objectID": "releases/2024-feb-14/highlights.html#release-highlights",
    "href": "releases/2024-feb-14/highlights.html#release-highlights",
    "title": "February 14, 2024",
    "section": "",
    "text": "Documentation templates have been updated to support logging each metric run as a unique result, making it possible to run the same test across different datasets or models. To make use of this new feature, you simply add a unique result_id identifier as a suffix to a content_id identifier in the content block definition of a metric or test content type.\nFor example, the following content blocks with the suffixes training_data and test_data enable you to log two individual results for the same test validmind.data_validation.Skewness:\n- content_type: test\n  content_id: validmind.data_validation.Skewness:training_data\n- content_type: metric\n  content_id: validmind.data_validation.Skewness:test_data\nYou can configure each of these unique content_id identifiers by passing the appropriate config and inputs in run_documentation_tests() or run_test(). For example, to configure two separate tests for Skewness using different datasets and parameters:\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.Skewness:training_data\",\n    params={\n        \"max_threshold\": 1\n    },\n    dataset=vm_train_ds,\n)\ntest.log()\n\ntest = vm.tests.run_test(\n    test_id=\"validmind.data_validation.Skewness:test_data\",\n    params={\n        \"max_threshold\": 1.5\n    },\n    dataset=vm_test_ds\n)\ntest.log()\nTry it yourself: Rendering more than one unique result for the same metric\n\n\n\n\nThe run_documentation_tests() function has been updated to allow passing both test inputs and params via the config parameter.\nPreviously, config could already pass params to each test that you declare. In this example, the test SomeTest receives a custom value for the param min_threshold:\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        ...\n    },\n    config={\n        \"validmind.data_validation.SomeTest\": {\n            \"min_threshold\": 1\n        }\n    }\n)\nWith the updated function, config can now pass both params and inputs to each declared test. For example, to specify what model should be passed to each individual test instance:\nfull_suite = vm.run_documentation_tests(\n    inputs = {\n        \"dataset\": vm_dataset,\n        \"model\": xgb_model\n    },\n    config = {\n        \"validmind..model_validation.Accuracy:xgb_model\": {\n            \"params\": { threshold: 0.5 },\n            \"inputs\": { \"model\": xgb_model }\n        },\n        \"validmind..model_validation.Accuracy:lr_model\": {\n            \"params\": { threshold: 0.3 },\n            \"inputs\": { \"model\": lr_model }\n        },\n    }\n)\nHere, the top-level inputs parameter acts as a global inputs parameter, and the individual tests can customize what they see as the input model via their own config parameters.\n\n\n\n\nTo enable model developers to know what task types and tags are available to filter on, we have made some updates to our developer framework:\n\nNew list_task_types() and list_tags() endpoints enable you to list all available task_type and tags across all test classes\nNew list_tasks_and_tags() endpoint enables you to list which tags are associated to which task_type\n\n\n\nTry it: Exploring Tests in the developer framework\n\n\n\n\nWe have added a new feature that tracks which datasets and models are used when running tests. Now, when you initialize datasets or models with vm.init_dataset() and vm.init_model(), we link those inputs with the test results they generate. This makes it clear which inputs were used for each result, improving transparency and making it easier to understand test outcomes. This update does not require any changes to your code and works with existing init methods.\n\n\n\n\n\n\n\nWe are now showing the name of the user who ran the action instead of a generic “Developer Framework” name whenever you generate documentation:\n\n\nBefore\n\n\n\n\nAfter\n\n\n\n\n\n\n\n\nWe simplified the instructions for getting started with the ValidMind Developer Framework in the ValidMind Platform UI. These instructions tell you how to use the code snippet for your model documentation with your own model or with one of our code samples."
  },
  {
    "objectID": "releases/2024-feb-14/highlights.html#enhancements",
    "href": "releases/2024-feb-14/highlights.html#enhancements",
    "title": "February 14, 2024",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nModel owners can edit model details\n\nModel owners can now edit the values for fields displayed on the model details page. Previously it was only possible to edit custom fields defined by your organization.\n\n\n\n\nPerformance improvements for the ValidMind Platform UI\n\nWe made improvements to page load times on our platform UI for a smoother user experience.\n\n\n\n\nAdded support for filtering model inventory by developers / validators\n\nEnhanced navigation of the Model Inventory by enabling filtering based on Developers and Validators involved with each model.\n\n\n\n\nSupport for custom model fields in the model inventory\n\nThe model inventory has been updated to allow organizations to add custom fields. This enhancement enables administrators to customize the model inventory data schema according to your specific organizational needs. This can be done by accessing Custom Fields in the Settings page.\n\n\nThe initial release supports the following field types:\n\nSingle Line Text\nLong Text\nSingle Select\nMultiple Select\nCheckbox\nNumber\nURL\nDate\nDate Time\nEmail\nLinked Record to User\n\n\n\nFilter for mentions in recent activity comments\n\nWe implemented a toggle feature in the Recent Activity &gt; Comments section to filter and display only specific mentions. By default, all comments where the logged-in user has been tagged are displayed by this filter.\n\n\n\n\nExpanded rich-text editor support\n\nForms in the Findings and Validation Report sections now support the rich-text editor interface. This support enables you to use the editor for your finding descriptions and remediation plans, for example."
  },
  {
    "objectID": "releases/2024-feb-14/highlights.html#bug-fixes",
    "href": "releases/2024-feb-14/highlights.html#bug-fixes",
    "title": "February 14, 2024",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nInvalid content blocks create errors for run_documentation_tests()\n\nFixed an issue where using an invalid test identifier would prevent run_documentation_tests() from running all available tests. The full test suite now runs as expected, even when an invalid test identifier causes an error for an individual test.\n\n\n\n\nShow all collapsed sections in documentation table of contents\n\nFixed an issue where the table of contents was not displaying every subsection that belongs to the parent section. The table of contents now accurately reflects the complete structure of the documentation, including all subsections.\n\n\n\n\nTemplate swap shows the wrong diff\n\nFixed an issue where the diff for validation reports was showing incorrectly when swapping templates. The correct diff between the current and the new template is now displayed.\n\n\n\n\nClicking on a recent activity item should link to the corresponding content block\n\nFixed an issue where clicking on a recent activity item would not redirect you to the corresponding content block. Clicking on a recent item now takes you to the correct content block as expected."
  },
  {
    "objectID": "releases/2024-feb-14/highlights.html#documentation-updates",
    "href": "releases/2024-feb-14/highlights.html#documentation-updates",
    "title": "February 14, 2024",
    "section": "Documentation updates",
    "text": "Documentation updates\n\n\nNew user management documentation\n\nOur user guide now includes end-to-end instructions for managing users on the ValidMind platform. This new content covers common tasks such as inviting new users, adding them to user groups, and managing roles and permissions. Learn more …\n\n\n\n\nUpdated sample notebooks with current input_id usage\n\nWe updated our sample notebooks to show the current, recommended usage for input_id when calling vm.init_dataset() or vm.init_model().\n\n\nLearn more:\n\nQuickStart for Customer Churn Model Documentation — Full Suite\nSentiment Analysis of Financial Data Using a Large Language Model (LLM)\nSummarization of Financial Data Using a Large Language Model (LLM)\nSentiment Analysis of Financial Data Using Hugging Face NLP Models\nSummarization of Financial Data Using Hugging Face NLP models\nPrompt Validation for Large Language Models (LLMs)\nQuickStart for California Housing Regression Model Documentation — Full Suite\nConfiguring and Using Dataset Features"
  },
  {
    "objectID": "releases/2024-feb-14/highlights.html#how-to-upgrade",
    "href": "releases/2024-feb-14/highlights.html#how-to-upgrade",
    "title": "February 14, 2024",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2023-nov-09/highlights.html",
    "href": "releases/2023-nov-09/highlights.html",
    "title": "November 9, 2023",
    "section": "",
    "text": "This release introduces support for several new models, a new user onboarding guide and other UI enhancements, and improved test descriptions in our user-facing documentation.\n\n\n\n\n\n\nThe ValidMind Developer Framework has added support for regression models. The updates include:\n\n\nAddition of new metrics, Errors and R-squared, to support regression model evaluation\nUse of existing tabular dataset metrics for data validation\n\n\n\n\n\n\nThe ValidMind Developer Framework has added support for clustering models. The updates include:\n\n\nAddition of new metrics, ClusterPerformanceMetrics, ClusterSizeDistribution, SilhouettePlot, HyperParametersTuning, and KMeansClustersOptimization, to support clustering model evaluation\n\n\n\n\n\n\nWe added initial support for text embeddings models in the ValidMind Developer Framework which enables you to create, use and test a BERT embeddings model utilizing the Hugging Face library. The updates include:\n\n\nA new folder in model_validation tests for embeddings, along with initial versions of tests for text embedding models\nSupport for feature_extraction tasks in the Hugging Face model wrapper of the ValidMind Developer Framework\n\n\n\n\n\n\n\n\n\nTo help you familiarize yourself with the features available in the ValidMind platform, a new onboarding guide is now available to all users:\n\nThe onboarding guide tracks six common tasks that can be completed in almost any order:\n\n\n\nGet your ValidMind account\nRegister your model\nStart on your model documentation\n\n\n\n\nCollaborate on your documentation\nSubmit your model documentation\nFind guides & resources\n\n\n\nYou can hover over each task for more details. The guide can be minimized, leaving a trigger in the bottom right corner, or dismissed entirely. To reactivate the guide, go to Settings &gt; Profile.\n\n\n\n\n\nWe’ve introduced new display options for search results in the Model Inventory and Documentation Projects pages:\n\n\n\n\n\n\n\n\n\nTable view: Offers a detailed, structured layout that simplifies comparison.\nCard view: Provides a visual, summarized presentation.\n\nYou can easily switch between views using the Display table or Display cards toggle next to the search results."
  },
  {
    "objectID": "releases/2023-nov-09/highlights.html#release-highlights",
    "href": "releases/2023-nov-09/highlights.html#release-highlights",
    "title": "November 9, 2023",
    "section": "",
    "text": "This release introduces support for several new models, a new user onboarding guide and other UI enhancements, and improved test descriptions in our user-facing documentation.\n\n\n\n\n\n\nThe ValidMind Developer Framework has added support for regression models. The updates include:\n\n\nAddition of new metrics, Errors and R-squared, to support regression model evaluation\nUse of existing tabular dataset metrics for data validation\n\n\n\n\n\n\nThe ValidMind Developer Framework has added support for clustering models. The updates include:\n\n\nAddition of new metrics, ClusterPerformanceMetrics, ClusterSizeDistribution, SilhouettePlot, HyperParametersTuning, and KMeansClustersOptimization, to support clustering model evaluation\n\n\n\n\n\n\nWe added initial support for text embeddings models in the ValidMind Developer Framework which enables you to create, use and test a BERT embeddings model utilizing the Hugging Face library. The updates include:\n\n\nA new folder in model_validation tests for embeddings, along with initial versions of tests for text embedding models\nSupport for feature_extraction tasks in the Hugging Face model wrapper of the ValidMind Developer Framework\n\n\n\n\n\n\n\n\n\nTo help you familiarize yourself with the features available in the ValidMind platform, a new onboarding guide is now available to all users:\n\nThe onboarding guide tracks six common tasks that can be completed in almost any order:\n\n\n\nGet your ValidMind account\nRegister your model\nStart on your model documentation\n\n\n\n\nCollaborate on your documentation\nSubmit your model documentation\nFind guides & resources\n\n\n\nYou can hover over each task for more details. The guide can be minimized, leaving a trigger in the bottom right corner, or dismissed entirely. To reactivate the guide, go to Settings &gt; Profile.\n\n\n\n\n\nWe’ve introduced new display options for search results in the Model Inventory and Documentation Projects pages:\n\n\n\n\n\n\n\n\n\nTable view: Offers a detailed, structured layout that simplifies comparison.\nCard view: Provides a visual, summarized presentation.\n\nYou can easily switch between views using the Display table or Display cards toggle next to the search results."
  },
  {
    "objectID": "releases/2023-nov-09/highlights.html#documentation-updates",
    "href": "releases/2023-nov-09/highlights.html#documentation-updates",
    "title": "November 9, 2023",
    "section": "Documentation updates",
    "text": "Documentation updates\n\nEnhanced test descriptions\n\nWe have improved the descriptions for tests available in the ValidMind Developer Framework, together with a new landing page that allows for easy browsing of all tests. Additionally, our documentation site search now indexes these descriptions to make them easier to find. Try it …"
  },
  {
    "objectID": "releases/2023-nov-09/highlights.html#how-to-upgrade",
    "href": "releases/2023-nov-09/highlights.html#how-to-upgrade",
    "title": "November 9, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html",
    "href": "releases/2023-aug-15/highlights.html",
    "title": "August 15, 2023",
    "section": "",
    "text": "This release includes a number of improvements for the developer experience when using the ValidMind Developer Framework, along with a visual redesign of the ValidMind Platform UI.\n\n\n\n\n\nImproved developer experience for the Metric test class. We made a number of enhancements to the ValidMind Developer Framework to improve the experience for developers:\n\nBetter test organization: Added a metadata and tags attribute to the Test base class for better categorization and filtering of tests.\nBetter filtering: Added a new task_type argument to the list_tests() function to allow simple filtering by task_type set in a test.\nClearer naming: Renamed required_context to required_inputs across the whole ValidMind SDK.\n\n\n\n\nEnhancements to test configuration discovery: To make the discovery of required context and default configuration easier, this update includes a number of changes to how required_context and default_parameters are presented:\n\nPreview template expansion: With vm.preview_template(), you can now view detailed, expandable information for each content block driven by a test. This includes the test’s name, description, required context, and default parameters.\nSimplified test listing: The vm.tests.list_tests() function has been refined to show only the ID, Name, and Description for each test, making it more user-friendly.\nDetailed test descriptions: You can now obtain complete test details, including required_context and default_params, using the vm.tests.describe_test() function.\nRetrieve test suite: The new function vm.get_test_suite() allows you to access a specific test suite instance via the high-level ValidMind API.\nAccess to default configuration: A new method, get_default_config(), is now part of both the TestPlan and TestSuite classes, letting you obtain a dictionary containing all the configuration parameters used by the tests.\nEasier required context: A complementary new method, get_required_context(), now returns a list of all the context variables needed for tests to run, making it easier for you to ensure that everything is set up correctly.\n\n\n\n\n\n\nRedesigned application and navigation bar layout. The new application design offers a cleaner, more intuitive interface that enhances user experience. The updated navigation bar layout specifically contributes to easier navigation, allowing users to find what they need with greater efficiency."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html#release-highlights",
    "href": "releases/2023-aug-15/highlights.html#release-highlights",
    "title": "August 15, 2023",
    "section": "",
    "text": "This release includes a number of improvements for the developer experience when using the ValidMind Developer Framework, along with a visual redesign of the ValidMind Platform UI.\n\n\n\n\n\nImproved developer experience for the Metric test class. We made a number of enhancements to the ValidMind Developer Framework to improve the experience for developers:\n\nBetter test organization: Added a metadata and tags attribute to the Test base class for better categorization and filtering of tests.\nBetter filtering: Added a new task_type argument to the list_tests() function to allow simple filtering by task_type set in a test.\nClearer naming: Renamed required_context to required_inputs across the whole ValidMind SDK.\n\n\n\n\nEnhancements to test configuration discovery: To make the discovery of required context and default configuration easier, this update includes a number of changes to how required_context and default_parameters are presented:\n\nPreview template expansion: With vm.preview_template(), you can now view detailed, expandable information for each content block driven by a test. This includes the test’s name, description, required context, and default parameters.\nSimplified test listing: The vm.tests.list_tests() function has been refined to show only the ID, Name, and Description for each test, making it more user-friendly.\nDetailed test descriptions: You can now obtain complete test details, including required_context and default_params, using the vm.tests.describe_test() function.\nRetrieve test suite: The new function vm.get_test_suite() allows you to access a specific test suite instance via the high-level ValidMind API.\nAccess to default configuration: A new method, get_default_config(), is now part of both the TestPlan and TestSuite classes, letting you obtain a dictionary containing all the configuration parameters used by the tests.\nEasier required context: A complementary new method, get_required_context(), now returns a list of all the context variables needed for tests to run, making it easier for you to ensure that everything is set up correctly.\n\n\n\n\n\n\nRedesigned application and navigation bar layout. The new application design offers a cleaner, more intuitive interface that enhances user experience. The updated navigation bar layout specifically contributes to easier navigation, allowing users to find what they need with greater efficiency."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html#how-to-upgrade",
    "href": "releases/2023-aug-15/highlights.html#how-to-upgrade",
    "title": "August 15, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/work-with-content-blocks.html",
    "href": "guide/work-with-content-blocks.html",
    "title": "Work with content blocks",
    "section": "",
    "text": "Make edits to your model documentation or validation reports by adding or removing content blocks directly in the online editor."
  },
  {
    "objectID": "guide/work-with-content-blocks.html#what-are-content-blocks",
    "href": "guide/work-with-content-blocks.html#what-are-content-blocks",
    "title": "Work with content blocks",
    "section": "What are content blocks?",
    "text": "What are content blocks?\nContent blocks provide you with sections that are part of a template. You can think of these sections as an empty canvas that you fill in with text, metrics, and test results. Multiple sections are joined to create a longer document with a table of contents that has different heading and subheading levels, such as 1., 1.1., and so on.\nTypes of content blocks:\n\nSimple text block\n\nCan be added anywhere on model documentation or validation reports and edited to include additional documentation in text format.\n\nTest-driven block\n\nCan be added to display one of the supported metrics or threshold test results collected by the developer framework."
  },
  {
    "objectID": "guide/work-with-content-blocks.html#prerequisites",
    "href": "guide/work-with-content-blocks.html#prerequisites",
    "title": "Work with content blocks",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Developer or Validator role\nThe model you are documenting or validating is registered in the model inventory."
  },
  {
    "objectID": "guide/work-with-content-blocks.html#add-content-blocks",
    "href": "guide/work-with-content-blocks.html#add-content-blocks",
    "title": "Work with content blocks",
    "section": "Add content blocks",
    "text": "Add content blocks\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation or Validation Report.\nYou can now jump to any section of the model documentation or validation report by expanding the table of contents on the left and selecting the relevant section you would like to add content to, such as 1.1 Model Overview.\nHover your mouse over the space where you want your new block to go until a horizontal dashed line with a  sign appears that indicates you can insert a new block:\n\nClick  and then select one of the available options:\n\nSimple text block: Adds a new section with a blank content block. After the new content block has been added, click  to edit the contents of the section like any other.\nTest-driven block: Select one of the options:\n\nMetric: Select one of the available metrics, such as Confusion Matrix.\nThreshold test: Select one of the available threshold tests, such as Data Quality: Skewness or Model Diagnosis: Overfit Regions.\n\n\nFor test-driven blocks, a preview of the available metrics or threshold test gets shown. Click Insert module when you are ready.\n\nAfter you have completed these steps, the new content block becomes a part of your model documentation."
  },
  {
    "objectID": "guide/work-with-content-blocks.html#remove-content-blocks",
    "href": "guide/work-with-content-blocks.html#remove-content-blocks",
    "title": "Work with content blocks",
    "section": "Remove content blocks",
    "text": "Remove content blocks\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation or Validation Report.\nYou can now jump to any section of the model documentation or validation report by expanding the table of contents on the left and selecting the relevant section, such as 1.1 Model Overview.\nSelect the block you wish to remove from the model documentation.\nClick on  located in either one of these locations:\n\nIn the toolbar for text blocks\n\n\n\nIn the single-button toolbar for the test-driven block\n\n\n\nAfter you have completed these steps, the content block is removed. Test-driven blocks can be re-added later on but text blocks are currently deleted permanently."
  },
  {
    "objectID": "guide/work-with-content-blocks.html#whats-next",
    "href": "guide/work-with-content-blocks.html#whats-next",
    "title": "Work with content blocks",
    "section": "What’s next",
    "text": "What’s next\n\nRegister models in the inventory\nWorking with model documentation\nPreparing validation reports"
  },
  {
    "objectID": "guide/get-started-developer-framework.html",
    "href": "guide/get-started-developer-framework.html",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "",
    "text": "The ValidMind Developer Framework helps you streamline model documentation by automating the generation of drafts. All you need to do is upload your documentation artifacts and test results to the ValidMind platform."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#what-is-the-var-vm_framework",
    "href": "guide/get-started-developer-framework.html#what-is-the-var-vm_framework",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "What is the ValidMind Developer Framework?",
    "text": "What is the ValidMind Developer Framework?\nValidMind’s developer framework provides a rich collection of documentation tools and test suites, from documenting descriptions of your dataset to validation testing your models for weak spots and overfit areas.\n\n\n\n\n\n\nValidMind offers two primary methods for automating model documentation:\n\nGenerate documentation: Through automation, the framework extracts metadata from associated datasets and models for you and generates model documentation based on a template. You can also add more documentation and tests manually using the documentation editing capabilities in the ValidMind UI.\nRun validation tests: The framework provides a suite of validation tests for common financial services use cases. For cases where these tests do not cover everything you need, you can also extend existing test suites with your own proprietary tests or testing providers.\n\nThe developer framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python client library already provides all the standard functionality you might need without requiring your developers to rewrite any functions.\n\n\n\n\n\n\n Key ValidMind concepts\n\n\n\n\n\n\n\n\nmodel documentation\n\nA structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses.\n\n\nWithin the realm of model risk management, this documentation serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n\nvalidation report\n\nA formal document produced after a model validation process, outlining the findings, assessments, and recommendations related to a specific model’s performance, appropriateness, and limitations. Provides a comprehensive review of the model’s conceptual framework, data sources and integrity, calibration methods, and performance outcomes.\n\n\nWithin model risk management, the validation report is crucial for ensuring transparency, demonstrating regulatory compliance, and offering actionable insights for model refinement or adjustments.\n\ntemplate, documentation template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n\n\nValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results. When rendered, produces a document that model developers can use for model validation.\n\ntest\n\nA function contained in the developer framework, designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind platform to generate the model documentation according to the template that is associated with the documentation.\n\n\nTests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n\nmetrics, custom metrics\n\nMetrics are a subset of tests that do not have thresholds. Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\n\n\nIn the context of ValidMind’s Jupyter notebooks, metrics and tests can be thought of as interchangeable concepts.\n\ninputs\n\nObjects to be evaluated and documented in the developer framework. They can be any of the following:\n\n\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model(). See the Model Documentation or the for more information.\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset(). See the Dataset Documentation for more information.\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\n\nparameters\n\nAdditional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\n\noutputs\n\nCustom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n\ntest suite\n\nA collection of tests which are run together to generate model documentation end-to-end for specific use cases.\n\n\nFor example, the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use cases."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#getting-started",
    "href": "guide/get-started-developer-framework.html#getting-started",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "Getting started",
    "text": "Getting started\n\n\n\n\n\nAfter you sign up for ValidMind to get access, try one our getting started guide:\n\n\n\n\n\n\n\nQuickstart for model documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidMind Introduction for Model Developers\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/get-started-developer-framework.html#learn-how-to-run-tests",
    "href": "guide/get-started-developer-framework.html#learn-how-to-run-tests",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "Learn how to run tests",
    "text": "Learn how to run tests\nValidMind provides many built-in tests and test suites which make it easy for developers to automate their model documentation. Start by running a pre-made test, then modify it, and finally create your own test:\n\n\n\n\n\n\n\nRun dataset based tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplement custom tests\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n Learn more: Run tests & test suites"
  },
  {
    "objectID": "guide/get-started-developer-framework.html#try-the-code-samples",
    "href": "guide/get-started-developer-framework.html#try-the-code-samples",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "Try the code samples",
    "text": "Try the code samples\nOur code samples showcase the capabilities of the ValidMind Developer Framework. Examples that you can build on and adapt for your own use cases include:\n\n\n\n\n\n\n\nCustomize metric outputs using output templates\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt validation for large language models (LLMs)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument a time series forecasting model\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n Try more: Code samples"
  },
  {
    "objectID": "guide/get-started-developer-framework.html#whats-next",
    "href": "guide/get-started-developer-framework.html#whats-next",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "What’s next",
    "text": "What’s next\nAfter you have tried out the developer framework, continue working with your model documentation in the AI Risk Platform (platform UI) online. There, you can:\n\nWork with documentation templates to customize them to your specific needs\nWork with model documentation in the UI to make edits, collaborate with validators, and submit your model documentation for approval\nExport your finalized model documentation"
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases? Our documentation includes how-to instructions for the following user roles:\n\nFor platform administrators — Learn how to configure the platform, from onboarding new users to setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your existing workflows, and more.\nFor model developers — Find information for ValidMind tests and test suites, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nWe have more code samples available that you can download and try out yourself.\nAlso check the Guides for how to integrate the developer framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers.\nCollaborate on documentation\n\n\nHave more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human.\n\n\nNeed help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/onboarding-users.html",
    "href": "guide/onboarding-users.html",
    "title": "Onboarding users",
    "section": "",
    "text": "Onboarding users involves controlling and organizing who has access to the ValidMind platform. Tasks include inviting your initial users, creating groups, and assigning roles and permissions."
  },
  {
    "objectID": "guide/onboarding-users.html#key-concepts",
    "href": "guide/onboarding-users.html#key-concepts",
    "title": "Onboarding users",
    "section": "Key concepts",
    "text": "Key concepts\nEffective user management is essential for maintaining security and operational efficiency. ValidMind uses role-based access control (RBAC) to manage user permissions systematically for our platform. With RBAC, access permissions are associated with roles rather than individual users. Users are assigned to specific roles, and these roles are then granted the necessary permissions to perform their respective tasks.\nThis approach simplifies user management by centralizing permissions and enhances security and efficiency. It ensures that your users have the appropriate level of access based on their roles, reducing the risk of unauthorized actions and fostering a structured and controlled environment within an organization’s digital ecosystem.\n\n\nKey terms\n\nUsers\n\nIndividuals interacting with the platform UI, each having a unique account.\n\nRoles\n\nSets of permissions defining which actions users can perform on the platform UI.\n\nGroups\n\nCollections of users managed at the organization or team level with similar roles or permissions, such as a specific model validation team or model developers in a specific business unit.\n\nPermissions\n\nSpecific actions or operations on resources allowed or denied to users or roles.\n\nResource actions\n\nSpecific CRUD (read, read, update, delete) actions, such as create_model or update_template.\n\n\n\n\nSupported Roles\nUser management utilizes role-based access control (RBAC), where roles determine permissions. Here are some common roles and their descriptions:\n\nDeveloper\n\nModel developers are responsible for documenting and testing models. They may have access to development tools and resources that interact with the platform UI and they can upload model documentation and test results through the developer framework.\n\nValidator\n\nModel validators review, test and validate models and model documentation. They may have limited access to modify content or configurations.\n\nGroup Admin\n\nGroup admins manage specific user groups, assigning and revoking permissions within those groups. They help ensure efficient group-level access control.\n\nCustomer Admin (Superuser)\n\nThe Customer Admin role is a superuser, with extensive control over the entire platform. This role can manage users, groups, and permissions at a broader level."
  },
  {
    "objectID": "guide/onboarding-users.html#example",
    "href": "guide/onboarding-users.html#example",
    "title": "Onboarding users",
    "section": "Example",
    "text": "Example\nIn the following example, RBAC has been configured for users and roles for two groups, the Marketing Group and the Finance & Risk Management group:\n\nOn the left, the possible CRUD (create, read, update, delete) operations are listed. There are four supported user roles, along with a number of users that have been defined. Each user is a member of one or more groups where they can perform actions based on the roles that they have been given."
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "QuickStart",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our developer framework in Jupyter Hub and to explore the ValidMind Platform UI online."
  },
  {
    "objectID": "guide/quickstart.html#before-you-begin",
    "href": "guide/quickstart.html#before-you-begin",
    "title": "QuickStart",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\n\nValidMind Developer Framework\nTo try the ValidMind Developer Framework, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory\nLocal developer environment\n\n\n\nValidMind Platform UI\nWe support most modern browsers, such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox.\nTo upload from the developer framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "QuickStart",
    "section": "Steps",
    "text": "Steps\n\nTry the ValidMind Developer Framework with Jupyter Hub (recommended) (15 minutes)\nTry our introductory Jupyter notebook to see the developer framework in action.\nExplore the ValidMind Platform UI (15 minutes):\n\nExplore sample model documentation\nRegister your first model\nGenerate documentation for your model\n\n\n\nWhat’s next\nIf you’re ready to do more with ValidMind, check out Next steps."
  },
  {
    "objectID": "guide/assign-documentation-section-statuses.html",
    "href": "guide/assign-documentation-section-statuses.html",
    "title": "Assign documentation section statuses",
    "section": "",
    "text": "Assign a completion status to individual sections of your model documentation that will be reflected in your Document Overview."
  },
  {
    "objectID": "guide/assign-documentation-section-statuses.html#prerequisites",
    "href": "guide/assign-documentation-section-statuses.html#prerequisites",
    "title": "Assign documentation section statuses",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Developer role\nThe model is already registered in the model inventory"
  },
  {
    "objectID": "guide/assign-documentation-section-statuses.html#steps",
    "href": "guide/assign-documentation-section-statuses.html#steps",
    "title": "Assign documentation section statuses",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nJump to any section of the model documentation by selecting the relevant section you would like to view.\nNext to the heading of each section, you can assign a status from the drop-down:\n\nIn Progress\nDone\n\n\n\n\n\n\n\nThe documentation completion status summary will then be updated on your Document Overview page."
  },
  {
    "objectID": "guide/assign-documentation-section-statuses.html#whats-next",
    "href": "guide/assign-documentation-section-statuses.html#whats-next",
    "title": "Assign documentation section statuses",
    "section": "What’s next",
    "text": "What’s next\n\nView documentation activity\nCollaborate with others"
  },
  {
    "objectID": "guide/store-credentials-in-env-file.html",
    "href": "guide/store-credentials-in-env-file.html",
    "title": "Store project credentials in .env files",
    "section": "",
    "text": "Learn how to store project identifier credentials in a .env file instead of using inline credentials. This topic is relevant for model developers who want to follow best practices for security when running notebooks."
  },
  {
    "objectID": "guide/store-credentials-in-env-file.html#why-is-this-recommended",
    "href": "guide/store-credentials-in-env-file.html#why-is-this-recommended",
    "title": "Store project credentials in .env files",
    "section": "Why is this recommended?",
    "text": "Why is this recommended?\nStoring credentials in a .env file is considered a best practice for security. Embedding credentials directly within the code makes them more susceptible to accidental exposure when sharing code or collaborating on projects. Keeing project credentials in a separate file also allows for precise access control and ensures that sensitive credentials are not publically accessible."
  },
  {
    "objectID": "guide/store-credentials-in-env-file.html#prerequisites",
    "href": "guide/store-credentials-in-env-file.html#prerequisites",
    "title": "Store project credentials in .env files",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nThe model is already registered in the model inventory"
  },
  {
    "objectID": "guide/store-credentials-in-env-file.html#steps",
    "href": "guide/store-credentials-in-env-file.html#steps",
    "title": "Store project credentials in .env files",
    "section": "Steps",
    "text": "Steps\n\nCreate a new file in the same folder as your notebook and name it .env. This is a hidden file, so you may need to change your settings to view it.\nLocate the code snippet for your model. These credentials can be found on the Getting started page for models registered in the model inventory. Copy the values from this page and paste them into your .env file in the following format:\nVM_API_PROJECT=&lt;Project Identifier&gt;\nVM_API_HOST=&lt;API Host URL&gt;\nVM_API_KEY=&lt;API Key&gt;\nVM_API_SECRET=&lt;API Secret&gt;\nInsert this code snippet above your project identifier credentials:\n%load_ext dotenv\n%dotenv dev.env\n\nThe updated notebook should look like this:\n%load_ext dotenv\n%dotenv .env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"...\"\n)\n\nRun the cell. Instead of using inline credentials, this cell will now load your project credentials from a .env file."
  },
  {
    "objectID": "guide/store-credentials-in-env-file.html#whats-next",
    "href": "guide/store-credentials-in-env-file.html#whats-next",
    "title": "Store project credentials in .env files",
    "section": "What’s next",
    "text": "What’s next\n\nWorking with model documentation\nGet started with the ValidMind Developer Framework"
  },
  {
    "objectID": "guide/preparing-validation-reports.html",
    "href": "guide/preparing-validation-reports.html",
    "title": "Preparing validation reports",
    "section": "",
    "text": "As part of preparing validation reports, you can view the guidelines, collaborate with model developers, add or edit content to your draft validation report, link to evidence, or work with project findings."
  },
  {
    "objectID": "guide/preparing-validation-reports.html#prerequisites",
    "href": "guide/preparing-validation-reports.html#prerequisites",
    "title": "Preparing validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Validator role\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation as Ready for Validation"
  },
  {
    "objectID": "guide/preparing-validation-reports.html#key-concepts",
    "href": "guide/preparing-validation-reports.html#key-concepts",
    "title": "Preparing validation reports",
    "section": "Key concepts",
    "text": "Key concepts\n\nA validation report is a comprehensive review that evaluates a model’s accuracy, performance, and suitability for its intended purpose. It encompasses the process of model risk assessment, identifying areas of potential error or risk within the model’s components, such as data inputs and algorithms. The report follows established validation guidelines to ensure consistency and adherence to internal and regulatory standards.\n\n\n\nmodel risk assessment\n\nThe process of identifying and evaluating risks associated with the use and potential errors in a financial model.\n\nmodel risk areas\n\nSpecific components or aspects of a model where risk might be present, such as data inputs, algorithms, or implementation.\n\nvalidation guidelines\n\nEstablished standards or procedures for conducting thorough and consistent model validations, usually aligned with principles within specific models or AI risk frameworks.\n\nfindings\n\nObservations or issues identified during model validation, including any deviations from expected performance or standards.\n\n\n\n\n\nactions\n\nRecommended steps or measures to address findings from model validation or risk assessments.\n\nevidence\n\nMaterial provided by the developer and reviewed by the validator, such as model documentation, source code, datasets, monitoring reports or previous validation reports.\n\nongoing monitoring report\n\nA periodic report assessing the model’s performance and compliance over time, ensuring it remains valid under changing conditions."
  },
  {
    "objectID": "guide/preparing-validation-reports.html#get-started",
    "href": "guide/preparing-validation-reports.html#get-started",
    "title": "Preparing validation reports",
    "section": "Get started",
    "text": "Get started\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Validation Report.\nYou can now jump to any section of the validation report by expanding the table of contents on the left and selecting the relevant section you would like to view.\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox.\nYou can now use the text editor functions to edit the content of the section. Whenever you make a change, an activity log entry is recorded under Section activity on the page, just below the section you are editing.\nSave your edits:\n\nClicking on the  icon in the editor toolbar.\nSelect Save current revision and enter a name.\nClick  to save the revision."
  },
  {
    "objectID": "guide/preparing-validation-reports.html#whats-next",
    "href": "guide/preparing-validation-reports.html#whats-next",
    "title": "Preparing validation reports",
    "section": "What’s next",
    "text": "What’s next\n\n\n\n\n\n\n\nView validation guidelines\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview model documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssess compliance\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with content blocks\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborate with others\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit for approval\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/work-with-model-findings.html",
    "href": "guide/work-with-model-findings.html",
    "title": "Work with model findings",
    "section": "",
    "text": "Model findings are detailed observations identified during the model validation process, highlighting any major or minor issues, deficiencies, model limitations, stability and robustness concerns, or needed adjustments.\nThese findings are critical for understanding the risk exposure and compliance status of models within an organization. To make them easier to track, findings are typically categorized by risk area, business unit, model status, and individual model, enabling targeted resolution and informed decision-making to mitigate identified risks and ensure model reliability and accuracy."
  },
  {
    "objectID": "guide/work-with-model-findings.html#prerequisites",
    "href": "guide/work-with-model-findings.html#prerequisites",
    "title": "Work with model findings",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Validator role\nThe model is already registered in the model inventory\nA model document project is completed or in progress"
  },
  {
    "objectID": "guide/work-with-model-findings.html#view-model-findings",
    "href": "guide/work-with-model-findings.html#view-model-findings",
    "title": "Work with model findings",
    "section": "View model findings",
    "text": "View model findings\nTo view all project findings:\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Findings.\nIn the page that opens, you can see a list of all model findings, including information for:\n\nTitle\nSeverity\nStatus\nInventory model\nRisk area\nAssignee\nDue date\n\nClick on any model finding for more information, including the proposed remediation plan."
  },
  {
    "objectID": "guide/work-with-model-findings.html#filter-project-findings",
    "href": "guide/work-with-model-findings.html#filter-project-findings",
    "title": "Work with model findings",
    "section": "Filter project findings",
    "text": "Filter project findings\nBy default, all model findings for all models are displayed and sorted by creation date, with the latest findings displayed first. To narrow down the findings to only those you want to see, you can apply some filters:\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Findings and then click  Filter.\nOn the Select your filters dialog that opens, select your filtering criteria for:\n\nStatus\nModel inventory\nRisk area\nBusiness units\nSeverity\nAssignee\n\nFor example: To filter by open findings for the Customer Churn Model assigned to Jane Doe:\n\nClick Apply Filters.\n\nFilters can be removed by clicking on the  next to them on the main Model Findings page where the results are displayed."
  },
  {
    "objectID": "guide/work-with-model-findings.html#add-project-findings",
    "href": "guide/work-with-model-findings.html#add-project-findings",
    "title": "Work with model findings",
    "section": "Add project findings",
    "text": "Add project findings\nAs part of the validation process, you may find issues with the model documentation that must be resolved. To indicate that there is an issue and to track the resolution, you add a new finding.\n\nFindings are logged in your Model Inventory, within a model’s documentation.\nWithin the ValidMind Platform UI, you’re able to add findings both on the overarching documentation overview page, and within each documentation section itself.\n\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\n\n\n\n\n\n\n\nYou can now either log a finding on this overview page, or via a specific documentation section. Both methods will allow you to associate a finding with a documentation section if desired.\n\n\n\n\nAdd finding via overview\nTo log a finding from the documentation overview:\n\nOn your model’s Documentation page, click Add Finding.\nOn the Add Model Finding page that opens, provide information for:\n\nTitle\nRisk area\nOwner\nSeverity\nDue date\nDocumentation section\nDescription\n\nAt a minimum, you must provide the required information indicated by *.\nWhen you are done, click Save to submit the finding.\n\n\n\nAdd finding via section\nWhile working within a section of your documentation, you can easily log a finding associated with that section without leaving the page:\n\nClick on ValidMind Insights™ to expand the insight panel.\nFor the section you want to add a Finding for, click  Add Finding beneath the Documentation Guidelines.\nOn the Add Model Finding page that opens, provide information for:\n\nTitle\nRisk area\nOwner\nSeverity\nDue date\nDocumentation section\nDescription\n\nAt a minimum, you must provide the required information indicated indicated by *. The documentation section will be auto-populated with the section you are working from — you are able to select another section if desired.\nWhen you are done, click Save to submit the finding."
  },
  {
    "objectID": "guide/work-with-model-findings.html#update-project-findings",
    "href": "guide/work-with-model-findings.html#update-project-findings",
    "title": "Work with model findings",
    "section": "Update project findings",
    "text": "Update project findings\nAs project findings get resolved or require other changes during the model validation process, you can update them:\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Findings.\nOn the page that opens, click on the finding you want to update or apply a filter to locate the correct finding first.\nOn the page that opens, make your updates to the finding. You can make updates to:\n\nDescription\nProposed remediation plan\nStatus\nSeverity\nRisk area\nDue date\nAssignee\nDocumentation section\n\n\nMost updates are applied immediately but you must click Save to make changes to the finding description and proposed remediation plan effective."
  },
  {
    "objectID": "guide/work-with-model-findings.html#whats-next",
    "href": "guide/work-with-model-findings.html#whats-next",
    "title": "Work with model findings",
    "section": "What’s next",
    "text": "What’s next\nTo get a summary view of all model findings and to identify specific areas of concerns for your model, you can view reports."
  },
  {
    "objectID": "guide/customize-documentation-templates.html",
    "href": "guide/customize-documentation-templates.html",
    "title": "Customize documentation templates",
    "section": "",
    "text": "Edit templates that get used for model documentation or for validation reports to configure templates for specific use cases or where the existing templates supplied by ValidMind need to be customized."
  },
  {
    "objectID": "guide/customize-documentation-templates.html#about-documentation-templates",
    "href": "guide/customize-documentation-templates.html#about-documentation-templates",
    "title": "Customize documentation templates",
    "section": "About documentation templates",
    "text": "About documentation templates\nDocumentation templates are stored as YAML files that you edit directly in the online editor. These templates are versioned and saving a documentation template after making changes or reverting to a previous version state always creates a new version."
  },
  {
    "objectID": "guide/customize-documentation-templates.html#prerequisites",
    "href": "guide/customize-documentation-templates.html#prerequisites",
    "title": "Customize documentation templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe template you want to edit must have been added to the ValidMind Platform already.\nIf you are not sure which template or which version of a template your model documentation is using, check the Documentation page of your model. The Document Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/customize-documentation-templates.html#template-schema",
    "href": "guide/customize-documentation-templates.html#template-schema",
    "title": "Customize documentation templates",
    "section": "Template schema",
    "text": "Template schema\n Schema Docs\n\n\n\n\n\n\n\n\n  Type: object     template_id Required     root    template_idType: string Unique identifier for the template.          template_name Required     root    template_nameType: string Name of the template.          version Required     root    versionType: string Version of the template.          description     root    descriptionType: string Description of the template.          sections Required     root    sectionsType: array Documentation sections of the template.  Each item of this array must be:   root    sections    sectionType: object     id Required     root    sections    sections items    idType: string Unique identifier for the section.          title Required     root    sections    sections items    titleType: string Title of the section.          description     root    sections    sections items    descriptionType: string Description of the section.          parent_section     root    sections    sections items    parent_sectionType: string ID of the parent section.          order     root    sections    sections items    orderType: integer Order of the section in the navigation menu. By default sections are ordered alphabetically. If order is specified, sections will be ordered by the order value, and then alphabetically.          default_text     root    sections    sections items    default_textType: string Default text for the section. If set, a metadata content row will be created with this text when installing the template          index_only     root    sections    sections items    index_onlyType: boolean If true, the section will be displayed in the navigation menu, but it will not be accessible via direct link.          condensed     root    sections    sections items    condensedType: boolean If true, the section will condense all of its subsections into a single section.          guidelines     root    sections    sections items    guidelinesType: array of string Documentation or validation guidelines for the section.  Each item of this array must be:   root    sections    sections items    guidelines    guidelines itemsType: string           contents     root    sections    sections items    contentsType: array Contents to be displayed on the section.  Each item of this array must be:   root    sections    sections items    contents    section_contentsType: object Single content block of the module.      content_type Required     root    sections    sections items    contents    contents items    content_typeType: enum (of string) Default: \"metadata_text\"  Must be one of: \"metadata_text\"\"dynamic\"\"metric\"\"test\"   Examples: \"metadata_text\"\n \"test\"\n          content_id     root    sections    sections items    contents    contents items    content_idType: string ID of the content to be displayed for the given content type (text, metric, testm, etc.).   Examples: \"sample_text\"\n \"section_intro\"\n          options     root    sections    sections items    contents    contents items    optionsType: object Options for the content block.   Examples: {\n    \"default_text\": \"This is a sample text block.\"\n}\n {\n    \"metric_id\": \"metric_1\",\n    \"title\": \"Custom Title for Metric 1\"\n}\n {\n    \"test_id\": \"adf_test\"\n}\n      default_text     root    sections    sections items    contents    contents items    options    default_textType: string Default text for the content block. Only applicable for metadata_text content blocks.          title     root    sections    sections items    contents    contents items    options    titleType: string Title of the content block. Only applicable for metric and test content blocks.                       Generated using json-schema-for-humans on 2023-06-08 at 14:44:40 -0700"
  },
  {
    "objectID": "guide/customize-documentation-templates.html#steps",
    "href": "guide/customize-documentation-templates.html#steps",
    "title": "Customize documentation templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Settings page, click Templates.\nSelect one of the tabs for the type of template you want to edit:\n\nDocumentation Templates\nValidation Report Templates\n\nLocate the template to edit and, at the bottom of the template card, click Edit Template.\nIn the YAML editor that opens, make your changes.\n\nUse See changes to view a side-by-side comparison of your changes with the latest version of the template.\nUse Reset changes to delete your changes and return to the latest version of the template.\n\nClick Prepare new version to save your changes.\n\nAdd a description in Version notes to track the changes that were made once the version is saved.\n\n\nAfter you have saved a new version, it becomes available for use with model documentation or validation reports."
  },
  {
    "objectID": "guide/customize-documentation-templates.html#troubleshooting",
    "href": "guide/customize-documentation-templates.html#troubleshooting",
    "title": "Customize documentation templates",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nThe documentation template editor validates the YAML changes you make and flags any errors that it finds. If you make a change that the editor cannot parse correctly, the editor will not let you save the changes until you correct the YAML.\nCommon issues with YAML include incorrect indenting, imbalanced quotes, or missing colons between keys and values. If you run into issues with incorrect YAML, check the error message provided by the template editor, as it might provide a line and column number where the error occurs."
  },
  {
    "objectID": "guide/view-documentation-activity.html",
    "href": "guide/view-documentation-activity.html",
    "title": "View documentation activity",
    "section": "",
    "text": "Use the audit trail functionality in the ValidMind Platform UI to track or audit all the information events associated with a specific model."
  },
  {
    "objectID": "guide/view-documentation-activity.html#prerequisites",
    "href": "guide/view-documentation-activity.html#prerequisites",
    "title": "View documentation activity",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis topic matters for model developers, model validators, and auditors.\n\nThe model you are documenting is registered in the model inventory\nModel documentation has already been created for this project\nA model developer has started generating documentation, either using the developer framework or via the online UI editor"
  },
  {
    "objectID": "guide/view-documentation-activity.html#steps",
    "href": "guide/view-documentation-activity.html#steps",
    "title": "View documentation activity",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Model Activity.\n\nThe table in this page shows a record of all activities, including:\n\nProject updates generated by the developer framework (API Client)\nActions performed on the project by users in your organization"
  },
  {
    "objectID": "guide/view-documentation-activity.html#whats-next",
    "href": "guide/view-documentation-activity.html#whats-next",
    "title": "View documentation activity",
    "section": "What’s next",
    "text": "What’s next\n\nCollaborate on documentation"
  },
  {
    "objectID": "guide/get-started-sandbox.html",
    "href": "guide/get-started-sandbox.html",
    "title": "Welcome to the ValidMind training environment!",
    "section": "",
    "text": "Gain hands-on experience and explore what ValidMind has to offer with our training environment.\nThe training environment mimics a production environment and includes comprehensive resources such as notebooks with sample code you can run, sample models registered in the model inventory, and draft documentation and validation reports."
  },
  {
    "objectID": "guide/get-started-sandbox.html#onboarding",
    "href": "guide/get-started-sandbox.html#onboarding",
    "title": "Welcome to the ValidMind training environment!",
    "section": "Onboarding",
    "text": "Onboarding\nStep-by-step instructions:\n\nGet your access credentials from ValidMind.\nIf you haven’t reached out to us yet, email info@validmind.ai to enquire about access to the training environment.\n\n\n\nWhen you receive your access credentials, make sure you can log in:\n\nValidMind Jupyter Hub: https://jupyterhub.validmind.ai/\nValidMind Platform UI: https://app.prod.validmind.ai\n\nJoin a kick-off session with ValidMind to get a free instructor-led hands-on demo.\nAfter we have received your request to try out the training environment, we will contact you to set up a kick-off session.\nTry the training environment: Getting started\n\n\n\n\n\n\n\nFor testing and evaluation purposes only\n\n\n\nDo not upload proprietary information — the training environment environment is not to be used for production. If you have questions about testing in the training environment, please provide feedback or ask for help.\n\n\n\nSample models\nSample models registered in the model inventory include:\n\nHousing prices prediction model\nCredit risk scorecard model\nCustomer churn prediction model\nInterest rate time series forecasting model\nLarge language model (LLM) demo application\n\nPlease note that we make updates to the models and datasets available in the training environment from time to time to provide you with our latest features.\n\n\nSample notebooks\nEach interactive notebook includes the sample code needed to automatically document a model:\n\n\n\nNotebook\nInventory model\n\n\n\n\nQuickstart for model documentationGets you started with the basic process of documenting models with ValidMind, from the developer framework to the platform UI.\n[Demo] Customer Churn Model\n\n\nValidMind introduction for model developersAs a model developer, learn how the end-to-end documentation process works based on common scenarios you encounter in model development settings.\n[Demo] Customer Churn Model\n\n\nDocument an application scorecard modelGuides you through building and documenting an application scorecard model using the Lending Club sample dataset from Kaggle.\n[Demo] Credit Risk Model\n\n\nPrompt validation for large language models (LLMs)Run and document prompt validation tests for a large language model (LLM) specialized in sentiment analysis for financial news.\n[Demo] Foundation Model - Text Sentiment Analysis"
  },
  {
    "objectID": "guide/get-started-sandbox.html#getting-started",
    "href": "guide/get-started-sandbox.html#getting-started",
    "title": "Welcome to the ValidMind training environment!",
    "section": "Getting started",
    "text": "Getting started\nWhat you try out first in the training environment depends on your interests:\n\n\nAutomated model testing & documentation\n\nExplore the sample notebooks\nRun tests and test suites\nGenerate model documentation\n\n\n\nValidMind Jupyter Hub\n\n\n\n\n\nModel risk management & governance\n\nExplore the validation report experience\nAssess compliance and link evidence\nView reports to identify risk areas\n\n\n\n\nValidMind Platform UI"
  },
  {
    "objectID": "guide/get-started-sandbox.html#provide-feedback-or-get-help",
    "href": "guide/get-started-sandbox.html#provide-feedback-or-get-help",
    "title": "Welcome to the ValidMind training environment!",
    "section": "Provide feedback or get help",
    "text": "Provide feedback or get help\n\nJoin our Slack community\nHave feedback or questions? We sponsor a Slack community where you can provide feedback or ask questions: Join Our Community Slack.\nThere is a dedicated channel for support: #community-support.\nOur growing Slack community is not just for our products but also aims to foster discussions between AI risk practitioners and those involved in model risk management (MRM). Feel free to take a look around the other channels that are available and stay a while.\n\n\nGet support\nTo get help from a human, send an email to support@validmind.ai."
  },
  {
    "objectID": "guide/get-started-sandbox.html#related-topics",
    "href": "guide/get-started-sandbox.html#related-topics",
    "title": "Welcome to the ValidMind training environment!",
    "section": "Related topics",
    "text": "Related topics\n\nGet started with the ValidMind Developer Framework\nWorking with model documentation\nPreparing validation reports"
  },
  {
    "objectID": "guide/test-descriptions.html",
    "href": "guide/test-descriptions.html",
    "title": "Test descriptions",
    "section": "",
    "text": "This topic describes the tests that are available as part of the ValidMind Developer Framework, grouped by type of validation test.\n\n\n\n\n\n\nTry the test sandbox (BETA)\n\n\n\nExplore our interactive sandbox to see what tests are available in the ValidMind Developer Framework.\n\n\n\n Data Validation Model Validation Prompt Validation\n\n\n\n\n\n\n\n\n\nACFandPACFPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVAOneWayTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoAR\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoMA\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoSeasonality\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoStationarity\n\n\n\n\n\n\n\n\n\n\n\n\n\nBivariateFeaturesBarPlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nBivariateHistograms\n\n\n\n\n\n\n\n\n\n\n\n\n\nBivariateScatterPlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nChiSquaredFeaturesTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassImbalance\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommonWords\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetSplit\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefaultRatesbyRiskBandPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptiveStatistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuplicates\n\n\n\n\n\n\n\n\n\n\n\n\n\nEngleGrangerCoint\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureTargetCorrelationPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nHashtags\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeatmapFeatureCorrelations\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighCardinality\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighPearsonCorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n\nIQROutliersBarPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nIQROutliersTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIsolationForestOutliers\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaggedCorrelationHeatmap\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguageDetection\n\n\n\n\n\n\n\n\n\n\n\n\n\nMentions\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissingValues\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissingValuesBarPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissingValuesRisk\n\n\n\n\n\n\n\n\n\n\n\n\n\nPearsonCorrelationMatrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiTCreditScoresHistogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiTPDHistogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolarityAndSubjectivity\n\n\n\n\n\n\n\n\n\n\n\n\n\nPunctuations\n\n\n\n\n\n\n\n\n\n\n\n\n\nRollingStatsPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonalDecompose\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkewness\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpreadPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nStopWords\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabularCategoricalBarPlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabularDateTimeHistograms\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabularDescriptionTables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTabularNumericalHistograms\n\n\n\n\n\n\n\n\n\n\n\n\n\nTargetRateBarPlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nTextDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeSeriesFrequency\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeSeriesHistogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeSeriesLinePlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeSeriesMissingValues\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeSeriesOutliers\n\n\n\n\n\n\n\n\n\n\n\n\n\nTooManyZeroValues\n\n\n\n\n\n\n\n\n\n\n\n\n\nToxicity\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniqueRows\n\n\n\n\n\n\n\n\n\n\n\n\n\nWOEBinPlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nWOEBinTable\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\nADF\n\n\n\n\n\n\n\n\n\n\n\n\n\nADFTest\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustedMutualInformation\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustedRandIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswerCorrectness\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswerRelevance\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswerSimilarity\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectCritique\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoARIMA\n\n\n\n\n\n\n\n\n\n\n\n\n\nBertScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nBleuScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxPierce\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifierPerformance\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusterCosineSimilarity\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusterDistribution\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusterPerformance\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusterPerformanceMetrics\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusterSizeDistribution\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompletenessScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfusionMatrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextEntityRecall\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextPrecision\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextRecall\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextRelevancy\n\n\n\n\n\n\n\n\n\n\n\n\n\nContextualRecall\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosineSimilarityComparison\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosineSimilarityDistribution\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosineSimilarityHeatmap\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulativePredictionProbabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\nDFGLSArch\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptiveAnalytics\n\n\n\n\n\n\n\n\n\n\n\n\n\nDurbinWatsonTest\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddingsVisualization2D\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuclideanDistanceComparison\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuclideanDistanceHeatmap\n\n\n\n\n\n\n\n\n\n\n\n\n\nFaithfulness\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureImportanceAndSignificance\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeaturesAUC\n\n\n\n\n\n\n\n\n\n\n\n\n\nFowlkesMallowsScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nGINITable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomogeneityScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperParametersTuning\n\n\n\n\n\n\n\n\n\n\n\n\n\nJarqueBera\n\n\n\n\n\n\n\n\n\n\n\n\n\nKMeansClustersOptimization\n\n\n\n\n\n\n\n\n\n\n\n\n\nKPSS\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorovSmirnov\n\n\n\n\n\n\n\n\n\n\n\n\n\nLJungBox\n\n\n\n\n\n\n\n\n\n\n\n\n\nLilliefors\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeteorScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimumAccuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimumF1Score\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimumROCAUCScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelMetadata\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelsPerformanceComparison\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitDiagnosis\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCAComponentsPairwisePlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDRatingClassPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermutationFeatureImportance\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhillipsPerronArch\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulationStabilityIndex\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecisionRecallCurve\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictionProbabilitiesHistogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nROCCurve\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionCoeffsPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionErrors\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionFeatureSignificance\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelForecastPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelForecastPlotLevels\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelInsampleComparison\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelOutsampleComparison\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelSensitivityPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelSummary\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelsCoeffs\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelsPerformance\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionModelsPerformanceComparison\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionPermutationFeatureImportance\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionR2Square\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegressionResidualsPlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidualsVisualInspection\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobustnessDiagnosis\n\n\n\n\n\n\n\n\n\n\n\n\n\nRougeScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunsTest\n\n\n\n\n\n\n\n\n\n\n\n\n\nSHAPGlobalImportance\n\n\n\n\n\n\n\n\n\n\n\n\n\nScorecardHistogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nShapiroWilk\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilhouettePlot\n\n\n\n\n\n\n\n\n\n\n\n\n\nStabilityAnalysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nStabilityAnalysisKeyword\n\n\n\n\n\n\n\n\n\n\n\n\n\nStabilityAnalysisRandomNoise\n\n\n\n\n\n\n\n\n\n\n\n\n\nStabilityAnalysisSynonyms\n\n\n\n\n\n\n\n\n\n\n\n\n\nStabilityAnalysisTranslation\n\n\n\n\n\n\n\n\n\n\n\n\n\nTSNEComponentsPairwisePlots\n\n\n\n\n\n\n\n\n\n\n\n\n\nTokenDisparity\n\n\n\n\n\n\n\n\n\n\n\n\n\nToxicityScore\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrainingTestDegradation\n\n\n\n\n\n\n\n\n\n\n\n\n\nVMeasure\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeakspotsDiagnosis\n\n\n\n\n\n\n\n\n\n\n\n\n\nZivotAndrewsArch\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\nBias\n\n\n\n\n\n\n\n\n\n\n\n\n\nClarity\n\n\n\n\n\n\n\n\n\n\n\n\n\nConciseness\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelimitation\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegativeInstruction\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobustness\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecificity\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/testing-overview.html",
    "href": "guide/testing-overview.html",
    "title": "Run tests and test suites",
    "section": "",
    "text": "ValidMind provides many built-in tests and test suites, which help you produce documentation during stages of the model development lifecycle where you need to validate that your work satisfies MRM (model risk management) requirements."
  },
  {
    "objectID": "guide/testing-overview.html#getting-started",
    "href": "guide/testing-overview.html#getting-started",
    "title": "Run tests and test suites",
    "section": "Getting started",
    "text": "Getting started\nStart by running a pre-made test, then modify it, and finally create your own test:\n\n\n\n\n\n\n\nRun dataset based tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplement custom tests\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/testing-overview.html#explore-tests-and-test-suites",
    "href": "guide/testing-overview.html#explore-tests-and-test-suites",
    "title": "Run tests and test suites",
    "section": "Explore tests and test suites",
    "text": "Explore tests and test suites\nNext, find available tests and test suites using the developer framework or the interactive test sandbox:\n\n\n\n\n\n\n\nExplore tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore test suites\n\n\n\n\n\n\n\n\n\n\n\n\n\n Test sandbox (BETA)\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/testing-overview.html#intermediate",
    "href": "guide/testing-overview.html#intermediate",
    "title": "Run tests and test suites",
    "section": "Intermediate",
    "text": "Intermediate\nBuilding on previous sections, add your own test provider, set up datasets, run tests on individual sections in your model documentation, and more:\n\n\n\n\n\n\n\nIntegrate external test providers\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure dataset features\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun individual documentation sections\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun documentation tests with custom configurations\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun tests with multiple datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun unit metrics\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/testing-overview.html#advanced",
    "href": "guide/testing-overview.html#advanced",
    "title": "Run tests and test suites",
    "section": "Advanced",
    "text": "Advanced\nNeed more? Try some of the advanced features provided by the developer framework:\n\n\n\n\n\n\n\nDocument multiple results for the same test\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoad dataset predictions\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/testing-overview.html#when-do-i-use-tests-and-test-suites",
    "href": "guide/testing-overview.html#when-do-i-use-tests-and-test-suites",
    "title": "Run tests and test suites",
    "section": "When do I use tests and test suites?",
    "text": "When do I use tests and test suites?\nWhile you have the flexibility to decide when to use which ValidMind tests, we have identified a few typical scenarios with their own characteristics and needs:\n\n\nDataset testing\nTo document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test suite.\nFor time-series datasets: use the time_series_dataset test suite.\n\n\n\nModel testing\nTo document and validate your model:\n\nFor binary classification models: use the classifier test suite.\nFor time series models: use the timeseries test suite.\n\n\n\nEnd-to-end testing\nTo document a binary classification model and the relevant dataset end-to-end:\nUse the classifier_full_suite test suite."
  },
  {
    "objectID": "guide/testing-overview.html#can-i-use-my-own-tests",
    "href": "guide/testing-overview.html#can-i-use-my-own-tests",
    "title": "Run tests and test suites",
    "section": "Can I use my own tests?",
    "text": "Can I use my own tests?\nAbsolutely! ValidMind supports custom tests that you develop yourself or that are provided by third-party test libraries, also referred to as test providers. We provide instructions with code examples that you can adapt:\n\nImplement Custom Metrics and Threshold Tests\nIntegrate an External Test Provider"
  },
  {
    "objectID": "guide/testing-overview.html#reference",
    "href": "guide/testing-overview.html#reference",
    "title": "Run tests and test suites",
    "section": "Reference",
    "text": "Reference\nValidMind Developer Framework Reference"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "ValidMind’s developer framework provides out-of-the-box support for testing and documentation for an array of model types and modeling packages."
  },
  {
    "objectID": "guide/supported-models.html#what-is-a-supported-model",
    "href": "guide/supported-models.html#what-is-a-supported-model",
    "title": "Supported models",
    "section": "What is a supported model?",
    "text": "What is a supported model?\nA supported model refers to a model for which predefined testing or documentation functions exist in the ValidMind Developer Framework, provided that the model you are developing is documented using a supported version of our client library. These model types cover a very large portion of the models used in commercial and retail banking.\nGiven the rapid developments in the AI space, including the advent of large language models (LLMs), ValidMind product development has also focused on making sure that our developer framework is extensible to support future model types or modeling packages, so that we do not limit our users to specific model types. You always have the flexibility to:\n\nImplement custom tests\nIntegrate external test providers"
  },
  {
    "objectID": "guide/supported-models.html#supported-model-types",
    "href": "guide/supported-models.html#supported-model-types",
    "title": "Supported models",
    "section": "Supported model types",
    "text": "Supported model types\n\nTraditional statistical models\n\nLinear regression: Models relationship between a scalar response and one or more explanatory variables.\nLogistic regression: Predicts the probability of a binary outcome based on one or more predictor variables.\nTime series: Analyzes data points collected or sequenced over time.\n\n\n\nMachine learning models\n\nHugging Face-compatible models\n\n\nNatural language processing (NLP) text classification: Categorizes text into predefined classes.\nTabular classification: Assigns categories to tabular dataset entries.\nTabular regression: Predicts continuous outcomes from tabular data.\n\n\nTree-based models (XGBoost / CatBoost / random forest)\n\n\nClassification: Predicts categorical outcomes using decision trees.\nRegression: Predicts continuous outcomes using decision trees.\n\n\nK-nearest neighbors (KNN)\n\n\nClassification: Assigns class by majority vote of the k-nearest neighbors.\nRegression: Predicts value based on the average of the k-nearest neighbors.\n\n\nClustering\n\n\nK-means: Partitions n observations into k clusters in which each observation belongs to the cluster with the nearest mean.\n\n\nNeural networks\n\n\nLong short-term memory (LSTM): Processes sequences of data, remembering inputs over long periods.\nRecurrent neural network (RNN): Processes sequences by maintaining a state that reflects the history of processed elements.\nConvolutional neural network (CNN): Primarily used for processing grid-like data such as images.\n\n\n\n\n\nGenerative AI models\n\nLarge language models (LLMs)\n\n\nClassification: Categorizes input into predefined classes.\nText summarization: Generates concise summaries from longer texts."
  },
  {
    "objectID": "guide/supported-models.html#supported-modeling-libraries-and-other-tools",
    "href": "guide/supported-models.html#supported-modeling-libraries-and-other-tools",
    "title": "Supported models",
    "section": "Supported modeling libraries and other tools",
    "text": "Supported modeling libraries and other tools\n\n\n\nscikit-learn: A Python library for machine learning, offering a range of supervised and unsupervised learning algorithms.\nstatsmodels: A Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and exploring data.\nPyTorch: An open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing.\nHugging Face Transformers: Provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, and text generation.\nXGBoost: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable, implementing machine learning algorithms under the Gradient Boosting framework.\n\n\n\n\nCatBoost: An open-source gradient boosting on decision trees library with categorical feature support out of the box, for ranking, classification, regression, and other ML tasks.\nLightGBM: A fast, distributed, high-performance gradient boosting (GBDT, GBRT, GBM, or MART) framework based on decision tree algorithms, used for ranking, classification, and many other machine learning tasks.\nR models, via rpy2 - R in Python: Facilitates the integration of R’s statistical computing and graphics capabilities with Python, allowing for R models to be called from Python.\nLarge language models (LLMs), via OpenAI-compatible APIs: Access to advanced AI models trained by OpenAI for a variety of natural language tasks, including text generation, translation, and analysis, through a compatible API interface. This support includes both the OpenAI API and the Azure OpenAI Service via API."
  },
  {
    "objectID": "guide/supported-models.html#whats-next",
    "href": "guide/supported-models.html#whats-next",
    "title": "Supported models",
    "section": "What’s next",
    "text": "What’s next\n\nRun tests & test suites\nTest descriptions\nCode samples"
  },
  {
    "objectID": "guide/comment-on-validation-reports.html",
    "href": "guide/comment-on-validation-reports.html",
    "title": "Comment on validation reports",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed faucibus, urna vel dignissim fringilla, est justo rhoncus ex, nec gravida eros nisl a nisi. Suspendisse potenti.\n[Include a brief description of what this task is about and why it’s important.]"
  },
  {
    "objectID": "guide/comment-on-validation-reports.html#prerequisites",
    "href": "guide/comment-on-validation-reports.html#prerequisites",
    "title": "Comment on validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as user roles they need to hold, software prerequisites, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/comment-on-validation-reports.html#steps",
    "href": "guide/comment-on-validation-reports.html#steps",
    "title": "Comment on validation reports",
    "section": "Steps",
    "text": "Steps\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-validation-reports.html#troubleshooting",
    "href": "guide/comment-on-validation-reports.html#troubleshooting",
    "title": "Comment on validation reports",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/comment-on-validation-reports.html#whats-next",
    "href": "guide/comment-on-validation-reports.html#whats-next",
    "title": "Comment on validation reports",
    "section": "What’s next",
    "text": "What’s next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html",
    "href": "guide/release-notes-2023-jun-22.html",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our developer framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with ValidMind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test suites. Learn more …\n\n\n\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. \n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#release-highlights",
    "href": "guide/release-notes-2023-jun-22.html#release-highlights",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our developer framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with ValidMind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test suites. Learn more …\n\n\n\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. \n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#enhancements",
    "href": "guide/release-notes-2023-jun-22.html#enhancements",
    "title": "June 22, 2023",
    "section": "Enhancements",
    "text": "Enhancements\nWe revised our QuickStart guide to be more modular and to highlight that our suggested starting point with the ValidMind Developer Framework is now Jupyter Hub. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "title": "June 22, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html",
    "href": "guide/samples-jupyter-notebooks.html",
    "title": "Code samples",
    "section": "",
    "text": "Our code samples, based on Jupyter notebooks, showcase the capabilities and features of the ValidMind Developer Framework, while also providing you with useful examples that you can build on and adapt for your own use cases."
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html#quickstart-for-customer-churn-model-documentation-full-suite",
    "href": "guide/samples-jupyter-notebooks.html#quickstart-for-customer-churn-model-documentation-full-suite",
    "title": "Code samples",
    "section": "QuickStart for Customer Churn Model Documentation — Full Suite",
    "text": "QuickStart for Customer Churn Model Documentation — Full Suite\nLearn how to use dataset and model documentation function on a simple customer churn model. The easiest way to try the QuickStart is on JupyterHub or Google Colaboratory:\n\n\n\nJupyter Hub\n\nA a web-based platform when you can interact with Jupyter Notebook instances on a shared server. It is commonly used as a collaborative and interactive computing environment for data analysis, scientific research, and programming.\n\n\n\n\n Try it on JupyterHub:\n\n\n\n\n\n\nGoogle Colaboratory (Colab)\n\nA free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time.\n\n\n\n\n Try it on Colab:"
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html#code-samples-for-your-use-case",
    "href": "guide/samples-jupyter-notebooks.html#code-samples-for-your-use-case",
    "title": "Code samples",
    "section": "Code samples for your use case",
    "text": "Code samples for your use case"
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling"
  },
  {
    "objectID": "guide/working-with-model-documentation.html",
    "href": "guide/working-with-model-documentation.html",
    "title": "Working with model documentation",
    "section": "",
    "text": "After you upload initial model documentation through the developer framework, use the platform UI to make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval."
  },
  {
    "objectID": "guide/working-with-model-documentation.html#prerequisites",
    "href": "guide/working-with-model-documentation.html#prerequisites",
    "title": "Working with model documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Developer role\nThe model is already registered in the model inventory\nYou have provided some content for the documentation, either by uploading it through the ValidMind Developer Framework or via the online UI editor"
  },
  {
    "objectID": "guide/working-with-model-documentation.html#key-concepts",
    "href": "guide/working-with-model-documentation.html#key-concepts",
    "title": "Working with model documentation",
    "section": "Key concepts",
    "text": "Key concepts\n\nEach section of your model documentation should address critical aspects of the model’s lifecycle, from conceptualization and data preparation through development and ongoing management. This comprehensive documentation approach is essential for ensuring the model’s reliability, relevance, and compliance with business and regulatory standards.\n\n\n\nconceptual soundness\n\nEstablishes the foundation of the model, covering the model overview, intended use and business use case, regulatory requirements, model limitations, and the rationale behind the model selection. It emphasizes the model’s purpose, scope, and constraints, which are crucial for stakeholders to understand the model’s applicability and limitations.\n\ndata preparation\n\nDetails the data description, including dataset summary, data quality tests, descriptive statistics, correlations and interactions, and feature selection and engineering. It provides transparency into the data used for model training, ensuring that the model is built on a solid and relevant dataset.\n\n\n\n\n\nmodel development\n\nDiscusses the model training, evaluation, explainability, interpretability, and diagnosis, including model weak spots, overfit regions, and robustness. This section is vital for understanding how the model was developed, how it performs, and its areas of strength and weakness.\n\nmonitoring and governance\n\nFocuses on the model’s ongoing monitoring plan, implementation, and governance plan. It outlines strategies for maintaining the model’s performance over time and ensuring that it remains compliant with regulatory requirements and ethical standards."
  },
  {
    "objectID": "guide/working-with-model-documentation.html#document-overview",
    "href": "guide/working-with-model-documentation.html#document-overview",
    "title": "Working with model documentation",
    "section": "Document Overview",
    "text": "Document Overview\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nThis is your Document Overview. This page shows a section-by-section outline of your project’s documentation, as well as summaries of:\n\nAny unresolved conversations\nThe number of model findings\nThe completion status for your model’s documentation\n\n\n\nUnresolved conversations\nHere on this documentation overview page, hover over the  # icon to see a preview of the unresolved conversation associated with your documentation section.\n\n\n\n\n\n\nResolve conversations directly from this popup, or click to expand the conversation.\n\n\n\n\n\nModel findings\nHere on this documentation overview page, hover over over the  # icon to see a preview of model findings associated with your documentation section.\n\n\n\n\n\n\nClick to expand the finding.\n\n\n\n\n\nRecent activity\nAt the bottom of the Document Overview, you will also see quick view of any documentation activity.\n\n\n\n\n\n\nClick on  See all Activity to be taken to the Model Activity page.\n\n\n\n\n\nAdd or edit documentation\n\nFrom the Document Overview, jump to any section of the model documentation by selecting the relevant section you would like to view.\nIn any section of the documentation, hover over text content and click to add or edit content.\nYou can now use the text editor functions to edit the content of the section. Whenever you make a change, an activity log entry is recorded under Section activity on the page, just below the section you are editing.\nSave your edits as a revision:\n\nClick on the  icon in the editor toolbar.\nSelect Save current revision and enter a name.\nClick  to save the current revision."
  },
  {
    "objectID": "guide/working-with-model-documentation.html#whats-next",
    "href": "guide/working-with-model-documentation.html#whats-next",
    "title": "Working with model documentation",
    "section": "What’s next",
    "text": "What’s next\n\n\n\n\n\n\n\nView documentation guidelines\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with content blocks\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssign documentation section statuses\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborate with others\n\n\n\n\n\n\n\n\n\n\n\n\n\nView documentation activity\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit for approval\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "href": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the developer framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/faq-data-handling.html",
    "href": "guide/faq-data-handling.html",
    "title": "Data handling",
    "section": "",
    "text": "The developer framework executes test suites and functions locally in your environment and is not limited by dataset size."
  },
  {
    "objectID": "guide/faq-data-handling.html#does-validmind-handle-large-datasets",
    "href": "guide/faq-data-handling.html#does-validmind-handle-large-datasets",
    "title": "Data handling",
    "section": "",
    "text": "The developer framework executes test suites and functions locally in your environment and is not limited by dataset size."
  },
  {
    "objectID": "guide/faq-data-handling.html#what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-data-handling.html#what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling",
    "section": "What about the confidentiality of data sent to ValidMind?",
    "text": "What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside of your environment. Additionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard.\nFor more information, refer to our data privacy policy."
  },
  {
    "objectID": "guide/faq-data-handling.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-data-handling.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-data-handling.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-data-handling.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the developer framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer makes a change in their development environment, such as in a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind automatically recreates the relevant figures and tables, updating them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-data-handling.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-data-handling.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur developer framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more.\nEach test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/faq-data-handling.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-data-handling.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers will be able to register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/quickstart-explore-sample-model-documentation.html",
    "href": "guide/quickstart-explore-sample-model-documentation.html",
    "title": "Explore sample model documentation",
    "section": "",
    "text": "First, let’s take a look at how the ValidMind handles model documentation. The best place to start is with the ValidMind Platform UI.\nThe ValidMind Platform UI is the central place to:"
  },
  {
    "objectID": "guide/quickstart-explore-sample-model-documentation.html#before-you-begin",
    "href": "guide/quickstart-explore-sample-model-documentation.html#before-you-begin",
    "title": "Explore sample model documentation",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account."
  },
  {
    "objectID": "guide/quickstart-explore-sample-model-documentation.html#steps",
    "href": "guide/quickstart-explore-sample-model-documentation.html#steps",
    "title": "Explore sample model documentation",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nClick Model Inventory.\nLocate or search for the [QuickStart] Customer Churn Model - Initial Validation and select it.\n\nOn the model overview page that opens, you can find important information about the model, such as:\n\nThe use case\nThe owners, validators, developers, and business unit associated with the model\nThe risk tier, model status, and current version\nModel findings, recent activity, and much more\n\nIn the left sidebar, you can find helpful links to the model documentation, model findings, validation report, (activity) archive, and getting started information for integrating with the ValidMind Developer Framework.\n\n\n\n\n\n\nNote that the model status is In Initial Validation. This is the status that a model starts in as part of the default workflow. You can click  See workflow to visualize the entire workflow that this model will go through.\n\n\n\nIn the left sidebar, select Documentation &gt; 2. Data preparation &gt; 2.1. Data description.\n\n\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically whenever a test result is above or below a certain threshold.\n\nIn the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the developer framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nIf you expand the ValidMind Insights right sidebar, the documentation guidelines can tell you more about what these sections mean and help you with the task of documenting the model as a developer. If you are a validator, this is also where you can add findings.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the developer framework, but they get added by the model developer in the platform UI. You can add both new text and test sections, called blocks to your model documentation."
  },
  {
    "objectID": "guide/quickstart-explore-sample-model-documentation.html#whats-next",
    "href": "guide/quickstart-explore-sample-model-documentation.html#whats-next",
    "title": "Explore sample model documentation",
    "section": "What’s next",
    "text": "What’s next\nContinue with Register your first model to learn more about using the ValidMind Platform hands-on."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend."
  },
  {
    "objectID": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "href": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-var-vm_dev",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-var-vm_dev",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the developer framework?",
    "text": "Can the documentation be initialized from the UI instead of the developer framework?\nValidMind allows you to write documentation directly in the online UI editor, without having to use the developer framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the developer framework, you can execute test suites and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-colab.html",
    "href": "guide/quickstart-try-developer-framework-with-colab.html",
    "title": "Try it with Google Colaboratory",
    "section": "",
    "text": "Learn how to document a model with ValidMind on Google Colaboratory."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-colab.html#before-you-begin",
    "href": "guide/quickstart-try-developer-framework-with-colab.html#before-you-begin",
    "title": "Try it with Google Colaboratory",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\nYou must have access to Google Colaboratory (Colab).\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there.\n\n\n\n\n\n\nAbout our Jupyter notebooks\n\n\n\nNotebooks from ValidMind are safe to run — If you get a warning that this notebook was not authored by Google, we welcome you to inspect the notebook source.  Runtime errors — We recommend that you not use the Run all option. Run each cell individually to see what is happening in the notebook. If you do see errors, re-run the notebook cells."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-colab.html#steps",
    "href": "guide/quickstart-try-developer-framework-with-colab.html#steps",
    "title": "Try it with Google Colaboratory",
    "section": "Steps",
    "text": "Steps\n\nOpen the QuickStart notebook in Google Colaboratory: \n\nClick File &gt; Save a copy in Drive to make a copy of the QuickStart notebook so that you can modify it later.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-colab.html#whats-next",
    "href": "guide/quickstart-try-developer-framework-with-colab.html#whats-next",
    "title": "Try it with Google Colaboratory",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore sample model documentation to learn more about using the ValidMind AI risk platform hands-on."
  },
  {
    "objectID": "guide/manage-roles-and-permissions.html",
    "href": "guide/manage-roles-and-permissions.html",
    "title": "Manage roles & permissions",
    "section": "",
    "text": "These steps guide you through the process of configuring and overseeing roles and permissions in the ValidMind Platform UI. By carefully defining what users can and cannot do, you can ensure that the right individuals have the appropriate level of access to resources and can perform the correct actions."
  },
  {
    "objectID": "guide/manage-roles-and-permissions.html#prerequisites",
    "href": "guide/manage-roles-and-permissions.html#prerequisites",
    "title": "Manage roles & permissions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo perform these steps, you must be a group administrator or a superuser."
  },
  {
    "objectID": "guide/manage-roles-and-permissions.html#add-new-roles",
    "href": "guide/manage-roles-and-permissions.html#add-new-roles",
    "title": "Manage roles & permissions",
    "section": "Add new roles",
    "text": "Add new roles\nTo add a new role:\n\nLog in to the ValidMind UI.\nUnder User Permissions in the sidebar, select Roles & Permissions.\nClick on the Add New Role button to initiate the role creation process.\n\nSpecify a name for the new role.\nDefine the role’s description and purpose.\nAssign permissions to the role based on the actions it should be allowed to perform.\nSave the new role configuration.\n\nVerify that the new role appears in the list of available roles, ensuring it is correctly set up for use."
  },
  {
    "objectID": "guide/manage-roles-and-permissions.html#manage-permissions",
    "href": "guide/manage-roles-and-permissions.html#manage-permissions",
    "title": "Manage roles & permissions",
    "section": "Manage permissions",
    "text": "Manage permissions\nTo manage the permissions for existing roles:\n\nLog in to the ValidMind UI.\nNavigate to the Roles & Permissions page.\nLocate the role for which you want to manage permissions.\n\nClick on the role to access its details.\n\nWithin the role’s details, you can:\n\nView the current permissions assigned to the role.\nAdd new permissions by specifying the actions or resources the role should have access to.\nRemove permissions that are no longer necessary for the role.\nSave the updated permissions configuration for the role.\n\n\n\n\n\n\n\n\nBe careful when modifying permissions to ensure they align with the role’s intended responsibilities and access requirements."
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe developer framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "href": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe developer framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the developer framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the developer framework. You will also be able to connect your own custom tests with the developer framework. These custom tests will be configurable and able to run programmatically, just like the rest of the developer framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur developer framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the developer framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s developer framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/quickstart-register-your-first-model.html",
    "href": "guide/quickstart-register-your-first-model.html",
    "title": "Register your first model",
    "section": "",
    "text": "To be able to document models with the ValidMind Platform, you need to register them in the model inventory first. Let’s show you how."
  },
  {
    "objectID": "guide/quickstart-register-your-first-model.html#before-you-begin",
    "href": "guide/quickstart-register-your-first-model.html#before-you-begin",
    "title": "Register your first model",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account."
  },
  {
    "objectID": "guide/quickstart-register-your-first-model.html#steps",
    "href": "guide/quickstart-register-your-first-model.html#steps",
    "title": "Register your first model",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nTo register a model for use with the QuickStart notebook, select:\n\nDocumentation template: Binary classification\nUse case: Attrition/Churn Management\n\n\nYou can fill in other options according to your own preference.\nClick Continue.\nAfter the model is registered, you get initial model documentation and a validation report based on the documentation template."
  },
  {
    "objectID": "guide/quickstart-register-your-first-model.html#whats-next",
    "href": "guide/quickstart-register-your-first-model.html#whats-next",
    "title": "Register your first model",
    "section": "What’s next",
    "text": "What’s next\nContinue with Generate documentation for your model to learn how to use the QuickStart notebook you looked at earlier with your first model."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "View the guidelines for validation reports associated with a template to ensure that you are compliant with the requirements for validation reports."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Validator role\nThe model you are documenting is registered in the model inventory\nModel documentation has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform\n\n\n\n\n\n\n\nConfiguring the validation guidelines for each template requires the Customer Admin role."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Validation Report.\n\n\n\nIn any section of the validation report, click ValidMind Insights in the top-right corner to expand the ValidMind Insights sidebar:\n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s next",
    "text": "What’s next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/register-models-in-model-inventory.html",
    "href": "guide/register-models-in-model-inventory.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "To be able to document models using ValidMind’s model documentation and validation features, you need to register them in the model inventory first."
  },
  {
    "objectID": "guide/register-models-in-model-inventory.html#prerequisites",
    "href": "guide/register-models-in-model-inventory.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are the model owner\nYou hold the Developer role"
  },
  {
    "objectID": "guide/register-models-in-model-inventory.html#steps",
    "href": "guide/register-models-in-model-inventory.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nClick Model Inventory in the left sidebar. \nClick Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models-in-model-inventory.html#whats-next",
    "href": "guide/register-models-in-model-inventory.html#whats-next",
    "title": "Register models in the inventory",
    "section": "What’s next",
    "text": "What’s next\n\nEdit model inventory fields\nGenerate documentation for your model"
  },
  {
    "objectID": "guide/assess-compliance.html",
    "href": "guide/assess-compliance.html",
    "title": "Assess compliance",
    "section": "",
    "text": "To provide an unbiased starting point that enables more efficient discussions between validators and developers, you assess compliance with guidelines based on analyzing evidence and findings."
  },
  {
    "objectID": "guide/assess-compliance.html#prerequisites",
    "href": "guide/assess-compliance.html#prerequisites",
    "title": "Assess compliance",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Validator role\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation as Ready for Validation"
  },
  {
    "objectID": "guide/assess-compliance.html#link-evidence-to-reports",
    "href": "guide/assess-compliance.html#link-evidence-to-reports",
    "title": "Assess compliance",
    "section": "Link evidence to reports",
    "text": "Link evidence to reports\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Validation Report and then 2. Validation.\nYou can now jump to any subsection of the validation report by expanding the table of contents on the left and selecting the relevant section you would like to work with.\nFor example: Select 2.1.1. Assumptions.\nIn any section of the documentation where the button is available, hover over text content and click  Link evidence to report.\n\nOn the Link evidence to validation report page that opens:\n\nFrom the Tests or Metrics tab, select the evidence that is related to your assessment.\nIf you are not sure if something is relevant, you can toggle the section to expand it for more details.\n\nClick Update linked evidence.\n\nThe newly linked-to evidence now gets shown under Developer Evidence."
  },
  {
    "objectID": "guide/assess-compliance.html#link-findings-to-reports",
    "href": "guide/assess-compliance.html#link-findings-to-reports",
    "title": "Assess compliance",
    "section": "Link findings to reports",
    "text": "Link findings to reports\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Validation Report and then 2. Validation.\nYou can now jump to any subsection of the validation report by expanding the table of contents on the left and selecting the relevant section you would like to work with.\nFor example: Select 2.1.1. Assumptions.\nIn any section of the documentation where the button is available, hover over text content and click  Link finding to report.\n\nOn the Link finding to report page that opens, select from the list of available findings, or create a new finding.\nClick Update linked findings.\n\nThe newly linked-to finding now gets shown under Findings."
  },
  {
    "objectID": "guide/assess-compliance.html#provide-your-compliance-assessments",
    "href": "guide/assess-compliance.html#provide-your-compliance-assessments",
    "title": "Assess compliance",
    "section": "Provide your compliance assessments",
    "text": "Provide your compliance assessments\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Validation Report and then 2. Validation.\nYou can now jump to any subsection of the validation report by expanding the table of contents on the left and selecting the relevant section you would like to work with.\nFor example: Select 2.1.1. Assumptions.\nIn any section of the documentation where the Assessment dropdown menu is available, select one of the available options:\n\nNo compliance — No tests have been run and no other evidence has been provided\nSome compliance — Evidence provided but not properly documented or verifiable\nStrong compliance — Evidence provided and documented, but not validated\nFull compliance — Evidence provided, documented, and validated\n\nFor example: To indicate that there is some compliance based on the evidence or findings you linked to:\n\nOn the Link finding to report page that opens, select from the list of available findings, or create a new finding.\nUnder Risk Assessment Notes, add any relevant notes that explain your assessment further.\n\nA compliance summary gets shown for each subsection under 2. Validation and provides a quick overview for current qualitative and quantitative risk assessments:"
  },
  {
    "objectID": "guide/join-community.html",
    "href": "guide/join-community.html",
    "title": "ValidMind",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "When you’re ready, verify the approval workflow, and then submit your model documentation or validation report for approval."
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Developer role and are ready to submit your model documentation for review\nYou hold the Validator role and are ready submit their validation report for review\nThe model you are documenting is registered in the model inventory\n\n\n\n\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/submit-for-approval.html#verify-the-workflow-and-view-current-status",
    "href": "guide/submit-for-approval.html#verify-the-workflow-and-view-current-status",
    "title": "Submit for approval",
    "section": "Verify the workflow and view current status",
    "text": "Verify the workflow and view current status\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nThe current status of the project is displayed under Document Status.\nFor example:\n\nIn Documentation means that the model is currently being documented and can be submitted for validation next.\nIn Validation means that the model is currently being validated and can be submitted for review and then approval.\n\nClick  See workflow to visualize the entire workflow that this model will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#advance-model-documentation-or-validation-reports-in-the-workflow",
    "href": "guide/submit-for-approval.html#advance-model-documentation-or-validation-reports-in-the-workflow",
    "title": "Submit for approval",
    "section": "Advance model documentation or validation reports in the workflow",
    "text": "Advance model documentation or validation reports in the workflow\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nExamples:\n\nTo submit model documentation for validation: Click Ready for Validation to indicate that a model developer has completed the initial model documentation and is ready to have it validated. Add any notes that need to be included and then click Ready for Validation.\nTo submit validation reports for review and approval: Click Ready for Review to indicate that you have completed your initial model validation report, fill in the mandatory notes, and submit.\nTo request revisions to model documentation or validation reports: Click Request Revision, fill in the mandatory notes to explain the changes that are required, and submit.\nTo have your model documentation and validation report approved: Click Ready for Approval, fill in the mandatory notes, and submit.\n\nFor you to be able to transition through the approval workflow, your model documentation or validation report must have completed all required steps. For example, you cannot submit a validation report for review until the model documentation itself has been submitted."
  },
  {
    "objectID": "guide/generate-model-documentation.html",
    "href": "guide/generate-model-documentation.html",
    "title": "Generate model documentation",
    "section": "",
    "text": "A typical high-level workflow for model developers consists of four major steps:\n\n\n\n\ngraph LR\n    A[Develop&lt;br&gt;model] --&gt; B[Generate model&lt;br&gt;documentation]\n    B --&gt; C[Refine model&lt;br&gt;documentation]\n    C --&gt; D[Submit for review]\n    C --&gt; B\n\n\n\n\n\n\n\nDevelop your model: In your existing developer environment, build one or more candidate models that need to be validated. This step includes all the usual activities you already follow as a model developer.\nGenerate model documentation: With the ValidMind Developer Framework, generate automated model documentation and run validation tests. This step includes making use of the automation and testing functionality provided by the framework and uploading the output to the platform UI. You can iteratively regenerate the documentation as you work though the next step of refining your documentation.\nRefine model documentation: In the ValidMind Platform UI, review the generated documentation and test output. Iterate over the documentation and test output to refine your model documentation. Collaborate with other developers and model validators to finalize the model documentation and get it ready for review.\nSubmit for review: In the ValidMind Platform UI, you submit the model documentation for review which moves the documentation workflow moves to the next phase where a model validator will review it.\n\nBefore you can use the Developer Framework, you need to verify that the current documentation template contains all the necessary tests for the model you are developing:\n\nThe template might already be sufficient and you only need to run the template within the Developer Framework to populate documentation.\nOr, more likely, the template might need additional tests that you can add these tests via the Developer Framework."
  },
  {
    "objectID": "guide/generate-model-documentation.html#how-do-i-use-the-framework",
    "href": "guide/generate-model-documentation.html#how-do-i-use-the-framework",
    "title": "Generate model documentation",
    "section": "",
    "text": "A typical high-level workflow for model developers consists of four major steps:\n\n\n\n\ngraph LR\n    A[Develop&lt;br&gt;model] --&gt; B[Generate model&lt;br&gt;documentation]\n    B --&gt; C[Refine model&lt;br&gt;documentation]\n    C --&gt; D[Submit for review]\n    C --&gt; B\n\n\n\n\n\n\n\nDevelop your model: In your existing developer environment, build one or more candidate models that need to be validated. This step includes all the usual activities you already follow as a model developer.\nGenerate model documentation: With the ValidMind Developer Framework, generate automated model documentation and run validation tests. This step includes making use of the automation and testing functionality provided by the framework and uploading the output to the platform UI. You can iteratively regenerate the documentation as you work though the next step of refining your documentation.\nRefine model documentation: In the ValidMind Platform UI, review the generated documentation and test output. Iterate over the documentation and test output to refine your model documentation. Collaborate with other developers and model validators to finalize the model documentation and get it ready for review.\nSubmit for review: In the ValidMind Platform UI, you submit the model documentation for review which moves the documentation workflow moves to the next phase where a model validator will review it.\n\nBefore you can use the Developer Framework, you need to verify that the current documentation template contains all the necessary tests for the model you are developing:\n\nThe template might already be sufficient and you only need to run the template within the Developer Framework to populate documentation.\nOr, more likely, the template might need additional tests that you can add these tests via the Developer Framework."
  },
  {
    "objectID": "guide/generate-model-documentation.html#how-do-i-generate-documentation",
    "href": "guide/generate-model-documentation.html#how-do-i-generate-documentation",
    "title": "Generate model documentation",
    "section": "How do I generate documentation?",
    "text": "How do I generate documentation?\nThis process of verifying the suitability of the the current documentation template and adding more tests to the template is an iterative process:\n\n\n\n\ngraph LR\n    A[Verify template] --&gt; B[Build template]\n    B --&gt; D[Add tests and&lt;br&gt;content blocks]\n    D --&gt; E[Add external&lt;br&gt;test providers]\n    E --&gt; C[Run template]\n    C --&gt; B\n\n\n\n\n\n\n\n\nBuild the template\n\nWhen the documentation template requires more tests to be added, or if the documentation template does not include a specific content or test block you need:\n\n\n\nFor functionality provided by the Developer Framework: Add the relevant tests or content blocks for the model use case.\nFor tests not provided by the framework: Add your own external test provider.\n\nRun the template : When you have registered all the required tests as content blocks in the documentation template, populate the necessary model documentation by adding this call to your model:\nrun_documentation_tests()\n\n\n\n\n\n\nValidMind may not support all potential use cases or provide a universally applicable documentation template. Typically, you initiate the process of putting ValidMind into production by constructing a template specific for your own use case and then refine your the documentation project."
  },
  {
    "objectID": "guide/generate-model-documentation.html#end-to-end-workflow",
    "href": "guide/generate-model-documentation.html#end-to-end-workflow",
    "title": "Generate model documentation",
    "section": "End-to end workflow",
    "text": "End-to end workflow\n\nIn your modeling environment\n\nBuild your model.\nExport the datasets and model.\n\nNext, go to With the Developer Framework, Step 2. \n\n\nWith the Developer Framework\n\nCreate a notebook to select and build the relevant tests.\n From your modeling environment, load the trained datasets and models.\n Use the instructions from In the platform UI, Step 3, initialize the ValidMind Developer Framework.\nSelect the relevant tests.\nReview if all tests are covered by ValidMind or your external test provider:\n\nIf all tests are NOT covered: Create and register additional tests.\nIf all tests are covered:\n\nRun the selected tests.\nReview your test results.\n\n\n\nNext, go to In the ValidMind Platform UI, Step 5. \n\n\nIn the ValidMind Platform UI\n\nRegister a new model.\nReview the template structure.\nLocate the framework integration instructions.\nGo to With the Developer Framework, Step 3. \n After With the Developer Framework, Step 6, add content blocks to your model documentation:\nSelect the block type:\n\nFor test-driven blocks: Select from available test provider results\nFor text blocks:\n\nFor new block:\n\nAdd new editable text content block\nReview and collaborate on the content block\n\nFor existing blocks: Select from available texts from content provider\n\n\nSubmit your documentation project for review."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html",
    "href": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html",
    "title": "Try it in your own developer environment",
    "section": "",
    "text": "Learn how to document a model with ValidMind locally in your own developer environment. You can either clone our open-source repository or download the code samples to run the notebook."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#before-you-begin",
    "href": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#before-you-begin",
    "title": "Try it in your own developer environment",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account.\n\n\nYou should be familiar with using standard software development tools, including the command line, and have a developer environment set up locally, such as Visual Studio Code. To run our sample Jupyter notebooks locally, your developer environment must support Python ≧3.8 and &lt;3.11.\nTo clone our open-source repository, you must have access to GitHub. Alternatively, you must be able to download a .zip file containing our code samples that include the introductory Jupyter notebook."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#option-1-clone-the-open-source-repository",
    "href": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#option-1-clone-the-open-source-repository",
    "title": "Try it in your own developer environment",
    "section": "Option 1: Clone the open-source repository",
    "text": "Option 1: Clone the open-source repository\n\nOpen a terminal or command prompt.\nNavigate to the directory where you want to clone the repository.\nRun the command:\ngit clone https://github.com/validmind/developer-framework.git\nAfter the cloning process is complete, open notebooks/code_samples/quickstart_customer_churn_full_suite.ipynb in your developer environment and run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will guide you through installing the ValidMind Developer Framework, initializing the Python environment, and finally initializing the ValidMind Client Library by connecting to the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the developer framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the platform UI."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#option-2-download-the-code-samples",
    "href": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#option-2-download-the-code-samples",
    "title": "Try it in your own developer environment",
    "section": "Option 2: Download the code samples",
    "text": "Option 2: Download the code samples\n\nIn a browser, download notebooks.zip.\nNavigate to the location where you saved notebooks.zip and extract the contents.\nDepending on your operating system, you can right-click on notebooks.zip and select Extract All … or Extract Here, for example.\nNavigate to the folder where you extracted the files.\nOpen notebooks/code_samples/quickstart_customer_churn_full_suite.ipynb in your developer environment and run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will guide you through installing the ValidMind Developer Framework, initializing the Python environment, and finally initializing the ValidMind Client Library by connecting to the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the developer framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the platform UI."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#whats-next",
    "href": "guide/quickstart-try-developer-framework-in-your-own-developer-environment.html#whats-next",
    "title": "Try it in your own developer environment",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore sample model documentation to learn more about using the ValidMind AI risk platform hands-on."
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in machine learning (ML), natural language processing (NLP), and large language models (LLMs). The platform offers tools that cater to both model developers and validators, simplifying key aspects of model risk management."
  },
  {
    "objectID": "guide/get-started.html#welcome-to-validmind",
    "href": "guide/get-started.html#welcome-to-validmind",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in machine learning (ML), natural language processing (NLP), and large language models (LLMs). The platform offers tools that cater to both model developers and validators, simplifying key aspects of model risk management."
  },
  {
    "objectID": "guide/get-started.html#what-do-i-use-the-validmind-platform-for",
    "href": "guide/get-started.html#what-do-i-use-the-validmind-platform-for",
    "title": "Get started",
    "section": "What do I use the ValidMind platform for?",
    "text": "What do I use the ValidMind platform for?\nModel developers and validators play important roles in managing model risk, including risk that stems from generative AI and machine learning models. From complying with regulations to ensuring that institutional standards are followed, your team members are tasked with the careful documentation, testing, and independent validation of models.\nThe purpose of these efforts is to ensure that good risk management principles are followed throughout the model lifecycle. To assist you with these processes of documenting and validating models, ValidMind provides a number of tools that you can employ regardless of the technology used to build your models.\n\n\n\n\n\n\nThe ValidMind AI risk platform provides two main products components:\n\nThe ValidMind Developer Framework is a library of tools and methods designed to automate generating model documentation and running validation tests. The framework is designed to be platform agnostic and integrates with your existing development environment.\nFor Python developers, a single installation command provides access to all the functions:\npip install validmind\nThe ValidMind Platform UI is an easy-to-use web-based UI that enables you to track the model lifecycle:\n\n\nCustomize workflows to manage the model documentation and validation process.\nReview and edit the documentation and test metrics generated by the developer framework.\nCollaborate with and capture feedback from model developers and model validators.\nGenerate validation reports and approvals.\n\nFor more information about the benefits that ValidMind can offer, check out the ValidMind overview.\n\n\n\n\n\n\n Key ValidMind concepts\n\n\n\n\n\n\n\n\nmodel documentation\n\nA structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses.\n\n\nWithin the realm of model risk management, this documentation serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n\nvalidation report\n\nA formal document produced after a model validation process, outlining the findings, assessments, and recommendations related to a specific model’s performance, appropriateness, and limitations. Provides a comprehensive review of the model’s conceptual framework, data sources and integrity, calibration methods, and performance outcomes.\n\n\nWithin model risk management, the validation report is crucial for ensuring transparency, demonstrating regulatory compliance, and offering actionable insights for model refinement or adjustments.\n\ntemplate, documentation template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n\n\nValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results. When rendered, produces a document that model developers can use for model validation.\n\ntest\n\nA function contained in the developer framework, designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind platform to generate the model documentation according to the template that is associated with the documentation.\n\n\nTests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n\nmetrics, custom metrics\n\nMetrics are a subset of tests that do not have thresholds. Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\n\n\nIn the context of ValidMind’s Jupyter notebooks, metrics and tests can be thought of as interchangeable concepts.\n\ninputs\n\nObjects to be evaluated and documented in the developer framework. They can be any of the following:\n\n\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model(). See the Model Documentation or the for more information.\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset(). See the Dataset Documentation for more information.\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\n\nparameters\n\nAdditional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\n\noutputs\n\nCustom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n\ntest suite\n\nA collection of tests which are run together to generate model documentation end-to-end for specific use cases.\n\n\nFor example, the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use cases."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nOn the ValidMind platform, everything starts with the model inventory: you first register a new model and then manage the model lifecycle through the different activities that are part of your existing model risk management processes.\n\nApproval workflow\nA typical high-level model approval workflow looks like this:\n\n\n\n\ngraph LR\n    A[Model&lt;br&gt;registration] --&gt; B[Initial&lt;br&gt;validation]\n    B --&gt; C[Validation&lt;br&gt;approval]\n    C --&gt; D[In production]\n    D --&gt; E[Periodic review&lt;br&gt;and revalidation]\n    E --&gt; B\n\n\n\n\n\n\nNew model registration\n\nSelect a documentation template when registering a new inventory model to start your model documentation. You then use the model inventory to manage the metadata associated with the model, including all compliance and regulatory attributes.\n\nInitial validation\n\nTriggers a new documentation workflow to yield a model that will be ready for production deployment after its documentation and validation reports have been approved.\n\nValidation approval\n\nPerform validation of the model to ensure that it meets the needs for which it was designed. You can also connect to third-party systems to send events when a model has been approved for production.\n\nIn production\n\nUse the model in production while ensuring its ongoing reliability, accuracy, and compliance with regulations by monitoring the model’s performance.\n\nPeriodic review and revalidation\n\nAs part of regular performance monitoring or change management, you follow a process similar to that seen in the Initial validation step."
  },
  {
    "objectID": "guide/get-started.html#next-steps",
    "href": "guide/get-started.html#next-steps",
    "title": "Get started",
    "section": "Next steps",
    "text": "Next steps\nThe fastest way to explore what ValidMind can offer is with our QuickStart:\n\nTry out the ValidMind Developer Framework\nExplore the ValidMind Platform UI\n\nIf you have already tried the QuickStart, more how-to instructions and links to our FAQs can be found under Next steps.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account."
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to platform UI.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html",
    "href": "guide/release-notes-2023-may-30.html",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export model documentation to Word documents from the platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind Developer Framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#release-highlights",
    "href": "guide/release-notes-2023-may-30.html#release-highlights",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export model documentation to Word documents from the platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind Developer Framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#bugfixes",
    "href": "guide/release-notes-2023-may-30.html#bugfixes",
    "title": "May 30, 2023",
    "section": "Bugfixes",
    "text": "Bugfixes\n\nFixed the display alignment in certain pages of the UI.\nFixed display issues related to Helvetica Neue font not available for Windows users.\nFixed an issue preventing users to drag & drop image files directly in the online editor.\nAdjusted filters for the Model Inventory search box."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "href": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "title": "May 30, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, refresh your browser.\nTo upgrade the ValidMind Developer Framework:\n\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/test-sandbox.html",
    "href": "guide/test-sandbox.html",
    "title": " Test sandbox (BETA)",
    "section": "",
    "text": "Explore our interactive sandbox to see what tests are available in the ValidMind Developer Framework and how you can use them in your own code."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "To keep your network traffic private and minimizes its attack surface, configure AWS PrivateLink to establish a private connection between ValidMind and your company network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\nEmail support@validmind.com\nEmail support@validmind.com"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the developer framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s next",
    "text": "What’s next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/collaborate-with-others.html",
    "href": "guide/collaborate-with-others.html",
    "title": "Collaborate with others",
    "section": "",
    "text": "Use the real-time collaboration features to track changes, add comments, and access the revision history for model documentation and validation reports."
  },
  {
    "objectID": "guide/collaborate-with-others.html#feature-highlights",
    "href": "guide/collaborate-with-others.html#feature-highlights",
    "title": "Collaborate with others",
    "section": "Feature highlights",
    "text": "Feature highlights\n\nReal-time collaboration. Simultaneously edit model documentation, leave and respond to comments suggestions, and access revision history. Changes to model documentation are also automatically added to ValidMind’s activity feed.\nSpell and grammar checker. Have your content checked automatically by the built-in spell and grammar checker.\nMath formulas. Add math formulas to documentation by clicking the MathType button and using the toolbar, or switch to handwriting."
  },
  {
    "objectID": "guide/collaborate-with-others.html#prerequisites",
    "href": "guide/collaborate-with-others.html#prerequisites",
    "title": "Collaborate with others",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are logged into the ValidMind Platform\nYou hold the Developer or Validator role\nThe model you are documenting is registered in the model inventory\nModel documentation has submitted for review or validation by the model validation team"
  },
  {
    "objectID": "guide/collaborate-with-others.html#tracking-changes",
    "href": "guide/collaborate-with-others.html#tracking-changes",
    "title": "Collaborate with others",
    "section": "Tracking changes",
    "text": "Tracking changes\n\nSuggest a change\n\nClick the Track changes button in the toolbar to turn on suggestion mode.\nMake your changes to the model documentation. When changes tracking is enabled, other contributors can accept or decline the suggested changes.\n\n\n\nResolve changes\n\nSuggested changes appear in green or red highlighted text, depending on if the change is adding or removing content. To accept or decline a change, click the highlighted text, then click  or . You can also reply to a suggested change.\nTo mass accept or decline suggestions, click the dropdown arror next to the Track changes button and click the desired option."
  },
  {
    "objectID": "guide/collaborate-with-others.html#revision-history",
    "href": "guide/collaborate-with-others.html#revision-history",
    "title": "Collaborate with others",
    "section": "Revision history",
    "text": "Revision history\n\nSave a version\n\nClick the Revision history button in the toolbar.\nIn the dropdown, click Save current version. Optionally, enter a version name. The default name is the date and time the latest change was made.\n\n\n\nView revision history\n\nClick the Revision history button in the toolbar, then click Open revision history. Here, you can view a history of all saved versions and your current version.\nTo see the the change made with each version, select the version in the right sidebar. Changes made in that version are highlighted. Hover over the highlighted content to see who made the change.\n\n\n\nRestore a version\n\nTo restore a version, select the desired version and click Restore this version.\nThe restored version will now appear under revision history with the name: “Restored: ‘version name’”. To exit revision history without restoring a version, click Back to editing."
  },
  {
    "objectID": "guide/collaborate-with-others.html#commenting",
    "href": "guide/collaborate-with-others.html#commenting",
    "title": "Collaborate with others",
    "section": "Commenting",
    "text": "Commenting\n\nPost comments\n\nIn any section of the model documentation, select the portion of text you want to comment on, and click the Comment button in the toolbar.\nEnter your comment and click Comment.\nYou can view the comment by clicking the highlighted text. Comments will also appear in the right sidebar.\n\n\n\nReply to an existing comment\n\nClick the highlighted text portion to view the comment thread.\nEnter your comment and click Reply.\nYou can view the comment thread by clicking the highlighted text.\n\n\n\n\n\n\n\n\n\nAll users associated with a model, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page.\n\n\n\n\n\nResolve comment threads and viewing archived comments\n\nClick the highlighted text portion to view the thread, then click  to resolve the thread.\nTo view the resolved comment thread, click the Comment archive button in the toolbar. You can view a history of all archived comments in the Comment archive.\nTo reopen a comment thread, reply to the comment thread in the Comment archive or click the Reopen button that appears next to the highlighted text portion.\n\n\n\nEditing and deleting comments\n\nClick the highlighted text portion to access the comment thread.\nTo edit a comment in the thread, click the More options icon for the corresponding comment and click Edit.\nEdit your comment and click Save.\nTo edit a comment in a resolved thread, follow the same steps but click the Comments archive button first to access the resolved thread.\n\n\n\n\n\n\n\n\n\n\n\nAll users associated with a model, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/collaborate-with-others.html#whats-next",
    "href": "guide/collaborate-with-others.html#whats-next",
    "title": "Collaborate with others",
    "section": "What’s next",
    "text": "What’s next\n\nPreparing validation reports\nWork with model findings"
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html",
    "href": "guide/release-notes-2023-jul-24.html",
    "title": "July 24, 2023",
    "section": "",
    "text": "This release improves the developer experience within the ValidMind Developer Framework and introduces an updated notebook to demonstrate support for NLP models, now using CatBoost for greater performance. Moreover, we’ve upgraded our platform UI with several major new features, including an all-new text editor, the capability to insert new content blocks into model documentation, template swapping for keeping your model documentation current, and much more.\n\n\n\n\n\nUpdated notebook for NLP use case. Now with enhanced performance by using CatBoost compared to the original PyTorch-based notebook, this updated notebook showcases natural language processing (NLP) data analysis and text classification using the ValidMind Developer Framework with emphasis on the in-depth analysis and preprocessing of COVID-19 tweets. \nImproved developer experience when you encounter error conditions. Unhandled errors now return an error code with a more descriptive message that can help you with looking them up in the troubleshooting section of our documentation. We also improved error handling when tests fail to run and the test results summary widget now display error information for each test that fails to run.\n\n\n\n\n\nNew full-featured text editor. When collaborating on model documentation in the platform UI with others, you now have a new text editor at your disposal. The interface should be intuitive and familiar, allowing you to manage model documentation and validation reports with greater ease. In addition to the new editing features, you can use the editor to participate in comment threads, track changes, see the revision history, enter math formulas, and more. Try it …\n\n\n\nAdd new content blocks to model documentation. For model developers and model validators who want to add new sections to model documentation, you can now do just that. The new content blocks feature provide you with additional sections that you fill in with text, metrics, and test results. Supported content block types:\nSimple text block: Can be added anywhere on model documentation pages and edited to include additional documentation in text format.\nTest-driven block: Can be added to display one of the supported metrics or threshold test results collected by the developer framework.\n\nYou can add new content block to an existing model documentation simply by hovering over the dashed line between sections. Try it …\n\n\nSwap templates for model documentation. This feature is designed to enhance the flexibility of documenting models. With it, you can seamlessly update an existing model documentation by either switching to an entirely different template or by simply upgrading to a more recent version of the template currently in use. This new feature ensures that your model documentation can remain up-to-date and align with the latest preferred template without the need to start from scratch. Try it …\nValidMind Platform UI enhancements: \n\nRenamed Client Integration page to Getting Started. To better reflect when you use the credentials to generate model documentation with the ValidMind Developer Framework, we have renamed the Client Integration page to Getting Started.\nImproved activity widget: We enhanced the user interface with new tabs, enabling you to filter events based on activity type.\nImproved Findings section and Findings Details page: We made UI enhancements for both project findings and the findings details sections for a better look and feel.\nRefined Models and Projects lists : We refined the user interface, featuring more intuitive default sorting and more user-friendly filtering."
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#release-highlights",
    "href": "guide/release-notes-2023-jul-24.html#release-highlights",
    "title": "July 24, 2023",
    "section": "",
    "text": "This release improves the developer experience within the ValidMind Developer Framework and introduces an updated notebook to demonstrate support for NLP models, now using CatBoost for greater performance. Moreover, we’ve upgraded our platform UI with several major new features, including an all-new text editor, the capability to insert new content blocks into model documentation, template swapping for keeping your model documentation current, and much more.\n\n\n\n\n\nUpdated notebook for NLP use case. Now with enhanced performance by using CatBoost compared to the original PyTorch-based notebook, this updated notebook showcases natural language processing (NLP) data analysis and text classification using the ValidMind Developer Framework with emphasis on the in-depth analysis and preprocessing of COVID-19 tweets. \nImproved developer experience when you encounter error conditions. Unhandled errors now return an error code with a more descriptive message that can help you with looking them up in the troubleshooting section of our documentation. We also improved error handling when tests fail to run and the test results summary widget now display error information for each test that fails to run.\n\n\n\n\n\nNew full-featured text editor. When collaborating on model documentation in the platform UI with others, you now have a new text editor at your disposal. The interface should be intuitive and familiar, allowing you to manage model documentation and validation reports with greater ease. In addition to the new editing features, you can use the editor to participate in comment threads, track changes, see the revision history, enter math formulas, and more. Try it …\n\n\n\nAdd new content blocks to model documentation. For model developers and model validators who want to add new sections to model documentation, you can now do just that. The new content blocks feature provide you with additional sections that you fill in with text, metrics, and test results. Supported content block types:\nSimple text block: Can be added anywhere on model documentation pages and edited to include additional documentation in text format.\nTest-driven block: Can be added to display one of the supported metrics or threshold test results collected by the developer framework.\n\nYou can add new content block to an existing model documentation simply by hovering over the dashed line between sections. Try it …\n\n\nSwap templates for model documentation. This feature is designed to enhance the flexibility of documenting models. With it, you can seamlessly update an existing model documentation by either switching to an entirely different template or by simply upgrading to a more recent version of the template currently in use. This new feature ensures that your model documentation can remain up-to-date and align with the latest preferred template without the need to start from scratch. Try it …\nValidMind Platform UI enhancements: \n\nRenamed Client Integration page to Getting Started. To better reflect when you use the credentials to generate model documentation with the ValidMind Developer Framework, we have renamed the Client Integration page to Getting Started.\nImproved activity widget: We enhanced the user interface with new tabs, enabling you to filter events based on activity type.\nImproved Findings section and Findings Details page: We made UI enhancements for both project findings and the findings details sections for a better look and feel.\nRefined Models and Projects lists : We refined the user interface, featuring more intuitive default sorting and more user-friendly filtering."
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#documentation",
    "href": "guide/release-notes-2023-jul-24.html#documentation",
    "title": "July 24, 2023",
    "section": "Documentation",
    "text": "Documentation\nTo make it easier to try out our Jupyter notebooks, we now provide a download button for all notebooks used in our documentation:\n\n\nDownload Notebooks\n\n\nThis download includes:\n\nQuickStart notebooks\nUse case notebooks\nTesting notebooks"
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jul-24.html#how-to-upgrade",
    "title": "July 24, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/guides.html",
    "href": "guide/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Our guides offer step-by-step instructions for frequent tasks you perform on the ValidMind platform, organized by category:"
  },
  {
    "objectID": "guide/guides.html#onboarding",
    "href": "guide/guides.html#onboarding",
    "title": "Guides",
    "section": "Onboarding",
    "text": "Onboarding\nTo onboard your organization, teams or business units, and users onto the ValidMind platform:\n\n\n\n\n\n\n\nOnboarding users\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure AWS PrivateLink\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/guides.html#model-inventory",
    "href": "guide/guides.html#model-inventory",
    "title": "Guides",
    "section": "Model inventory",
    "text": "Model inventory\nTo configure the model inventory to your organization’s requirements and to register new models:\n\n\n\n\n\n\n\nEdit model inventory fields\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegister models in the inventory\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/guides.html#model-documentation",
    "href": "guide/guides.html#model-documentation",
    "title": "Guides",
    "section": "Model documentation",
    "text": "Model documentation\nTo document and test your models in your own model development environment:\n\n\n\n\n\n\n\nGet started with the ValidMind Developer Framework\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode samples\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nTo work with documentation, documentation templates, and model documentation in the platform UI, and to collaborate with model validators:\n\n\n\n\n\n\n\nWorking with documentation templates\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with model documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\nExport documentation\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/guides.html#model-validation",
    "href": "guide/guides.html#model-validation",
    "title": "Guides",
    "section": "Model validation",
    "text": "Model validation\nTo set up approvals, prepare validation reports, collaborate with model developers, link evidence to your reports, or work with model documentation findings:\n\n\n\n\n\n\n\nPreparing validation reports\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork with model findings\n\n\n\n\n\n\n\n\n\n\n\n\n\nView reports\n\n\n\n\n\n\n\n\n\n\n\n\n\nExport documentation\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "href": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the developer framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s developer framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python ≧3.8 and &lt;3.11 and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-var-vm_dev-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-var-vm_dev-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the developer framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the developer framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the developer framework, such as in CSV format."
  },
  {
    "objectID": "guide/manage-users.html",
    "href": "guide/manage-users.html",
    "title": "Manage users",
    "section": "",
    "text": "Managing users is fundamental to maintaining access control and efficient user management in the ValidMind system. This task topic will guide you through the process of inviting new users, assigning them to specific groups, and configuring their roles and permissions. Whether you’re a group administrator or a superuser, understanding these steps is crucial for ensuring the right individuals have the appropriate access and permissions within the system."
  },
  {
    "objectID": "guide/manage-users.html#viewing-your-account-profile",
    "href": "guide/manage-users.html#viewing-your-account-profile",
    "title": "Manage users",
    "section": "Viewing your account profile",
    "text": "Viewing your account profile\n\nLog in to the ValidMind UI. quart\nGo to Settings in the sidebar and select Profile.\nYou can find the following on your profile page:\n\nProfile details\nRoles attached to your account\nGroups assigned to your account\nValues for API Key and Secret Key"
  },
  {
    "objectID": "guide/manage-users.html#prerequisites",
    "href": "guide/manage-users.html#prerequisites",
    "title": "Manage users",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo perform these next steps, you must be a group administrator or a superuser."
  },
  {
    "objectID": "guide/manage-users.html#user-invitations",
    "href": "guide/manage-users.html#user-invitations",
    "title": "Manage users",
    "section": "User invitations",
    "text": "User invitations\n\nInvite new users\n\nLog in to the ValidMind UI.\nClick Settings.\n\nNavigate to User Permissions in the sidebar.\nClick Invite New Users.\nEnter the user’s email address.\nAssign the user to a group from the Group dropdown menu.\nAdd roles to the user by clicking the  icon and selecting the appropriate roles from the list.\nClick Assign Roles to invitee.\n\nUsers now will have the option to accept the invitation through the email they receive.\n\n\n\nMonitor user invitations\n\nMonitor the status of invitations in the Invites section. Look for Pending status for users who haven’t accepted their invites yet.\nConfirm that the new user is successfully added and that their group and roles are correctly configured to meet your access control requirements.\nReview the history of previously accepted invitations in the Invite History section."
  },
  {
    "objectID": "guide/manage-users.html#user-roles",
    "href": "guide/manage-users.html#user-roles",
    "title": "Manage users",
    "section": "User roles",
    "text": "User roles\n\nAssign new roles to a user\n\nLog in to the ValidMind UI.\nClick Settings.\nUnder User Permissions in the sidebar, select User Directory.\nUse one of these methods to assign roles to users:\n\nSelect the checkbox next to one or more users and click Assign New Role or Assign New Role to n Users; OR\nNext to any individual user, click the  icon.\n\nOn the pop-up that appears, select the roles to be assigned and click Assign Roles to User.\n\n\n\nRemove roles from a user\n\nLog in to the ValidMind UI.\nClick Settings.\nUnder User Permissions in the sidebar, select User Directory.\nClick the  in the pill next to the role for the user that you want to remove."
  },
  {
    "objectID": "guide/manage-users.html#user-groups",
    "href": "guide/manage-users.html#user-groups",
    "title": "Manage users",
    "section": "User groups",
    "text": "User groups\n\nView group members\n\nLog in to the ValidMind UI.\nClick Settings.\nUnder User Permissions in the sidebar, select Groups.\nExpand See Details to:\n\nView the complete list of members in any group\nScroll further down to view the list of models visible to this group"
  },
  {
    "objectID": "guide/manage-users.html#add-or-remove-group-members",
    "href": "guide/manage-users.html#add-or-remove-group-members",
    "title": "Manage users",
    "section": "Add or remove group members",
    "text": "Add or remove group members\n\nLog in to the ValidMind UI.\nClick Settings.\nUnder User Permissions in the sidebar, select Groups.\nTo add a member to a group:\n\nClick See Details.\nClick  Add New Member.\nSelect members by checking the boxes next to their names in the pop-up.\nClick  Add to Group.\n\nTo remove a member from a group:\n\nClick See Details.\nSelect members by checking the boxes next to their names.\nClick Remove Member."
  },
  {
    "objectID": "guide/view-documentation-templates.html",
    "href": "guide/view-documentation-templates.html",
    "title": "View documentation templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-documentation-templates.html#prerequisites",
    "href": "guide/view-documentation-templates.html#prerequisites",
    "title": "View documentation templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo view templates, you must be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nTo configure templates, you must hold the Customer Admin role."
  },
  {
    "objectID": "guide/view-documentation-templates.html#steps",
    "href": "guide/view-documentation-templates.html#steps",
    "title": "View documentation templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Settings and then Templates.\nClick on one of the available templates.\nIn the page that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the developer framework that feed into the template"
  },
  {
    "objectID": "guide/view-documentation-templates.html#whats-next",
    "href": "guide/view-documentation-templates.html#whats-next",
    "title": "View documentation templates",
    "section": "What’s next",
    "text": "What’s next\n\nGet started with the ValidMind Developer Framework\nCollaborate on documentation"
  },
  {
    "objectID": "guide/tutorials.html",
    "href": "guide/tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our tutorials provide a more targeted learning experience and cover specific scenarios or use cases."
  },
  {
    "objectID": "guide/tutorials.html#whats-next",
    "href": "guide/tutorials.html#whats-next",
    "title": "Tutorials",
    "section": "What’s next",
    "text": "What’s next\nBesides our tutorials, we also offer a QuickStart that walks you through the full experience from the developer framework to the platform UI."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "If you need access to your model model documentation or validation reports outside of the ValidMind platform, you can export your model documentation and validation report as Word or PDF files."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Developer or Validator role\nThe model is already registered in the model inventory\nModel documentation is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nIn right sidebar, click Export Document.\nConfigure the export options:\n\n\nChoose the file format for export. We currently support exporting to .docx for Microsoft Word format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\n\n\n\nIn the left sidebar that appears for your model, click Validation Report.\nIn right sidebar, click Export Document.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#whats-next",
    "href": "guide/export-documentation.html#whats-next",
    "title": "Export documentation",
    "section": "What’s next",
    "text": "What’s next\n\nWorking with model documentation\nCollaborate on documentation\nSubmit for approval"
  },
  {
    "objectID": "guide/manage-users-and-roles.html",
    "href": "guide/manage-users-and-roles.html",
    "title": "Manage users and roles",
    "section": "",
    "text": "User management involves controlling and organizing user accounts within the ValidMind Platform UI. It encompasses tasks like user creation, authentication, authorization, and access control. Effective user management is essential for maintaining security and operational efficiency."
  },
  {
    "objectID": "guide/manage-users-and-roles.html#prerequisites",
    "href": "guide/manage-users-and-roles.html#prerequisites",
    "title": "Manage users and roles",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#add-users",
    "href": "guide/manage-users-and-roles.html#add-users",
    "title": "Manage users and roles",
    "section": "Add users",
    "text": "Add users\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#delete-users",
    "href": "guide/manage-users-and-roles.html#delete-users",
    "title": "Manage users and roles",
    "section": "Delete users",
    "text": "Delete users\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#update-user-roles",
    "href": "guide/manage-users-and-roles.html#update-user-roles",
    "title": "Manage users and roles",
    "section": "Update user roles",
    "text": "Update user roles\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "href": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test suite execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test suites\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The developer framework executes test suites and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a developer framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the developer framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur developer framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/manage-groups.html",
    "href": "guide/manage-groups.html",
    "title": "Manage groups",
    "section": "",
    "text": "This task involves managing user groups within ValidMind, allowing for organized access control."
  },
  {
    "objectID": "guide/manage-groups.html#prerequisites",
    "href": "guide/manage-groups.html#prerequisites",
    "title": "Manage groups",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo perform these steps, you must be a group administrator or a superuser."
  },
  {
    "objectID": "guide/manage-groups.html#add-new-groups",
    "href": "guide/manage-groups.html#add-new-groups",
    "title": "Manage groups",
    "section": "Add new groups",
    "text": "Add new groups\n\nLog in to the ValidMind UI.\nClick Settings.\nUnder User Permissions in the sidebar, select Groups.\nClick + Create New Group to initiate the group creation process:\n\nProvide a name and description for the new group, click Continue.\nOptionally, specify a description for the group’s purpose.\nSelect users from the list to add them to the group.\nClick Create Group to save.\n\nConfirm that the new group is created and appears in the list of available groups, ensuring it is correctly configured for its intended use."
  },
  {
    "objectID": "guide/manage-groups.html#see-group-details",
    "href": "guide/manage-groups.html#see-group-details",
    "title": "Manage groups",
    "section": "See group details",
    "text": "See group details\n\nLog in to the ValidMind UI.\nClick Settings.\nUnder User Permissions in the sidebar, select Groups.\nLocate the group for which you want to see details, expand See Details &gt; to:\n\nView the group’s name and description.\nSee a list of users who are members of the group.\nReview any additional information or settings related to the group. For example, scroll further down to view the list of models visible to this group.\n\n\n\n\n\n\n\n\nEnsure that the group details align with your requirements for access control and group management."
  },
  {
    "objectID": "guide/view-reports.html",
    "href": "guide/view-reports.html",
    "title": "View reports",
    "section": "",
    "text": "Reports provide quick insights into your model validation efforts, detailing critical findings, risk exposure, and compliance status to ensure effective oversight and management of model-related risks.\nTypical uses for reports include:\nHow reports are organized:"
  },
  {
    "objectID": "guide/view-reports.html#prerequisites",
    "href": "guide/view-reports.html#prerequisites",
    "title": "View reports",
    "section": "Prerequisites",
    "text": "Prerequisites\nReports are primarily designed for model validators or Chief Risk Officers (CROs) who require an understanding of findings related to model validation activities."
  },
  {
    "objectID": "guide/view-reports.html#steps",
    "href": "guide/view-reports.html#steps",
    "title": "View reports",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Reports.\n\nIn the page that opens, you can see bar charts for different types of findings, including:\n\nFindings by risk area\nFindings by business unit\nFindings by status\nFindings by model\n\nFor each of the bar charts, you can click on the individual bars to get a more detailed view.\nFor example: To see all inventory models where there are findings related to data quality:\n\nUnder Findings by Risk Area, click on Data Quality.\nOn the page that opens, the applicable findings are listed, including the inventory model they apply to, the severity of the finding and its current status, and who the finding has been assigned to.\nTo get more details on the finding itself, click on the title of the finding to take you to the Model Findings section for the specific model the finding applies to."
  },
  {
    "objectID": "guide/view-reports.html#whats-next",
    "href": "guide/view-reports.html#whats-next",
    "title": "View reports",
    "section": "What’s next",
    "text": "What’s next\nViewing reports in the ValidMind Platform UI is closely related to working with model findings. For example, you can drill down into specific model findings which will take you to the Model Findings page."
  },
  {
    "objectID": "guide/troubleshooting.html",
    "href": "guide/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Learn how to resolve commonly encountered issues with the developer framework."
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-install-the-var-vm_framework",
    "href": "guide/troubleshooting.html#cannot-install-the-var-vm_framework",
    "title": "Troubleshooting",
    "section": "Cannot install the ValidMind Developer Framework",
    "text": "Cannot install the ValidMind Developer Framework\nIssue: You cannot run pip install validmind or import validmind as vm in the ValidMind Developer Framework notebooks.\nFix: Make sure you are installing the latest version of the developer framework by running this command:\n%pip install --upgrade validmind"
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "href": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "title": "Troubleshooting",
    "section": "Cannot initialize ValidMind client library",
    "text": "Cannot initialize ValidMind client library\nIssue: When you run vm.init(), you encounter an error message like this:\nMissingAPICredentialsError: API key and secret must be provided either as environment variables or as arguments to init.\nor\nInvalidProjectError: Invalid project ID. Please ensure that you have provided a project ID that belongs to your organization.\nFix: Make sure that you are using the correct initialization credentials for the project you are trying to connect to.\nFollow the steps in Install and initialize the developer framework for detailed instructions on how to integrate the developer framework and upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/troubleshooting.html#additional-resources",
    "href": "guide/troubleshooting.html#additional-resources",
    "title": "Troubleshooting",
    "section": "Additional resources",
    "text": "Additional resources\nCheck out our FAQ page to browse through common questions, or contact our support team for more help troubleshooting technical issues."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the developer framework",
    "section": "",
    "text": "ValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run a Jupyter notebook."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the developer framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the developer framework and to be able to upload to the ValidMind Platform, you must provide the following information through a code snippet that you copy from the ValidMind Platform UI:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier\n\n\n\nIf you do not have a suitable model with a code snippet to use, you can register a model first.\nThe developer framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#locate-the-framework-integration-instructions",
    "href": "guide/install-and-initialize-developer-framework.html#locate-the-framework-integration-instructions",
    "title": "Install and initialize the developer framework",
    "section": "Locate the framework integration instructions",
    "text": "Locate the framework integration instructions\nFor existing projects, this information can be found in the ValidMind UI:\n\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Getting Started.\nLocate the code snippet and click  Copy snippet to clipboard.\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nUse the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the developer framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the developer framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"API_KEY\",\n  api_secret = \"API_SECRET\",\n  project = \"PROJECT-IDENTIFIER\"\n)\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nClick  Copy snippet to clipboard to copy everything.\n\n\nAfter you have pasted the code snippet into your development source code and run your code, the Python client library will connect and register with ValidMind. You can now use the developer framework to document and test your models, and to upload model documentation and test results to the ValidMind Platform."
  },
  {
    "objectID": "guide/working-with-documentation-templates.html",
    "href": "guide/working-with-documentation-templates.html",
    "title": "Working with documentation templates",
    "section": "",
    "text": "Documentation templates offer a standardized approach to creating consistent and comprehensive model documentation and validation reports. You customize these templates to fit your specific project needs.\n\n\n\n\n\n\n\nView documentation templates\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize documentation templates\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwap documentation templates\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the developer framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s developer framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "View the guidelines for model documentation associated with a template to ensure that you are compliant with documentation requirements."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Developer role\nThe model you are documenting is registered in the model inventory\nModel documentation has already been created for this project\nA model developer has started generating documentation, either using the ValidMind Developer Framework or via the online editor in the ValidMind Platform UI\n\n\n\n\n\n\n\nConfiguring the documentation guidelines for each template requires the Customer Admin role."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nIn any section of the documentation for a model, click ValidMind Insights in the top right corner to expand the ValidMind Insights sidebar:\n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#whats-next",
    "href": "guide/view-documentation-guidelines.html#whats-next",
    "title": "View documentation guidelines",
    "section": "What’s next",
    "text": "What’s next\n\nGet started with the ValidMind Developer Framework\nWorking with model documentation\nCollaborate on documentation"
  },
  {
    "objectID": "guide/quickstart-generate-documentation-for-your-model.html",
    "href": "guide/quickstart-generate-documentation-for-your-model.html",
    "title": "Generate documentation for your model",
    "section": "",
    "text": "You are now ready to modify the QuickStart notebook you used earlier and run it to generate model documentation and test result with the ValidMind Developer Framework. The resulting artifacts are then uploaded to your model documentation in the ValidMind Platform UI."
  },
  {
    "objectID": "guide/quickstart-generate-documentation-for-your-model.html#before-you-begin",
    "href": "guide/quickstart-generate-documentation-for-your-model.html#before-you-begin",
    "title": "Generate documentation for your model",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account."
  },
  {
    "objectID": "guide/quickstart-generate-documentation-for-your-model.html#steps",
    "href": "guide/quickstart-generate-documentation-for-your-model.html#steps",
    "title": "Generate documentation for your model",
    "section": "Steps",
    "text": "Steps\n\nIn the left sidebar, go to the Getting Started section of the model you registered previously:\n\n\n\n\n\nThe page that opens provides you with a code snippet to use with the ValidMind Developer Framework, including:\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThis code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\nLocate the code snippet and click  Copy snippet to clipboard.\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nUse the  Copy snippet to clipboard button to copy the API_SECRET to your clipboard.\n\n\nReopen the QuickStart notebook you accessed earlier (in Jupyter Hub, for example).\nIn the QuickStart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you registered your new model:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to the ValidMind Platform."
  },
  {
    "objectID": "guide/quickstart-generate-documentation-for-your-model.html#whats-next",
    "href": "guide/quickstart-generate-documentation-for-your-model.html#whats-next",
    "title": "Generate documentation for your model",
    "section": "What’s next",
    "text": "What’s next\nYou can now switch back to the platform UI to view the documentation that has been generated by the developer framework.\nReady to learn more about how you can use ValidMind? Check out Next Steps."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-jupyterhub.html",
    "href": "guide/quickstart-try-developer-framework-with-jupyterhub.html",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "",
    "text": "Learn how to document a model with ValidMind on Jupyter Hub."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-jupyterhub.html#before-you-begin",
    "href": "guide/quickstart-try-developer-framework-with-jupyterhub.html#before-you-begin",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nNeed an account for ValidMind?\n\n\n\nSigning up is FREE — Create your account."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-jupyterhub.html#steps",
    "href": "guide/quickstart-try-developer-framework-with-jupyterhub.html#steps",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "Steps",
    "text": "Steps\n\nIn a web browser, open Quickstart for model documentation.\nThis link takes you to ValidMind’s Jupyter Hub instance where you can log in with the Auth0 credentials for your ValidMind account to access the QuickStart notebook.\n\n\n\n\nAfter the notebook opens, run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will guide you through installing the ValidMind Developer Framework, initializing the Python environment, and finally initializing the ValidMind Client Library by connecting to the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the developer framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the platform UI."
  },
  {
    "objectID": "guide/quickstart-try-developer-framework-with-jupyterhub.html#whats-next",
    "href": "guide/quickstart-try-developer-framework-with-jupyterhub.html#whats-next",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore sample model documentation to learn more about using the ValidMind AI risk platform hands-on."
  },
  {
    "objectID": "guide/swap-documentation-templates.html",
    "href": "guide/swap-documentation-templates.html",
    "title": "Swap documentation templates",
    "section": "",
    "text": "Swapping templates allows you to switch to a completely different template, upgrade to a more recent version of your current template, or make changes to both the template and template version at the same time.\nWhen swapping templates, only the document structure is changed. Any modifications that you might have made to content is preserved inside each content block or section. If you are not sure which template or which version of a template your model documentation or validation report is using, you can also check and compare between different templates and versions of templates."
  },
  {
    "objectID": "guide/swap-documentation-templates.html#prerequisites",
    "href": "guide/swap-documentation-templates.html#prerequisites",
    "title": "Swap documentation templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou must hold the Developer role to update the template for model documentation.\nYou must hold the Validator role to update the template for validation reports."
  },
  {
    "objectID": "guide/swap-documentation-templates.html#view-current-templates",
    "href": "guide/swap-documentation-templates.html#view-current-templates",
    "title": "Swap documentation templates",
    "section": "View current templates",
    "text": "View current templates\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nCheck the right sidebar:\n\nDocument Template — lists the template used for model documentation\nValidation Report Template — lists the template used for validation reports"
  },
  {
    "objectID": "guide/swap-documentation-templates.html#swap-templates",
    "href": "guide/swap-documentation-templates.html#swap-templates",
    "title": "Swap documentation templates",
    "section": "Swap templates",
    "text": "Swap templates\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nIn the right sidebar, select one of the templates currently in use under one of the following:\n\nDocument Template\nValidation Report Template\n\nThe window pane that opens shows the YAML for the current template along with other information, such as the name and the current version, and which models are using this template.\nClick Swap Template.\nThe window pane now shows the YAML for two templates side-by-side:\n\nOn the left, your current template is shown.\nOn the right, you can select a different template and version.\n\nInitially, both templates are the same.\nOn the right, select a different template or version:\n\nTemplate: Change to a different template entirely\nVersion: Change to a different version of the template you selected\n\nFor example: Select a previous version of the template currently in use to revert to that version.\nAfter you select a different template or version, the YAML differences between the templates are highlighted.\nClick Prepare Swap.\nEnter a note to enable completing the swap and click Swap Template.\n\nAfter your model documentation template has been swapped successfully, you can continue to work on your model documentation or validation report.\n\n\n\n\n\n\nIs there any content missing from the new template?\n\n\n\nIf you added a simple text block to your old template and want to reuse the content, you can temporarily switch back to the old template, copy the content, swap back to the new template, and then paste in the content."
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "href": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Edit individual model detailed fields in the model inventory to customize them to your needs or to ensure that model details are accurate and up to date."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nThe model is already registered in the model inventory.\nYou are the model owner for the specific model you would like edit the details of, or an administrator."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nLog in to the ValidMind UI.\nOn the Model Inventory page, select a model to view the model details.\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#whats-next",
    "href": "guide/edit-model-inventory-fields.html#whats-next",
    "title": "Edit model inventory fields",
    "section": "What’s next",
    "text": "What’s next\n\nRegister models in the inventory"
  },
  {
    "objectID": "releases/2024-may-22/release-notes.html",
    "href": "releases/2024-may-22/release-notes.html",
    "title": "May 22, 2024",
    "section": "",
    "text": "Our new end-to-end notebook gives you a full introduction to the ValidMind Developer Framework. You can use this notebook to learn how the end-to-end documentation process works, based on common scenarios you encounter in model development settings.\n\n\n\n\nTry it on JupyterHub\n\n\n\n\n\nHigh level sections include:\n\nInitializing the ValidMind Developer Framework — The developer framework provides tools for documentation and testing, from dataset descriptions to model validation using various open-source frameworks.\nStarting the model development process — Access the test repository to use individual tests as building blocks for model development. Learn to run tests, analyze results, and add evidence to model documentation.\nSee the full list of tests at Test descriptions or try the Test sandbox.\nImplementing custom tests — Include custom tests in model documentation. Learn how to add these tests and use them as additional evidence.\nFinalizing testing and documentation — Ensure model documentation includes custom tests and configure settings for all tests in the template. By the end, you should have a fully documented model ready for review.\n\n\n\n\n\n\n\nYou can now run documentation tests without passing a Python-native model object. This change enables you to document:\n\n\n\n\nRead developer documentation\n\n\n\n\n\nModels that are developed in non-Python environments\nNon-standard model interfaces:\n\nModels deployed as APIs, such as SageMaker model endpoints\nTools such as Spark where a model is not a typical object that exposes a predict() interface\n\n\nTo run tests for these models, you typically must load model predictions from a file, dataset, and so on. The new init_model interface does not enforce a Python model object anymore. You only need to pass attributes that describe the model which is required as a best practice for model documentation.\n\n\nSince there is no native Python object to pass to init_model, you instead pass attributes that describe the model:\n# Assume you want to load predictions for a PySpark ML model\n\nmodel_attributes = {\n    \"architecture\": \"Spark\",\n    \"language\": \"PySpark\",\n}\n\n# Or maybe you're loading predictions for a SageMaker endpoint (model API)\nmodel_attributes = {\n    \"architecture\": \"SageMaker Model\",\n    \"language\": \"Python\",\n}\n\n# Call `init_model` without passing a model. Pass `attributes` instead.\nvm_model = vm.init_model(\n    attributes=model_attributes,\n    input_id=\"model\",\n)\n\n\n\nSince there’s no model object available, the developer framework won’t be able to call model.predict() or model.predict_proba(). You need to load predictions and probabilities manually. For example:\nvm_train_ds.assign_predictions(\n    model=vm_model,\n    prediction_values=prediction_values,\n    prediction_probabilities=prediction_probabilities,\n)\nYou can proceed to run tests on your data as you would under normal conditions, without needing to modify any other parts of your code.\n\n\n\n\n\n\n\nWe introduced a new metric decorator that turns any function into a ValidMind Metric that you can use in your documentation. To learn what this decorator can do for you, try our code sample on JupyterHub!\n\n\n\n\nTry it on JupyterHub\n\n\n\n\nCustom metrics offer added flexibility. They allow you to extend the library of default metrics provided by ValidMind which enables you to document any type of model or use case.\nThis new decorator simplifies creating and using custom metrics by almost completely eliminating the boilerplate code required to define and register a custom metric.\n\n\n\n\n\n\n\n\n\nOur new documentation is designed to help you find the information you need more quickly.\nNow more distinct from the Model Details page, the new overview page provides easier navigation and enhanced data visualization to better understand the progress of the documentation stage.\n\n\n\n\n\n\nOur new Generate with AI button is now available when you edit text blocks in model documentation or model validation reports. This button replaces the old button that was used to AI-generate content.\nClicking on Generate Text with AI pops up a modal showing the AI content generation:\n\nHere, you can choose to Accept Text or Try Again:\n\n\n\n\n\nWe created a new documentation outline page which replaces the existing project overview page. This page shows a section-by-section outline of your project’s documentation:\n\n\n\nIt also includes a count of every unresolved conversation within each section. From here, you can hover over the chat icon to see a preview of all unresolved conversations. Click a chat icon to jump to a conversations, or resolve conversations directly from the popup.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have introduced several updates to the Organization settings page, enabling you to manage business units and risk areas for your organization.\nThe following features are now available:\n\nAdd or remove business units from your organization.\nRemove business units from the organization settings page.\nRemove risk areas from the organization settings page.\nAdd or remove risk areas from your organization.\n\n\n\n\n\n\nThis update allows you to edit documentation templates visually, eliminating the need to manually edit YAML files. Add, remove, and rename sections with the new editor.\n\n\n\nThe edit button next to the version dropdown is now hidden.\nA new edit button is displayed above the accordions. Clicking this button activates the edit mode for the accordion.\n\n\n\n\n\nIn edit mode, you see a cancel button to exit edit mode and a button to publish a new version of the template with the current changes.\nYou can edit section titles when the accordion is in edit mode.\nYou can insert new sections before or after an existing section or insert a sub-section.\nYou can remove sections, with a confirmation alert to ensure the action.\n\nThis new visual editing interface streamlines the template editing process, making it more intuitive and user-friendly:"
  },
  {
    "objectID": "releases/2024-may-22/release-notes.html#release-highlights",
    "href": "releases/2024-may-22/release-notes.html#release-highlights",
    "title": "May 22, 2024",
    "section": "",
    "text": "Our new end-to-end notebook gives you a full introduction to the ValidMind Developer Framework. You can use this notebook to learn how the end-to-end documentation process works, based on common scenarios you encounter in model development settings.\n\n\n\n\nTry it on JupyterHub\n\n\n\n\n\nHigh level sections include:\n\nInitializing the ValidMind Developer Framework — The developer framework provides tools for documentation and testing, from dataset descriptions to model validation using various open-source frameworks.\nStarting the model development process — Access the test repository to use individual tests as building blocks for model development. Learn to run tests, analyze results, and add evidence to model documentation.\nSee the full list of tests at Test descriptions or try the Test sandbox.\nImplementing custom tests — Include custom tests in model documentation. Learn how to add these tests and use them as additional evidence.\nFinalizing testing and documentation — Ensure model documentation includes custom tests and configure settings for all tests in the template. By the end, you should have a fully documented model ready for review.\n\n\n\n\n\n\n\nYou can now run documentation tests without passing a Python-native model object. This change enables you to document:\n\n\n\n\nRead developer documentation\n\n\n\n\n\nModels that are developed in non-Python environments\nNon-standard model interfaces:\n\nModels deployed as APIs, such as SageMaker model endpoints\nTools such as Spark where a model is not a typical object that exposes a predict() interface\n\n\nTo run tests for these models, you typically must load model predictions from a file, dataset, and so on. The new init_model interface does not enforce a Python model object anymore. You only need to pass attributes that describe the model which is required as a best practice for model documentation.\n\n\nSince there is no native Python object to pass to init_model, you instead pass attributes that describe the model:\n# Assume you want to load predictions for a PySpark ML model\n\nmodel_attributes = {\n    \"architecture\": \"Spark\",\n    \"language\": \"PySpark\",\n}\n\n# Or maybe you're loading predictions for a SageMaker endpoint (model API)\nmodel_attributes = {\n    \"architecture\": \"SageMaker Model\",\n    \"language\": \"Python\",\n}\n\n# Call `init_model` without passing a model. Pass `attributes` instead.\nvm_model = vm.init_model(\n    attributes=model_attributes,\n    input_id=\"model\",\n)\n\n\n\nSince there’s no model object available, the developer framework won’t be able to call model.predict() or model.predict_proba(). You need to load predictions and probabilities manually. For example:\nvm_train_ds.assign_predictions(\n    model=vm_model,\n    prediction_values=prediction_values,\n    prediction_probabilities=prediction_probabilities,\n)\nYou can proceed to run tests on your data as you would under normal conditions, without needing to modify any other parts of your code.\n\n\n\n\n\n\n\nWe introduced a new metric decorator that turns any function into a ValidMind Metric that you can use in your documentation. To learn what this decorator can do for you, try our code sample on JupyterHub!\n\n\n\n\nTry it on JupyterHub\n\n\n\n\nCustom metrics offer added flexibility. They allow you to extend the library of default metrics provided by ValidMind which enables you to document any type of model or use case.\nThis new decorator simplifies creating and using custom metrics by almost completely eliminating the boilerplate code required to define and register a custom metric.\n\n\n\n\n\n\n\n\n\nOur new documentation is designed to help you find the information you need more quickly.\nNow more distinct from the Model Details page, the new overview page provides easier navigation and enhanced data visualization to better understand the progress of the documentation stage.\n\n\n\n\n\n\nOur new Generate with AI button is now available when you edit text blocks in model documentation or model validation reports. This button replaces the old button that was used to AI-generate content.\nClicking on Generate Text with AI pops up a modal showing the AI content generation:\n\nHere, you can choose to Accept Text or Try Again:\n\n\n\n\n\nWe created a new documentation outline page which replaces the existing project overview page. This page shows a section-by-section outline of your project’s documentation:\n\n\n\nIt also includes a count of every unresolved conversation within each section. From here, you can hover over the chat icon to see a preview of all unresolved conversations. Click a chat icon to jump to a conversations, or resolve conversations directly from the popup.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have introduced several updates to the Organization settings page, enabling you to manage business units and risk areas for your organization.\nThe following features are now available:\n\nAdd or remove business units from your organization.\nRemove business units from the organization settings page.\nRemove risk areas from the organization settings page.\nAdd or remove risk areas from your organization.\n\n\n\n\n\n\nThis update allows you to edit documentation templates visually, eliminating the need to manually edit YAML files. Add, remove, and rename sections with the new editor.\n\n\n\nThe edit button next to the version dropdown is now hidden.\nA new edit button is displayed above the accordions. Clicking this button activates the edit mode for the accordion.\n\n\n\n\n\nIn edit mode, you see a cancel button to exit edit mode and a button to publish a new version of the template with the current changes.\nYou can edit section titles when the accordion is in edit mode.\nYou can insert new sections before or after an existing section or insert a sub-section.\nYou can remove sections, with a confirmation alert to ensure the action.\n\nThis new visual editing interface streamlines the template editing process, making it more intuitive and user-friendly:"
  },
  {
    "objectID": "releases/2024-may-22/release-notes.html#enhancements",
    "href": "releases/2024-may-22/release-notes.html#enhancements",
    "title": "May 22, 2024",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nAdd extra columns on the fly\n\n\nWe added support for two new VMDataset methods:\n\nadd_extra_column()\nget_extra_column()\n\n\n\n\n\nRead developer documentation\n\n\n\n\n\nadd_extra_column()\nYou can now register arbitrary extra columns in a dataset when a test needs to compute metrics outside of the existing sets of columns (features, targets, predictions).\nFor example, credit risk-related metrics may require access to a list of scores computed from predictions. In this case, an extra column called scores is needed for computing the metrics.\nExample usage:\n# Init your dataset as usual\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=customer_churn.target_column,\n)\n\n# Generate scores using a user defined function:\nscores = compute_my_scores(x_train)\n\n# Assign a new \"scores\" column to vm_train_ds:\nvm_train_ds.add_extra_column(\"scores\", scores)\nThis function returns an error if no column values are passed:\nvm_train_ds.add_extra_column(\"scores\")\n\nValueError: Column values must be provided when the column doesn't exist in the dataset\nIt’s also possible to use init_dataset with a dataset that has precomputed scores, for example:\n&gt; train_df.columns\nIndex(['CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n       'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited',\n       'Geography_France', 'Geography_Germany', 'Geography_Spain'],\n      dtype='object')\n&gt; train_df[\"my_scores\"] = scores\n&gt; train_df.columns\nIndex(['CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n       'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited',\n       'Geography_France', 'Geography_Germany', 'Geography_Spain',\n       'my_scores'],\n      dtype='object')\nMake sure you set the feature_columns correctly:\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"another_ds\",\n    feature_columns=[\n        \"CreditScore\",\n        \"Gender\",\n        \"Age\",\n        \"Tenure\",\n        \"Balance\",\n        \"NumOfProducts\",\n        \"HasCrCard\",\n        \"IsActiveMember\",\n        \"EstimatedSalary\",\n        \"Exited\",\n        \"Geography_France\",\n        \"Geography_Germany\",\n        \"Geography_Spain\",\n    ],\n    target_column=customer_churn.target_column,\n)\nThen, call add_extra_column() to register the extra column:\n&gt; another_ds.add_extra_column(column_name=\"my_scores\")\nColumn my_scores exists in the dataset, registering as an extra column\n\n\nget_extra_column()\nYou can use this inside a test to retrieve the extra column values.\nExample usage:\nscores = self.inputs.dataset.get_extra_column(\"scores\")\n\n\n\n\nCreate composite metrics by combining multiple individual unit metrics\nValidMind now supports the ability to compose multiple Unit Metrics into complex outputs.\nThese composite metrics can be logged as a single result that can be used as a content block in your documentation. The results from composite metrics automatically update whenever you re-run the documentation test suite.\n\n\n\nNew text data validation tests\nThe following tests for text data validation have been added:\n\nvalidmind.data_validation.nlp.LanguageDetection\nvalidmind.data_validation.nlp.Toxicity\nvalidmind.data_validation.nlp.PolarityAndSubjectivity\nvalidmind.data_validation.nlp.Sentiment\n\n\n\n\n\nUse metric decorators with test providers\n\n\nContinuing our efforts to simplify the process for getting your custom code working in ValidMind, we now support functional metrics for internal tests and test providers. Functional metrics are tests that can be defined as pure functions without any boilerplate.\n\n\n\n\nTry it on JupyterHub\n\n\n\n\n\n\n\nSupport for metadata in new metric decorator\nWe added new decorators to support task type and tag metadata in functional metrics.\nUsage example:\nfrom sklearn.metrics import accuracy_score\n\nfrom validmind import tags, tasks\n\n\n@tasks(\"classification\")\n@tags(\"classification\", \"sklearn\", \"accuracy\")\ndef Accuracy(dataset, model):\n    \"\"\"Calculates the accuracy of a model\"\"\"\n    return accuracy_score(dataset.y, dataset.y_pred(model))\n\n\n# the above decorator is syntactic sugar for the following:\nAccuracy.__tags__ = [\"classification\"]\nAccuracy.__tasks__ = [\"classification\", \"sklearn\", \"accuracy\"]\n\n\n\nAssign prediction probabilities\n\n\nWe added support for assigning prediction_probabilities to assign_predictions. This support enables you to:\n\n\n\n\nTry it on GitHub\n\n\n\n\n\nAssign prediction values and probabilities that have been computed outside ValidMind.\nIncorporate prediction values and probabilities from datasets that already have prediction columns.\nAutomate the assignment of prediction values and probabilities within VM.\n\n\n\n\nAssociate findings with a documentation section\nYou can now associate Model Findings with sections within your model documentation. Doing so will allow you to track findings by severity, section-by-section, in the Documentation overview page.\nYou can see the total number of findings at the top of the Documentation overview page, as well as the individual counts per section. To view the related findings, hover over the finding icon in each section and click on one to jump to it.\n\n\n\n\n\n\n\nBetter UI for workflow customization\nOur revamped workflows UI pages enable more granular management of model and documentation lifecycles and deep integration with model inventory attributes.\nThe new workflows UI includes the following features:\n\nAbility to require a user action (approve, reject, request changes, etc.) before updating the status of a resource. The user action is presented to relevant users as an action button.\nAbility to define the conditions that need to be met before allowing state transitions on a workflow. These conditions are evaluated from attribute values of the inventory model.\nSupport for approval steps. Approval steps allow you to define the model attribute where a list of approvers needs retrieval and specify the percentage of approvals needed for a successful approval.\nApproval steps allow defining approved and rejected outcomes.\n\n\n\n\nSet section status for model documentation and overview page\nWe added a status picker to each section of the model documentation page.\nThis picker allows you to set whether the section is In Progress or Done. The Documentation overview page displays a total count of how many sections have been completed, as well as a checkmark indicating that the section is done.\n\n\n\n\n\n\n\n\n\n\n\nSpecify a template for rich text custom fields\nWe added a new property to the Long Text custom field: ENABLE RICH TEXT FORMATTING.\n\nToggling this property changes the field to a rich text editor rather than a simple text area.\nAnother field called Template is also available and enables you to specify the default value within the rich text editor. This feature can be useful for defining procedures or guidelines that all models need to follow.\n\n\n\n\n\n\n\n\nValidation report overview page\nWe created a new Validation Report overview page which shows a section-by-section outline of a model’s validation report, including a total compliance summary for all risk areas associated with the model.\n\nYou can hover over any section in the report outline to view the current compliance status in the document section. The new validation report page also adds an Add finding button.\n\n\n\nSpecify vendor info during inventory registration and improved inventory model filters\nWe added the ability to flag models as Is Vendor Model and specify a vendor name.\nHow you add Vendor Model as a filtering flag:\n\nAlso available is an improved look and functionality for model inventory filtering:\n\n\n\n\nDisplay group information for inventory models\nAdmin users can now modify the group the inventory model belongs to:"
  },
  {
    "objectID": "releases/2024-may-22/release-notes.html#bug-fixes",
    "href": "releases/2024-may-22/release-notes.html#bug-fixes",
    "title": "May 22, 2024",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nMissing required field card view\n\n\nWe added a tooltip for required missing fields on Inventory Model card view.\n\n\n\n\n\n\n\n\nTemplate bug fixes\n\n\nWe now validate whether a template has duplicate section IDs and return an error if a duplicate ID is found."
  },
  {
    "objectID": "releases/2024-may-22/release-notes.html#documentation",
    "href": "releases/2024-may-22/release-notes.html#documentation",
    "title": "May 22, 2024",
    "section": "Documentation",
    "text": "Documentation\n\n\n\nImproved site navigation with new “About” section\n\n\nAbout ValidMind\nWe’ve revamped our documentation for a cleaner, more intuitive experience. The update features a brand new About section:\n\nFeatures includes product overviews and our glossary.\nContributing includes community information and our new style guide.\nReleases includes our latest product updates.\nFine print includes our data privacy policy, and our software license agreement.\n\n\nContributing\nFind a brand new set of guides on contributing to our open source software. Learn how to engage with the ValidMind community, read about our brand voice and vision, and more:\n\nValidMind community: You’re part of the ValidMind community. Come learn and play with us.\nValidMind style guide: Check out the first official version of the ValidMind style guide.\n\n\n\n\nValidMind style guide\nCheck out the first official version of the ValidMind style guide!\n\nLearn about the ValidMind voice\nUnderstand our shared vision and goals\nSee our reference for formatting conventions\n\n\n\n\n\nMore contextual information in Jupyter notebooks\nMany of our Jupyter notebooks have received improvements to make them easier to consume and more standalone:\n\nIntroductions now include more contextual information\nA new table of contents makes notebooks easier to navigate\nKey concepts are explained in the context where you might need that information\nNext steps make it easier to find additional learning resources\n\n\n\nTry it on JupyterHub\n\n\n\n\n\n\nQuickStart improvements\nWe reworked our QuickStart experience to shorten the number of clicks it takes to get you started.\nYou can now access the QuickStart directly from the homepage of our docs site, where we direct you to the preferred QuickStart on JupyterHub right away.\n\nNew QuickStart video\n\n\nA new three-minute video walks you through documenting a model with ValidMind and is now included in the QuickStart for JupyterHub.\n\n\n\n\n\n\n\n\n\nSandbox getting started\nWe added getting started information for the new ValidMind sandbox environment, which is currently available on request. You can use the sandbox to gain hands-on experience and explore what ValidMind has to offer.\nThe sandbox mimics a production environment. It includes comprehensive resources such as notebooks with sample code you can run, sample models registered in the model inventory, and draft documentation and validation reports.\nMost of our model documentation features are available for you to test in the sandbox. These include:\n\nAutomated model testing & documentation\nPreparing model validation reports\nLarge language model (LLM) support\n\nThese features provide a rich context for testing and evaluation. You can use realistic models and datasets without any risk to your production environment. Learn more…"
  },
  {
    "objectID": "releases/2024-mar-27/highlights.html",
    "href": "releases/2024-mar-27/highlights.html",
    "title": "March 27, 2024",
    "section": "",
    "text": "New metric metadata available in the ValidMind Platform UI makes it easy to see information about the test results that were generated with the ValidMind Developer Framework. Test results metadata includes:\n\nThe history of values for test results in your model documentation\nUser attribution to tell you who updated the test results\nRelevant inputs associated with the test results\n\nTest results metadata is available directly in your model documentation:\n\n\nYour browser does not support the video tag.\n\n\n\n\n\n\n\n\nWe’re introducing two important new features to our platform: the Model Findings and Reports pages. The Reports page offers access to essential information about your model validation efforts, summarizing findings, while the Model Findings page facilitates efficient tracking of issues identified during model validation. Together, these pages improve your model validation workflow and simplify compliance with organizational standards and regulatory requirements.\n\n\n\n\n\n\n\nNew Model Findings page\nNew Reports page\n\n\n\n\n\n\n\n\n\nThe new Model Findings page allows you to efficiently track all findings flagged during model validation, such as major and minor issues, deficiencies, model limitations, or other concerns that must be addressed. To facilitate working with findings across all models undergoing validation in your organization, you can filter and view findings by specific criteria, including:\n\nInventory model\nSeverity\nStatus\nRisk area\nAssignee\nDue date\n\nThe new Reports page provides quick insights into your model validation efforts to ensure effective oversight and management of model-related risks. Reports are grouped into bar charts for different types of findings, including:\n\nFindings by risk area\nFindings by business unit\nFindings by status\nFindings by model\n\nAdditionally, you can seamlessly navigate between the Reports page and relevant model findings by selecting information from the available bar charts to drill down into specific areas of concern.\nTry it:\n\nModel Findings\nReports\n\n\n\n\n\n\n\nWe enhanced both the Model Inventory and Model Findings pages to allow for the customization of table views. This feature enables you to customize these pages according to your own preferences, so that specific columns that are of interest are always shown. Columns can also be sorted by clicking on the column header.\nOn the Model Inventory page, you can show or hide table columns by clicking the Manage Columns button and toggling individual columns on or off:\n\n\nYour browser does not support the video tag.\n\n\nThe same customization is available on the Model Findings page:\n\nTry it:\n\nModel Inventory\nModel Findings\n\n\n\n\n\nWhen registering new models in the model inventory, you can now designate custom fields as mandatory. Users must complete these custom fields before they can proceed with registering a model. This feature ensures that all necessary information is provided upfront, improving the completeness and accuracy of the model registration process.\nYou can specify the custom fields that are required under Settings &gt; Custom Fields:\n\nYour browser does not support the video tag.\n\n\nTry it: Custom Fields\n\n\n\n\n\n\n\n\n\nWe made a number of improvements to how you manage user groups:\n\nAdded support for the creation of new groups\nEnabled the addition of members during group creation\nAdded a section to show what models are visible to a group\nIncluded the assigned groups to user profile pages\n\nThese improvements enable you to manage collections of users at the organization or team level with similar roles or permissions more effectively.\nYou can create new user groups under Settings &gt; Groups:\n\nYour browser does not support the video tag.\n\n\nTry it: Groups\nWe also made improvements to how the invitation history gets displayed when you add new users to your organization:\n\nAdded an Invite History table to show accepted invitations\nRenamed the existing table to Pending Invites and updated it to show only users that have not yet accepted\n\nCombined, these changes make it easier to see both pending and accepted invitations for new users joining your organization:\n\nTry it: Invite New Users"
  },
  {
    "objectID": "releases/2024-mar-27/highlights.html#release-highlights",
    "href": "releases/2024-mar-27/highlights.html#release-highlights",
    "title": "March 27, 2024",
    "section": "",
    "text": "New metric metadata available in the ValidMind Platform UI makes it easy to see information about the test results that were generated with the ValidMind Developer Framework. Test results metadata includes:\n\nThe history of values for test results in your model documentation\nUser attribution to tell you who updated the test results\nRelevant inputs associated with the test results\n\nTest results metadata is available directly in your model documentation:\n\n\nYour browser does not support the video tag.\n\n\n\n\n\n\n\n\nWe’re introducing two important new features to our platform: the Model Findings and Reports pages. The Reports page offers access to essential information about your model validation efforts, summarizing findings, while the Model Findings page facilitates efficient tracking of issues identified during model validation. Together, these pages improve your model validation workflow and simplify compliance with organizational standards and regulatory requirements.\n\n\n\n\n\n\n\nNew Model Findings page\nNew Reports page\n\n\n\n\n\n\n\n\n\nThe new Model Findings page allows you to efficiently track all findings flagged during model validation, such as major and minor issues, deficiencies, model limitations, or other concerns that must be addressed. To facilitate working with findings across all models undergoing validation in your organization, you can filter and view findings by specific criteria, including:\n\nInventory model\nSeverity\nStatus\nRisk area\nAssignee\nDue date\n\nThe new Reports page provides quick insights into your model validation efforts to ensure effective oversight and management of model-related risks. Reports are grouped into bar charts for different types of findings, including:\n\nFindings by risk area\nFindings by business unit\nFindings by status\nFindings by model\n\nAdditionally, you can seamlessly navigate between the Reports page and relevant model findings by selecting information from the available bar charts to drill down into specific areas of concern.\nTry it:\n\nModel Findings\nReports\n\n\n\n\n\n\n\nWe enhanced both the Model Inventory and Model Findings pages to allow for the customization of table views. This feature enables you to customize these pages according to your own preferences, so that specific columns that are of interest are always shown. Columns can also be sorted by clicking on the column header.\nOn the Model Inventory page, you can show or hide table columns by clicking the Manage Columns button and toggling individual columns on or off:\n\n\nYour browser does not support the video tag.\n\n\nThe same customization is available on the Model Findings page:\n\nTry it:\n\nModel Inventory\nModel Findings\n\n\n\n\n\nWhen registering new models in the model inventory, you can now designate custom fields as mandatory. Users must complete these custom fields before they can proceed with registering a model. This feature ensures that all necessary information is provided upfront, improving the completeness and accuracy of the model registration process.\nYou can specify the custom fields that are required under Settings &gt; Custom Fields:\n\nYour browser does not support the video tag.\n\n\nTry it: Custom Fields\n\n\n\n\n\n\n\n\n\nWe made a number of improvements to how you manage user groups:\n\nAdded support for the creation of new groups\nEnabled the addition of members during group creation\nAdded a section to show what models are visible to a group\nIncluded the assigned groups to user profile pages\n\nThese improvements enable you to manage collections of users at the organization or team level with similar roles or permissions more effectively.\nYou can create new user groups under Settings &gt; Groups:\n\nYour browser does not support the video tag.\n\n\nTry it: Groups\nWe also made improvements to how the invitation history gets displayed when you add new users to your organization:\n\nAdded an Invite History table to show accepted invitations\nRenamed the existing table to Pending Invites and updated it to show only users that have not yet accepted\n\nCombined, these changes make it easier to see both pending and accepted invitations for new users joining your organization:\n\nTry it: Invite New Users"
  },
  {
    "objectID": "releases/2024-mar-27/highlights.html#enhancements",
    "href": "releases/2024-mar-27/highlights.html#enhancements",
    "title": "March 27, 2024",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nAdded SSO login option to auth flow\n\nIn this release, we are introducing the option to log in to ValidMind using SSO. Single sign on (SSO) allows your enterprise organizations to have a centralized authentication mechanism, more integrated control over internal applications, and integration with existing identity management systems, such as Google Workspace, LDAP or Active Directory.\n\n\nIn order for this feature to be enabled for your organization, we can configure our Auth0 authentication provider to integrate with your organization’s identity provider. The steps required to make SSO available include:\n\nCreating an Auth0 Enterprise SAML connection to your organization’s identity provider\nConfiguring your organizations identity provider and adding a new SSO application for ValidMind\nEnabling SSO on the ValidMind backend by associating the domain and the Auth0 SAML connection with your organization\n\n\n\n\n\n\nRemoved documentation projects from the UI\n\nDocumentation projects have been removed from the ValidMind Platform UI, streamlining navigation for accessing model documentation. You can now easily locate a model of interest and then access its documentation or validation report directly within the context of that model.\n\n\nThis change aims to improve navigation by providing you with a single entry point from the Model Inventory page. Previously, both the Model Inventory and the Documentation Projects pages offered similar navigation pathways which were redundant. All actions and information previously available for model documentation or validation reports continue to be available, now on the Document Overview page."
  },
  {
    "objectID": "releases/2024-mar-27/highlights.html#bug-fixes",
    "href": "releases/2024-mar-27/highlights.html#bug-fixes",
    "title": "March 27, 2024",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nUser permissions settings menu shown for users without the correct role\n\nWe fixed an issue where the user permissions settings menu was displayed even when a user did not possess the required Customer admin role. The menu is now hidden from view for these users.\n\n\n\n\nUI showing incorrect message to edit text\n\nFixed an issue where users would see a message reading “Start editing by typing here …” even when they didn’t have permissions to edit documentation for the given model. This message is now no longer displayed.\n\n\n\n\n\nUse input_id to track inputs being accessed\n\nFixed a bug where input metadata was not tracked correctly because input_id was not being used to look up inputs. Input metadata is now tracked as expected."
  },
  {
    "objectID": "releases/2024-mar-27/highlights.html#documentation-updates",
    "href": "releases/2024-mar-27/highlights.html#documentation-updates",
    "title": "March 27, 2024",
    "section": "Documentation updates",
    "text": "Documentation updates\n\n\nImproved Guides section\n\nThe main user guides section of our documentation site now features a new landing page that organizes common tasks by product area to assist with setting up ValidMind in a production environment. A redesigned left sidebar enhances navigation through content by product area or major feature, simplifying the search for relevant information based on your role or interest in specific product features.\n\n\nLearn more …\n\n\nImproved model documentation and validation report sections\n\nThe user guide sections for working with model documentation and preparing validation reports have been expanded with new landing pages, better procedural task steps, and more contextual information.\n\n\nLearn more:\n\nWorking with model documentation\nPreparing validation reports\n\n\n\nImproved model documentation and validation report sections\n\nThe user guide sections for model documentation and validation reports have been enhanced with new landing pages, improved procedural steps, and additional context. This update makes it easier to work with model documentation and prepare validation reports.\n\n\nLearn more:\n\nWorking with model documentation\nPreparing validation reports\n\n\n\nNew QuickStart for local developer environments\n\nA new QuickStart is available for first-time users that shows you how to run our introductory Jupyter notebook locally in your own developer environment.\n\n\nWe also removed the option to try out ValidMind in Docker. JupyterHub and Google Colab remain available as other QuickStart options to try, as before.\nLearn more …\n\n\nImproved supported models page\n\nWe improved our supported models documentation with additional information about supported model types, modeling libraries, and other tools.\n\n\nLearn more …\n\n\nA new dedicated data privacy page\n\nA new data privacy policy page now aggregates all the information that explains how we protect your personal information in one convenient location.\n\n\nLearn more …\n\n\nRemoved roadmap commitments\n\nWe updated the FAQ section to remove some stale references to roadmap items.\n\n\n\n\nEasier docs site previews\n\nWe made it easier for contributors to preview and render the docs site locally, without having to first fetch some additional content from other repos first."
  },
  {
    "objectID": "releases/2024-mar-27/highlights.html#how-to-upgrade",
    "href": "releases/2024-mar-27/highlights.html#how-to-upgrade",
    "title": "March 27, 2024",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2023-oct-25/highlights.html",
    "href": "releases/2023-oct-25/highlights.html",
    "title": "October 25, 2023",
    "section": "",
    "text": "This release introduces a new guide for modifying configurations and parameters within the ValidMind Developer Framework. It also brings new features to the ValidMind Platform UI that enable you to remove blocks of content from model documentation and work with your settings more effectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can now remove blocks of text or test-related content from model documentation in the editor of the platform UI. This new feature gives you more control over your documentation and enables you to remove content that is no longer needed.\n\nTo remove text blocks and test-driven blocks from model documentation, you first select the block you want to remove and click , either in the text-block’s toolbar or in the test-driven’s block single-button toolbar:\nTry it …\n\n\n\n\n\nA new Settings landing page now organizes more of your settings for the ValidMind platform in one convenient place.\n\n\n\nNew Settings page\n\n\nFrom this page you can manage:\n\nYour organization, including the name and the API and secret key you use to connect to the ValidMind platform.\nThe documentation templates that standardize the documentation table of contents for your projects and configure the required validation tests for specific model use cases.\nWorkflows that determine the statuses of your model documentation and how they transition through the workflow according to your requirements.\n\nTry it …"
  },
  {
    "objectID": "releases/2023-oct-25/highlights.html#release-highlights",
    "href": "releases/2023-oct-25/highlights.html#release-highlights",
    "title": "October 25, 2023",
    "section": "",
    "text": "This release introduces a new guide for modifying configurations and parameters within the ValidMind Developer Framework. It also brings new features to the ValidMind Platform UI that enable you to remove blocks of content from model documentation and work with your settings more effectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can now remove blocks of text or test-related content from model documentation in the editor of the platform UI. This new feature gives you more control over your documentation and enables you to remove content that is no longer needed.\n\nTo remove text blocks and test-driven blocks from model documentation, you first select the block you want to remove and click , either in the text-block’s toolbar or in the test-driven’s block single-button toolbar:\nTry it …\n\n\n\n\n\nA new Settings landing page now organizes more of your settings for the ValidMind platform in one convenient place.\n\n\n\nNew Settings page\n\n\nFrom this page you can manage:\n\nYour organization, including the name and the API and secret key you use to connect to the ValidMind platform.\nThe documentation templates that standardize the documentation table of contents for your projects and configure the required validation tests for specific model use cases.\nWorkflows that determine the statuses of your model documentation and how they transition through the workflow according to your requirements.\n\nTry it …"
  },
  {
    "objectID": "releases/2023-oct-25/highlights.html#enhancements",
    "href": "releases/2023-oct-25/highlights.html#enhancements",
    "title": "October 25, 2023",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nRouge and Bert Score metrics now show average scores: Introduced RougeMetricsAggregate and BertScoreAggregate to offer a high-level overview of model performance across a large number of text rows. These metrics complement the detailed row-by-row analysis provided by RougeMetrics and BertScore. \n\n\n\nAdded metrics for safety toxicity and bias in text summarization. We introduced several new metrics to evaluate safety and bias risks in text summarization:\n\nToxicityScore: Measures safety risk\nToxicityHistogram: Provides a distribution of safety risk scores\nRegardScore: Evaluates bias risk\nRegardHistogram: Shows distribution of bias risk scores"
  },
  {
    "objectID": "releases/2023-oct-25/highlights.html#bug-fixes",
    "href": "releases/2023-oct-25/highlights.html#bug-fixes",
    "title": "October 25, 2023",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nShap metric issue resolved. - We set matplotlib to version 3.7.x in pyproject.toml to fix an incompatibility with the latest matplotlib version (3.8.0). This incompatibility was causing SHAP plot errors. We will keep track of matplotlib releases for future updates. Once fixed, we will consider updating the version."
  },
  {
    "objectID": "releases/2023-oct-25/highlights.html#documentation-updates",
    "href": "releases/2023-oct-25/highlights.html#documentation-updates",
    "title": "October 25, 2023",
    "section": "Documentation updates",
    "text": "Documentation updates\n\n\nPlatform overview rewrite. We expanded our platform overview to provide more background information about what ValidMind offers and how we enable you to comply with policies and regulations such as SR 11-7 and SS1/23. Try it …\n\n\n\nQuickStart updates for the closed beta. We updated the QuickStart section of our documentation to reflect recent UI and sign-up flow changes. Try it …\n\n\n\n\nImproved handling of Jupyter notebooks for code samples and testing how-to content. We now programmatically embed these notebooks in our documentation site and generate a downloadable notebooks.zip file with all notebooks and supporting datasets. Try it:\n\nCode samples\nTests and test suites"
  },
  {
    "objectID": "releases/2023-oct-25/highlights.html#how-to-upgrade",
    "href": "releases/2023-oct-25/highlights.html#how-to-upgrade",
    "title": "October 25, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html",
    "href": "releases/2023-sep-27/highlights.html",
    "title": "September 27, 2023",
    "section": "",
    "text": "In this release, we’ve added support for large language models (LLMs) to enhance the capabilities of the ValidMind Developer Framework in preparation for the closed beta, along with a number of new demo notebooks that you can try out. Other enhancements provide improvements for the developer experience and with our documentation site."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#release-highlights",
    "href": "releases/2023-sep-27/highlights.html#release-highlights",
    "title": "September 27, 2023",
    "section": "",
    "text": "In this release, we’ve added support for large language models (LLMs) to enhance the capabilities of the ValidMind Developer Framework in preparation for the closed beta, along with a number of new demo notebooks that you can try out. Other enhancements provide improvements for the developer experience and with our documentation site."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#var-vm_framework-v1.19.0",
    "href": "releases/2023-sep-27/highlights.html#var-vm_framework-v1.19.0",
    "title": "September 27, 2023",
    "section": "ValidMind Developer Framework (v1.19.0)",
    "text": "ValidMind Developer Framework (v1.19.0)\n\n\nLarge language model (LLM) support\nWe added initial support for large language models (LLMs) in ValidMind via the new FoundationModel class. You can now create an instance of a FoundationModel and specify predict_fn and a prompt, and pass that into any test suite, for example. The predict_fn must be defined by the user and implements the logic for calling the Foundation LLM, usually via the API.\nTo demonstrate the capabilities of LLM support, this release also includes new demo notebooks:\n\n\nPrompt validation demo notebook for LLMs. As a proof of concept, we added initial native prompt validation tests to the developer framework, including a notebook and simple template to test out these metrics on a sentiment analysis LLM model we built.\n\n\n\n\nText summarization model demo notebook for LLMs. We added a new notebook in the developer framework that includes the financial news dataset, initializes a Hugging Face summarization model using the init_model interface, implements relevant metrics for testing, and demonstrates how to run a text summarization metrics test suite for an LLM instructed as a financial news summarizer.\n\n\n\n\n\n\n\nInterested in our LLM support?\n\n\n\nLarge language model support and more will be available in our closed beta. Read the announcement and sign up to take the first step in exploring all that ValidMind has to offer:\n\n\nJoin the waitlist\n\n\n\n\n\n\n\nSupport for Hugging Face models\nValidMind can now validate pre-trained models from the HuggingFace Hub, including any language model compatible with the HF transformers API.\nTo illustrate this new feature, we have included a financial news sentiment analysis demo that runs documentation tests for a Hugging Face model with text classification using the financial_phrasebank. Try it …\n\n\n\n\nA better developer experience with run_test\nWe added a new run_test helper function that streamlines running tests for you. This function allows executing any individual test independent of a test suite or a documentation template. A one-line command can execute a test, making it easier to run tests with various parameters and options. For example:\nrun_test(\"ClassImbalance\", dataset=dataset, params=params, send=True)\n\nWe also updated the QuickStart notebook to have a consistent experience: Try it …\nThis notebook:\n\nNow runs vm.preview_template() after initializing ValidMind\nNow runs vm.run_documentation_tests() instead of running a test suite that is not connected to the template\n\n\nExample usage for run_test\nDiscover existing tests by calling list_tests() or describe_test():\nlist_tests():\n\ndescribe_test():\n\nView the tests associated with a documentation template by running preview_template():\n\nUsing the test ID, run a given test and pass in additional configuration parameters and inputs:\n# No params\ntest_results = vm.tests.run_test(\n    \"class_imbalance\",\n    dataset=vm_dataset\n)\n\n# Custom params\ntest_results = vm.tests.run_test(\n    \"class_imbalance\",\n    params={\"min_percent_threshold\": 30},\n    dataset=vm_dataset\n)\nOutput: \nSend the results of the test to ValidMind by calling .log():\ntest_results.log()"
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#enhancements",
    "href": "releases/2023-sep-27/highlights.html#enhancements",
    "title": "September 27, 2023",
    "section": "Enhancements",
    "text": "Enhancements\n\n\nMulti-class test improvements. We made a number of changes to metrics to improve the developer experience:\n\nA new fail_fast argument can be passed to run_test_plan, run_test_suite and run_documentation_tests, used to fail and raise an exception on the first error encountered. This change is useful for debugging.\nClassifierPerformance test now determines if you are testing a binary or a multi-class model. When testing a multi-class model, we now report additional per-class, macro and weighted average metrics.\nFixed F1 score metric so it works correctly for binary and multi-class models.\n\n\n\n\nAdded multi-class classification support. The developer framework now supports a multi-class version of some the existing metrics, such as confusion matrix, accuracy, precision, recall, and more. Also, the dataset and model interfaces now support dealing with multiple targets.\n\n\n\nImplemented classification model comparison metrics. Added a model performance comparison test for classification tasks. The test includes metrics such as accuracy, F1, precision, recall, and roc_auc score.\n\n\n\nTrack additional test metadata. Added a metadata property to every ValidMind test class. The metadata property includes a task_types field and a tags field which both serve to categorize the tests based on what data and model types they work with, what category of test they fall into, and more.\n\n\n\nFilter tests by task type and tags. We added a new search feature to the validmind.tests.list_tests function to allow for better test discoverability. The list_tests function in the tests module now supports the following arguments:\n\nfilter: If set, will match tests by ID, task_types or tags using a combination of substring and fuzzy string matching. Defaults to None.\ntask: If set, will further narrow matching tests (assuming filter has been passed) by exact matching the task to the test’s task_type metadata. Defaults to None.\ntags: If a list is passed, will again narrow the matched tests by exact matching on tags. Defaults to None."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#documentation-updates",
    "href": "releases/2023-sep-27/highlights.html#documentation-updates",
    "title": "September 27, 2023",
    "section": "Documentation updates",
    "text": "Documentation updates\n\n\nUser journey improvements. We enhanced the architecture and content of our external docs site to make the user journey more efficient for model developers and model validators who are new to our products:\n\nReworked the “Get Started” section to include more conceptual information and an overview of the high-level workflows. Try it …\nRevised the “Developer Framework” section to provide an end-to-end overview of the workflow that model developers should follow as they adopt the framework. Try it …\n\n\n\n\nDocs site improvements. We made a number of incremental improvements to our user guide:\n\nNew dropdown for the developer framework that gives faster access to the most important bits, such as our code samples and the reference documentation.\n\nPublication date for each page that reflects the last time the source file was touched.\nPrevious and next topic footers** for related topics that make it easier to keep reading.\nExpanded overview for key ValidMind concepts with some additional information.\nLighter background for diagrams that improves legibility."
  },
  {
    "objectID": "releases/2023-sep-27/highlights.html#how-to-upgrade",
    "href": "releases/2023-sep-27/highlights.html#how-to-upgrade",
    "title": "September 27, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "releases/2023-dec-13/highlights.html",
    "href": "releases/2023-dec-13/highlights.html",
    "title": "December 13, 2023",
    "section": "",
    "text": "This release includes bug fixes for the ValidMind Platform UI and reduces the external dependencies for our user-facing documentation site."
  },
  {
    "objectID": "releases/2023-dec-13/highlights.html#release-highlights",
    "href": "releases/2023-dec-13/highlights.html#release-highlights",
    "title": "December 13, 2023",
    "section": "",
    "text": "This release includes bug fixes for the ValidMind Platform UI and reduces the external dependencies for our user-facing documentation site."
  },
  {
    "objectID": "releases/2023-dec-13/highlights.html#bug-fixes",
    "href": "releases/2023-dec-13/highlights.html#bug-fixes",
    "title": "December 13, 2023",
    "section": "Bug fixes",
    "text": "Bug fixes\n\n\nContextualize references to the current user. We fixed an issue where activity logs indicated user references out of context, displaying the current user’s full name instead of referring to them as “you”.\n\n\n\nRespect width and height for plots when available. We fixed an issue where some plots were being cut off due to having more height than available in the current viewport. This issue was fixed by respecting the plot’s original defined height instead of setting a maximum pre-configured height on the frontend."
  },
  {
    "objectID": "releases/2023-dec-13/highlights.html#documentation-updates",
    "href": "releases/2023-dec-13/highlights.html#documentation-updates",
    "title": "December 13, 2023",
    "section": "Documentation updates",
    "text": "Documentation updates\n\nRemove safe external dependencies. To improve loading times and reduce the reliance on being connected to the internet, we removed some external dependencies for our docs site:\n\nFonts are now stored and imported locally\nSome unneeded font references in our template schema docs have been removed"
  },
  {
    "objectID": "releases/2023-dec-13/highlights.html#how-to-upgrade",
    "href": "releases/2023-dec-13/highlights.html#how-to-upgrade",
    "title": "December 13, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "training/training-for-model-validators.html#prerequisites",
    "href": "training/training-for-model-validators.html#prerequisites",
    "title": "Training for Model Validators",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo try out this training module, you need to have been onboarded onto the ValidMind training environment.\nLog into the ValidMind Platform UI to check your access:\n\n\n\nLog in\n\n\n\nBe sure to return to this page afterwards."
  },
  {
    "objectID": "training/training-for-model-validators.html#review-documentation",
    "href": "training/training-for-model-validators.html#review-documentation",
    "title": "Training for Model Validators",
    "section": "1. Review documentation",
    "text": "1. Review documentation\n\n\nVerify model documentation is complete and complies with regulatory standards by cross-checking against your guidelines.\n\nHave a question? Ask your model developer right in their model documentation to collaborate.\n\n\n\n\n\n\n\nShow me how\n\n\n\n\n\n\nShow me how"
  },
  {
    "objectID": "training/training-for-model-validators.html#analyze-test-results",
    "href": "training/training-for-model-validators.html#analyze-test-results",
    "title": "Training for Model Validators",
    "section": "2. Analyze test results",
    "text": "2. Analyze test results\n\n\nLocate test results in the model documentation, review the data, and identify issues with the model.\n\nBased on your review of the model documentation, add findings and link evidence for your validation report.\n\n\n\n\n\n\n\nShow me how\n\n\n\n\n\nExplore sections:\n\n\n\n\nData Preparation\n\n\nModel Development\n\n\n\n\n\n\n\n\n\nShow me how"
  },
  {
    "objectID": "training/training-for-model-validators.html#prepare-validation-reports",
    "href": "training/training-for-model-validators.html#prepare-validation-reports",
    "title": "Training for Model Validators",
    "section": "3. Prepare validation reports",
    "text": "3. Prepare validation reports\nUse predefined templates within the ValidMind platform to create validation reports.\nEnsure all necessary data from model tests and evaluations is included.\n\n\n\nShow me how"
  },
  {
    "objectID": "training/training-for-model-validators.html#track-approvals",
    "href": "training/training-for-model-validators.html#track-approvals",
    "title": "Training for Model Validators",
    "section": "4. Track approvals",
    "text": "4. Track approvals\nSubmit validation reports for review through the ValidMind platform, monitor the approval status, and address any feedback or required changes.\n\n\n\nShow me how"
  },
  {
    "objectID": "training/training-for-model-validators.html#preview-use-the-ai-based-documentation-assistant",
    "href": "training/training-for-model-validators.html#preview-use-the-ai-based-documentation-assistant",
    "title": "Training for Model Validators",
    "section": "PREVIEW: Use the AI-based documentation assistant",
    "text": "PREVIEW: Use the AI-based documentation assistant\nUse the ValidMind AI tool to auto-generate initial report content based on model metadata and to gain insight into test results."
  },
  {
    "objectID": "training/training-for-model-validators.html#whats-next",
    "href": "training/training-for-model-validators.html#whats-next",
    "title": "Training for Model Validators",
    "section": "What’s next",
    "text": "What’s next\nFind your next learning resource: ValidMind training\n\n\n\nValidMind Academy | Home"
  },
  {
    "objectID": "training/training-for-administrators.html#onboard-your-organization",
    "href": "training/training-for-administrators.html#onboard-your-organization",
    "title": "Training for Administrators",
    "section": "1. Onboard your organization",
    "text": "1. Onboard your organization\nInvite initial users by entering their email addresses and sending out invitations. Create groups to organize users based on departments or project teams. Assign roles and permissions to users, defining their access levels and responsibilities."
  },
  {
    "objectID": "training/training-for-administrators.html#manage-users-and-roles",
    "href": "training/training-for-administrators.html#manage-users-and-roles",
    "title": "Training for Administrators",
    "section": "2. Manage users and roles",
    "text": "2. Manage users and roles\nAdd new users by entering their details and assigning them to appropriate groups. Edit user profiles to update information or change their roles and permissions. Remove users who no longer need access to the platform, ensuring security."
  },
  {
    "objectID": "training/training-for-administrators.html#manage-permissions",
    "href": "training/training-for-administrators.html#manage-permissions",
    "title": "Training for Administrators",
    "section": "3. Manage permissions",
    "text": "3. Manage permissions\nEnsure compliance with organizational policies by reviewing and updating user permissions. Set role-specific permissions to control what each user can access and modify within the platform."
  },
  {
    "objectID": "training/training-for-administrators.html#manage-templates",
    "href": "training/training-for-administrators.html#manage-templates",
    "title": "Training for Administrators",
    "section": "4. Manage templates",
    "text": "4. Manage templates\nCustomize documentation templates to create consistent and comprehensive model documentation and validation reports. These templates are standardized to ensure uniformity but can be tailored to meet your specific project needs."
  },
  {
    "objectID": "training/training-for-administrators.html#manage-workflows",
    "href": "training/training-for-administrators.html#manage-workflows",
    "title": "Training for Administrators",
    "section": "5. Manage workflows",
    "text": "5. Manage workflows\nEnsure compliance by verifying your current approval workflow, advance documentation through tailored workflow states, and customize workflows with multiple review stages for your organizational needs."
  },
  {
    "objectID": "training/training-for-administrators.html#whats-next",
    "href": "training/training-for-administrators.html#whats-next",
    "title": "Training for Administrators",
    "section": "What’s next",
    "text": "What’s next\nFind your next learning resource: ValidMind training\n\n\n\nValidMind Academy | Home"
  },
  {
    "objectID": "training/training-for-model-developers.html#prerequisites",
    "href": "training/training-for-model-developers.html#prerequisites",
    "title": "Training for Model Developers",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo try out this training module, you need to have been onboarded onto the ValidMind training environment.\nLog into the ValidMind Platform UI to check your access:\n\n\n\nLog in\n\n\n\nBe sure to return to this page afterwards."
  },
  {
    "objectID": "training/training-for-model-developers.html#initialize-the-developer-framework",
    "href": "training/training-for-model-developers.html#initialize-the-developer-framework",
    "title": "Training for Model Developers",
    "section": "1. Initialize the Developer Framework",
    "text": "1. Initialize the Developer Framework\nInstall the client library, register a new model to document in the model inventory, and preview the documentation template."
  },
  {
    "objectID": "training/training-for-model-developers.html#start-the-model-development-process",
    "href": "training/training-for-model-developers.html#start-the-model-development-process",
    "title": "Training for Model Developers",
    "section": "2. Start the model development process",
    "text": "2. Start the model development process\nInitialize datasets, access ValidMind’s repository of tests, run pre-built tests, investigate the results, and add evidence to documentation."
  },
  {
    "objectID": "training/training-for-model-developers.html#edit-model-documentation",
    "href": "training/training-for-model-developers.html#edit-model-documentation",
    "title": "Training for Model Developers",
    "section": "3. Edit model documentation",
    "text": "3. Edit model documentation\nUnderstand how to use text and test-driven content blocks in the Platform UI to expand the existing template structure of your model documentation, and edit documentation."
  },
  {
    "objectID": "training/training-for-model-developers.html#finalize-testing-and-documentation",
    "href": "training/training-for-model-developers.html#finalize-testing-and-documentation",
    "title": "Training for Model Developers",
    "section": "4. Finalize testing and documentation",
    "text": "4. Finalize testing and documentation\nEnsure model documentation includes custom tests and apply configuration changes. Finalize the process with a fully documented model ready for review."
  },
  {
    "objectID": "training/training-for-model-developers.html#implement-custom-tests-and-integrate-external-test-providers",
    "href": "training/training-for-model-developers.html#implement-custom-tests-and-integrate-external-test-providers",
    "title": "Training for Model Developers",
    "section": "1. Implement custom tests and integrate external test providers",
    "text": "1. Implement custom tests and integrate external test providers\nInclude your own custom tests and include them in model documentation as additional evidence, and register an external test provider to run your own tests."
  },
  {
    "objectID": "training/training-for-model-developers.html#collaborate-with-others",
    "href": "training/training-for-model-developers.html#collaborate-with-others",
    "title": "Training for Model Developers",
    "section": "2. Collaborate with others",
    "text": "2. Collaborate with others\nExplore methods for collaborating with model validators on documentation projects."
  },
  {
    "objectID": "training/training-for-model-developers.html#view-documentation-activity",
    "href": "training/training-for-model-developers.html#view-documentation-activity",
    "title": "Training for Model Developers",
    "section": "3. View documentation activity",
    "text": "3. View documentation activity\nTrack changes and updates made to model documentation over time."
  },
  {
    "objectID": "training/training-for-model-developers.html#submit-for-approval",
    "href": "training/training-for-model-developers.html#submit-for-approval",
    "title": "Training for Model Developers",
    "section": "4. Submit for approval",
    "text": "4. Submit for approval\nFollow the process for submitting documentation for review and approval to ensure compliance."
  },
  {
    "objectID": "training/training-for-model-developers.html#preview-use-the-ai-based-documentation-assistant",
    "href": "training/training-for-model-developers.html#preview-use-the-ai-based-documentation-assistant",
    "title": "Training for Model Developers",
    "section": "PREVIEW: Use the AI-based documentation assistant",
    "text": "PREVIEW: Use the AI-based documentation assistant\n\n\nUse the ValidMind AI tool to auto-generate a starting point for sections of the model documentation and to gain insight into test results.\n\nTBD"
  },
  {
    "objectID": "training/training-for-model-developers.html#whats-next",
    "href": "training/training-for-model-developers.html#whats-next",
    "title": "Training for Model Developers",
    "section": "What’s next",
    "text": "What’s next\nFind your next learning resource: ValidMind training\n\n\n\nValidMind Academy | Home"
  },
  {
    "objectID": "tests/prompt_validation/Robustness.html",
    "href": "tests/prompt_validation/Robustness.html",
    "title": "Robustness",
    "section": "",
    "text": "Robustness\nAssesses the robustness of prompts provided to a Large Language Model under varying conditions and contexts.\nPurpose: The Robustness test is meant to evaluate the resilience and reliability of prompts provided to a Language Learning Model (LLM). The aim of this test is to guarantee that the prompts consistently generate accurate and the expected outputs, despite being in diverse or challenging scenarios.\nTest Mechanism: The Robustness test appraises prompts under various conditions, alterations, and contexts to ascertain their stability in producing consistent responses from the LLM. Factors evaluated range from different phrasings, inclusion of potential distracting elements, and various input complexities. By default, the test generates 10 inputs for a prompt but can be adjusted according to test parameters.\nSigns of High Risk:\n\nIf the output from the tests diverges extensively from the expected results, this indicates high risk.\nWhen the prompt doesn’t give a consistent performance across various tests.\nA high risk is indicated when the prompt is susceptible to breaking, especially when the output is expected to be of a specific type.\n\nStrengths:\n\nThe robustness test helps to ensure stable performance of the LLM prompts and lowers the chances of generating unexpected or off-target outputs.\nThis test is vital for applications where predictability and reliability of the LLM’s output are crucial.\n\nLimitations:\n\nCurrently, the test only supports single-variable prompts, which restricts its application to more complex models.\nWhen there are too many target classes (over 10), the test is skipped, which can leave potential vulnerabilities unchecked in complex multi-class models.\nThe test may not account for all potential conditions or alterations that could show up in practical use scenarios."
  },
  {
    "objectID": "tests/prompt_validation/Conciseness.html",
    "href": "tests/prompt_validation/Conciseness.html",
    "title": "Conciseness",
    "section": "",
    "text": "Conciseness\nAnalyzes and grades the conciseness of prompts provided to a Large Language Model.\nPurpose: The Conciseness Assessment is designed to evaluate the brevity and succinctness of prompts provided to a Language Learning Model (LLM). A concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed.\nTest Mechanism: Using an LLM, this test conducts a conciseness analysis on input prompts. The analysis grades the prompt on a scale from 1 to 10, where the grade reflects how well the prompt delivers clear instructions without being verbose. Prompts that score equal to or above a predefined threshold (default set to 7) are deemed successfully concise. This threshold can be adjusted to meet specific requirements.\nSigns of High Risk:\n\nPrompts that consistently score below the predefined threshold.\nPrompts that are overly wordy or contain unnecessary information.\nPrompts that create confusion or ambiguity due to excess or unnecessary information.\n\nStrengths:\n\nEnsures clarity and effectiveness of the prompts.\nPromotes brevity and preciseness in prompts without sacrificing essential information.\nUseful for models like LLMs, where input prompt length and clarity greatly influence model performance.\nProvides a quantifiable measure of prompt conciseness.\n\nLimitations:\n\nThe conciseness score is based on an AI’s assessment, which might not fully capture human interpretation of conciseness.\nThe predefined threshold for conciseness could be subjective and might need adjustment based on application.\nThe test is dependent on the LLM’s understanding of conciseness, which might vary from model to model."
  },
  {
    "objectID": "tests/prompt_validation/Clarity.html",
    "href": "tests/prompt_validation/Clarity.html",
    "title": "Clarity",
    "section": "",
    "text": "Clarity\nEvaluates and scores the clarity of prompts in a Large Language Model based on specified guidelines.\nPurpose: The Clarity evaluation metric is used to assess how clear the prompts of a Large Language Model (LLM) are. This assessment is particularly important because clear prompts assist the LLM in more accurately interpreting and responding to instructions.\nTest Mechanism: The evaluation uses an LLM to scrutinize the clarity of prompts, factoring in considerations such as the inclusion of relevant details, persona adoption, step-by-step instructions, usage of examples and specification of desired output length. Each prompt is rated on a clarity scale of 1 to 10, and any prompt scoring at or above the preset threshold (default of 7) will be marked as clear. It is important to note that this threshold can be adjusted via test parameters, providing flexibility in the evaluation process.\nSigns of High Risk:\n\nPrompts that consistently score below the clarity threshold\nRepeated failure of prompts to adhere to guidelines for clarity. These guidelines could include detail inclusion, persona adoption, explicit step-by-step instructions, use of examples, and specification of output length.\n\nStrengths:\n\nEncourages the development of more effective prompts that aid the LLM in interpreting instructions accurately.\nApplies a quantifiable measure (a score from 1 to 10) to evaluate the clarity of prompts.\nThreshold for clarity is adjustable, allowing for flexible evaluation depending on the context.\n\nLimitations:\n\nScoring system is subjective and relies on the AI’s interpretation of ‘clarity’.\nThe test assumes that all required factors (detail inclusion, persona adoption, step-by-step instructions, use of examples, and specification of output length) contribute equally to clarity, which might not always be the case.\nThe evaluation may not be as effective if used on non-textual models."
  },
  {
    "objectID": "tests/model_validation/ModelMetadata.html",
    "href": "tests/model_validation/ModelMetadata.html",
    "title": "ModelMetadata",
    "section": "",
    "text": "ModelMetadata\nExtracts and summarizes critical metadata from a machine learning model instance for comprehensive analysis.\nPurpose: This test is designed to collect and summarize important metadata related to a particular machine learning model. Such metadata includes the model’s architecture (modeling technique), the version and type of modeling framework used, and the programming language the model is written in.\nTest Mechanism: The mechanism of this test consists of extracting information from the model instance. It tries to extract the model information such as the modeling technique used, the modeling framework version, and the programming language. It decorates this information into a data frame and returns a summary of the results.\nSigns of High Risk:\n\nHigh risk could be determined by a lack of documentation or inscrutable metadata for the model.\nUnidentifiable language, outdated or unsupported versions of modeling frameworks, or undisclosed model architectures reflect risky situations, as they could hinder future reproducibility, support, and debugging of the model.\n\nStrengths:\n\nThe strengths of this test lie in the increased transparency and understanding it brings regarding the model’s setup.\nKnowing the model’s architecture, the specific modeling framework version used, and the language involved, provides multiple benefits: supports better error understanding and debugging, facilitates model reuse, aids compliance of software policies, and assists in planning for model obsolescence due to evolving or discontinuing software and dependencies.\n\nLimitations:\n\nNotably, this test is largely dependent on the compliance and correctness of information provided by the model or the model developer.\nIf the model’s built-in methods for describing its architecture, framework or language are incorrect or lack necessary information, this test will hold limitations.\nMoreover, it is not designed to directly evaluate the performance or accuracy of the model, rather it provides supplementary information which aids in comprehensive analysis."
  },
  {
    "objectID": "tests/model_validation/embeddings/CosineSimilarityHeatmap.html",
    "href": "tests/model_validation/embeddings/CosineSimilarityHeatmap.html",
    "title": "CosineSimilarityHeatmap",
    "section": "",
    "text": "CosineSimilarityHeatmap"
  },
  {
    "objectID": "tests/model_validation/embeddings/PCAComponentsPairwisePlots.html",
    "href": "tests/model_validation/embeddings/PCAComponentsPairwisePlots.html",
    "title": "PCAComponentsPairwisePlots",
    "section": "",
    "text": "PCAComponentsPairwisePlots"
  },
  {
    "objectID": "tests/model_validation/embeddings/StabilityAnalysis.html",
    "href": "tests/model_validation/embeddings/StabilityAnalysis.html",
    "title": "StabilityAnalysis",
    "section": "",
    "text": "StabilityAnalysis\nBase class for embeddings stability analysis tests\nrequired_inputs = [“model”, “dataset”] default_params = { mean_similarity_threshold”: 0.7, } metadata = { task_types”: [“feature_extraction”], tags”: [“llm”, “text_data”, “text_embeddings”, “visualization”], }\n@abstractmethod def perturb_data(self, data: str) -&gt; str: Perturb a string of text (overriden by subclasses)"
  },
  {
    "objectID": "tests/model_validation/embeddings/StabilityAnalysisSynonyms.html",
    "href": "tests/model_validation/embeddings/StabilityAnalysisSynonyms.html",
    "title": "StabilityAnalysisSynonyms",
    "section": "",
    "text": "StabilityAnalysisSynonyms\nEvaluates the stability of text embeddings models when words in test data are replaced by their synonyms randomly.\nThis test uses WordNet to find synonyms for words in the test dataset and expects a parameter probability that determines the probability of swapping a word with a synonym.\nPurpose: The Stability Analysis Synonyms test is designed to gauge the robustness and stability of an embeddings model on text-based data. The test does so by introducing random word changes through replacing words in the test dataset with their synonyms.\nTest Mechanism: This test utilizes WordNet to find synonyms for a given word present in the test data, replacing the original word with this synonym based on a given probability. The probability is defined as a parameter and determines the likelihood of swapping a word with its synonym. By default, this is set at 0.02 but can be adjusted based on specific test requirements. This methodology enables an evaluation of how such replacements can affect the model’s performance.\nSigns of High Risk:\n\nThe model’s performance or predictions change significantly after swapping words with their synonyms.\nThe model shows high sensitivity to small perturbations, like modifying the data with synonyms.\nThe embeddings model fails to identify similar meanings between the original words and their synonyms, which means it lacks semantic understanding.\n\nStrengths:\n\nThe test is flexible in its application. The ‘probability’ parameter can be adjusted based on the degree of synonym swapping required.\nEfficient in gauging a model’s sensitivity or robustness with respect to small changes in input data.\nThe test can provide insights into the semantic understanding of the model as it monitors the impact of swapping words with synonyms.\n\nLimitations:\n\nThe ability to perturb data is reliant on the availability of synonyms, limiting its efficiency.\nThe test assumes that the synonyms provided by WordNet are accurate and interchangeable in all contexts, which may not always be the case given the intricacies of language and context-specific meanings.\nThis test does not consider the influence of multi-word expressions or phrases, as synonyms are considered at the word level only.\nRelies solely on the WordNet corpus for synonyms, which would limit its effectiveness for specialized or domain-specific jargon not included in that corpus.\nIt does not consider the semantic role of the words in the sentence, so the swapped synonym could potentially alter the overall meaning of the sentence, leading to a false perception of the model’s stability."
  },
  {
    "objectID": "tests/model_validation/embeddings/EuclideanDistanceHeatmap.html",
    "href": "tests/model_validation/embeddings/EuclideanDistanceHeatmap.html",
    "title": "EuclideanDistanceHeatmap",
    "section": "",
    "text": "EuclideanDistanceHeatmap"
  },
  {
    "objectID": "tests/model_validation/embeddings/EmbeddingsVisualization2D.html",
    "href": "tests/model_validation/embeddings/EmbeddingsVisualization2D.html",
    "title": "EmbeddingsVisualization2D",
    "section": "",
    "text": "EmbeddingsVisualization2D\nVisualizes 2D representation of text embeddings generated by a model using t-SNE technique.\n1. Purpose: The objective of this metric is to provide a visual 2D representation of the embeddings created by a text embedding machine learning model. By doing so, it aids in analyzing the embedding space created by the model and helps in understanding how the learned embeddings are distributed and how they relate to each other.\n2. Test Mechanism: This metric uses the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique, which is a tool for visualizing high-dimensional data by reducing the dimensionality to 2. The perplexity parameter for t-SNE is set to the value provided by the user. If the input perplexity value is greater than the number of samples, the perplexity is adjusted to be one less than the number of samples. Following the reduction of dimensionality, a scatter plot is produced depicting each embedding as a data point in the visualized 2D plane.\n3. Signs of High Risk:\n\nIf the embeddings are highly concentrated in a specific region of the plane, it might indicate that the model is not learning diverse representations of the text.\nWide gaps or partitions in the visualization could suggest that the model is over-segmenting in the embedding space and may lead to poor generalization.\n\n4. Strengths:\n\nOffers a powerful visual tool that can assist in understanding and interpreting high-dimensional embeddings, which could otherwise be difficult to visualize.\nIt is model-agnostic and can be used with any machine learning model that produces text embeddings.\nt-SNE visualization helps in focusing on local structures and preserves the proximity of points that are close together in the original high-dimensional space.\n\n5. Limitations:\n\nThe reduction of high-dimensional data to 2D can result in loss of some information, which may lead to misinterpretation.\nDue to its stochastic nature, t-SNE can produce different results when run multiple times with the same parameters, leading to potential inconsistency in interpretation.\nIt is designed for visual exploration and not for downstream tasks; that is, the 2D embeddings generated should not be directly used for further training or analysis."
  },
  {
    "objectID": "tests/model_validation/embeddings/StabilityAnalysisRandomNoise.html",
    "href": "tests/model_validation/embeddings/StabilityAnalysisRandomNoise.html",
    "title": "StabilityAnalysisRandomNoise",
    "section": "",
    "text": "StabilityAnalysisRandomNoise\nEvaluate robustness of embeddings models to random noise introduced by using a probability parameter to choose random locations in the text to apply random perturbations. These perturbations include:\n\nSwapping two adjacent words\nIntroducing a random typo in a word\nDeleting a random word\nInserting a random word at a random position\n\nPurpose: The purpose of the stability analysis is to evaluate the robustness of a text embeddings model to random noise. Random perturbations such as swapping adjacent words, introducing random typos, deleting random words, or inserting random words at random positions in the text are introduced, to gauge the model’s performance and stability.\nTest Mechanism: The test mechanism includes a series of function-defined random perturbations like swapping two words random_swap, introducing a typo in a word introduce_typo, deleting a random word random_deletion, and inserting a random word at a random position random_insertion. A probability parameter determines the likelihood or frequency of these perturbations within the text data.\nThe perturb_data function initally tokenizes the string data based on spaces then applies selected random perturbations based on the provided probability for each word in the text.\nSigns of High Risk: - High error rates in model predictions or classifications after the introduction of the random noise - Greater sensitivity to certain types and degrees of noise such as typographical errors, insertion or deletion of words - Significant change in loss function or accuracy metric - Inconsistency in model outputs for slightly perturbed inputs\nStrengths: - Measures model robustness against noise thereby reflecting real-world scenarios where data may contain errors or inconsistencies. - Easy to implement with adjustable perturbation severity through probability parameter. - Enables identification of model sensitivity to certain noise types, providing grounding for model improvement. - Useful for testing models designed to handle text data.\nLimitations: - The test might not be effective for models that have been designed with a high resistance to noise or models that are inherently designed to handle such perturbations. - Pseudo-randomness may not accurately represent real-world distribution of noise or typographical errors, which could be biased towards certain types of errors or malware injections. - Highly dependent on the probability parameter to introduce noise, with artificial adjusting required to achieve an optimal balance. - Only validates the model’s performance against noise in input data, not its ability to capture complex language structures or semantics. - Does not guarantee the model’s performance in new, unseen, real-world data beyond what is represented by the noise-introduced test data."
  },
  {
    "objectID": "tests/model_validation/ClusterSizeDistribution.html",
    "href": "tests/model_validation/ClusterSizeDistribution.html",
    "title": "ClusterSizeDistribution",
    "section": "",
    "text": "ClusterSizeDistribution\nCompares and visualizes the distribution of cluster sizes in model predictions and actual data for assessing clustering model performance.\nPurpose: The purpose of the ClusterSizeDistribution metric is to assess the performance of clustering models. It does this by comparing the distribution of cluster sizes in the predictions made by the model and the actual data. Observing the cluster distribution helps gain insights into whether the model’s output aligns well with the actual dataset distribution.\nTest Mechanism: The testing mechanism for ClusterSizeDistribution involves first running the clustering model on the training dataset, storing predictions, and comparing these predictions with the actual output. The actual and predicted outputs are then converted into pandas dataframes, which conveniently enables the use of pandas built-in functions to derive cluster size distributions. Two histograms are constructed from this data: one for the actual distribution and one for the predicted distribution. These histograms are then plotted side-by-side for visual comparison.\nSigns of High Risk: * Discrepancies between the actual cluster size distribution and the predicted cluster size distribution may indicate high risk. * An irregular distribution of data across clusters in the predicted outcomes points towards an inaccurate prediction model. * A high number of outlier clusters could indicate that the model has trouble correctly grouping data.\nStrengths: * ClusterSizeDistribution provides a visual and intuitive way to compare the performance of the clustering model against the actual data. * This metric can effectively reveal where the model might be over- or underestimating cluster sizes. * It works well with any clustering models, making it a versatile comparison tool.\nLimitations: * The metric assumes that the actual cluster distribution is optimal, which may not always be the case. * It relies heavily on visual comparison, which might be subjective and may not provide a precise numerical measure of model performance. * The metric might not fully capture other important aspects of clustering such as cluster density, distances between clusters, and the shape of clusters."
  },
  {
    "objectID": "tests/model_validation/TokenDisparity.html",
    "href": "tests/model_validation/TokenDisparity.html",
    "title": "TokenDisparity",
    "section": "",
    "text": "TokenDisparity"
  },
  {
    "objectID": "tests/model_validation/ragas/ContextRecall.html",
    "href": "tests/model_validation/ragas/ContextRecall.html",
    "title": "ContextRecall",
    "section": "",
    "text": "ContextRecall"
  },
  {
    "objectID": "tests/model_validation/ragas/ContextPrecision.html",
    "href": "tests/model_validation/ragas/ContextPrecision.html",
    "title": "ContextPrecision",
    "section": "",
    "text": "ContextPrecision"
  },
  {
    "objectID": "tests/model_validation/ragas/Faithfulness.html",
    "href": "tests/model_validation/ragas/Faithfulness.html",
    "title": "Faithfulness",
    "section": "",
    "text": "Faithfulness"
  },
  {
    "objectID": "tests/model_validation/ragas/AnswerCorrectness.html",
    "href": "tests/model_validation/ragas/AnswerCorrectness.html",
    "title": "AnswerCorrectness",
    "section": "",
    "text": "AnswerCorrectness"
  },
  {
    "objectID": "tests/model_validation/sklearn/RegressionErrors.html",
    "href": "tests/model_validation/sklearn/RegressionErrors.html",
    "title": "RegressionErrors",
    "section": "",
    "text": "RegressionErrors\nPurpose: This metric is used to measure the performance of a regression model. It gauges the model’s accuracy by computing several error metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Bias Deviation (MBD) on both the training and testing dataset.\nTest Mechanism: The test computes each of the aforementioned metrics. MAE calculates the average of the absolute difference between the true value and the predicted value. MSE squares the difference before averaging it. RMSE then takes the square root of the MSE. MAPE evaluates the average of the absolute difference between true and predicted values divided by the true value, expressed as a percentage. Lastly, MBD is a measure of average bias in the prediction. The results are compared between the training dataset and the testing dataset.\nSigns of High Risk: High values for any of the metrics, or particularly different metric outcomes for the training set versus the test set, are signs of high risk. Specifically, high MAE, MSE, RMSE, or MAPE values could indicate poor model performance and overfitting. If MBD is significantly different from zero, it could signify that the model’s predictions are systematically biased.\nStrengths: These metrics collectively provide a comprehensive view of model performance and error distribution. Individually, MAE provides a linear score that could be more interpretable, while MSE gives more weight to larger errors. RMSE is useful because it is in the same unit as the target variable. MAPE expresses error as a percentage, making it a good measure of prediction accuracy. MBD helps to detect systematic bias in predictions.\nLimitations: Each of these metrics has its own limitations. MAE and MSE are sensitive to outliers. While RMSE is good for giving high weight to larger errors, it might too heavily penalize these errors. MAPE might be biased if actual values are near zero, and MBD would not work well if the difference between predictions and actual values changes with the magnitude of the actual values. Overall, these metrics will not capture all model performance nuances, and they should be used with contextual understanding of the problem at hand."
  },
  {
    "objectID": "tests/model_validation/sklearn/RegressionR2Square.html",
    "href": "tests/model_validation/sklearn/RegressionR2Square.html",
    "title": "RegressionR2Square",
    "section": "",
    "text": "RegressionR2Square\nPurpose: The purpose of the RegressionR2Square Metric test is to measure the overall goodness-of-fit of a regression model. Specifically, this Python-based test evaluates the R-squared (R2) and Adjusted R-squared (Adj R2) scores: two statistical measures within regression analysis used to evaluate the strength of the relationship between the model’s predictors and the response variable.\nTest Mechanism: The test deploys the ‘r2_score’ method from the Scikit-learn metrics module, measuring the R2 score on both training and test sets. This score reflects the proportion of the variance in the dependent variable that is predictable from independent variables. The test also considers the Adjusted R2 score, accounting for the number of predictors in the model, to penalize model complexity and thus reduce overfitting. The Adjusted R2 score will be smaller if unnecessary predictors are included in the model.\nSigns of High Risk: Indicators of high risk in this test may include a low R2 or Adjusted R2 score, which would suggest that the model does not explain much variation in the dependent variable. The occurrence of overfitting is also a high-risk sign, evident when the R2 score on the training set is significantly higher than on the test set, indicating that the model is not generalizing well to unseen data.\nStrengths: The R2 score is a widely-used measure in regression analysis, providing a sound general indication of model performance. It is easy to interpret and understand, as it is essentially representing the proportion of the dependent variable’s variance explained by the independent variables. The Adjusted R2 score complements the R2 score well by taking into account the number of predictors in the model, which helps control overfitting.\nLimitations: R2 and Adjusted R2 scores can be sensitive to the inclusion of unnecessary predictors in the model (even though Adjusted R2 is intended to penalize complexity). Their reliability might also lessen in cases of non-linear relationships or when the underlying assumptions of linear regression are violated. Additionally, while they summarize how well the model fits the data, they do not provide insight on whether the correct regression was used, or whether certain key assumptions have been fulfilled."
  },
  {
    "objectID": "tests/model_validation/sklearn/ModelsPerformanceComparison.html",
    "href": "tests/model_validation/sklearn/ModelsPerformanceComparison.html",
    "title": "ModelsPerformanceComparison",
    "section": "",
    "text": "ModelsPerformanceComparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy, precision, recall, and F1 score.\nPurpose: This metric test aims to evaluate and compare the performance of various Machine Learning models using test data. It employs multiple metrics such as accuracy, precision, recall, and the F1 score, among others, to assess model performance and assist in selecting the most effective model for the designated task.\nTest Mechanism: The test employs Scikit-learn’s performance metrics to evaluate each model’s performance for both binary and multiclass classification tasks. To compare performances, the test runs each model against the test dataset, then produces a comprehensive classification report. This report includes metrics such as accuracy, precision, recall, and the F1 score. Based on whether the task at hand is binary or multiclass classification, it calculates metrics all the classes and their weighted averages, macro averages, and per class metrics. The test will be skipped if no models are supplied.\nSigns of High Risk: - Low scores in accuracy, precision, recall, and F1 metrics indicate a potentially high risk. - A low area under the Receiver Operating Characteristic (ROC) curve (roc_auc score) is another possible indicator of high risk. - If the metrics scores are significantly lower than alternative models, this might suggest a high risk of failure.\nStrengths: - The test provides a simple way to compare the performance of multiple models, accommodating both binary and multiclass classification tasks. - It provides a holistic view of model performance through a comprehensive report of key performance metrics. - The inclusion of the ROC AUC score is advantageous, as this robust performance metric can effectively handle class imbalance issues.\nLimitations: - This test may not be suitable for more complex performance evaluations that consider factors such as prediction speed, computational cost, or business-specific constraints. - The test’s reliability depends on the provided test dataset; hence, the selected models’ performance could vary with unseen data or changes in the data distribution. - The ROC AUC score might not be as meaningful or easily interpretable for multilabel/multiclass tasks."
  },
  {
    "objectID": "tests/model_validation/sklearn/HyperParametersTuning.html",
    "href": "tests/model_validation/sklearn/HyperParametersTuning.html",
    "title": "HyperParametersTuning",
    "section": "",
    "text": "HyperParametersTuning\nExerts exhaustive grid search to identify optimal hyperparameters for the model, improving performance.\nPurpose: The “HyperParametersTuning” metric being used here is intended to find the optimal set of hyperparameters for a given model. The test essentially aims to enhance the performance of the model under scrutiny by determining the best configuration of hyperparameters. The parameters that are being optimized are defined by the parameter grid that is passed to the metric.\nTest Mechanism: The HyperParametersTuning test employs a grid search mechanism using the function GridSearchCV from the scikit-learn library. The grid search algorithm is exhaustive: it systematically works through multiple combinations of the parameter tunes, cross-validated to determine which tune gives the best model performance. The chosen model and the parameters grid that are to be passed for tuning are the required inputs. Once the grid search is complete, the test caches and returns the details of the best model and its associated parameters.\nSigns of High Risk: - The test raises a SkipTestError if the param_grid is not supplied. This suggests that there are no specific parameters to optimize, which is a risk in certain model types that rely heavily on parameter tuning. - Poorly chosen scoring metrics that don’t align well with the specific model or problem at hand might also reflect as a potential risk or failure in achieving the best performance.\nStrengths: - The test is a comprehensive exploratory mechanism that figures out the best set of hyperparameters for the supplied model, thereby helping improve its performance. - The implementation of GridSearchCV simplifies and automates the time-consuming task of hyperparameter tuning.\nLimitations: - The grid search algorithm can be computationally expensive, particularly with a large dataset or complex models. This grid search approach can be time-consuming as it tries out all possible combinations within the specified parameter grid. - The suitability of the tuning heavily relies on the quality of the data and it only accepts datasets with numerical or ordered categories. - The functionality assumes that the same set of hyperparameters is optimal for all problem sets, which may not hold true in every scenario. - There is a potential risk of overfitting the model if the training set is not representative of the data the model will be applied to."
  },
  {
    "objectID": "tests/model_validation/sklearn/PopulationStabilityIndex.html",
    "href": "tests/model_validation/sklearn/PopulationStabilityIndex.html",
    "title": "PopulationStabilityIndex",
    "section": "",
    "text": "PopulationStabilityIndex\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model’s predictions across different datasets.\nPurpose: The Population Stability Index (PSI) serves as a quantitative assessment for evaluating the stability of a machine learning model’s output distributions when comparing two different datasets. Typically, these would be a development and a validation dataset or two datasets collected at different periods. The PSI provides a measurable indication of any significant shift in the model’s performance over time or noticeable changes in the characteristics of the population the model is making predictions for.\nTest Mechanism: The implementation of the PSI in this script involves calculating the PSI for each feature between the training and test datasets. Data from both datasets is sorted and placed into either a predetermined number of bins or quantiles. The boundaries for these bins are initially determined based on the distribution of the training data. The contents of each bin are calculated and their respective proportions determined. Subsequently, the PSI is derived for each bin through a logarithmic transformation of the ratio of the proportions of data for each feature in the training and test datasets. The PSI, along with the proportions of data in each bin for both datasets, are displayed in a summary table, a grouped bar chart, and a scatter plot.\nSigns of High Risk:\n\nA high PSI value is a clear indicator of high risk. Such a value suggests a significant shift in the model predictions or severe changes in the characteristics of the underlying population.\nThis ultimately suggests that the model may not be performing as well as expected and that it may be less reliable for making future predictions.\n\nStrengths:\n\nThe PSI provides a quantitative measure of the stability of a model over time or across different samples, making it an invaluable tool for evaluating changes in a model’s performance.\nIt allows for direct comparisons across different features based on the PSI value.\nThe calculation and interpretation of the PSI are straightforward, facilitating its use in model risk management.\nThe use of visual aids such as tables and charts further simplifies the comprehension and interpretation of the PSI.\n\nLimitations:\n\nThe PSI test does not account for the interdependence between features: features that are dependent on one another may show similar shifts in their distributions, which in turn may result in similar PSI values.\nThe PSI test does not inherently provide insights into why there are differences in distributions or why the PSI values may have changed.\nThe test may not handle features with significant outliers adequately.\nAdditionally, the PSI test is performed on model predictions, not on the underlying data distributions which can lead to misinterpretations. Any changes in PSI could be due to shifts in the model (model drift), changes in the relationships between features and the target variable (concept drift), or both. However, distinguishing between these causes is non-trivial."
  },
  {
    "objectID": "tests/model_validation/sklearn/TrainingTestDegradation.html",
    "href": "tests/model_validation/sklearn/TrainingTestDegradation.html",
    "title": "TrainingTestDegradation",
    "section": "",
    "text": "TrainingTestDegradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold.\nPurpose: The ‘TrainingTestDegradation’ class serves as a test to verify that the degradation in performance between the training and test datasets does not exceed a predefined threshold. This test serves as a measure to check the model’s ability to generalize from its training data to unseen test data. It assesses key classification metric scores such as accuracy, precision, recall and f1 score, to verify the model’s robustness and reliability.\nTest Mechanism: The code applies several predefined metrics including accuracy, precision, recall and f1 scores to the model’s predictions for both the training and test datasets. It calculates the degradation as the difference between the training score and test score divided by the training score. The test is considered successful if the degradation for each metric is less than the preset maximum threshold of 10%. The results are summarized in a table showing each metric’s train score, test score, degradation percentage, and pass/fail status.\nSigns of High Risk: - A degradation percentage that exceeds the maximum allowed threshold of 10% for any of the evaluated metrics. - A high difference or gap between the metric scores on the training and the test datasets. - The ‘Pass/Fail’ column displaying ‘Fail’ for any of the evaluated metrics.\nStrengths: - This test provides a quantitative measure of the model’s ability to generalize to unseen data, which is key for predicting its practical real-world performance. - By evaluating multiple metrics, it takes into account different facets of model performance and enables a more holistic evaluation. - The use of a variable predefined threshold allows the flexibility to adjust the acceptability criteria for different scenarios.\nLimitations: - The test compares raw performance on training and test data, but does not factor in the nature of the data. Areas with less representation in the training set, for instance, might still perform poorly on unseen data. - It requires good coverage and balance in the test and training datasets to produce reliable results, which may not always be available. - The test is currently only designed for classification tasks."
  },
  {
    "objectID": "tests/model_validation/sklearn/HomogeneityScore.html",
    "href": "tests/model_validation/sklearn/HomogeneityScore.html",
    "title": "HomogeneityScore",
    "section": "",
    "text": "HomogeneityScore\nAssesses clustering homogeneity by comparing true and predicted labels, scoring from 0 (heterogeneous) to 1 (homogeneous).\nPurpose: The Homogeneity Score encapsulated in this performance test is used to measure the homogeneity of the clusters formed by a machine learning model. In simple terms, a clustering result satisfies homogeneity if all of its clusters contain only points which are members of a single class.\nTest Mechanism: This test uses the homogeneity_score function from the sklearn.metrics library to compare the ground truth class labels of the training and testing sets with the labels predicted by the given model. The returned score is a metric of the clustering accuracy, and ranges from 0.0 to 1.0, with 1.0 denoting the highest possible degree of homogeneity.\nSigns of High Risk: - A score close to 0: This denotes that clusters are highly heterogenous and points within the same cluster might not belong to the same class. - A significantly lower score for testing data compared to the score for training data: This can indicate overfitting, where the model has learned to perfectly match the training data but fails to perform well on unseen data.\nStrengths: - It provides a simple quantitative measure of the degree to which clusters contain points from only one class. - Useful for validating clustering solutions where the ground truth - class membership of points - is known. - It’s agnostic to the absolute labels, and cares only that the points within the same cluster have the same class label.\nLimitations: - The Homogeneity Score is not useful for clustering solutions where the ground truth labels are not known. - It doesn’t work well with differently sized clusters since it gives predominance to larger clusters. - The score does not address the actual number of clusters formed, or the evenness of cluster sizes. It only checks the homogeneity within the given clusters created by the model."
  },
  {
    "objectID": "tests/model_validation/sklearn/SHAPGlobalImportance.html",
    "href": "tests/model_validation/sklearn/SHAPGlobalImportance.html",
    "title": "SHAPGlobalImportance",
    "section": "",
    "text": "SHAPGlobalImportance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification.\nPurpose: The SHAP (SHapley Additive exPlanations) Global Importance metric aims to elucidate model outcomes by attributing them to the contributing features. It assigns a quantifiable global importance to each feature via their respective absolute Shapley values, thereby making it suitable for tasks like classification (both binary and multiclass). This metric forms an essential part of model risk management.\nTest Mechanism: The exam begins with the selection of a suitable explainer which aligns with the model’s type. For tree-based models like XGBClassifier, RandomForestClassifier, CatBoostClassifier, TreeExplainer is used whereas for linear models like LogisticRegression, XGBRegressor, LinearRegression, it is the LinearExplainer. Once the explainer calculates the Shapley values, these values are visualized using two specific graphical representations:\n\nMean Importance Plot: This graph portrays the significance of individual features based on their absolute Shapley values. It calculates the average of these absolute Shapley values across all instances to highlight the global importance of features.\nSummary Plot: This visual tool combines the feature importance with their effects. Every dot on this chart represents a Shapley value for a certain feature in a specific case. The vertical axis is denoted by the feature whereas the horizontal one corresponds to the Shapley value. A color gradient indicates the value of the feature, gradually changing from low to high. Features are systematically organized in accordance with their importance. These plots are generated by the function _generate_shap_plot().\n\nSigns of High Risk:\n\nOveremphasis on certain features in SHAP importance plots, thus hinting at the possibility of model overfitting\nAnomalies such as unexpected or illogical features showing high importance, which might suggest that the model’s decisions are rooted in incorrect or undesirable reasoning\nA SHAP summary plot filled with high variability or scattered data points, indicating a cause for concern\n\nStrengths:\n\nSHAP does more than just illustrating global feature significance, it offers a detailed perspective on how different features shape the model’s decision-making logic for each instance.\nIt provides clear insights into model behavior.\n\nLimitations:\n\nHigh-dimensional data can convolute interpretations.\nAssociating importance with tangible real-world impact still involves a certain degree of subjectivity."
  },
  {
    "objectID": "tests/model_validation/sklearn/RobustnessDiagnosis.html",
    "href": "tests/model_validation/sklearn/RobustnessDiagnosis.html",
    "title": "RobustnessDiagnosis",
    "section": "",
    "text": "RobustnessDiagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring performance.\nPurpose:\nThe purpose of this test code is to evaluate the robustness of a machine learning model. Robustness refers to a model’s ability to maintain a high level of performance in the face of perturbations or changes—particularly noise—added to its input data. This test is designed to help gauge how well the model can handle potential real-world scenarios where the input data might be incomplete or corrupted.\nTest Mechanism:\nThis test is conducted by adding Gaussian noise, proportional to a particular standard deviation scale, to numeric input features of both the training and testing datasets. The model performance in the face of these perturbed features is then evaluated using metrics (default: ‘accuracy’). This process is iterated over a range of scale factors. The resulting accuracy trend against the amount of noise introduced is illustrated with a line chart. A predetermined threshold determines what level of accuracy decay due to perturbation is considered acceptable.\nSigns of High Risk: - Substantial decreases in accuracy when noise is introduced to feature inputs. - The decay in accuracy surpasses the configured threshold, indicating that the model is not robust against input noise. - Instances where one or more elements provided in the features list don’t match with the training dataset’s numerical feature columns.\nStrengths: - Provides an empirical measure of the model’s performance in tackling noise or data perturbations, revealing insights into the model’s stability. - Offers flexibility with the ability to choose specific features to perturb and control the level of noise applied. - Detailed results visualization helps in interpreting the outcome of robustness testing.\nLimitations: - Only numerical features are perturbed, leaving out non-numerical features, which can lead to an incomplete analysis of robustness. - The default metric used is accuracy, which might not always give the best measure of a model’s success, particularly for imbalanced datasets. - The test is contingent on the assumption that the added Gaussian noise sufficiently represents potential data corruption or incompleteness in real-world scenarios. - There might be a requirement to fine-tune the set decay threshold for accuracy with the help of domain knowledge or specific project requisites. - The robustness test might not deliver the expected results for datasets with a text column."
  },
  {
    "objectID": "tests/model_validation/sklearn/AdjustedMutualInformation.html",
    "href": "tests/model_validation/sklearn/AdjustedMutualInformation.html",
    "title": "AdjustedMutualInformation",
    "section": "",
    "text": "AdjustedMutualInformation\nEvaluates clustering model performance by measuring mutual information between true and predicted labels, adjusting for chance.\n1. Purpose: The purpose of this metric (Adjusted Mutual Information) is to evaluate the performance of a machine learning model, more specifically, a clustering model. It measures the mutual information between the true labels and the ones predicted by the model, adjusting for chance.\n2. Test Mechanism: The Adjusted Mutual Information (AMI) uses sklearn’s adjusted_mutual_info_score function. This function calculates the mutual information between the true labels and the ones predicted while correcting for the chance correlation expected due to random label assignments. This test requires the model, the training dataset, and the test dataset as inputs.\n3. Signs of High Risk: - Low Adjusted Mutual Information Score: This score ranges between 0 and 1. A low score (closer to 0) can indicate poor model performance as the predicted labels do not align well with the true labels. - In case of high dimensional data, if the algorithm shows high scores, this could also be a potential risk as AMI may not perform reliably.\n4. Strengths: - The AMI metric takes into account the randomness of the predicted labels, which makes it more robust than the simple Mutual Information. - The scale of AMI is not dependent on the sizes of the clustering, allowing for comparability between different datasets or models. - Good for comparing the output of clustering algorithms where the number of clusters is not known a priori.\n5. Limitations: - Adjusted Mutual Information does not take into account the continuous nature of some data. As a result, it may not be the best choice for regression or other continuous types of tasks. - AMI has the drawback of being biased towards clusterings with a higher number of clusters. - In comparison to other metrics, AMI can be slower to compute. - The interpretability of the score can be complex as it depends on the understanding of information theory concepts."
  },
  {
    "objectID": "tests/model_validation/sklearn/VMeasure.html",
    "href": "tests/model_validation/sklearn/VMeasure.html",
    "title": "VMeasure",
    "section": "",
    "text": "VMeasure\nEvaluates homogeneity and completeness of a clustering model using the V Measure Score.\n1. Purpose: The purpose of this metric, V Measure Score (V Score), is to evaluate the performance of a clustering model. It measures the homogeneity and completeness of a set of cluster labels, where homogeneity refers to each cluster containing only members of a single class and completeness meaning all members of a given class are assigned to the same cluster.\n2. Test Mechanism: ClusterVMeasure is a class that inherits from another class, ClusterPerformance. It uses the v_measure_score function from the sklearn module’s metrics package. The required inputs to perform this metric are the model, train dataset, and test dataset. The test is appropriate for models tasked with clustering.\n3. Signs of High Risk:\n\nLow V Measure Score: A low V Measure Score indicates that the clustering model has poor homogeneity or completeness, or both. This might signal that the model is failing to correctly cluster the data.\n\n4. Strengths:\n\nThe V Measure Score is a harmonic mean between homogeneity and completeness. This ensures that both attributes are taken into account when evaluating the model, providing an overall measure of its cluster validity.\nThe metric does not require knowledge of the ground truth classes when measuring homogeneity and completeness, making it applicable in instances where such information is unavailable.\n\n5. Limitations:\n\nThe V Score can be influenced by the number of clusters, which means that it might not always reflect the quality of the clustering. Partitioning the data into many small clusters could lead to high homogeneity but low completeness, leading to a low V Score even if the clustering might be useful.\nIt assumes equal importance of homogeneity and completeness. In some applications, one may be more important than the other. The V Score does not provide flexibility in assigning different weights to homogeneity and completeness."
  },
  {
    "objectID": "tests/model_validation/sklearn/PermutationFeatureImportance.html",
    "href": "tests/model_validation/sklearn/PermutationFeatureImportance.html",
    "title": "PermutationFeatureImportance",
    "section": "",
    "text": "PermutationFeatureImportance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature values are randomly rearranged.\nPurpose: The purpose of the Permutation Feature Importance (PFI) metric is to assess the importance of each feature used by the Machine Learning model. The significance is measured by evaluating the decrease in the model’s performance when the feature’s values are randomly arranged.\nTest Mechanism: PFI is calculated via the permutation_importance method from the sklearn.inspection module. This method shuffles the columns of the feature dataset and measures the impact on the model’s performance. A significant decrease in performance after permutating a feature’s values deems the feature as important. On the other hand, if performance remains the same, the feature is likely not important. The output of the PFI metric is a figure illustrating the importance of each feature.\nSigns of High Risk: - The model heavily relies on a feature with highly variable or easily permutable values, indicating instability. - A feature, deemed unimportant by the model but based on domain knowledge should have a significant effect on the outcome, is not influencing the model’s predictions.\nStrengths: - PFI provides insights into the importance of different features and may reveal underlying data structure. - It can indicate overfitting if a particular feature or set of features overly impacts the model’s predictions. - The metric is model-agnostic and can be used with any classifier that provides a measure of prediction accuracy before and after feature permutation.\nLimitations: - The feature importance calculated does not imply causality, it only presents the amount of information that a feature provides for the prediction task. - The metric does not account for interactions between features. If features are correlated, the permutation importance may allocate importance to one and not the other. - PFI cannot interact with certain libraries like statsmodels, pytorch, catboost, etc, thus limiting its applicability."
  },
  {
    "objectID": "tests/model_validation/sklearn/OverfitDiagnosis.html",
    "href": "tests/model_validation/sklearn/OverfitDiagnosis.html",
    "title": "OverfitDiagnosis",
    "section": "",
    "text": "OverfitDiagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets.\nPurpose: The OverfitDiagnosis test is devised to detect areas within a Machine Learning model that might be prone to overfitting. It achieves this by comparing the model’s performance on both the training and testing datasets. These datasets are broken down into distinct sections defined by a Feature Space. Areas, where the model underperforms by displaying high residual values or a significant amount of overfitting, are highlighted, prompting actions for mitigation using regularization techniques such as L1 or L2 regularization, Dropout, Early Stopping or data augmentation.\nTest Mechanism: The metric conducts the test by executing the method ‘run’ on the default parameters and metrics with ‘accuracy’ as the specified metric. It segments the feature space by binning crucial feature columns from both the training and testing datasets. Then, the method computes the prediction results for each defined region. Subsequently, the prediction’s efficacy is evaluated, i.e., the model’s performance gap (defined as the discrepancy between the actual and the model’s predictions) for both datasets is calculated and compared with a preset cut-off value for the overfitting condition. A test failure presents an overfit model, whereas a pass signifies a fit model. Meanwhile, the function also prepares figures further illustrating the regions with overfitting.\nSigns of High Risk: Indicators of a high-risk model are: - A high ‘gap’ value indicating discrepancies in the training and testing data accuracy signals an overfit model. - Multiple or vast overfitting zones within the feature space suggest overcomplication of the model.\nStrengths: - Presents a visual perspective by plotting regions with overfit issues, simplifying understanding of the model structure. - Permits a feature-focused assessment, which promotes specific, targeted modifications to the model. - Caters to modifications of the testing parameters such as ‘cut_off_percentage’ and ‘features_column’ enabling a personalized analysis. - Handles both numerical and categorical features.\nLimitations: - Does not currently support regression tasks and is limited to classification tasks only. - Ineffectual for text-based features, which in turn restricts its usage for Natural Language Processing models. - Primarily depends on the bins setting, responsible for segmenting the feature space. Different bin configurations might yield varying results. - Utilization of a fixed cut-off percentage for making overfitting decisions, set arbitrarily, leading to a possible risk of inaccuracy. - Limitation of performance metrics to accuracy alone might prove inadequate for detailed examination, especially for imbalanced datasets."
  },
  {
    "objectID": "tests/model_validation/sklearn/MinimumROCAUCScore.html",
    "href": "tests/model_validation/sklearn/MinimumROCAUCScore.html",
    "title": "MinimumROCAUCScore",
    "section": "",
    "text": "MinimumROCAUCScore\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold.\nPurpose: This test metric, Minimum ROC AUC Score, is used to determine the model’s performance by ensuring that the Receiver Operating Characteristic Area Under the Curve (ROC AUC) score on the validation dataset meets or exceeds a predefined threshold. The ROC AUC score is an indicator of how well the model is capable of distinguishing between different classes, making it a crucial measure in binary and multiclass classification tasks.\nTest Mechanism: This test implementation calculates the multiclass ROC AUC score on the true target values and the model’s prediction. The test converts the multi-class target variables into binary format using LabelBinarizer before computing the score. If this ROC AUC score is higher than the predefined threshold (defaulted to 0.5), the test passes; otherwise, it fails. The results, including the ROC AUC score, the threshold, and whether the test passed or failed, are then stored in a ThresholdTestResult object.\nSigns of High Risk: - A high risk or failure in the model’s performance as related to this metric would be represented by a low ROC AUC score, specifically any score lower than the predefined minimum threshold. This suggests that the model is struggling to distinguish between different classes effectively.\nStrengths: - The test considers both the true positive rate and false positive rate, providing a comprehensive performance measure. - ROC AUC score is threshold-independent meaning it measures the model’s quality across various classification thresholds. - Works robustly with binary as well as multi-class classification problems.\nLimitations: - ROC AUC may not be useful if the class distribution is highly imbalanced; it could perform well in terms of AUC but still fail to predict the minority class. - The test does not provide insight into what specific aspects of the model are causing poor performance if the ROC AUC score is unsatisfactory. - The use of macro average for multiclass ROC AUC score implies equal weightage to each class, which might not be appropriate if the classes are imbalanced."
  },
  {
    "objectID": "tests/model_validation/sklearn/PrecisionRecallCurve.html",
    "href": "tests/model_validation/sklearn/PrecisionRecallCurve.html",
    "title": "PrecisionRecallCurve",
    "section": "",
    "text": "PrecisionRecallCurve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve.\nPurpose: The Precision Recall Curve metric is intended to evaluate the trade-off between precision and recall in classification models, particularly binary classification models. It assesses the model’s capacity to produce accurate results (high precision), as well as its ability to capture a majority of all positive instances (high recall).\nTest Mechanism: The test extracts ground truth labels and prediction probabilities from the model’s test dataset. It applies the precision_recall_curve method from the sklearn metrics module to these extracted labels and predictions, which computes a precision-recall pair for each possible threshold. This calculation results in an array of precision and recall scores that can be plotted against each other to form the Precision-Recall Curve. This curve is then visually represented by using Plotly’s scatter plot.\nSigns of High Risk: * A lower area under the Precision-Recall Curve signifies high risk. * This corresponds to a model yielding a high amount of false positives (low precision) and/or false negatives (low recall). * If the curve is closer to the bottom left of the plot, rather than being closer to the top right corner, it can be a sign of high risk.\nStrengths: * This metric aptly represents the balance between precision (minimizing false positives) and recall (minimizing false negatives), which is especially critical in scenarios where both values are significant. * Through the graphic representation, it enables an intuitive understanding of the model’s performance across different threshold levels.\nLimitations: * This metric is only applicable to binary classification models - it raises errors for multiclass classification models or Foundation models. * It may not fully represent the overall accuracy of the model if the cost of false positives and false negatives are extremely different, or if the dataset is heavily imbalanced."
  },
  {
    "objectID": "tests/model_validation/FeaturesAUC.html",
    "href": "tests/model_validation/FeaturesAUC.html",
    "title": "FeaturesAUC",
    "section": "",
    "text": "FeaturesAUC\nEvaluates the discriminatory power of each individual feature within a binary classification model by calculating the Area Under the Curve (AUC) for each feature separately.\nPurpose: The central objective of this metric is to quantify how well each feature on its own can differentiate between the two classes in a binary classification problem. It serves as a univariate analysis tool that can help in pre-modeling feature selection or post-modeling interpretation.\nTest Mechanism: For each feature, the metric treats the feature values as raw scores to compute the AUC against the actual binary outcomes. It provides an AUC value for each feature, offering a simple yet powerful indication of each feature’s univariate classification strength.\nSigns of High Risk: - A feature with a low AUC score may not be contributing significantly to the differentiation between the two classes, which could be a concern if it is expected to be predictive. - Conversely, a surprisingly high AUC for a feature not believed to be informative may suggest data leakage or other issues with the data.\nStrengths: - By isolating each feature, it highlights the individual contribution of features to the classification task without the influence of other variables. - Useful for both initial feature evaluation and for providing insights into the model’s reliance on individual features after model training.\nLimitations: - Does not reflect the combined effects of features or any interaction between them, which can be critical in certain models. - The AUC values are calculated without considering the model’s use of the features, which could lead to different interpretations of feature importance when considering the model holistically. - This metric is applicable only to binary classification tasks and cannot be directly extended to multiclass classification or regression without modifications."
  },
  {
    "objectID": "tests/model_validation/RegressionResidualsPlot.html",
    "href": "tests/model_validation/RegressionResidualsPlot.html",
    "title": "RegressionResidualsPlot",
    "section": "",
    "text": "RegressionResidualsPlot\nEvaluates regression model performance using residual distribution and actual vs. predicted plots.\nPurpose: The RegressionResidualsPlot metric aims to evaluate the performance of regression models. By generating and analyzing two plots – a distribution of residuals and a scatter plot of actual versus predicted values – this tool helps to visually appraise how well the model predicts and the nature of errors it makes.\nTest Mechanism: The process begins by extracting the true output values (y_true) and the model’s predicted values (y_pred). Residuals are computed by subtracting predicted from true values. These residuals are then visualized using a histogram to display their distribution. Additionally, a scatter plot is derived to compare true values against predicted values, together with a “Perfect Fit” line, which represents an ideal match (predicted values equal actual values), facilitating the assessment of the model’s predictive accuracy.\nSigns of High Risk: - Residuals showing a non-normal distribution, especially those with frequent extreme values. - Significant deviations of predicted values from actual values in the scatter plot. - Sparse density of data points near the “Perfect Fit” line in the scatter plot, indicating poor prediction accuracy. - Visible patterns or trends in the residuals plot, suggesting the model’s failure to capture the underlying data structure adequately.\nStrengths: - Provides a direct, visually intuitive assessment of a regression model’s accuracy and handling of data. - Visual plots can highlight issues of underfitting or overfitting. - Can reveal systematic deviations or trends that purely numerical metrics might miss. - Applicable across various regression model types.\nLimitations: - Relies on visual interpretation, which can be subjective and less precise than numerical evaluations. - May be difficult to interpret in cases with multi-dimensional outputs due to the plots’ two-dimensional nature. - Overlapping data points in the residuals plot can complicate interpretation efforts. - Does not summarize model performance into a single quantifiable metric, which might be needed for comparative or summary analyses."
  },
  {
    "objectID": "tests/model_validation/RegardScore.html",
    "href": "tests/model_validation/RegardScore.html",
    "title": "RegardScore",
    "section": "",
    "text": "RegardScore"
  },
  {
    "objectID": "tests/model_validation/ContextualRecall.html",
    "href": "tests/model_validation/ContextualRecall.html",
    "title": "ContextualRecall",
    "section": "",
    "text": "ContextualRecall"
  },
  {
    "objectID": "tests/model_validation/statsmodels/Lilliefors.html",
    "href": "tests/model_validation/statsmodels/Lilliefors.html",
    "title": "Lilliefors",
    "section": "",
    "text": "Lilliefors\nAssesses the normality of feature distributions in an ML model’s training dataset using the Lilliefors test.\nPurpose: The purpose of this metric is to utilize the Lilliefors test, named in honor of the Swedish statistician Hubert Lilliefors, in order to assess whether the features of the machine learning model’s training dataset conform to a normal distribution. This is done because the assumption of normal distribution plays a vital role in numerous statistical procedures as well as numerous machine learning models. Should the features fail to follow a normal distribution, some model types may not operate at optimal efficiency. This can potentially lead to inaccurate predictions.\nTest Mechanism: The application of this test happens across all feature columns within the training dataset. For each feature, the Lilliefors test returns a test statistic and p-value. The test statistic quantifies how far the feature’s distribution is from an ideal normal distribution, whereas the p-value aids in determining the statistical relevance of this deviation. The final results are stored within a dictionary, the keys of which correspond to the name of the feature column, and the values being another dictionary which houses the test statistic and p-value.\nSigns of High Risk:\n\nIf the p-value corresponding to a specific feature sinks below a pre-established significance level, generally set at 0.05, then it can be deduced that the distribution of that feature significantly deviates from a normal distribution. This can present a high risk for models that assume normality, as these models may perform inaccurately or inefficiently in the presence of such a feature.\n\nStrengths:\n\nOne advantage of the Lilliefors test is its utility irrespective of whether the mean and variance of the normal distribution are known in advance. This makes it a more robust option in real-world situations where these values might not be known.\nSecond, the test has the ability to screen every feature column, offering a holistic view of the dataset.\n\nLimitations:\n\nDespite the practical applications of the Lilliefors test in validating normality, it does come with some limitations.\nFirstly, it is only capable of testing unidimensional data, thus rendering it ineffective for datasets with interactions between features or multi-dimensional phenomena.\nAdditionally, the test might not be as sensitive as some other tests (like the Anderson-Darling test) in detecting deviations from a normal distribution.\nLastly, like any other statistical test, Lilliefors test may also produce false positives or negatives. Hence, banking solely on this test, without considering other characteristics of the data, may give rise to risks."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelInsampleComparison.html",
    "href": "tests/model_validation/statsmodels/RegressionModelInsampleComparison.html",
    "title": "RegressionModelInsampleComparison",
    "section": "",
    "text": "RegressionModelInsampleComparison\nEvaluates and compares in-sample performance of multiple regression models using R-Squared, Adjusted R-Squared, MSE, and RMSE.\nPurpose: The RegressionModelInsampleComparison test metric is utilized to evaluate and compare the performance of multiple regression models trained on the same dataset. Key performance indicators for this comparison include statistics related to the goodness of fit - R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\nTest Mechanism: The methodology behind this test is as follows - - Firstly, a verification that the list of models to be tested is indeed not empty occurs. - Once confirmed, the In-Sample performance of the models is calculated by a private function, _in_sample_performance_ols, that executes the following steps: - Iterates through each model in the supplied list. - For each model, the function extracts the features (X) and the target (y_true) from the training dataset and computes the predicted target values (y_pred). - The performance metrics for the model are calculated using formulas for R-Squared, Adjusted R-Squared, MSE, and RMSE. - The results, including the computed metrics, variables of the model, and the model’s identifier, are stored in a dictionary that is appended to a list. - The collected results are finally returned as a pandas dataframe.\nSigns of High Risk: - Significantly low values for R-Squared or Adjusted R-Squared. - Significantly high values for MSE and RMSE. Please note that what constitutes as “low” or “high” will vary based on the specific context or domain in which the model is being utilized.\nStrengths: - Enables comparison of in-sample performance across different models on the same dataset, providing insights into which model fits the data the best. - Utilizes multiple evaluation methods (R-Squared, Adjusted R-Squared, MSE, RMSE), offering a comprehensive review of a model’s performance.\nLimitations: - The test measures only in-sample performance, i.e., how well a model fits the data it was trained on. However, it does not give any information on the performance of the model on new, unseen, or out-of-sample data. - Higher in-sample performance might be a result of overfitting, where the model is just memorizing the training data. This test is sensitive to such cases. - The test does not consider additional key factors such as the temporal dynamics of the data, that is, the pattern of changes in data over time. - The test does not provide an automated mechanism to determine if the reported metrics are within acceptable ranges, necessitating human judgment."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelSummary.html",
    "href": "tests/model_validation/statsmodels/RegressionModelSummary.html",
    "title": "RegressionModelSummary",
    "section": "",
    "text": "RegressionModelSummary\nEvaluates regression model performance using metrics including R-Squared, Adjusted R-Squared, MSE, and RMSE.\nPurpose: This metric test evaluates the performance of regression models by measuring their predictive ability with regards to dependent variables given changes in the independent variables. Its measurement tools include conventional regression metrics such as R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\nTest Mechanism: This test employs the ‘train_ds’ attribute of the model to gather and analyze the training data. Initially, it fetches the independent variables and uses the model to make predictions on these given features. Subsequently, it calculates several standard regression performance metrics including R-Square, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE), which quantify the approximation of the predicted responses to the actual responses.\nSigns of High Risk: - Low R-Squared and Adjusted R-Squared values. A poor fit between the model predictions and the true responses is indicated by low values of these metrics, suggesting the model explains a small fraction of the variance in the target variable. - High MSE and RMSE values represent a high prediction error and point to poor model performance.\nStrengths: - Offers an extensive evaluation of regression models by combining four key measures of model accuracy and fit. - Provides a comprehensive view of the model’s performance. - Both the R-Squared and Adjusted R-Squared measures are readily interpretable. They represent the proportion of total variation in the dependent variable that can be explained by the independent variables.\nLimitations: - Applicable exclusively to regression models. It is not suited for evaluating binary classification models or time series models, thus limiting its scope. - Although RMSE and MSE are sound measures of prediction error, they might be sensitive to outliers, potentially leading to an overestimation of the model’s prediction error. - A high R-squared or adjusted R-squared may not necessarily indicate a good model, especially in cases where the model is possibly overfitting the data."
  },
  {
    "objectID": "tests/model_validation/statsmodels/ADFTest.html",
    "href": "tests/model_validation/statsmodels/ADFTest.html",
    "title": "ADFTest",
    "section": "",
    "text": "ADFTest\nAssesses the stationarity of time series data using the Augmented Dickey-Fuller (ADF) test.\nPurpose: The Augmented Dickey-Fuller (ADF) metric test is designed to evaluate the presence of a unit root in a time series. This essentially translates to assessing the stationarity of a time series dataset. This is vital in time series analysis, regression tasks, and forecasting, as these often need the data to be stationary.\nTest Mechanism: This test application utilizes the “adfuller” function from Python’s “statsmodels” library. It applies this function to each column of the training dataset, subsequently calculating the ADF statistic, p-value, the number of lags used, and the number of observations in the sample for each column. If a column’s p-value is lower than the predetermined threshold (usually 0.05), the series is considered stationary, and the test is deemed passed for that column.\nSigns of High Risk: - A p-value that surpasses the threshold value indicates a high risk or potential model performance issue. - A high p-value suggests that the null hypothesis (of a unit root being present) cannot be rejected. This in turn suggests that the series is non-stationary which could potentially yield unreliable and falsified results for the model’s performance and forecast.\nStrengths: - Archetypal Test for Stationarity: The ADF test is a comprehensive approach towards testing the stationarity of time series data. Such testing is vital for many machine learning and statistical models. - Detailed Output: The function generates detailed output, including the number of lags used and the number of observations, which adds to understanding a series’ behaviour.\nLimitations: - Dependence on Threshold: The result of this test freights heavily on the threshold chosen. Hence, an imprudent threshold value might lead to false acceptance or rejection of the null hypothesis. - Not Effective for Trending Data: The test suffers when it operates under the assumption that the data does not encapsulate any deterministic trend. In the presence of such a trend, it might falsely identify a series as non-stationary. - Potential for False Positives: The ADF test especially in the case of larger datasets, tends to reject the null hypothesis, escalating the chances of false positives."
  },
  {
    "objectID": "tests/model_validation/statsmodels/DFGLSArch.html",
    "href": "tests/model_validation/statsmodels/DFGLSArch.html",
    "title": "DFGLSArch",
    "section": "",
    "text": "DFGLSArch\nExecutes Dickey-Fuller GLS metric to determine order of integration and check stationarity in time series data.\nPurpose: The Dickey-Fuller GLS (DFGLS) Arch metric is utilized to determine the order of integration in time series data. For machine learning models dealing with time series and forecasting, this metric evaluates the existence of a unit root, thereby checking whether a time series is non-stationary. This analysis is a crucial initial step when dealing with time series data.\nTest Mechanism: This code implements the Dickey-Fuller GLS unit root test on each attribute of the dataset. This process involves iterating through every column of the dataset and applying the DFGLS test to assess the presence of a unit root. The resulting information, including the test statistic (‘stat’), the p-value (‘pvalue’), the quantity of lagged differences utilized in the regression (‘usedlag’), and the number of observations (‘nobs’), is subsequently stored.\nSigns of High Risk: - A high p-value for the DFGLS test represents a high risk. Specifically, a p-value above a typical threshold of 0.05 suggests that the time series data is quite likely to be non-stationary, thus presenting a high risk for generating unreliable forecasts.\nStrengths: - The Dickey-Fuller GLS test is a potent tool for checking the stationarity of time series data. - It helps to verify the assumptions of the models before the actual construction of the machine learning models proceeds. - The results produced by this metric offer a clear insight into whether the data is appropriate for specific machine learning models, especially those demanding the stationarity of time series data.\nLimitations: - Despite its benefits, the DFGLS test does present some drawbacks. It can potentially lead to inaccurate conclusions if the time series data incorporates a structural break. - If the time series tends to follow a trend while still being stationary, the test might misinterpret it, necessitating further detrending. - The test also presents challenges when dealing with shorter time series data or volatile data, not producing reliable results in these cases."
  },
  {
    "objectID": "tests/model_validation/statsmodels/DurbinWatsonTest.html",
    "href": "tests/model_validation/statsmodels/DurbinWatsonTest.html",
    "title": "DurbinWatsonTest",
    "section": "",
    "text": "DurbinWatsonTest\nAssesses autocorrelation in time series data features using the Durbin-Watson statistic.\nPurpose: The Durbin-Watson Test metric detects autocorrelation in time series data (where a set of data values influences their predecessors). Autocorrelation is a crucial factor for regression tasks as these often assume the independence of residuals. A model with significant autocorrelation may give unreliable predictions.\nTest Mechanism: Utilizing the durbin_watson function in the statsmodels Python library, the Durbin-Watson (DW) Test metric generates a statistical value for each feature of the training dataset. The function is looped over all columns of the dataset, calculating and caching the DW value for each column for further analysis. A DW metric value nearing 2 indicates no autocorrelation. Conversely, values approaching 0 suggest positive autocorrelation, and those leaning towards 4 imply negative autocorrelation.\nSigns of High Risk: - If a feature’s DW value significantly deviates from 2, it could signal a high risk due to potential autocorrelation issues in the dataset. - A value closer to ‘0’ could imply positive autocorrelation, while a value nearer to ‘4’ could point to negative autocorrelation, both leading to potentially unreliable prediction models.\nStrengths: - The metric specializes in identifying autocorrelation in prediction model residuals. - Autocorrelation detection assists in diagnosing violation of various modeling technique assumptions, particularly in regression analysis and time-series data modeling.\nLimitations: - The Durbin-Watson Test mainly detects linear autocorrelation and could overlook other types of relationships. - The metric is highly sensitive to data points order. Shuffling the order could lead to notably different results. - The test only checks for first-order autocorrelation (between a variable and its immediate predecessor) and fails to detect higher order autocorrelation."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelForecastPlot.html",
    "href": "tests/model_validation/statsmodels/RegressionModelForecastPlot.html",
    "title": "RegressionModelForecastPlot",
    "section": "",
    "text": "RegressionModelForecastPlot\nGenerates plots to visually compare the forecasted outcomes of one or more regression models against actual observed values over a specified date range.\nPurpose: The “regression_forecast_plot” is intended to visually depict the performance of one or more regression models by comparing the model’s forecasted outcomes against actual observed values within a specified date range. This metric is especially useful in time-series models or any model where the outcome changes over time, allowing direct comparison of predicted vs actual values.\nTest Mechanism: This test generates a plot for each fitted model in the list. The x-axis represents the date ranging from the specified “start_date” to the “end_date”, while the y-axis shows the value of the outcome variable. Two lines are plotted: one representing the forecasted values and the other representing the observed values. The “start_date” and “end_date” can be parameters of this test; if these parameters are not provided, they are set to the minimum and maximum date available in the dataset. The test verifies that the provided date range is within the limits of the available data.\nSigns of High Risk:\n\nHigh risk or failure signs could be deduced visually from the plots if the forecasted line significantly deviates from the observed line, indicating the model’s predicted values are not matching actual outcomes.\nA model that struggles to handle the edge conditions like maximum and minimum data points could also be considered a sign of risk.\n\nStrengths:\n\nVisualization: The plot provides an intuitive and clear illustration of how well the forecast matches the actual values, making it straightforward even for non-technical stakeholders to interpret.\nFlexibility: It allows comparison for multiple models and for specified time periods.\nModel Evaluation: It can be useful in identifying overfitting or underfitting situations, as these will manifest as discrepancies between the forecasted and observed values.\n\nLimitations:\n\nInterpretation Bias: Interpretation of the plot is subjective and can lead to different conclusions by different evaluators.\nLack of Precision: Visual representation might not provide precise values of the deviation.\nInapplicability: Limited to cases where the order of data points (time-series) matters, it might not be of much use in problems that are not related to time series prediction."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RunsTest.html",
    "href": "tests/model_validation/statsmodels/RunsTest.html",
    "title": "RunsTest",
    "section": "",
    "text": "RunsTest\nExecutes Runs Test on ML model to detect non-random patterns in output data sequence.\nPurpose: The Runs Test is a statistical procedure used to determine whether the sequence of data extracted from the ML model behaves randomly or not. Specifically, it analyzes runs, sequences of consecutive positives or negatives, in the data to check if there are more or fewer runs than expected under the assumption of randomness. This can be an indication of some pattern, trend, or cycle in the model’s output which may need attention.\nTest Mechanism: The testing mechanism applies the Runs Test from the statsmodels module on each column of the training dataset. For every feature in the dataset, a Runs Test is executed, whose output includes a Runs Statistic and P-value. A low P-value suggests that data arrangement in the feature is not likely to be random. The results are stored in a dictionary where the keys are the feature names, and the values are another dictionary storing the test statistic and the P-value for each feature.\nSigns of High Risk: - High risk is indicated when the P-value is close to zero. - If the p-value is less than a predefined significance level (like 0.05), it suggests that the runs (series of positive or negative values) in the model’s output are not random and are longer or shorter than what is expected under a random scenario. - This would mean there’s a high risk of non-random distribution of errors or model outcomes, suggesting potential issues with the model.\nStrengths: - The strength of the Runs Test is that it’s straightforward and fast for detecting non-random patterns in data sequence. - It can validate assumptions of randomness, which is particularly valuable for checking error distributions in regression models, trendless time series data, and making sure a classifier doesn’t favour one class over another. - Moreover, it can be applied to both classification and regression tasks, making it versatile.\nLimitations: - The test assumes that the data is independently and identically distributed (i.i.d.), which might not be the case for many real-world datasets. - The conclusion drawn from the low p-value indicating non-randomness does not provide information about the type or the source of the detected pattern. - Also, it is sensitive to extreme values (outliers), and overly large or small run sequences can influence the results. - Furthermore, this test does not provide model performance evaluation; it is used to detect patterns in the sequence of outputs only."
  },
  {
    "objectID": "tests/model_validation/statsmodels/PDRatingClassPlot.html",
    "href": "tests/model_validation/statsmodels/PDRatingClassPlot.html",
    "title": "PDRatingClassPlot",
    "section": "",
    "text": "PDRatingClassPlot\nAssesses and visualizes credit risk distribution across different rating classes within a dataset via default probabilities.\nPurpose: The purpose of the Probability of Default (PD) Rating Class Plot test is to measure and evaluate the distribution of calculated default probabilities across different rating classes. This is critical for understanding and inferring credit risk and can provide insights into how effectively the model is differentiating between different risk levels in a credit dataset.\nTest Mechanism: This metric is implemented via a visualization mechanism. It sorts the predicted probabilities of defaults into user-defined rating classes defined in “rating_classes” in default parameters. When it has classified the probabilities, it then calculates the average default rates within each rating class. Subsequently, it produces bar plots for each of these rating classes, illustrating the average likelihood of a default within each class. This process is executed separately for both the training and testing data sets. The classification of predicted probabilities utilizes the pandas “cut” function, sorting and sectioning the data values into bins.\nSigns of High Risk:\n\nIf lower rating classes present higher average likelihoods of default than higher rating classes\nIf there is poor differentiation between the averages across the different rating classes\nIf the model generates a significant contrast between the likelihoods for the training set and the testing set, suggestive of model overfitting\n\nStrengths:\n\nPresents a clear visual representation of how efficient the model is at predicting credit risk across different risk levels\nAllows for rapid identification and understanding of model performance per rating class\nHighlights potential overfitting issues by including both training and testing datasets in the analysis\n\nLimitations:\n\nMaking an incorrect choice for the number of rating classes, either oversimplifying or overcomplicating the distribution of default rates\nRelying on the assumption that the rating classes are effective at differentiating risk levels and that the boundaries between classes truly represent the risk distribution\nNot accounting for data set class imbalance, which could cause skewed average probabilities\nInability to gauge the overall performance of the model only based on this metric, emphasizing the requirement of combining it with other evaluation metrics"
  },
  {
    "objectID": "tests/model_validation/statsmodels/ScorecardHistogram.html",
    "href": "tests/model_validation/statsmodels/ScorecardHistogram.html",
    "title": "ScorecardHistogram",
    "section": "",
    "text": "ScorecardHistogram\nCreates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model.\nPurpose: The Scorecard Histogram test metric provides a visual interpretation of the credit scores generated by a machine learning model for credit-risk classification tasks. It aims to compare the alignment of the model’s scoring decisions with the actual outcomes of credit loan applications. It helps in identifying potential discrepancies between the model’s predictions and real-world risk levels.\nTest Mechanism: This metric uses logistic regression to generate a histogram of credit scores for both default (negative class) and non-default (positive class) instances. Using both training and test datasets, the metric calculates the credit score of each instance with a scorecard method, considering the impact of different features on the likelihood of default. İncludes the default point to odds (PDO) scaling factor and predefined target score and odds settings. Histograms for training and test sets are computed and plotted separately to offer insights into the model’s generalizability to unseen data.\nSigns of High Risk: - Discrepancies between the distributions of training and testing data, indicating a model’s poor generalisation ability - Skewed distributions favouring specific scores or classes, representing potential bias\nStrengths: - Provides a visual interpretation of the model’s credit scoring system, enhancing comprehension of model behavior - Enables a direct comparison between actual and predicted scores for both training and testing data - Its intuitive visualization helps understand the model’s ability to differentiate between positive and negative classes - Can unveil patterns or anomalies not easily discerned through numerical metrics alone\nLimitations: - Despite its value for visual interpretation, it doesn’t quantify the performance of the model, and therefore may lack precision for thorough model evaluation - The quality of input data can strongly influence the metric, as bias or noise in the data will affect both the score calculation and resultant histogram - Its specificity to credit scoring models limits its applicability across a wider variety of machine learning tasks and models - The metric’s effectiveness is somewhat tied to the subjective interpretation of the analyst, since it relies on the analyst’s judgment of the characteristics and implications of the plot."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelOutsampleComparison.html",
    "href": "tests/model_validation/statsmodels/RegressionModelOutsampleComparison.html",
    "title": "RegressionModelOutsampleComparison",
    "section": "",
    "text": "RegressionModelOutsampleComparison\nComputes MSE and RMSE for multiple regression models using out-of-sample test to assess model’s prediction accuracy on unseen data.\nPurpose: The RegressionModelOutsampleComparison test is designed to evaluate the predictive performance of multiple regression models by means of an out-of-sample test. The primary aim of this test is to validate the model’s ability to generalize to unseen data, a common challenge in the context of overfitting. It does this by computing two critical metrics — Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), which provide a quantifiable measure of the model’s prediction accuracy on the testing dataset.\nTest Mechanism: This test requires multiple models (specifically Ordinary Least Squares - OLS regression models) and a test dataset as inputs. Each model generates predictions using the test dataset. The residuals are then calculated and used to compute the MSE and RMSE for each model. The test outcomes, which include the model’s name, its MSE, and RMSE, are recorded and returned in a structured dataframe format.\nSigns of High Risk: - High values of MSE or RMSE indicate significant risk, signifying that the model’s predictions considerably deviate from the actual values in the test dataset. - Consistently large discrepancies between training and testing performance across various models may indicate an issue with the input data itself or the model selection strategies employed.\nStrengths: - This test offers a comparative evaluation of multiple models’ out-of-sample performance, enabling the selection of the best performing model. - The use of both MSE and RMSE provides insights into the model’s prediction error. While MSE is sensitive to outliers, emphasizing larger errors, RMSE provides a more interpretable measure of average prediction error given that it’s in the same unit as the dependent variable.\nLimitations: - The applicability of this test is limited to regression tasks, specifically OLS models. - The test operates under the assumption that the test dataset is a representative sample of the population. This might not always hold true and can result in less accurate insights. - The interpretability and the objectivity of the output (MSE and RMSE) can be influenced when the scale of the dependent variable varies significantly, or the distribution of residuals is heavily skewed or contains outliers."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionModelsPerformance.html",
    "href": "tests/model_validation/statsmodels/RegressionModelsPerformance.html",
    "title": "RegressionModelsPerformance",
    "section": "",
    "text": "RegressionModelsPerformance\nEvaluates and compares regression models’ performance using R-squared, Adjusted R-squared, and MSE metrics.\nPurpose: This metric is used to evaluate and compare the performance of various regression models. Through the use of key statistical measures such as R-squared, Adjusted R-squared, and Mean Squared Error (MSE), the performance of different models in predicting dependent variables can be assessed both on the data used for training (in-sample) and new, unseen data (out-of-sample).\nTest Mechanism: The test evaluates a list of provided regression models. For each model, it calculates their in-sample and out-of-sample performance by deriving the model predictions for the training and testing datasets respectively, and then comparing these predictions to the actual values. In doing so, it calculates R-squared, Adjusted R-squared, and MSE for each model, stores the results, and returns them for comparison.\nSigns of High Risk: - High Mean Squared Error (MSE) values. - Strikingly low values of R-squared and Adjusted R-squared. - A significant drop in performance when transitioning from in-sample to out-of-sample evaluations, signaling a potential overfitting issue.\nStrengths: - The test permits comparisons of multiple models simultaneously, providing an objective base for identifying the top-performing model. - It delivers both in-sample and out-of-sample evaluations, presenting performance data on unseen data. - The utilization of R-squared and Adjusted R-squared in conjunction with MSE allows for a detailed view of the model’s explainability and error rate.\nLimitations: - This test is built around the assumption that the residuals of the regression model are normally distributed, which is a fundamental requirement for Ordinary Least Squares (OLS) regression; thus, it could be not suitable for models where this assumption is broken. - The test does not consider cases where higher R-squared or lower MSE values do not necessarily correlate with better predictive performance, particularly in instances of excessively complex models."
  },
  {
    "objectID": "tests/model_validation/statsmodels/RegressionCoeffsPlot.html",
    "href": "tests/model_validation/statsmodels/RegressionCoeffsPlot.html",
    "title": "RegressionCoeffsPlot",
    "section": "",
    "text": "RegressionCoeffsPlot\nVisualizes regression coefficients with 95% confidence intervals to assess predictor variables’ impact on response variable.\nPurpose: The Regression Coefficients with Confidence Intervals plot and metric aims to understand the impact of predictor variables on the response variable in question. This understanding is achieved via the visualization and analysis of the regression model by presenting the coefficients derived from the model along with their associated 95% confidence intervals. By doing so, it offers insights into the variability and uncertainty associated with the model’s estimates.\nTest Mechanism: The test begins by extracting the estimated coefficients and their related standard errors from the regression model under test. It then calculates and draws confidence intervals based on a 95% confidence level (a standard convention in statistics). These intervals provide a range wherein the true value can be expected to fall 95% of the time if the same regression were re-run multiple times with samples drawn from the same population. This information is then visualized as a bar plot, with the predictor variables and their coefficients on the x-axis and y-axis respectively and the confidence intervals represented as error bars.\nSigns of High Risk: * If the calculated confidence interval contains the zero value, it could mean the feature/coefficient in question doesn’t significantly contribute to prediction in the model. * If there are multiple coefficients exhibiting this behavior, it might raise concerns about overall model reliability. * Very wide confidence intervals might indicate high uncertainty in the associated coefficient estimates.\nStrengths: * This metric offers a simple and easily comprehendible visualization of the significance and impact of individual predictor variables in a regression model. * By including confidence intervals, it enables an observer to evaluate the uncertainty around each coefficient estimate.\nLimitations: * The test is dependent on a few assumptions about the data, namely normality of residuals and independence of observations, which may not always be true for all types of datasets. * The test does not consider multi-collinearity (correlation among predictor variables), which can potentially distort the model and make interpretation of coefficients challenging. * The test’s application is limited to regression tasks and tabular datasets and is not suitable for other types of machine learning assignments or data structures."
  },
  {
    "objectID": "tests/model_validation/statsmodels/KolmogorovSmirnov.html",
    "href": "tests/model_validation/statsmodels/KolmogorovSmirnov.html",
    "title": "KolmogorovSmirnov",
    "section": "",
    "text": "KolmogorovSmirnov\nExecutes a feature-wise Kolmogorov-Smirnov test to evaluate alignment with normal distribution in datasets.\nPurpose: This metric employs the Kolmogorov-Smirnov (KS) test to evaluate the distribution of a dataset’s features. It specifically gauges whether the data from each feature aligns with a normal distribution, a common presumption in many statistical methods and machine learning models.\nTest Mechanism: This KS test calculates the KS statistic and the corresponding p-value for each column in a dataset. It achieves this by contrasting the cumulative distribution function of the dataset’s feature with an ideal normal distribution. Subsequently, a feature-by-feature KS statistic and p-value are stored in a dictionary. The specific threshold p-value (the value below which we reject the hypothesis that the data is drawn from a normal distribution) is not firmly set within this implementation, allowing for definitional flexibility depending on the specific application.\nSigns of High Risk: - Elevated KS statistic for a feature combined with a low p-value. This suggests a significant divergence between the feature’s distribution and a normal one. - Features with notable deviations. These could create problems if the applicable model makes assumptions about normal data distribution, thereby representing a risk.\nStrengths: - The KS test is acutely sensitive to differences in the location and shape of the empirical cumulative distribution functions of two samples. - It is non-parametric and does not presuppose any specific data distribution, making it adaptable to a range of datasets. - With its focus on individual features, it offers detailed insights into data distribution.\nLimitations: - The sensitivity of the KS test to disparities in data distribution tails can be excessive. Such sensitivity might prompt false alarms about non-normal distributions, particularly in situations where these tail tendencies are irrelevant to the model. - It could become less effective when applied to multivariate distributions, considering that it’s primarily configured for univariate distributions. - As a goodness-of-fit test, the KS test does not identify specific types of non-normality, such as skewness or kurtosis, that could directly impact model fitting."
  },
  {
    "objectID": "tests/model_validation/statsmodels/PhillipsPerronArch.html",
    "href": "tests/model_validation/statsmodels/PhillipsPerronArch.html",
    "title": "PhillipsPerronArch",
    "section": "",
    "text": "PhillipsPerronArch\nExecutes Phillips-Perron test to assess the stationarity of time series data in each ML model feature.\nPurpose: The Phillips-Perron (PP) test is used to establish the order of integration in time series data, testing a null hypothesis that a time series is unit-root non-stationary. This is vital in forecasting and understanding the stochastic behavior of data within machine learning models. Essentially, the PP test aids in confirming the robustness of results and generating valid predictions from regression analysis models.\nTest Mechanism: The PP test is conducted for each feature in the dataset. A data frame is created from the dataset, and for each column in this frame, the PhillipsPerron method calculates the statistic value, p-value, used lags, and number of observations. This process computes the PP metric for each feature and stores the results for future reference.\nSigns of High Risk: - A high P-value could imply that the series has a unit root and is therefore non-stationary. - Test statistic values that surpass critical values indicate additional evidence of non-stationarity. - A high ‘usedlag’ value for a series could point towards autocorrelation issues which could further impede the model’s performance.\nStrengths: - Resilience against heteroskedasticity in the error term is a significant strength of the PP test. - Its capacity to handle long time series data. - Its ability to determine whether the time series is stationary or not, influencing the selection of suitable models for forecasting.\nLimitations: - The PP test can only be employed within a univariate time series framework. - The test relies on asymptotic theory, which means the test’s power can significantly diminish for small sample sizes. - The need to convert non-stationary time series into stationary series through differencing might lead to loss of vital data points."
  },
  {
    "objectID": "tests/model_validation/statsmodels/JarqueBera.html",
    "href": "tests/model_validation/statsmodels/JarqueBera.html",
    "title": "JarqueBera",
    "section": "",
    "text": "JarqueBera\nAssesses normality of dataset features in an ML model using the Jarque-Bera test.\nPurpose: The purpose of the Jarque-Bera test as implemented in this metric is to determine if the features in the dataset of a given Machine Learning model follows a normal distribution. This is crucial for understanding the distribution and behavior of the model’s features, as numerous statistical methods assume normal distribution of the data.\nTest Mechanism: The test mechanism involves computing the Jarque-Bera statistic, p-value, skew, and kurtosis for each feature in the dataset. It utilizes the ‘jarque_bera’ function from the ‘statsmodels’ library in Python, storing the results in a dictionary. The test evaluates the skewness and kurtosis to ascertain whether the dataset follows a normal distribution. A significant p-value (typically less than 0.05) implies that the data does not possess normal distribution.\nSigns of High Risk: - A high Jarque-Bera statistic and a low p-value (usually less than 0.05) indicates high-risk conditions. - Such results suggest the data significantly deviates from a normal distribution. If a machine learning model expects feature data to be normally distributed, these findings imply that it may not function as intended.\nStrengths: - This test provides insights into the shape of the data distribution, helping determine whether a given set of data follows a normal distribution. - This is particularly useful for risk assessment for models that assume a normal distribution of data. - By measuring skewness and kurtosis, it provides additional insights into the nature and magnitude of a distribution’s deviation.\nLimitations: - The Jarque-Bera test only checks for normality in the data distribution. It cannot provide insights into other types of distributions. - Datasets that aren’t normally distributed but follow some other distribution might lead to inaccurate risk assessments. - The test is highly sensitive to large sample sizes, often rejecting the null hypothesis (that data is normally distributed) even for minor deviations in larger datasets."
  },
  {
    "objectID": "tests/model_validation/statsmodels/FeatureImportanceAndSignificance.html",
    "href": "tests/model_validation/statsmodels/FeatureImportanceAndSignificance.html",
    "title": "FeatureImportanceAndSignificance",
    "section": "",
    "text": "FeatureImportanceAndSignificance\nEvaluates and visualizes the statistical significance and feature importance using regression and decision tree models.\nPurpose: The ‘FeatureImportanceAndSignificance’ test evaluates the statistical significance and the importance of features in the context of the machine learning model. By comparing the p-values from a regression model and the feature importances from a decision tree model, this test aids in determining the most significant variables from a statistical and a machine learning perspective, assisting in feature selection during the model development process.\nTest Mechanism: The test first compares the p-values from a regression model and the feature importances from a decision tree model. These values are normalized to ensure a uniform comparison. The ‘p_threshold’ parameter is used to determine what p-value is considered statistically significant and if the ‘significant_only’ parameter is true, only features with p-values below this threshold are included in the final output. The output from this test includes an interactive visualization displaying normalized p-values and the associated feature importances. The test throws an error if it does not receive both a regression model and a decision tree model.\nSigns of High Risk: - Exceptionally high or low p-values, which suggest that a feature may not be significant or meaningful in the context of the model. - If many variables with small feature importance values have significant p-values, this could indicate that the model might be overfitting.\nStrengths: - Combines two perspectives statistical significance (p-values) and feature importance (decision tree model), making it a robust feature selection test. - Provides an interactive visualization making it easy to interpret and understand the results.\nLimitations: - The test only works with a regression model and a decision tree model which may limit its applicability. - The test does not take into account potential correlations or causative relationships between features which may lead to misinterpretations of significance and importance. - Over-reliance on the p-value as a cut-off for feature significance can be seen as arbitrary and may not truly reflect the real-world importance of the feature."
  },
  {
    "objectID": "tests/data_validation/BivariateScatterPlots.html",
    "href": "tests/data_validation/BivariateScatterPlots.html",
    "title": "BivariateScatterPlots",
    "section": "",
    "text": "BivariateScatterPlots\nGenerates bivariate scatterplots to visually inspect relationships between pairs of predictor variables in machine learning classification tasks.\nPurpose: This metric is intended for visual inspection and monitoring of relationships between pairs of variables in a machine learning model targeting classification tasks. It is especially useful for understanding how predictor variables (features) behave in relation to each other and how they are distributed for different classes of the target variable, which could inform feature selection, model-building strategies, and even alert to possible biases and irregularities in the data.\nTest Mechanism: This metric operates by creating a scatter plot for each pair of the selected features in the dataset. If the parameters “features_pairs” are not specified, an error will be thrown. The metric offers flexibility by allowing the user to filter on a specific target class - specified by the “target_filter” parameter - for more granified insights. Each scatterplot is then color-coded based on the category of the target variable for better visual differentiation. The seaborn scatterplot library is used for generating the plots.\nSigns of High Risk: - Visual patterns which might suggest non-linear relationships, substantial skewness, multicollinearity, clustering, or isolated outlier points in the scatter plot. - Such issues could affect the assumptions and performance of some models, especially the ones assuming linearity like linear regression or logistic regression.\nStrengths: - Scatterplots are simple and intuitive for users to understand, providing a visual tool to pinpoint complex relationships between two variables. - They are useful for outlier detection, identification of variable associations and trends, including non-linear patterns which can be overlooked by other linear-focused metrics or tests. - The implementation also supports visualizing binary or multi-class classification datasets.\nLimitations: - Scatterplots are limited to bivariate analysis - the relationship of two variables at a time - and might not reveal the full picture in higher dimensions or where interactions are present. - They are not ideal for very large datasets as points will overlap and render the visualization less informative. - Scatterplots are more of an exploratory tool rather than a formal statistical test, so they don’t provide any quantitative measure of model quality or performance. - Interpretation of scatterplots relies heavily on the domain knowledge and judgment of the viewer, which can introduce subjective bias."
  },
  {
    "objectID": "tests/data_validation/AutoSeasonality.html",
    "href": "tests/data_validation/AutoSeasonality.html",
    "title": "AutoSeasonality",
    "section": "",
    "text": "AutoSeasonality\nAutomatically identifies and quantifies optimal seasonality in time series data to improve forecasting model performance.\nPurpose: The AutoSeasonality metric’s purpose is to automatically detect and identify the best seasonal order or period for each variable in a time series dataset. This detection helps to quantify periodic patterns and seasonality that reoccur at fixed intervals of time in the data. This is especially significant for forecasting-based models, where understanding the seasonality component can drastically improve prediction accuracy.\nTest Mechanism: This metric uses the seasonal decomposition method from the Statsmodels Python library. The function takes the additive’ model type for each variable and applies it within the prescribed range of ‘min_period’ and max_period’. The function decomposes the seasonality for each period in the range and calculates the mean residual error for each period. The seasonal period that results in the minimum residuals is marked as the ‘Best Period’. The test results include the ‘Best Period’, the calculated residual errors, and a determination of ‘Seasonality’ or No Seasonality’.\nSigns of High Risk:\n\nIf the optimal seasonal period (or ‘Best Period’) is consistently at the maximum or minimum limit of the offered range for a majority of variables, it may suggest that the range set does not adequately capture the true seasonal pattern in the series.\nA high average ‘Residual Error’ for the selected ‘Best Period’ could indicate issues with the model’s performance.\n\nStrengths:\n\nThe metric offers an automatic approach to identifying and quantifying the optimal seasonality, providing a robust method for analyzing time series datasets.\nIt is applicable to multiple variables in a dataset, providing a comprehensive evaluation of each variable’s seasonality.\nThe use of concrete and measurable statistical methods improves the objectivity and reproducibility of the model.\n\nLimitations:\n\nThis AutoSeasonality metric may not be suitable if the time series data exhibits random walk behaviour or lacks clear seasonality, as the seasonal decomposition model may not be appropriate.\nThe defined range for the seasonal period (min_period and max_period) can influence the outcomes. If the actual seasonality period lies outside this range, this method will not be able to identify the true seasonal order.\nThis metric may not be able to fully interpret complex patterns that go beyond the simple additive model for seasonal decomposition.\nThe tool may incorrectly infer seasonality if random fluctuations in the data match the predefined seasonal period range."
  },
  {
    "objectID": "tests/data_validation/DatasetSplit.html",
    "href": "tests/data_validation/DatasetSplit.html",
    "title": "DatasetSplit",
    "section": "",
    "text": "DatasetSplit\nEvaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML model.\nPurpose: The DatasetSplit test is designed to evaluate and visualize the distribution of data among training, testing, and validation datasets, if available, within a given machine learning model. The main purpose is to assess whether the model’s datasets are split appropriately, as an imbalanced split might affect the model’s ability to learn from the data and generalize to unseen data.\nTest Mechanism: The DatasetSplit test first calculates the total size of all available datasets in the model. Then, for each individual dataset, the methodology involves determining the size of the dataset and its proportion relative to the total size. The results are then conveniently summarized in a table that shows dataset names, sizes, and proportions. Absolute size and proportion of the total dataset size are displayed for each individual dataset.\nSigns of High Risk:\n\nA very small training dataset, which may result in the model not learning enough from the data.\nA very large training dataset and a small test dataset, which may lead to model overfitting and poor generalization to unseen data.\nA small or non-existent validation dataset, which might complicate the model’s performance assessment.\n\nStrengths:\n\nThe DatasetSplit test provides a clear, understandable visualization of dataset split proportions, which can highlight any potential imbalance in dataset splits quickly.\nIt covers a wide range of task types including classification, regression, and text-related tasks.\nThe metric is not tied to any specific data type and is applicable to tabular data, time series data, or text data.\n\nLimitations:\n\nThe DatasetSplit test does not provide any insight into the quality or diversity of the data within each split, just the size and proportion.\nThe test does not give any recommendations or adjustments for imbalanced datasets.\nPotential lack of compatibility with more complex modes of data splitting (for example, stratified or time-based splits) could limit the applicability of this test."
  },
  {
    "objectID": "tests/data_validation/HeatmapFeatureCorrelations.html",
    "href": "tests/data_validation/HeatmapFeatureCorrelations.html",
    "title": "HeatmapFeatureCorrelations",
    "section": "",
    "text": "HeatmapFeatureCorrelations\nCreates a heatmap to visually represent correlation patterns between pairs of numerical features in a dataset.\nPurpose: The HeatmapFeatureCorrelations metric is utilized to evaluate the degree of interrelationships between pairs of input features within a dataset. This metric allows us to visually comprehend the correlation patterns through a heatmap, which can be essential in understanding which features may contribute most significantly to the performance of the model. Features that have high intercorrelation can potentially reduce the model’s ability to learn, thus impacting the overall performance and stability of the machine learning model.\nTest Mechanism: The metric executes the correlation test by computing the Pearson correlations for all pairs of numerical features. It then generates a heatmap plot using seaborn, a Python data visualization library. The colormap ranges from -1 to 1, indicating perfect negative correlation and perfect positive correlation respectively. A ‘declutter’ option is provided which, if set to true, removes variable names and numerical correlations from the plot to provide a more streamlined view. The size of feature names and correlation coefficients can be controlled through ‘fontsize’ parameters.\nSigns of High Risk:\n\nIndicators of potential risk include features with high absolute correlation values.\nA significant degree of multicollinearity might lead to instabilities in the trained model and can also result in overfitting.\nThe presence of multiple homogeneous blocks of high positive or negative correlation within the plot might indicate redundant or irrelevant features included within the dataset.\n\nStrengths:\n\nThe strength of this metric lies in its ability to visually represent the extent and direction of correlation between any two numeric features, which aids in the interpretation and understanding of complex data relationships.\nThe heatmap provides an immediate and intuitively understandable representation, hence, it is extremely useful for high-dimensional datasets where extracting meaningful relationships might be challenging.\n\nLimitations:\n\nThe central limitation might be that it can only calculate correlation between numeric features, making it unsuitable for categorical variables unless they are already numerically encoded in a meaningful manner.\nIt uses Pearson’s correlation, which only measures linear relationships between features. It may perform poorly in cases where the relationship is non-linear.\nLarge feature sets might result in cluttered and difficult-to-read correlation heatmaps, especially when the declutter’ option is set to false."
  },
  {
    "objectID": "tests/data_validation/BivariateHistograms.html",
    "href": "tests/data_validation/BivariateHistograms.html",
    "title": "BivariateHistograms",
    "section": "",
    "text": "BivariateHistograms\nGenerates bivariate histograms for paired features, aiding in visual inspection of categorical variables distributions and correlations.\nPurpose: This metric, dubbed BivariateHistograms, is primarily used for visual data analysis via the inspection of variable distribution, specifically categorical variables. Its main objective is to ascertain any potential correlations between these variables and distributions within each defined target class. This is achieved by offering an intuitive avenue into gaining insights into the characteristics of the data and any plausible patterns therein.\nTest Mechanism: The working mechanism of the BivariateHistograms module revolves around an input dataset and a series of feature pairs. It uses seaborn’s histogram plotting function and matplotlib techniques to create bivariate histograms for each feature pair in the dataset. Two histograms, stratified by the target column status, are produced for every pair of features. This enables the telling apart of different target statuses through color differentiation. The module also offers optional functionality for restricting the data by a specific status through the target_filter parameter.\nSigns of High Risk: - Irregular or unexpected distributions of data across the different categories. - Highly skewed data distributions. - Significant deviations from the perceived ‘normal’ or anticipated distributions. - Large discrepancies in distribution patterns between various target statuses.\nStrengths: - Owing to its simplicity, the histogram-based approach is easy to implement and interpret which translates to quick insights. - The metrics provides a consolidated view of the distribution of data across different target conditions for each variable pair, thereby assisting in highlighting potential correlations and patterns. - It proves advantageous in spotting anomalies, comprehending interactions among features, and facilitating exploratory data analysis.\nLimitations: - Its simplicity may be a drawback when it comes to spotting intricate or complex patterns in data. - Overplotting might occur when working with larger datasets. - The metric is only applicable to categorical data, and offers limited insights for numerical or continuous variables. - The interpretation of visual results hinges heavily on the expertise of the observer, possibly leading to subjective analysis."
  },
  {
    "objectID": "tests/data_validation/HighPearsonCorrelation.html",
    "href": "tests/data_validation/HighPearsonCorrelation.html",
    "title": "HighPearsonCorrelation",
    "section": "",
    "text": "HighPearsonCorrelation\nIdentifies highly correlated feature pairs in a dataset suggesting feature redundancy or multicollinearity.\nPurpose: The High Pearson Correlation test measures the linear relationship between features in a dataset, with the main goal of identifying high correlations that might indicate feature redundancy or multicollinearity. Identification of such issue allows developers and risk management teams to properly deal with potential impacts on the machine learning model’s performance and interpretability.\nTest Mechanism: The test works by generating pairwise Pearson correlations for all features in the dataset, then sorting and eliminating duplicate and self-correlations. It assigns a Pass or Fail based on whether the absolute value of the correlation coefficient surpasses a pre-set threshold (defaulted at 0.3). It lastly returns the top ten strongest correlations regardless of passing or failing status.\nSigns of High Risk: - A high risk indication would be the presence of correlation coefficients exceeding the threshold. - If the features share a strong linear relationship, this could lead to potential multicollinearity and model overfitting. - Redundancy of variables can undermine the interpretability of the model due to uncertainty over the authenticity of individual variable’s predictive power.\nStrengths: - The High Pearson Correlation test provides a quick and simple means of identifying relationships between feature pairs. - It generates a transparent output which not only displays pairs of correlated variables but also delivers the Pearson correlation coefficient and a Pass or Fail status for each. - It aids early identification of potential multicollinearity issues that may disrupt model training.\nLimitations: - The Pearson correlation test can only delineate linear relationships. It fails to shed light on nonlinear relationships or dependencies. - It is sensitive to outliers where a few outliers could notably affect the correlation coefficient. - It is limited to identifying redundancy only within feature pairs. When three or more variables are linearly dependent, it may fail to spot this complex relationship. - The top 10 result filter might not fully capture the richness of the data; an option to configure the number of retained results could be helpful."
  },
  {
    "objectID": "tests/data_validation/PiTPDHistogram.html",
    "href": "tests/data_validation/PiTPDHistogram.html",
    "title": "PiTPDHistogram",
    "section": "",
    "text": "PiTPDHistogram\nAssesses credit risk prediction accuracy of a model by comparing actual and predicted defaults at a chosen point in time.\nPurpose: The PiTPDHistogram metric uses Probability of Default (PD) calculations for individual instances within both training and test data sets in order to assess a model’s proficiency in predicting credit risk. A distinctive point in time (PiT) is chosen for these PD calculations, and the results for both actual and predicted defaults are presented in histogram form. This visualization is aimed at simplifying the understanding of model prediction accuracy.\nTest Mechanism: Instances are categorized into two groups - those for actual defaults and those for predicted defaults, with ‘1’ indicating a default and ‘0’ indicating non-default. PD is calculated for each instance, and based on these calculations, two histograms are created, one for actual defaults and one for predicted defaults. If the predicted default frequency matches that of the actual defaults, the model’s performance is deemed effective.\nSigns of High Risk: - Discrepancies between the actual and predicted default histograms may suggest model inefficiency. - Variations in histogram shapes or divergences in default probability distributions could be concerning. - Significant mismatches in peak default probabilities could also be red flags.\nStrengths: - Provides a visual comparison between actual and predicted defaults, aiding in the understanding of model performance. - Helps reveal model bias and areas where the model’s performance could be improved. - Easier to understand than purely numerical evaluations or other complicated visualization measures.\nLimitations: - The metric remains largely interpretive and subjective, as the extent and relevance of visual discrepancies often need to be evaluated manually, leading to potentially inconsistent results across different analyses. - This metric alone may not capture all the complexities and nuances of model performance. - The information provided is limited to a specific point in time, potentially neglecting the model’s performance under various circumstances or different time periods."
  },
  {
    "objectID": "tests/data_validation/IQROutliersBarPlot.html",
    "href": "tests/data_validation/IQROutliersBarPlot.html",
    "title": "IQROutliersBarPlot",
    "section": "",
    "text": "IQROutliersBarPlot\nVisualizes outlier distribution across percentiles in numerical data using Interquartile Range (IQR) method.\nPurpose: The InterQuartile Range Outliers Bar Plot (IQROutliersBarPlot) metric aims to visually analyze and evaluate the extent of outliers in numeric variables based on percentiles. Its primary purpose is to clarify the dataset’s distribution, flag possible abnormalities in it and gauge potential risks associated with processing potentially skewed data, which can affect the machine learning model’s predictive prowess.\nTest Mechanism: The examination invokes a series of steps:\n\nFor every numeric feature in the dataset, the 25th percentile (Q1) and 75th percentile (Q3) are calculated before deriving the Interquartile Range (IQR), the difference between Q1 and Q3.\nSubsequently, the metric calculates the lower and upper thresholds by subtracting Q1 from the threshold times IQR and adding Q3 to threshold times IQR, respectively. The default threshold is set at 1.5.\nAny value in the feature that falls below the lower threshold or exceeds the upper threshold is labeled as an outlier.\nThe number of outliers are tallied for different percentiles, such as [0-25], [25-50], [50-75], and [75-100].\nThese counts are employed to construct a bar plot for the feature, showcasing the distribution of outliers across different percentiles.\n\nSigns of High Risk: High risk or a potential lapse in the model’s performance could be unveiled by the following signs:\n\nA prevalence of outliers in the data, potentially skewing its distribution.\nOutliers dominating higher percentiles (75-100) which implies the presence of extreme values, capable of severely influencing the model’s performance.\nCertain features harboring most of their values as outliers, which signifies that these features might not contribute positively to the model’s forecasting ability.\n\nStrengths:\n\nEffectively identifies outliers in the data through visual means, facilitating easier comprehension and offering insights into the outliers’ possible impact on the model.\nProvides flexibility by accommodating all numeric features or a chosen subset.\nTask-agnostic in nature; it is viable for both classification and regression tasks.\nCan handle large datasets as its operation does not hinge on computationally heavy operations.\n\nLimitations:\n\nIts application is limited to numerical variables and does not extend to categorical ones.\nRelies on a predefined threshold (default being 1.5) for outlier identification, which may not be suitable for all cases.\nOnly reveals the presence and distribution of outliers and does not provide insights into how these outliers might affect the model’s predictive performance.\nThe assumption that data is unimodal and symmetric may not always hold true. In cases with non-normal distributions, the results can be misleading."
  },
  {
    "objectID": "tests/data_validation/TabularDateTimeHistograms.html",
    "href": "tests/data_validation/TabularDateTimeHistograms.html",
    "title": "TabularDateTimeHistograms",
    "section": "",
    "text": "TabularDateTimeHistograms\nGenerates histograms to provide graphical insight into the distribution of time intervals in model’s datetime data.\nPurpose: The TabularDateTimeHistograms metric is designed to provide graphical insight into the distribution of time intervals in a machine learning model’s datetime data. By plotting histograms of differences between consecutive date entries in all datetime variables, it enables an examination of the underlying pattern of time series data and identification of anomalies.\nTest Mechanism: This test operates by first identifying all datetime columns and extracting them from the dataset. For each datetime column, it next computes the differences (in days) between consecutive dates, excluding zero values, and visualizes these differences in a histogram. The seaborn library’s histplot function is used to generate histograms, which are labeled appropriately and provide a graphical representation of the frequency of different day intervals in the dataset.\nSigns of High Risk: - If no datetime columns are detected in the dataset, this would lead to a ValueError. Hence, the absence of datetime columns signifies a high risk. - A severely skewed or irregular distribution depicted in the histogram may indicate possible complications with the data, such as faulty timestamps or abnormalities.\nStrengths: - The metric offers a visual overview of time interval frequencies within the dataset, supporting the recognition of inherent patterns. - Histogram plots can aid in the detection of potential outliers and data anomalies, contributing to an assessment of data quality. - The metric is versatile, compatible with a range of task types, including classification and regression, and can work with multiple datetime variables if present.\nLimitations: - A major weakness of this metric is its dependence on the visual examination of data, as it does not provide a measurable evaluation of the model. - The metric might overlook complex or multi-dimensional trends in the data. - The test is only applicable to datasets containing datetime columns and will fail if such columns are unavailable. - The interpretation of the histograms relies heavily on the domain expertise and experience of the reviewer."
  },
  {
    "objectID": "tests/data_validation/LaggedCorrelationHeatmap.html",
    "href": "tests/data_validation/LaggedCorrelationHeatmap.html",
    "title": "LaggedCorrelationHeatmap",
    "section": "",
    "text": "LaggedCorrelationHeatmap\nAssesses and visualizes correlation between target variable and lagged independent variables in a time-series dataset.\nPurpose: The LaggedCorrelationHeatmap metric is utilized to appraise and illustrate the correlation between the target variable and delayed copies (lags) of independent variables in a time-series dataset. It assists in revealing relationships in time-series data where the influence of an independent variable on the dependent variable is not immediate but occurs after a period (lags).\nTest Mechanism: To execute this test, Python’s Pandas library pairs with Plotly to perform computations and present the visualization in the form of a heatmap. The test begins by extracting the target variable and corresponding independent variables from the dataset. Then, generation of lags of independent variables takes place, followed by the calculation of correlation between these lagged variables and the target variable. The outcome is a correlation matrix that gets recorded and illustrated as a heatmap, where different color intensities represent the strength of the correlation, making patterns easier to identify.\nSigns of High Risk: - Insignificant correlations across the heatmap, indicating a lack of noteworthy relationships between variables. - Correlations that break intuition or previous understanding, suggesting potential issues with the dataset or the model.\nStrengths: - This metric serves as an exceptional tool for exploring and visualizing time-dependent relationships between features and the target variable in a time-series dataset. - It aids in identifying delayed effects that might go unnoticed with other correlation measures. - The heatmap offers an intuitive visual representation of time-dependent correlations and influences.\nLimitations: - The metric presumes linear relationships between variables, potentially ignoring non-linear relationships. - The correlation considered is linear; therefore, intricate non-linear interactions might be overlooked. - The metric is only applicable for time-series data, limiting its utility outside of this context. - The number of lags chosen can significantly influence the results; too many lags can render the heatmap difficult to interpret, while too few might overlook delayed effects. - This metric does not take into account any causal relationships, but merely demonstrates correlation."
  },
  {
    "objectID": "tests/data_validation/TimeSeriesMissingValues.html",
    "href": "tests/data_validation/TimeSeriesMissingValues.html",
    "title": "TimeSeriesMissingValues",
    "section": "",
    "text": "TimeSeriesMissingValues\nValidates time-series data quality by confirming the count of missing values is below a certain threshold.\nPurpose: This test is designed to validate the quality of a historical time-series dataset by verifying that the number of missing values is below a specified threshold. As time-series models greatly depend on the continuity and temporality of data points, missing values could compromise the model’s performance. Consequently, this test aims to ensure data quality and readiness for the machine learning model, safeguarding its predictive capacity.\nTest Mechanism: The test method commences by validating if the dataset has a datetime index, if not, an error is raised. It establishes a lower limit threshold for missing values and performs a missing values check on each column of the dataset. An object for the test result is created stating whether the number of missing values is within the specified threshold. Additionally, the test calculates the percentage of missing values alongside the raw count.\nTo aid in data visualization, the test generates two plots - a bar plot and a heatmap, to better illustrate the distribution and quantity of missing values per variable. The test results, a count of missing values, the percentage of missing values, and a pass/fail status are returned in a results table.\nSigns of High Risk: - The number of missing values in any column of the dataset surpasses the threshold, marking a failure and a high-risk scenario. The reasons could range from incomplete data collection, faulty sensors to data preprocessing errors. - A continuous visual ‘streak’ in the heatmap may indicate a systematic error during data collection, pointing towards another potential risk source.\nStrengths: - Effectively identifies missing values which could adversely affect the model’s performance. - Applicable and customizable through the threshold parameter across different data sets. - Goes beyond raw numbers by calculating the percentage of missing values, offering a more relative understanding of data scarcity. - Includes a robust visualization mechanism for easy and fast understanding of data quality.\nLimitations: - Although it identifies missing values, the test does not provide solutions to handle them. - The test demands that the dataset should have a datetime index, hence limiting its use only to time series analysis. - The test’s sensitivity to the ‘min_threshold’ parameter may raise false alarms if set too strictly or may overlook problematic data if set too loosely. - Solely focuses on the ‘missingness’ of the data and might fall short in addressing other aspects of data quality."
  },
  {
    "objectID": "tests/data_validation/TimeSeriesOutliers.html",
    "href": "tests/data_validation/TimeSeriesOutliers.html",
    "title": "TimeSeriesOutliers",
    "section": "",
    "text": "TimeSeriesOutliers\nIdentifies and visualizes outliers in time-series data using z-score method.\nPurpose: This test is designed to identify outliers in time-series data using the z-score method. It’s vital for ensuring data quality before modeling, as outliers can skew predictive models and significantly impact their overall performance.\nTest Mechanism: The test processes a given dataset which must have datetime indexing, checks if a zscore_threshold’ parameter has been supplied, and identifies columns with numeric data types. After finding numeric columns, the implementer then applies the z-score method to each numeric column, identifying outliers based on the threshold provided. Each outlier is listed together with their variable name, z-score, timestamp and relative threshold in a dictionary and converted to a DataFrame for convenient output. Additionally, it produces visual plots for each time series illustrating outliers in the context of the broader dataset. The zscore_threshold’ parameter sets the limit beyond which a data point will be labeled as an outlier. The default threshold is set at 3, indicating that any data point that falls 3 standard deviations away from the mean will be marked as an outlier.\nSigns of High Risk: - If many or substantial outliers are present within a dataset, this may be an indicator of high risk as it suggests that the dataset contains significant anomalies. - This could potentially affect the performance of the machine learning models, if not properly addressed. - Data points with z-scores higher than the set threshold would be flagged as outliers and could be considered as high risk.\nStrengths: - The z-score method is a popular and robust method for identifying outliers in a dataset. - Time series maintenance is simplified through requiring a datetime index. - Outliers are identified for each numeric feature individually. - Provides an elaborate report which shows variables, date, z-score and whether the test passed or failed. - Offers visual inspection for detected outliers in the respective time-series through plots.\nLimitations: - This test only identifies outliers in numeric columns, and won’t identify outliers in categorical variables. - The utility and accuracy of z-scores can be limited if the data doesn’t follow a normal distribution. - The method relies on a subjective z-score threshold for deciding what constitutes an outlier, which might not always be suitable depending on the dataset and the use case. - It does not address possible ways to handle identified outliers in the data. - The necessity for a datetime index could limit the extent of its application."
  },
  {
    "objectID": "tests/data_validation/DescriptiveStatistics.html",
    "href": "tests/data_validation/DescriptiveStatistics.html",
    "title": "DescriptiveStatistics",
    "section": "",
    "text": "DescriptiveStatistics\nPerforms a detailed descriptive statistical analysis of both numerical and categorical data within a model’s dataset.\nPurpose: The purpose of the Descriptive Statistics metric is to provide a comprehensive summary of both numerical and categorical data within a dataset. This involves statistics such as count, mean, standard deviation, minimum and maximum values for numerical data. For categorical data, it calculates the count, number of unique values, most common value and its frequency, and the proportion of the most frequent value relative to the total. The goal is to visualize the overall distribution of the variables in the dataset, aiding in understanding the model’s behavior and predicting its performance.\nTest Mechanism: The testing mechanism utilizes two in-built functions of pandas dataframes: describe() for numerical fields and value_counts() for categorical fields. The describe() function pulls out several summary statistics, while value_counts() accounts for unique values. The resulting data is formatted into two distinct tables, one for numerical and another for categorical variable summaries. These tables provide a clear summary of the main characteristics of the variables, which can be instrumental in assessing the model’s performance.\nSigns of High Risk: - Skewed data or significant outliers can represent high risk. For numerical data, this may be reflected via a significant difference between the mean and median (50% percentile). - For categorical data, a lack of diversity (low count of unique values), or overdominance of a single category (high frequency of the top value) can indicate high risk.\nStrengths: - This metric provides a comprehensive summary of the dataset, shedding light on the distribution and characteristics of the variables under consideration. - It is a versatile and robust method, applicable to both numerical and categorical data. - It helps highlight crucial anomalies such as outliers, extreme skewness, or lack of diversity, which are vital in understanding model behavior during testing and validation.\nLimitations: - While this metric offers a high-level overview of the data, it may fail to detect subtle correlations or complex patterns. - It does not offer any insights on the relationship between variables. - Alone, descriptive statistics cannot be used to infer properties about future unseen data. - It should be used in conjunction with other statistical tests to provide a comprehensive understanding of the model’s data."
  },
  {
    "objectID": "tests/data_validation/TabularCategoricalBarPlots.html",
    "href": "tests/data_validation/TabularCategoricalBarPlots.html",
    "title": "TabularCategoricalBarPlots",
    "section": "",
    "text": "TabularCategoricalBarPlots\nGenerates and visualizes bar plots for each category in categorical features to evaluate dataset’s composition.\nPurpose: The purpose of this metric is to visually analyze categorical data using bar plots. It is intended to evaluate the dataset’s composition by displaying the counts of each category in each categorical feature.\nTest Mechanism: The provided dataset is first checked to determine if it contains any categorical variables. If no categorical columns are found, the tool raises a ValueError. For each categorical variable in the dataset, a separate bar plot is generated. The number of occurrences for each category is calculated and displayed on the plot. If a dataset contains multiple categorical columns, multiple bar plots are produced.\nSigns of High Risk:\n\nHigh risk could occur if the categorical variables exhibit an extreme imbalance, with categories having very few instances possibly being underrepresented in the model, which could affect the model’s performance and its ability to generalize.\nAnother sign of risk is if there are too many categories in a single variable, which could lead to overfitting and make the model complex.\n\nStrengths: This metric provides a visual and intuitively understandable representation of categorical data, which aids in the analysis of variable distributions. By presenting model inputs in this way, we can easily identify imbalances or rare categories that could affect the model’s performance.\nLimitations:\n\nThis method only works with categorical data, meaning it won’t apply to numerical variables.\nIn addition, the method does not provide any informative value when there are too many categories, as the bar chart could become cluttered and hard to interpret.\nIt offers no insights into the model’s performance or precision, but rather provides a descriptive analysis of the input."
  },
  {
    "objectID": "tests/data_validation/BivariateFeaturesBarPlots.html",
    "href": "tests/data_validation/BivariateFeaturesBarPlots.html",
    "title": "BivariateFeaturesBarPlots",
    "section": "",
    "text": "BivariateFeaturesBarPlots\nGenerates visual bar plots to analyze the relationship between paired features within categorical data in the model.\nPurpose: The BivariateFeaturesBarPlots metric is intended to perform a visual analysis of categorical data within the model. The goal is to assess and understand the specific relationships between various feature pairs, while simultaneously highlighting the model’s target variable. This form of bivariate plotting is immensely beneficial in uncovering trends, correlations, patterns, or inconsistencies that may not be readily apparent within raw tabular data.\nTest Mechanism: These tests establish bar plots for each pair of features defined within the parameters. The dataset is grouped by each feature pair and then calculates the mean of the target variable within each specific grouping. Each group is represented via a bar in the plot, and the height of this bar aligns with the calculated mean. The colors assigned to these bars are based on the categorical section to which they pertain: these colors can either come from a colormap or generated anew if the total number of categories exceeds the current colormap’s scope.\nSigns of High Risk: - If any values are found missing or inconsistent within the feature pairs. - If there exist large discrepancies or irregularities between the mean values of certain categories within feature pairs. - If the parameters for feature pairs have not been specified or if they were wrongly defined.\nStrengths: - The BivariateFeaturesBarPlots provides a clear, visual comprehension of the relationships between feature pairs and the target variable. - It allows an easy comparison between different categories within feature pairs. - The metric can handle a diverse array of categorical data, enhancing its universal applicability. - It is highly customizable due to its allowance for users to define feature pairs based on their specific requirements.\nLimitations: - It can only be used with categorical data, limiting its usability with numerical or textual data. - It relies on manual input for feature pairs, which could result in the overlooking of important feature pairs if not chosen judiciously. - The generated bar plots could become overly cluttered and difficult to decipher when dealing with feature pairs with a large number of categories. - This metric only provides a visual evaluation and fails to offer any numerical or statistical measures to quantify the relationship between feature pairs."
  },
  {
    "objectID": "tests/data_validation/MissingValuesRisk.html",
    "href": "tests/data_validation/MissingValuesRisk.html",
    "title": "MissingValuesRisk",
    "section": "",
    "text": "MissingValuesRisk\nAssesses and quantifies the risk related to missing values in a dataset used for training an ML model.\nPurpose: The Missing Values Risk metric is specifically designed to assess and quantify the risk associated with missing values in the dataset used for machine learning model training. It measures two specific risks: the percentage of total data that are missing, and the percentage of all variables (columns) that contain some missing values.\nTest Mechanism: Initially, the metric calculates the total number of data points in the dataset and the count of missing values. It then inspects each variable (column) to determine how many contain at least one missing datapoint. By methodically counting missing datapoints across the entire dataset and each variable (column), it identifies the percentage of missing values in the entire dataset and the percentage of variables (columns) with such values.\nSigns of High Risk:\n\nRecord high percentages in either of the risk measures could suggest a high risk.\nIf the dataset indicates a high percentage of missing values, it might significantly undermine the model’s performance and credibility.\nIf a significant portion of variables (columns) in the dataset are missing values, this could make the model susceptible to bias and overfitting.\n\nStrengths:\n\nThe metric offers valuable insights into the readiness of a dataset for model training as missing values can heavily destabilize both the model’s performance and predictive capabilities.\nThe metric’s quantification of the risks caused by missing values allows for the use of targeted methods to manage these values correctly- either through removal, imputation, or alternative strategies.\nThe metric has the flexibility to be applied to both classification and regression assignments, maintaining its utility across a wide range of models and scenarios.\n\nLimitations:\n\nThe metric primarily identifies and quantifies the risk associated with missing values without suggesting specific mitigation strategies.\nThe metric does not ascertain whether the missing values are random or associated with an underlying issue in the stages of data collection or preprocessing.\nHowever, the identification of the presence and scale of missing data is the essential initial step towards improving data quality."
  },
  {
    "objectID": "tests/data_validation/nlp/Hashtags.html",
    "href": "tests/data_validation/nlp/Hashtags.html",
    "title": "Hashtags",
    "section": "",
    "text": "Hashtags\nAssesses hashtag frequency in a text column, highlighting usage trends and potential dataset bias or spam.\nPurpose: The Hashtags test is designed to measure the frequency of hashtags used within a given text column in a dataset. It is particularly useful for natural language processing tasks such as text classification and text summarization. The goal is to identify common trends and patterns in the use of hashtags, which can serve as critical indicators or features within a machine learning model.\nTest Mechanism: The test implements a regular expression (regex) to extract all hashtags from the specified text column. For each hashtag found, it makes a tally of its occurrences. It then outputs a list of the top N hashtags (default is 25, but customizable), sorted by their counts in descending order. The results are also visualized in a bar plot, with frequency counts on the y-axis and the corresponding hashtags on the x-axis.\nSigns of High Risk: - A low diversity in the usage of hashtags, as indicated by a few hashtags being used disproportionately more than others. - Repeated usage of one or few hashtags can be indicative of spam or a biased dataset. - If there are no or extremely few hashtags found in the dataset, it perhaps signifies that the text data does not contain structured social media data.\nStrengths: - It provides a concise visual representation of the frequency of hashtags, which can be critical for understanding trends about a particular topic in text data. - It is instrumental in tasks specifically related to social media text analytics, such as opinion analysis and trend discovery. - The test is adaptable, allowing the flexibility to determine the number of top hashtags to be analyzed.\nLimitations: - The test assumes the presence of hashtags and therefore may not be applicable for text datasets that do not contain hashtags (e.g., formal documents, scientific literature). - Language-specific limitations of hashtag formulations are not taken into account. - It does not account for typographical errors, variations, or synonyms in hashtags. - This test does not provide context or sentiment associated with the hashtags, so the information provided may have limited utility on its own."
  },
  {
    "objectID": "tests/data_validation/nlp/Mentions.html",
    "href": "tests/data_validation/nlp/Mentions.html",
    "title": "Mentions",
    "section": "",
    "text": "Mentions\nCalculates and visualizes frequencies of ‘@’ prefixed mentions in a text-based dataset for NLP model analysis.\nPurpose: This test, termed “Mentions”, is designed to gauge the quality of data in a Natural Language Processing (NLP) or text-focused Machine Learning model. The primary objective is to identify and calculate the frequency of ‘mentions’ within a chosen text column of a dataset. A ‘mention’ in this context refers to individual text elements that are prefixed by ‘@’. The output of this test reveals the most frequently mentioned entities or usernames, which can be integral for applications such as social media analyses, customer sentiment analyses, and so on.\nTest Mechanism: The test first verifies the existence of a text column in the provided dataset. It then employs a regular expression pattern to extract mentions from the text. Subsequently, the frequency of each unique mention is calculated. The test selects the most frequent mentions based on default or user-defined parameters, the default being the top 25, for representation. This process of thresholding forms the core of the test. A treemap plot visualizes the test results, where the size of each rectangle corresponds to the frequency of a particular mention.\nSigns of High Risk: - The lack of a valid text column in the dataset, which would result in the failure of the test execution. - The absence of any mentions within the text data, indicating that there might not be any text associated with @’. This situation could point towards sparse or poor-quality data, thereby hampering the model’s generalization or learning capabilities.\nStrengths: - The test is specifically optimized for text-based datasets which gives it distinct power in the context of NLP. - It enables quick identification and visually appealing representation of the predominant elements or mentions. - It can provide crucial insights about the most frequently mentioned entities or usernames.\nLimitations: - The test only recognizes mentions that are prefixed by ‘@’, hence useful textual aspects not preceded by ’@ might be ignored. - This test isn’t suited for datasets devoid of textual data. - It does not provide insights on less frequently occurring data or outliers, which means potentially significant patterns could be overlooked."
  },
  {
    "objectID": "tests/data_validation/nlp/Punctuations.html",
    "href": "tests/data_validation/nlp/Punctuations.html",
    "title": "Punctuations",
    "section": "",
    "text": "Punctuations\nAnalyzes and visualizes the frequency distribution of punctuation usage in a given text dataset.\n1. Purpose: The Punctuations Metric’s primary purpose is to analyze the frequency of punctuation usage within a given text dataset. This is often used in Natural Language Processing tasks, such as text classification and text summarization.\n2. Test Mechanism: The test begins by verifying that the input “dataset” is of the type VMDataset. Following that, a corpus is created from the dataset by splitting its text on spaces. Each unique punctuation character in the text corpus is then tallied. Then, the frequency distribution of each punctuation symbol is visualized as a bar graph, with these results being stored as Figures and associated with the main Punctuations object.\n3. Signs of High Risk:\n\nHigh risk can be indicated by the excessive or unusual frequency of specific punctuation marks, potentially denoting dubious quality, data corruption, or skewed data.\n\n4. Strengths:\n\nThe Punctuations Metric provides valuable insights into the distribution of punctuation usage in a text dataset.\nThis insight can be important in validating the quality, consistency, and nature of the data.\nIt can provide hints about the style or tonality of the text corpus. For example, frequent usage of exclamation marks may suggest a more informal and emotional context.\n\n5. Limitations:\n\nThe metric focuses solely on punctuation usage and can miss other important textual characteristics.\nIt’s important not to make general cultural or tonality assumptions based solely on punctuation distribution, since these can vary greatly across different languages and contexts.\nThe metric may be less effective with languages that use non-standard or different punctuation.\nThe visualization may lack interpretability when there are many unique punctuation marks in the dataset."
  },
  {
    "objectID": "tests/data_validation/nlp/PolarityAndSubjectivity.html",
    "href": "tests/data_validation/nlp/PolarityAndSubjectivity.html",
    "title": "PolarityAndSubjectivity",
    "section": "",
    "text": "PolarityAndSubjectivity"
  },
  {
    "objectID": "tests/data_validation/nlp/CommonWords.html",
    "href": "tests/data_validation/nlp/CommonWords.html",
    "title": "CommonWords",
    "section": "",
    "text": "CommonWords\nIdentifies and visualizes the 40 most frequent non-stopwords in a specified text column within a dataset.\nPurpose: The CommonWords metric is used to identify and visualize the most prevalent words within a specified text column of a dataset. This provides insights into the prevalent language patterns and vocabulary, especially useful in Natural Language Processing (NLP) tasks such as text classification and text summarization.\nTest Mechanism: The test methodology involves splitting the specified text column’s entries into words, collating them into a corpus, and then counting the frequency of each word using the Counter. The forty most frequently occurring non-stopwords are then visualized in a bar chart, where the x-axis represents the words, and the y-axis indicates their frequency of occurrence.\nSigns of High Risk: - A lack of distinct words within the list, or the most common words being stopwords. - Frequent occurrence of irrelevant or inappropriate words could point out a poorly curated or noisy dataset. - An error returned due to the absence of a valid Dataset object indicates high risk as the metric cannot be effectively implemented without it.\nStrengths: - The metric provides clear insights into the language features – specifically word frequency – of unstructured text data. - It can reveal prominent vocabulary and language patterns, which prove vital for feature extraction in NLP tasks. - The visualization helps in quickly capturing the patterns and understanding the data intuitively.\nLimitations: - The test disregards semantic or context-related information as it solely focuses on word frequency. - It intentionally ignores stopwords which might carry necessary significance in certain scenarios. - The applicability is limited to English language text data as English stopwords are used for filtering, hence cannot account for data in other languages. - The metric requires a valid Dataset object, indicating a dependency condition that limits its broader applicability."
  },
  {
    "objectID": "tests/data_validation/MissingValuesBarPlot.html",
    "href": "tests/data_validation/MissingValuesBarPlot.html",
    "title": "MissingValuesBarPlot",
    "section": "",
    "text": "MissingValuesBarPlot\nCreates a bar plot showcasing the percentage of missing values in each column of the dataset with risk categorization based on a user-defined threshold.\nPurpose: The ‘MissingValuesBarPlot’ metric provides a color-coded visual representation of the percentage of missing values for each column in an ML model’s dataset. The primary purpose of this metric is to easily identify and quantify missing data, which are essential steps in data preprocessing. The presence of missing data can potentially skew the model’s predictions and decrease its accuracy. Additionally, this metric uses a pre-set threshold to categorize various columns into ones that contain missing data above the threshold (high risk) and below the threshold (less risky).\nTest Mechanism: The test mechanism involves scanning each column in the input dataset and calculating the percentage of missing values. It then compares each column’s missing data percentage with the predefined threshold, categorizing columns with missing data above the threshold as high-risk. The test generates a bar plot in which columns with missing data are represented on the y-axis and their corresponding missing data percentages are displayed on the x-axis. The color of each bar reflects the missing data percentage in relation to the threshold: grey for values below the threshold and light coral for those exceeding it. The user-defined threshold is represented by a red dashed line on the plot.\nSigns of High Risk:\n\nColumns with higher percentages of missing values beyond the threshold are high-risk. These are visually represented by light coral bars on the bar plot.\n\nStrengths:\n\nHelps in quickly identifying and quantifying missing data across all columns of the dataset.\nFacilitates pattern recognition through visual representation.\nEnables customization of the level of risk tolerance via a user-defined threshold.\nSupports both classification and regression tasks, sharing its versatility.\n\nLimitations:\n\nIt only considers the quantity of missing values, not differentiating between different types of missingness (Missing completely at random - MCAR, Missing at random - MAR, Not Missing at random - NMAR).\nIt doesn’t offer insights into potential approaches for handling missing entries, such as various imputation strategies.\nThe metric does not consider possible impacts of the missing data on the model’s accuracy or precision.\nInterpretation of the findings and the next steps might require an expert understanding of the field."
  },
  {
    "objectID": "tests/data_validation/AutoMA.html",
    "href": "tests/data_validation/AutoMA.html",
    "title": "AutoMA",
    "section": "",
    "text": "AutoMA\nAutomatically selects the optimal Moving Average (MA) order for each variable in a time series dataset based on minimal BIC and AIC values.\nPurpose: The AutoMA metric serves an essential role of automated decision-making for selecting the optimal Moving Average (MA) order for every variable in a given time series dataset. The selection is dependent on the minimalization of BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion); these are established statistical tools used for model selection. Furthermore, prior to the commencement of the model fitting process, the algorithm conducts a stationarity test (Augmented Dickey-Fuller test) on each series.\nTest Mechanism: Starting off, the AutoMA algorithm checks whether the max_ma_order parameter has been provided. It consequently loops through all variables in the dataset, carrying out the Dickey-Fuller test for stationarity. For each stationary variable, it fits an ARIMA model for orders running from 0 to max_ma_order. The result is a list showcasing the BIC and AIC values of the ARIMA models based on different orders. The MA order, which yields the smallest BIC, is chosen as the ‘best MA order’ for every single variable. The final results include a table summarizing the auto MA analysis and another table listing the best MA order for each variable.\nSigns of High Risk: - When a series is non-stationary (p-value&gt;0.05 in the Dickey-Fuller test), the produced result could be inaccurate. - Any error that arises in the process of fitting the ARIMA models, especially with a higher MA order, can potentially indicate risks and might need further investigation.\nStrengths: - The metric facilitates automation in the process of selecting the MA order for time series forecasting. This significantly saves time and reduces efforts conventionally necessary for manual hyperparameter tuning. - The use of both BIC and AIC enhances the likelihood of selecting the most suitable model. - The metric ascertains the stationarity of the series prior to model fitting, thus ensuring that the underlying assumptions of the MA model are fulfilled.\nLimitations: - If the time series fails to be stationary, the metric may yield inaccurate results. Consequently, it necessitates pre-processing steps to stabilize the series before fitting the ARIMA model. - The metric adopts a rudimentary model selection process based on BIC and doesn’t consider other potential model selection strategies. Depending on the specific dataset, other strategies could be more appropriate. - The ‘max_ma_order’ parameter must be manually input which doesn’t always guarantee optimal performance, especially when configured too low. - The computation time increases with the rise in max_ma_order, hence, the metric may become computationally costly for larger values."
  },
  {
    "objectID": "tests/data_validation/RollingStatsPlot.html",
    "href": "tests/data_validation/RollingStatsPlot.html",
    "title": "RollingStatsPlot",
    "section": "",
    "text": "RollingStatsPlot\nThis test evaluates the stationarity of time series data by plotting its rolling mean and standard deviation.\nPurpose: The RollingStatsPlot metric is employed to gauge the stationarity of time series data in a given dataset. This metric specifically evaluates the rolling mean and rolling standard deviation of the dataset over a pre-specified window size. The rolling mean provides an understanding of the average trend in the data, while the rolling standard deviation gauges the volatility of the data within the window. It is critical in preparing time series data for modeling as it reveals key insights into data behavior across time.\nTest Mechanism: This mechanism is comprised of two steps. Initially, the rolling mean and standard deviation for each of the dataset’s columns are calculated over a window size, which can be user-specified or by default set to 12 data points. Then, the calculated rolling mean and standard deviation are visualized via separate plots, illustrating the trends and volatility in the dataset. A straightforward check is conducted to ensure the existence of columns in the dataset, and to verify that the given dataset has been indexed by its date and time—a necessary prerequisites for time series analysis.\nSigns of High Risk: - The presence of non-stationary patterns in either the rolling mean or the rolling standard deviation plots, which could indicate trends or seasonality in the data that may affect the performance of time series models. - Missing columns in the dataset, which would prevent the execution of this metric correctly. - The detection of NaN values in the dataset, which may need to be addressed before the metric can proceed successfully.\nStrengths: - Offers visualizations of trending behaviour and volatility within the data, facilitating a broader understanding of the dataset’s inherent characteristics. - Checks of the dataset’s integrity, such as existence of all required columns and the availability of a datetime index. - Adjusts to accommodate various window sizes, thus allowing accurate analysis of data with differing temporal granularities. - Considers each column of the data individually, thereby accommodating multi-feature datasets.\nLimitations: - For all columns, a fixed-size window is utilised. This may not accurately capture patterns in datasets where different features may require different optimal window sizes. - Requires the dataset to be indexed by date and time, hence it may not be useable for datasets without a timestamp index. - Primarily serves for data visualization as it does not facilitate any quantitative measures for stationarity, such as through statistical tests. Therefore, the interpretation is subjective and depends heavily on modeler discretion."
  },
  {
    "objectID": "tests/data_validation/UniqueRows.html",
    "href": "tests/data_validation/UniqueRows.html",
    "title": "UniqueRows",
    "section": "",
    "text": "UniqueRows\nVerifies the diversity of the dataset by ensuring that the count of unique rows exceeds a prescribed threshold.\nPurpose: The UniqueRows test is designed to gauge the quality of the data supplied to the machine learning model by verifying that the count of distinct rows in the dataset exceeds a specific threshold, thereby ensuring a varied collection of data. Diversity in data is essential for training an unbiased and robust model that excels when faced with novel data.\nTest Mechanism: The testing process starts with calculating the total number of rows in the dataset. Subsequently, the count of unique rows is determined for each column in the dataset. If the percentage of unique rows (calculated as the ratio of unique rows to the overall row count) is less than the prescribed minimum percentage threshold given as a function parameter, the test is passed. The results are cached and a final pass or fail verdict is given based on whether all columns have successfully passed the test.\nSigns of High Risk: - A lack of diversity in data columns, demonstrated by a count of unique rows that falls short of the preset minimum percentage threshold, is indicative of high risk. - This lack of variety in the data signals potential issues with data quality, possibly leading to overfitting in the model and issues with generalization, thus posing a significant risk.\nStrengths: - The UniqueRows test is efficient in evaluating the data’s diversity across each information column in the dataset. - This test provides a quick, systematic method to assess data quality based on uniqueness, which can be pivotal in developing effective and unbiased machine learning models.\nLimitations: - A limitation of the UniqueRows test is its assumption that the data’s quality is directly proportionate to its uniqueness, which may not always hold true. There might be contexts where certain non-unique rows are essential and should not be overlooked. - The test does not consider the relative ‘importance’ of each column in predicting the output, treating all columns equally. - This test may not be suitable or useful for categorical variables, where the count of unique categories is inherently limited."
  },
  {
    "objectID": "tests/data_validation/FeatureTargetCorrelationPlot.html",
    "href": "tests/data_validation/FeatureTargetCorrelationPlot.html",
    "title": "FeatureTargetCorrelationPlot",
    "section": "",
    "text": "FeatureTargetCorrelationPlot\nVisualizes the correlation between input features and model’s target output in a color-coded horizontal bar plot.\nPurpose: This test is designed to graphically illustrate the correlations between distinct input features and the target output of a Machine Learning model. Understanding how each feature influences the model’s predictions is crucial - a higher correlation indicates stronger influence of the feature on the target variable. This correlation study is especially advantageous during feature selection and for comprehending the model’s operation.\nTest Mechanism: This FeatureTargetCorrelationPlot test computes and presents the correlations between the features and the target variable using a specific dataset. These correlations are calculated, graphically represented in a horizontal bar plot, and color-coded based on the strength of the correlation. A hovering template can also be utilized for informative tooltips. It is possible to specify the features to be analyzed and adjust the graph’s height according to need.\nSigns of High Risk: - There are no strong correlations (either positive or negative) between features and the target variable. This could suggest high risk as the supplied features do not appear to significantly impact the prediction output. - The presence of duplicated correlation values might hint at redundancy in the feature set.\nStrengths: - Provides visual assistance to interpreting correlations more effectively. - Gives a clear and simple tour of how each feature affects the model’s target variable. - Beneficial for feature selection and grasping the model’s prediction nature. - Precise correlation values for each feature are offered by the hover template, contributing to a granular-level comprehension.\nLimitations: - The test only accepts numerical data, meaning variables of other types need to be prepared beforehand. - The plot assumes all correlations to be linear, thus non-linear relationships might not be captured effectively. - Not apt for models that employ complex feature interactions, like Decision Trees or Neural Networks, as the test may not accurately reflect their importance."
  },
  {
    "objectID": "tests/data_validation/ANOVAOneWayTable.html",
    "href": "tests/data_validation/ANOVAOneWayTable.html",
    "title": "ANOVAOneWayTable",
    "section": "",
    "text": "ANOVAOneWayTable\nApplies one-way ANOVA (Analysis of Variance) to identify statistically significant numerical features in the dataset.\nPurpose: The ANOVA (Analysis of Variance) One-Way Table metric is utilized to determine whether the mean of numerical variables differs across different groups identified by target or categorical variables. Its primary purpose is to scrutinize the significant impact of categorical variables on numerical ones. This method proves essential in identifying statistically significant features corresponding to the target variable present in the dataset.\nTest Mechanism: The testing mechanism involves the ANOVA F-test’s performance on each numerical variable against the target. If no specific features are mentioned, all numerical features are tested. A p-value is produced for each test and compared against a certain threshold (default being 0.05 if not specified). If the p-value is less than or equal to this threshold, the feature is marked as ‘Pass’, indicating significant mean difference across the groups. Otherwise, it’s marked as ‘Fail’. The test produces a DataFrame that includes variable name, F statistic value, p-value, threshold, and pass/fail status for every numerical variable.\nSigns of High Risk: - A large number of ‘Fail’ results in the ANOVA F-test could signify high risk or underperformance in the model. This issue may arise when multiple numerical variables in the dataset don’t exhibit any significant difference across the target variable groups. - Features with high p-values also indicate a high risk as they imply a greater chance of obtaining observed data given that the null hypothesis is true.\nStrengths: - The ANOVA One Way Table is highly efficient in identifying statistically significant features by simultaneously comparing group means. - Its flexibility allows the testing of all numerical features in the dataset when no specific ones are mentioned. - This metric provides a convenient method to measure the statistical significance of numerical variables and assists in selecting those variables influencing the classifier’s predictions considerably.\nLimitations: - This metric assumes that the data is normally distributed, which may not always be the case leading to erroneous test results. - The sensitivity of the F-test to variance changes may hinder this metric’s effectiveness, especially for datasets with high variance. - The ANOVA One Way test does not specify which group means differ statistically from others; it strictly asserts the existence of a difference. - The metric fails to provide insights into variable interactions, and significant effects due to these interactions could easily be overlooked."
  },
  {
    "objectID": "tests/data_validation/MissingValues.html",
    "href": "tests/data_validation/MissingValues.html",
    "title": "MissingValues",
    "section": "",
    "text": "MissingValues\nEvaluates dataset quality by ensuring missing value ratio across all features does not exceed a set threshold.\nPurpose: This test is designed to evaluate the quality of a dataset by measuring the number of missing values across all features. The objective is to ensure that the ratio of missing data to total data is less than a predefined threshold, defaulting to 1, to maintain the data quality necessary for reliable predictive strength in a machine learning model.\nTest Mechanism: The mechanism for this test involves iterating through each column of the dataset, counting missing values (represented as NaNs), and calculating the percentage they represent against the total number of rows. The test then checks if these missing value counts are less than the predefined min_threshold. The results are shown in a table summarizing each column, the number of missing values, the percentage of missing values in each column, and a Pass/Fail status based on the threshold comparison.\nSigns of High Risk: - When the number of missing values in any column exceeds the min_threshold value, it indicates a high risk. - A high risk is also flagged when missing values are present across many columns. In both instances, the test would return a “Fail” mark.\nStrengths: - The test offers a quick and granular identification of missing data across each feature in the dataset. - It provides an effective, straightforward means of maintaining data quality, which is vital for constructing efficient machine learning models.\nLimitations: - Even though the test can efficiently identify missing values, it does not suggest the root causes of these missing values or recommend ways to impute or handle them. - The test might overlook features with a significant amount of missing data, but still less than the min_threshold. This could impact the model, especially if min_threshold is set too high. - The test does not account for data encoded as values (like “-999” or “None”), which might not technically classify as missing but could bear similar implications."
  },
  {
    "objectID": "tests/data_validation/ChiSquaredFeaturesTable.html",
    "href": "tests/data_validation/ChiSquaredFeaturesTable.html",
    "title": "ChiSquaredFeaturesTable",
    "section": "",
    "text": "ChiSquaredFeaturesTable\nExecutes Chi-Squared test for each categorical feature against a target column to assess significant association.\nPurpose: The ChiSquaredFeaturesTable metric is used to carry out a Chi-Squared test of independence for each categorical feature variable against a designated target column. The primary goal is to determine if a significant association exists between the categorical features and the target variable. This method typically finds its use in the context of Model Risk Management to understand feature relevance and detect potential bias in a classification model.\nTest Mechanism: The testing process generates a contingency table for each categorical variable and the target variable, after which a Chi-Squared test is performed. Using this approach, the Chi-Squared statistic and the p-value for each feature are calculated. The p-value threshold is a modifiable parameter, and a test will qualify as passed if the p-value is less than or equal to this threshold. If not, the test is labeled as failed. The outcome for each feature - comprising the variable name, Chi-squared statistic, p-value, threshold, and pass/fail status - is incorporated into a final summary table.\nSigns of High Risk: - High p-values (greater than the set threshold) for specific variables could indicate a high risk. - These high p-values allude to the absence of a statistically significant relationship between the feature and the target variables, resulting in a ‘Fail’ status. - A categorical feature lacking a relevant association with the target variable could be a warning that the machine learning model might not be performing optimally.\nStrengths: - The test allows for a comprehensive understanding of the interaction between a model’s input features and the target output, thus validating the relevance of categorical features. - It also produces an unambiguous ‘Pass/Fail’ output for each categorical feature. - The opportunity to adjust the p-value threshold contributes to flexibility in accommodating different statistical standards.\nLimitations: - The metric presupposes that data is tabular and categorical, which may not always be the case with all datasets. - It is distinctively designed for classification tasks, hence unsuitable for regression scenarios. - The Chi-squared test, akin to any hypothesis testing-based test, cannot identify causal relationships, but only associations. - Furthermore, the test hinges on an adjustable p-value threshold, and varying threshold selections might lead to different conclusions regarding feature relevance."
  },
  {
    "objectID": "tests/data_validation/TabularNumericalHistograms.html",
    "href": "tests/data_validation/TabularNumericalHistograms.html",
    "title": "TabularNumericalHistograms",
    "section": "",
    "text": "TabularNumericalHistograms\nGenerates histograms for each numerical feature in a dataset to provide visual insights into data distribution and detect potential issues.\nPurpose: The purpose of this test is to provide visual analysis of numerical data through the generation of histograms for each numerical feature in the dataset. Histograms aid in the exploratory analysis of data, offering insight into the distribution of the data, skewness, presence of outliers, and central tendencies. It helps in understanding if the inputs to the model are normally distributed which is a common assumption in many machine learning algorithms.\nTest Mechanism: This test scans the provided dataset and extracts all the numerical columns. For each numerical column, it constructs a histogram using plotly, with 50 bins. The deployment of histograms offers a robust visual aid, ensuring unruffled identification and understanding of numerical data distribution patterns.\nSigns of High Risk:\n\nA high degree of skewness\nUnexpected data distributions\nExistence of extreme outliers in the histograms These may indicate issues with the data that the model is receiving. If data for a numerical feature is expected to follow a certain distribution (like normal distribution) but does not, it could lead to sub-par performance by the model. As such these instances should be treated as high-risk indicators.\n\nStrengths:\n\nThis test provides a simple, easy-to-interpret visualization of how data for each numerical attribute is distributed.\nIt can help detect skewed values and outliers, that could potentially harm the AI model’s performance.\nIt can be applied to large datasets and multiple numerical variables conveniently.\n\nLimitations:\n\nThis test only works with numerical data, thus ignoring non-numerical or categorical data.\nIt does not analyze relationships between different features, only the individual feature distributions.\nIt is a univariate analysis, and may miss patterns or anomalies that only appear when considering multiple variables together.\nIt does not provide any insight into how these features affect the output of the model; it is purely an input analysis tool."
  },
  {
    "objectID": "about/data-privacy-policy.html",
    "href": "about/data-privacy-policy.html",
    "title": "Data privacy policy",
    "section": "",
    "text": "This page outlines ValidMind’s data privacy policy, explaining how we protect your personal information. Our goal is to provide transparency about our data handling practices and to demonstrate our commitment to protecting your privacy and data security."
  },
  {
    "objectID": "about/data-privacy-policy.html#what-is-validminds-data-privacy-policy",
    "href": "about/data-privacy-policy.html#what-is-validminds-data-privacy-policy",
    "title": "Data privacy policy",
    "section": "What is ValidMind’s data privacy policy?",
    "text": "What is ValidMind’s data privacy policy?\nThe key points of our data privacy policy include:\n\nNo personal identifiable information in documentation: When the ValidMind Developer Framework generates documentation, it ensures that no personally identifiable information (PII) is included. This practice is a critical part of our commitment to protecting your privacy and maintaining the confidentiality of your data.\nNo storage of customer data: ValidMind does not retain any customer datasets or models. This policy is in place in order to protect your data privacy and security. By not storing this information, ValidMind minimizes the risk of unauthorized access or data breaches.\n\nWe believe it is important for users of ValidMind’s products to understand these practices as they reflect our dedication to data security and privacy.\n\n\n\n\n\n\nValidMind does NOT:\n\nInclude any personal identifiable information (PII) when generating documentation reports.\nStore any customer datasets or models."
  },
  {
    "objectID": "about/data-privacy-policy.html#do-you-comply-with-the-soc-2-security-standard",
    "href": "about/data-privacy-policy.html#do-you-comply-with-the-soc-2-security-standard",
    "title": "Data privacy policy",
    "section": "Do you comply with the SOC 2 security standard?",
    "text": "Do you comply with the SOC 2 security standard?\nService Organization Control 2 (SOC 2) is a type of audit report that evaluates the security and privacy controls of a service organization, such as a software-as-a-service (SaaS) vendor like ValidMind. The report provides assurance to customers that an organization has implemented effective security and privacy controls to protect sensitive data.\nValidMind’s security and privacy controls are designed to align with the stringent requirements of the SOC 2 standard. This compliance means that ValidMind has established and consistently maintains a set of security measures and protocols that meet or exceed the benchmark set by SOC 2. We also regularly review and update these controls to ensure that they stay current with evolving security threats and regulatory requirements."
  },
  {
    "objectID": "about/data-privacy-policy.html#do-you-offer-additional-data-privacy-options",
    "href": "about/data-privacy-policy.html#do-you-offer-additional-data-privacy-options",
    "title": "Data privacy policy",
    "section": "Do you offer additional data privacy options?",
    "text": "Do you offer additional data privacy options?\nValidMind’s platform is a secure, multi-tenant solution that can be hosted on Amazon Web Services (AWS), Microsoft Azure Cloud (Azure), or Google Cloud Platform (GCP). For organizations that require a stricter trust model and the highest level of security, such as financial services organizations handling highly sensitive data, ValidMind also offers a Virtual Private ValidMind (VPV) option to host our solution in a dedicated single-tenant cloud instance.\nThe Virtual Private ValidMind option provides all the features and services of other editions of our products, but hosted within a separate environment that is isolated from other ValidMind accounts. VPV accounts do not share resources with non-VPV accounts.\nAccess to any edition is available through AWS PrivateLink, Azure Private Link, or GCP Private Service Connect, all of which provide private connectivity between ValidMind and your on-premises network without exposing your traffic to the public internet."
  },
  {
    "objectID": "about/data-privacy-policy.html#what-model-artifacts-are-imported-into-documentation",
    "href": "about/data-privacy-policy.html#what-model-artifacts-are-imported-into-documentation",
    "title": "Data privacy policy",
    "section": "What model artifacts are imported into documentation?",
    "text": "What model artifacts are imported into documentation?\nWhen you generate documentation or run tests, ValidMind imports the following artifacts into the documentation via our SaaS API endpoint integration:\n\n\nMetadata about datasets and models, used to look up programmatic documentation content, such as the stored definition for common logistic regression limitations when a logistic regression model has been passed to the ValidMind test suite to be run.\nQuality and performance metrics collected from datasets and models.\nOutput from tests and test suites that have been run.\nImages, plots, visuals that were generated as part of extracting metrics and running tests.\n\nThe ValidMind Developer Framework does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "about/data-privacy-policy.html#whats-next",
    "href": "about/data-privacy-policy.html#whats-next",
    "title": "Data privacy policy",
    "section": "What’s next",
    "text": "What’s next\n\nConfigure AWS PrivateLink"
  },
  {
    "objectID": "about/overview-model-documentation.html",
    "href": "about/overview-model-documentation.html",
    "title": "Automated model testing & documentation",
    "section": "",
    "text": "The ValidMind Developer Framework is a developer framework and documentation engine designed to streamline the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence/machine learning models, and large language models (LLMs). It offers model developers a systematic approach to documenting and testing risk models with repeatability and consistency, ensuring alignment with regulatory and compliance standards.\n\n\n\n\n\n\nThe developer framework consists of a client-side library, API integration for models and testing, and validation tests that streamline the model development process. Implemented as a series of independent libraries in Python and R, our framework ensures compatibility and flexibility with diverse sets of developer environments and requirements.\nWith the developer framework, you can:\n\nAutomate documentation — Add comprehensive documentation as metadata while you build models to be shared with model validators, streamlining and speeding up the process.\nRun test suites — Identify potential risks for a diverse range of statistical and AI/LLM/ML models by assessing data quality, model outcomes, robustness, and explainability.\nIntegrate with your development environment — Seamlessly incorporate the framework into your existing model development environment, connecting to your existing model code and data sets.\nUpload documentation data — Send qualitative and quantitative test data to the AI risk platform to generate the model documentation for review and approval, fostering effective collaboration with model reviewers and validators."
  },
  {
    "objectID": "about/overview-model-documentation.html#the-var-vm_framework",
    "href": "about/overview-model-documentation.html#the-var-vm_framework",
    "title": "Automated model testing & documentation",
    "section": "",
    "text": "The ValidMind Developer Framework is a developer framework and documentation engine designed to streamline the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence/machine learning models, and large language models (LLMs). It offers model developers a systematic approach to documenting and testing risk models with repeatability and consistency, ensuring alignment with regulatory and compliance standards.\n\n\n\n\n\n\nThe developer framework consists of a client-side library, API integration for models and testing, and validation tests that streamline the model development process. Implemented as a series of independent libraries in Python and R, our framework ensures compatibility and flexibility with diverse sets of developer environments and requirements.\nWith the developer framework, you can:\n\nAutomate documentation — Add comprehensive documentation as metadata while you build models to be shared with model validators, streamlining and speeding up the process.\nRun test suites — Identify potential risks for a diverse range of statistical and AI/LLM/ML models by assessing data quality, model outcomes, robustness, and explainability.\nIntegrate with your development environment — Seamlessly incorporate the framework into your existing model development environment, connecting to your existing model code and data sets.\nUpload documentation data — Send qualitative and quantitative test data to the AI risk platform to generate the model documentation for review and approval, fostering effective collaboration with model reviewers and validators."
  },
  {
    "objectID": "about/overview-model-documentation.html#simple-installation",
    "href": "about/overview-model-documentation.html#simple-installation",
    "title": "Automated model testing & documentation",
    "section": "Simple installation",
    "text": "Simple installation\nInstall the developer framework with: pip install validmind"
  },
  {
    "objectID": "about/overview-model-documentation.html#docs-as-code",
    "href": "about/overview-model-documentation.html#docs-as-code",
    "title": "Automated model testing & documentation",
    "section": "Docs-as-code",
    "text": "Docs-as-code\n\n\nWhat the developer framework offers:\n\nGenerates documentation artifacts utilizing the context of the model and dataset, the model’s metadata, and the chosen documentation template.\nCan be easily imported into your local model development environment. The supported platforms include Python and R.\nDual-licensed: the framework is available as open-source under AGPL v3 license and also with a commercial software license.\n\n\n\nimport validmind as vm\n\nvm.init(project=\"PROJECT_IDENTIFIER\")\nvm_dataset = vm. log_dataset(\n      df,\n      \"training\",\n      targets=targets,\n)\nvm. run_dataset_tests(df, vm_dataset=vm_dataset)\nvm. Log_model (model)\nvm. log_training_metrics (model, x_train, y_train)\nvm. run_model_tests (model, x_test, y_test)\n\n\nHow the developer framework works:\n\nThe tests and functions are executed automatically, following pre-configured templates tailored for specific model use cases. This ensures that minimum documentation requirements are consistently fulfilled.\nThe framework integrates with ETL/data processing pipelines using connector interfaces. This enables the extraction of relationships between raw data sources and their corresponding post-processed datasets, such as those preloaded session instances received from platforms like Spark and Snowflake."
  },
  {
    "objectID": "about/overview-model-documentation.html#extensible-by-design",
    "href": "about/overview-model-documentation.html#extensible-by-design",
    "title": "Automated model testing & documentation",
    "section": "Extensible by design",
    "text": "Extensible by design\n\n\nIn Financial Services, our platform supports various model types, including:\n\nTraditional machine learning models (ML) such as tree-based models and neural network models.\nNatural language processing models (NLP) for text analysis and understanding.\nLarge language models (LLMs) in beta testing phase, offering advanced language capabilities.\nTraditional statistical models like Ordinary Least Squares (OLS) regression, Logistic regression, Time Series models, and more.\n\nRead more …\n\n\nOur platform is designed to be highly extensible to cater to our customers’ specific requirements. You can expand its functionality in the following ways:\n\nYou can easily add support for new models and data types by defining new classes within the framework. We provide templates to guide you through this process. Read more …\nTo include custom tests in the library, you can define new functions. We offer templates to help you create these custom tests. Read more …\nYou have the flexibility to integrate third-party test libraries seamlessly. These libraries can be hosted either locally within your infrastructure or remotely, for example, on GitHub. Leverage additional testing capabilities and resources as needed. Read more …"
  },
  {
    "objectID": "about/overview-model-documentation.html#api-integration",
    "href": "about/overview-model-documentation.html#api-integration",
    "title": "Automated model testing & documentation",
    "section": "API integration",
    "text": "API integration\n\n\nValidMind imports the following artifacts into the documentation via SaaS API endpoint integration:\n\nMetadata about datasets and models, used to lookup programmatic documentation content, such as the stored definition for common logistic regression limitations when a logistic regression model has been passed to the ValidMind test plan to be run.\nQuality and performance metrics collected from datasets and models.\nOutput from test and test suites that have been run.\nImages, plots, visuals that were generated as part of extracting metrics and running tests.\n\n\n\n\n\n\n\n\n\n\n\n\nValidMind does NOT:\n\nSend any personal identifiable information (PII) when generating documentation reports.\nStore any customer datasets or models."
  },
  {
    "objectID": "about/overview-model-documentation.html#ready-to-try-out-validmind",
    "href": "about/overview-model-documentation.html#ready-to-try-out-validmind",
    "title": "Automated model testing & documentation",
    "section": "Ready to try out ValidMind?",
    "text": "Ready to try out ValidMind?\nOur QuickStart is the quickest and easiest way to try out our product features."
  },
  {
    "objectID": "about/license-agreement.html",
    "href": "about/license-agreement.html",
    "title": "SOFTWARE LICENSE AGREEMENT",
    "section": "",
    "text": "IMPORTANT: READ THIS SOFTWARE LICENSE AGREEMENT (THIS “AGREEMENT”) CAREFULLY BEFORE USING THE SOFTWARE. BY USING THE SOFTWARE, YOU ARE AGREEING TO BE BOUND BY THE TERMS OF THIS LICENSE AGREEMENT:\nAs between you and Licensor, this software and associated media, printed materials, and “online” or electronic documentation files (collectively, the “Software”), is the proprietary information of ValidMind Inc. (“Licensor”), and is protected under copyright and other intellectual property laws.\nNo part of this Software may be copied, reproduced, distributed, republished, displayed, posted or transmitted in any form or by any means, including, but not limited to, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of Licensor.\nBy installing, copying, or otherwise using the Software, you as a user of the Software (“you”) agree to be bound by the terms of this Agreement. If you do not agree to the terms of this Agreement, do not install or use the Software.\n\nLICENSE GRANT. Subject to the terms and conditions of this Agreement, Licensor grants you a non-exclusive, non-transferable, limited license (without the right to sublicense) during the term of this Agreement to install and use the Software only in object code or byte code form for the sole purpose of testing its functionality.\nOWNERSHIP. The Software is owned by Licensor and its licensors and is protected by copyright laws and international copyright treaties, as well as other intellectual property laws and treaties. As between you and Licensor, Licensor retains all right, title, and interest in and to the Software, including all intellectual property rights. This Agreement does not transfer any ownership of the Software to you.\nRESTRICTIONS. You will not use the Software for any commercial, production, or operational purposes. You will not (a) modify, adapt, translate, reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code of the Software including (without limitation) for the purpose of obtaining unauthorized access to the Software, (b) modify, translate, copy, or create derivative works based on the Software, (c) use the Software to create or develop a competitive product or service, (d) circumvent, remove, alter, or thwart any technological measure or content protections of the Software, (e) distribute, sublicense, rent, lease, or lend the Software to any third party, (f) remove or alter any copyright, trademark, or proprietary rights notice contained in the Software or (g) use the Software for any purpose except as expressly permitted under this Agreement.\nCONFIDENTIALITY. You agree to safeguard the Software and its related materials with the same care and precaution as you use to protect your own proprietary information and trade secrets, but in no event less than a reasonable degree of care so that no unauthorized use is made of them and no disclosure of any part of their contents is made to anyone other than your employees, agents or consultants whose duties reasonably require such disclosure, or as necessary in the ordinary course of business. You shall make all such persons fully aware of their responsibility to fulfill your obligations under this Agreement and agree to be responsible for any breach of this Agreement by such persons. You agree to promptly notify Licensor if you obtain information as to any unauthorized possession, use or disclosure of the Software by any person or entity, and further agrees to cooperate with Licensor in protecting Licensor’s proprietary rights. Upon the request of Licensor, copies and embodiments of the Software and its related materials shall be promptly returned to Licensor by you or destroyed by you, and you agree to certify such destruction in writing.\nTERM & TERMINATION. This Agreement is effective until terminated. The Licensee may terminate this Agreement at any time by destroying all copies of the Software and its related materials. The Licensor may terminate this License if you fail to comply with any term or condition of this Agreement. Upon termination, you must destroy all copies of the Software and its related materials in your possession.\nDISCLAIMER OF WARRANTY. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.\nLIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT SHALL LICENSOR BE LIABLE FOR ANY (A) INDIRECT, PUNITIVE, EXEMPLARY, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES (INCLUDING LOST PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR INABILITY TO USE THE SOFTWARE, EVEN IF LICENSEE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR (B) ANY AMOUNTS IN EXCESS OF THE AMOUNT PAID BY YOU FOR THE SOFTWARE.\nINDEMNIFICATION. You agree to indemnify and hold Licensor harmless from and against all loss, cost, expense or liability (including reasonable attorney’s fees) (i) arising out of a claim by a third party against Licensor based upon your use of the Software, or (ii) related to, or associated with your customizations, updates and/or corrections to the Software. Licensor shall have no liability to you for any damage sustained by you as a result of your use of the Software, whether such damages would arise as a result of breach of contract, tort or otherwise.\nGOVERNING LAW. This Agreement and any dispute arising hereunder shall be governed by and construed in accordance with the laws of the State of California and the federal laws of the United States of America, without giving effect to any principles of conflicts of law.\nENTIRE AGREEMENT. This Agreement constitutes the entire agreement between you and Licensor with respect to the Software and supersedes all prior or contemporaneous communications and proposals, whether oral or written, between you and Licensor.\nASSIGNMENT. You will not assign your rights and obligations under this Agreement without prior written consent of the Licensor. Licensor may freely assign its rights and obligations without your consent.\nMISCELLANEOUS. If any provision of this Agreement is found to be unenforceable or invalid, that provision will be limited or eliminated to the minimum extent necessary so that this Agreement will otherwise remain in full force and effect and enforceable. Without limiting anything herein, and except for your payment obligations, neither party shall have any liability for any failure or delay resulting from any condition beyond the reasonable control of such party, including but not limited to governmental action or acts of terrorism, earthquake or other acts of God, labor conditions, epidemics, pandemics, and power failures. For all purposes under this Agreement each party shall be and act as an independent contractor and shall not bind nor attempt to bind the other to any contract. Any notices in connection with this Agreement will be in writing.\n\nBy installing or using the Software, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.\nCopyright © 2023 ValidMind Inc. All rights reserved."
  },
  {
    "objectID": "about/overview-model-risk-management.html",
    "href": "about/overview-model-risk-management.html",
    "title": "Model risk management & governance",
    "section": "",
    "text": "Our ValidMind model risk management platform offers an integrated platform to manage validation reports, track findings, and report on model risk compliance across your model portfolio. Its main purpose is to enable your organization to monitor and manage models effectively, focusing on mitigating risks, maintaining governance, and ensuring compliance throughout the entire enterprise."
  },
  {
    "objectID": "about/overview-model-risk-management.html#the-var-vm_risk",
    "href": "about/overview-model-risk-management.html#the-var-vm_risk",
    "title": "Model risk management & governance",
    "section": "The ValidMind AI risk platform",
    "text": "The ValidMind AI risk platform\nThe ValidMind Platform UI provides a comprehensive suite of tools, guidelines, and best practices. You use the platform to review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements.\n\n\n\n\n\n\nThe platform employs a multi-tenant architecture, hosting the cloud-based user interface, APIs, databases, and internal services. The design ensures efficient resource utilization and offers a highly scalable solution for organizations of varying sizes.\nWith the ValidMind Platform, you can:\n\nTrack your model inventory — Manage the model lifecycle, track the workflow status for models, plan for upcoming validation dates, and more.\nWork on validation projects — Collaborate with developers and validators to review documentation, add findings, keep track of review statuses, and generate validation reports.\nConfigure workflows — Set up ValidMind to follow your existing model risk management processes, manage statuses for different parts of the workflow, and get an end-to-end view of workflows and who is involved.\nUse, create, or edit tests, test suites, and templates — Create and/or configure required validation tests, test suites, and documentation templates for specific model use cases, tailoring it to your own specific needs.\nIntegrate with your stack — Import and export model documentation and validation reports."
  },
  {
    "objectID": "about/overview-model-risk-management.html#regulatory-requirements",
    "href": "about/overview-model-risk-management.html#regulatory-requirements",
    "title": "Model risk management & governance",
    "section": "Regulatory requirements",
    "text": "Regulatory requirements\nValidMind’s platform is designed to cater to the regulatory compliance and model risk management (MRM) requirements of financial institutions, facilitating enhanced compliance with government regulations, policies concerning MRM, and emerging legislations addressing AI model risk, including risks associated with the use of large language models (LLMs).\nExamples of regulations or policies include:\n\n\nSR 11-7: Guidance on Model Risk Management\nThe Supervisory Guidance on model risk management issued by the Board of Governors of the Federal Reserve System and the Office of the Comptroller of the Currency in the United States in 2011. It provides comprehensive guidance to financial institutions on developing and maintaining a robust model risk management framework, covering aspects like model development, implementation, use, and validation. SR 11-7 is widely recognized and has become a benchmark in the industry for model risk management practices.\n\n\n \n\n\nSR 11-7 outlines these core requirements:\n\nModel Risk Management\n\n\nIdentify and mitigate risks associated with incorrect or inappropriate model usage, outputs, or implementation errors.\nEncourage “effective challenge” to identify model limitations and propose necessary changes.\nConsider materiality in model risk management based on the extent of model usage and its impact on the organization’s financial condition.\n\n\nModel Development, Implementation, and Use\n\n\nDevelop with a clear statement of purpose, sound design, theory, and logic.\nAssess rigorously data quality and relevance, robust methodologies, and appropriate documentation.\nTest to ensure accuracy, robustness, stability, and to evaluate limitations and assumptions.\n\n\nModel Validation\n\n\nBe an integral part for managing model risk, ensuring models perform as intended.\nIdentify and address potential errors or misuses.\n\n\nGovernance, Policies, and Controls\n\n\nEstablish a sound governance framework to oversee model risk management.\nImplement policies and controls for appropriate use and validation of models.\n\n\n\nThe regulation also mandates ongoing monitoring and periodic reviews to ensure models remain valid and effective.\nRead more …\n\n\n \n\n\nSS1/23 – Model Risk Management Principles for Banks\nA policy issued by the Prudential Regulation Authority (PRA) in the UK. It encapsulates the final model risk management principles following feedback on the earlier consultation paper CP6/22. The statement provides guidelines for banks in the UK on managing model risk effectively, with particular emphasis on strategic planning and technical capabilities. It outlines principles and amendments, like clarifications on model complexity factors, senior management function responsibilities, and inclusion of dynamic adjustments in model change management, aiming to standardize MRM practices across UK banks and foster the safe adoption of emerging technologies, such as machine learning, artificial intelligence, and large language models (LLMs).\n\n\nSS1/23 outlines these core principles:\n\nModel Identification and Model Risk Classification\n\n\nEnsure a structured approach to accurately identify and categorize models within the model risk management (MRM) framework.\nFacilitate the proper management and oversight of models, aiding in the alignment of model risk management efforts with organizational risks and objectives.\n\n\nGovernance\n\n\nEstablish a structured oversight mechanism for effective model risk management, delineating clear responsibilities and authorities.\nEnsure accountability, transparency, and effective communication within the organization regarding model risks and controls.\n\n\nModel Development, Implementation, and Use\n\n\nEmphasize the correct development, deployment, and utilization of models as per the guidelines laid down in the MRM framework.\nEnsure models are developed and utilized in a manner consistent with their intended purposes and within acceptable risk boundaries.\n\n\nIndependent Model Validation\n\n\nStress the importance of independent validation to ascertain model performance, accuracy, and identify potential issues.\nProvide an objective assessment of models to ensure they are functioning as intended and to identify any potential areas of improvement or correction.\n\n\nModel Risk Mitigants\n\n\nUnderline the necessity for measures to mitigate risks associated with model use, including the identification and implementation of controls.\nHelp in reducing the potential adverse impact of model risks on the organization’s financial condition, reputation, and regulatory compliance.\n\n\n\nThe regulation encourages a proportionate application of these principles based on the size and complexity of the institution.\nRead more …\n\nAround the globe\nOther, similar guidelines and policies that our platform is designed to help you with include:\n\n\nGuideline-E23: Enterprise-Wide Model Risk Management for Deposit-Taking Institutions\nIssued by the Office of the Superintendent of Financial Institutions (OSFI) in Canada, it outlines minimum prudent practices for model development, review, approval, use, and modification​​.\nRead more …\n\n\nPrinciples for Model Risk Management\nIssued by the Financial Services Agency (FSA) in Japan in June 2021, this document was finalized after a consultation period and outlines principles for managing model risk​​​.\nRead more …\n\n\n\n\nMeeting regulatory requirements with ValidMind\nValidMind, as a robust tool for implementing Model Risk Management (MRM) best practices, including the three lines of defense, significantly aids organizations in adhering to the regulatory guidelines set forth by SR 11:7 and SS1/23.\n\n\n\nFirst line of defense — model developers\n\nValidMind offers a suite of tools for model developers, facilitating thorough documentation and rigorous testing of models, aligning with the regulatory expectations of both SR 11:7 and SS1/23, particularly for models under regulatory purview.\n\nSecond line of defense — model validators\n\nThe platform empowers model validators with the ability to independently validate models ensuring adherence to the organization’s MRM principles throughout the model lifecycle, a core requirement of these regulations.\n\nThird line of defense — auditors\n\nEnabling internal and external audits provides an independent and objective assurance to the organization by assessing the effectiveness and efficiency of controls within the model risk management framework. It evaluates how well the first and second lines of defense are functioning, ensuring adherence to regulatory and organizational standards, thereby promoting a robust model risk management environment.\n\nModel inventory\n\nThe Model Inventory feature encapsulates a centralized repository for all models, aiding in streamlined tracking, management, and monitoring, simplifying compliance with the inventory mandates specified in SR 11:7 and SS1/23.\n\nLifecycle management and custom workflows\n\nValidMind’s capabilities extend to effective model lifecycle management through configurable workflows. This structured approach to managing model risks across various lifecycle stages significantly aids in meeting the rigorous management and oversight expectations set by SR 11:7 and SS1/23.\n\n\n\n\n\n\nModel documentation automation\n\nBy automating model documentation through configurable templates and test plans, ValidMind ensures consistent and accurate documentation capture, directly aligning with the documentation standards stipulated in these regulatory guidelines.\n\nModel validation and approval\n\nWith automated validation features and comprehensive risk assessment tools, ValidMind aligns with the effective validation criteria and thorough risk evaluation mandates of SR 11:7 and SS1/23.\n\nCommunication and tracking\n\nThe built-in communication and tracking functionality of ValidMind facilitates seamless collaboration and understanding among stakeholders regarding model usage, limitations, and risks, fostering a collaborative environment as encouraged by these regulations.\n\n\n\n\nBy integrating these features, ValidMind provides a comprehensive platform that not only simplifies the path to compliance with SR 11:7 and SS1/23 but also embeds a culture of rigorous and transparent model risk management within the organization."
  },
  {
    "objectID": "about/overview-model-risk-management.html#ready-to-try-out-validmind",
    "href": "about/overview-model-risk-management.html#ready-to-try-out-validmind",
    "title": "Model risk management & governance",
    "section": "Ready to try out ValidMind?",
    "text": "Ready to try out ValidMind?\nOur QuickStart is the quickest and easiest way to try out our product features."
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html",
    "href": "notebooks/quickstart_customer_churn_full_suite.html",
    "title": "Quickstart for model documentation",
    "section": "",
    "text": "Welcome! Let’s get you started with the basic process of documenting models with ValidMind.\nYou will learn how to initialize the ValidMind Developer Framework, load a sample dataset to train a simple classification model, and then run a ValidMind test suite to quickly generate documentation about the data and model.\nThis notebook uses the Bank Customer Churn Prediction sample dataset from Kaggle to train the classification model."
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#contents",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#contents",
    "title": "Quickstart for model documentation",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nGet your code snippet\n\nInitialize the client library\n\nInitialize the Python environment\n\nPreview the documentation template\n\n\nLoad the sample dataset\n\nDocument the model\n\nPrepocess the raw dataset\n\nInitialize the ValidMind datasets\n\nInitialize a model object\n\nAssign predictions to the datasets\n\nRun the full suite of tests\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#about-validmind",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#about-validmind",
    "title": "Quickstart for model documentation",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#install-the-client-library",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#install-the-client-library",
    "title": "Quickstart for model documentation",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#initialize-the-client-library",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#initialize-the-client-library",
    "title": "Quickstart for model documentation",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n ### Get your code snippet\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)"
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#initialize-the-python-environment",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#initialize-the-python-environment",
    "title": "Quickstart for model documentation",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nimport xgboost as xgb\n\n%matplotlib inline\n\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#load-the-sample-dataset",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#load-the-sample-dataset",
    "title": "Quickstart for model documentation",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{customer_churn.target_column}' \\n\\t• Class labels: {customer_churn.class_labels}\"\n)\n\nraw_df = customer_churn.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#document-the-model",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#document-the-model",
    "title": "Quickstart for model documentation",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\n\nPrepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\nInitialize XGBoost classifier: Creates an XGBClassifier object with early stopping rounds set to 10.\nSet evaluation metrics: Specifies metrics for model evaluation as “error,” “logloss,” and “auc.”\nFit the model: Trains the model on x_train and y_train using the validation set (x_val, y_val). Verbose output is disabled.\n\n\ntrain_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n\nx_train = train_df.drop(customer_churn.target_column, axis=1)\ny_train = train_df[customer_churn.target_column]\nx_val = validation_df.drop(customer_churn.target_column, axis=1)\ny_val = validation_df[customer_churn.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\nclass_labels — an optional value to map predicted classes to class labels\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_raw_dataset = vm.init_dataset(\n    dataset=raw_df,\n    input_id=\"raw_dataset\",\n    target_column=customer_churn.target_column,\n    class_labels=customer_churn.class_labels,\n)\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=customer_churn.target_column,\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df, input_id=\"test_dataset\", target_column=customer_churn.target_column\n)\n\n\n\n\nInitialize a model object\nAdditionally, you need to initialize a ValidMind model object (vm_model) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    input_id=\"model\",\n)\n\n\n\n\nAssign predictions to the datasets\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n\nvm_train_ds.assign_predictions(\n    model=vm_model,\n)\n\nvm_test_ds.assign_predictions(\n    model=vm_model,\n)\n\n\n\n\nRun the full suite of tests\nThis is where it all comes together: you are now ready to run the documentation tests for the model as defined by the documentation template you looked at earlier.\nThe vm.run_documentation_tests function finds and runs every test specified in the template and then uploads all the documentation and test artifacts that get generated to the ValidMind AI Risk Platform.\nThe function requires information about the inputs to use on every test. These inputs can be passed as an inputs argument if we want to use the same inputs for all tests. It’s also possible to pass a config argument that has information about the params and inputs that each test requires. The config parameter is a dictionary with the following structure:\nconfig = {\n    \"&lt;test-id&gt;\": {\n        \"params\": {\n            \"param1\": \"value1\",\n            \"param2\": \"value2\",\n            ...\n        },\n        \"inputs\": {\n            \"input1\": \"value1\",\n            \"input2\": \"value2\",\n            ...\n        }\n    },\n    ...\n}\nEach &lt;test-id&gt; above corresponds to the test driven block identifiers shown by vm.preview_template(). For this model, we will use the default parameters for all tests, but we’ll need to specify the input configuration for each one. The method get_demo_test_config() below constructs the default input configuration for our demo.\n\nfrom validmind.utils import preview_test_config\n\ntest_config = customer_churn.get_demo_test_config()\npreview_test_config(test_config)\n\nNow we can pass the input configuration to vm.run_documentation_tests() and run the full suite of tests. The variable full_suite then holds the result of these tests.\n\nfull_suite = vm.run_documentation_tests(config=test_config)"
  },
  {
    "objectID": "notebooks/quickstart_customer_churn_full_suite.html#next-steps",
    "href": "notebooks/quickstart_customer_churn_full_suite.html#next-steps",
    "title": "Quickstart for model documentation",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html",
    "title": "Integrate external test providers",
    "section": "",
    "text": "Register a custom test provider with the Validmind Developer Framework to run your own tests.\nThe ValidMind Developer framework offers the ability to extend the built-in library of tests (metrics) with custom tests. If you’ve followed along in the Implement custom tests notebook, you will be familiar with the process of creating custom tests and running them for use in your model documentation. In that notebook, the tests were defined inline, as a notebook cell, using the validmind.metric decorator. This works very well when you just want to create one-off, ad-hoc tests or for experimenting. But for more complex, reusable and shareable tests, it is recommended to use a more structured approach.\nThis is where the concept of External Test Providers come in. A test “Provider” is a Python class that gets registered with the ValidMind Framework and loads tests based on a test ID, for example my_test_provider.my_test_id. The built-in suite of tests that ValidMind offers is technically its own test provider. You can use one the built-in test provider offered by ValidMind (validmind.tests.test_providers.LocalTestProvider) or you can create your own. More than likely, you’ll want to use the LocalTestProvider to add a directory of custom tests but there’s flexibility to be able to load tests from any source.\nIn this notebook, we’ll take you through the process of creating a folder of custom tests from existing, inline tests. We’ll then show you how to defined and register a LocalTestProvider that points to that folder. Finally, we’ll show how the test ID works when using test providers and how you can run and use custom tests from your provider."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#contents",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#contents",
    "title": "Integrate external test providers",
    "section": "Contents",
    "text": "Contents\n\nPrerequisites\n\nKey concepts\n\nHigh-level steps\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\n\nInstall the client library\n\nInitialize the client library\n\nSet up custom tests\n\nRegistering the test provider\n\nTest providers overview\n\nLocal test provider\n\nRunning test provider tests\n\nSet up the model and dataset\n\nRun the tests\n\nAdd custom metrics to model documentation\n\nRun, save, and add other tests\n\n\nVerify that preview_template() now loads the tests from the test providers\n\nRun the documentation tests\n\n\nConclusion\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#prerequisites",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#prerequisites",
    "title": "Integrate external test providers",
    "section": "Prerequisites",
    "text": "Prerequisites\nAs mentioned above, its recommended that you have gone through the Implement custom tests notebook to understand the basics of creating custom tests. We will be using the same tests defined in that notebook.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#key-concepts",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#key-concepts",
    "title": "Integrate external test providers",
    "section": "Key concepts",
    "text": "Key concepts\n\nTest Provider: A Python class that gets registered with the ValidMind Framework and loads tests based on a test ID e.g. my_test_provider.my_test_id.\nLocalTestProvider: A built-in test provider that loads tests from a local directory.\nTest ID: A unique identifier for a test that is used to load the test from a test provider. Test IDs in ValidMind have multiple parts, separated by dots (.), that have special meaning/functions - namespace.path.test_name:result_id (e.g. validmind.data_validation.ClassImbalance:1):\n\nnamespace: The namespace of the test - built-in tests use the validmind namespace, custom test providers specify a custom namespace when they are registered\npath: The path to the test within the test provider. For internal tests, this is relative to the validmind/tests directory. When using a LocalTestProvider this is relative to the root directory you specify when creating the provider class instance. Note: This path-based approach is a convention and not a strict requirement when implementing your own test provider.\ntest_name: The name of the test module (file) as well as the name of the function or class that is the “test” itself. Again, this is a convention for internal and LocalTestProvider tests, but test files should be named the same as the test function or class they contain.\nresult_id: An optional identifier for the test. Especially useful, and required, for running tests multiple times in one section. This is separated from the previous parts by a colon : and can be effectively ignored when loading the test itself as its only used to uniquely identify the test result."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#high-level-steps",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#high-level-steps",
    "title": "Integrate external test providers",
    "section": "High-level steps",
    "text": "High-level steps\n\nCreate a folder of custom tests from existing, inline tests.\nDefine and register a LocalTestProvider that points to that folder.\nUse the test ID to run, view and log custom tests from your provider.\nAdd the test results to your documentation\nrun_documentation_tests() to run custom tests as part of your documentation template suite"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#about-validmind",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#about-validmind",
    "title": "Integrate external test providers",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference\n\n\n\nBefore you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#install-the-client-library",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#install-the-client-library",
    "title": "Integrate external test providers",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#initialize-the-client-library",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#initialize-the-client-library",
    "title": "Integrate external test providers",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select the Attrition/Churn Management use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"...\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#set-up-custom-tests",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#set-up-custom-tests",
    "title": "Integrate external test providers",
    "section": "Set up custom tests",
    "text": "Set up custom tests\nIf you’ve gone through the Implement custom tests notebook, you should have a good understanding of how custom tests are implemented. If you haven’t, we recommend going through that notebook first. In this notebook, we will take those custom tests and move them into separate modules in a folder. This is the logical progression from the previous notebook, as it allows you to take one-off tests and move them into an organized structure that makes it easier to manage, maintain and share them.\nFirst let’s set the path to our custom tests folder.\n\ntests_folder = \"my_tests\"\n\nNow let’s create this folder if it doesn’t exist. If it does, let’s empty it to start fresh.\n\nimport os\n\n# create tests folder\nos.makedirs(tests_folder, exist_ok=True)\n\n# remove existing tests\nfor f in os.listdir(tests_folder):\n    # remove files and pycache\n    if f.endswith(\".py\") or f == \"__pycache__\":\n        os.system(f\"rm -rf {tests_folder}/{f}\")\n\nLet’s go ahead and redefine the custom tests we created in the previous notebook.\n\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom validmind import metric\n\n\n@metric(\"my_custom_metrics.ConfusionMatrix\")\ndef confusion_matrix(dataset, model):\n    \"\"\"The confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known.\n\n    The confusion matrix is a 2x2 table that contains 4 values:\n\n    - True Positive (TP): the number of correct positive predictions\n    - True Negative (TN): the number of correct negative predictions\n    - False Positive (FP): the number of incorrect positive predictions\n    - False Negative (FN): the number of incorrect negative predictions\n\n    The confusion matrix can be used to assess the holistic performance of a classification model by showing the accuracy, precision, recall, and F1 score of the model on a single figure.\n    \"\"\"\n    confusion_matrix = metrics.confusion_matrix(dataset.y, dataset.y_pred(model))\n\n    cm_display = metrics.ConfusionMatrixDisplay(\n        confusion_matrix=confusion_matrix,\n        display_labels=[False, True]\n    )\n    cm_display.plot()\n\n    plt.close()  # close the plot to avoid displaying it\n\n    return cm_display.figure_  # return the figure object itself\n\nThe decorator thats used to register these as one-off custom tests adds a convenience method to the function object that allows us to simply call &lt;func_name&gt;.save() to save it to a file. This will save the function to a Python file in the current directory. You can also pass a path to a folder to save it to a different location. In our case, we can pass the variable tests_folder to save it to the custom tests folder we created.\nNormally, this will get you started by creating the file and saving the function code with the correct name. But it won’t automatically add any import or other functions/variables outside of the function that are needed for the test to run. In that case, you would need to manually add those to the file.\nBut for the sake of this demo, we will pass a list of imports into the save method to automatically add them to the file.\n\nconfusion_matrix.save(tests_folder, imports=[\"import matplotlib.pyplot as plt\", \"from sklearn import metrics\"])\n\nLet’s go ahead and do this for the rest of the tests.\n\n@metric(\"my_custom_metrics.Hyperparameters\")\ndef hyperparameters(model):\n    \"\"\"The hyperparameters of a machine learning model are the settings that control the learning process.\n    These settings are specified before the learning process begins and can have a significant impact on the\n    performance of the model.\n\n    The hyperparameters of a model can be used to tune the model to achieve the best possible performance\n    on a given dataset. By examining the hyperparameters of a model, you can gain insight into how the model\n    was trained and how it might be improved.\n    \"\"\"\n    hyperparameters = model.model.get_xgb_params() # dictionary of hyperparameters\n\n    # turn the dictionary into a table where each row contains a hyperparameter and its value\n    return [{\"Hyperparam\": k, \"Value\": v} for k, v in hyperparameters.items() if v]\n\n\nhyperparameters.save(tests_folder)\n\n\nimport requests\n\n\n@metric(\"my_custom_metrics.ExternalAPI\")\ndef external_api():\n    \"\"\"This metric calls an external API to get the current BTC price. It then creates\n    a table with the relevant data so it can be displayed in the documentation.\n\n    The purpose of this metric is to demonstrate how to call an external API and use the\n    data in a metric. A metric like this could even be setup to run in a scheduled\n    pipeline to keep your documentation in-sync with an external data source.\n    \"\"\"\n    url = \"https://api.coindesk.com/v1/bpi/currentprice.json\"\n    response = requests.get(url)\n    data = response.json()\n\n    # extract the time and the current BTC price in USD\n    return [\n        {\n            \"Time\": data[\"time\"][\"updated\"],\n            \"Price (USD)\": data[\"bpi\"][\"USD\"][\"rate\"],\n        }\n    ]\n\n\nexternal_api.save(tests_folder, imports=[\"import requests\"])\n\n\nimport plotly_express as px\n\n\n@metric(\"my_custom_metrics.ParameterExample\")\ndef parameter_example(plot_title=\"Default Plot Title\", x_col=\"sepal_width\", y_col=\"sepal_length\"):\n    \"\"\"This metric takes two parameters and creates a scatter plot based on them.\n\n    The purpose of this metric is to demonstrate how to create a metric that takes\n    parameters and uses them to generate a plot. This can be useful for creating\n    metrics that are more flexible and can be used in a variety of scenarios.\n    \"\"\"\n    # return px.scatter(px.data.iris(), x=x_col, y=y_col, color=\"species\")\n    return px.scatter(px.data.iris(), x=x_col, y=y_col, color=\"species\", title=plot_title)\n\n\nparameter_example.save(tests_folder, imports=[\"import plotly_express as px\"])\n\n\nimport numpy as np\nimport plotly_express as px\n\n@metric(\"my_custom_metrics.ComplexOutput\")\ndef complex_output():\n    \"\"\"This metric demonstrates how to return many tables and figures in a single metric\"\"\"\n    # create a couple tables\n    table = [{\"A\": 1, \"B\": 2}, {\"A\": 3, \"B\": 4}]\n    table2 = [{\"C\": 5, \"D\": 6}, {\"C\": 7, \"D\": 8}]\n\n    # create a few figures showing some random data\n    fig1 = px.line(x=np.arange(10), y=np.random.rand(10), title=\"Random Line Plot\")\n    fig2 = px.bar(x=[\"A\", \"B\", \"C\"], y=np.random.rand(3), title=\"Random Bar Plot\")\n    fig3 = px.scatter(x=np.random.rand(10), y=np.random.rand(10), title=\"Random Scatter Plot\")\n\n    return {\n        \"My Cool Table\": table,\n        \"Another Table\": table2,\n    }, fig1, fig2, fig3\n\n\ncomplex_output.save(tests_folder, imports=[\"import numpy as np\", \"import plotly_express as px\"])\n\n\nimport io\nimport matplotlib.pyplot as plt\n\n\n@metric(\"my_custom_metrics.Image\")\ndef image():\n    \"\"\"This metric demonstrates how to return an image in a metric\"\"\"\n\n    # create a simple plot\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3, 4])\n    ax.set_title(\"Simple Line Plot\")\n\n    # save the plot as a PNG image (in-memory buffer)\n    img_data = io.BytesIO()\n    fig.savefig(img_data, format=\"png\")\n    img_data.seek(0)\n\n    plt.close()  # close the plot to avoid displaying it\n\n    return img_data.read()\n\n\nimage.save(tests_folder, imports=[\"import io\", \"import matplotlib.pyplot as plt\"])"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#registering-the-test-provider",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#registering-the-test-provider",
    "title": "Integrate external test providers",
    "section": "Registering the test provider",
    "text": "Registering the test provider\n\n\nTest providers overview\nNow that we have a folder with our custom tests, we can create a test provider that will tell the ValidMind Developer Framework where to find these tests. ValidMind offers out-of-the-box test providers for local tests (i.e. tests in a folder) or a Github provider for tests in a Github repository. You can also create your own test provider by creating a class that has a load_test method that takes a test ID and returns the test function matching that ID. The protocol for test providers is below:\nclass ExternalTestProvider(Protocol):\n    \"\"\"Protocol for user-defined test providers\"\"\"\n\n    def load_test(self, test_id: str) -&gt; callable:\n        \"\"\"Load the test by test ID\n\n        Args:\n            test_id (str): The test ID (does not contain the namespace under which\n                the test is registered)\n\n        Returns:\n            callable: The test object\n\n        Raises:\n            FileNotFoundError: If the test is not found\n        \"\"\"\n        ...\n\n\n\nLocal test provider\nFor most use-cases, the local test provider should be sufficient. Let’s go ahead and see how we can do this with our custom tests.\n\nfrom validmind.tests import LocalTestProvider\n\n# initialize the test provider with the tests folder we created earlier\nmy_test_provider = LocalTestProvider(tests_folder)\n\nvm.tests.register_test_provider(\n    namespace=\"my_test_provider\",\n    test_provider=my_test_provider,\n)\n# `my_test_provider.load_test()` will be called for any test ID that starts with `my_test_provider`\n# e.g. `my_test_provider.ConfusionMatrix` will look for a function named `ConfusionMatrix` in `my_tests/ConfusionMatrix.py` file\n\n\n\n\nRunning test provider tests\nNow that we have our test provider set up, we can run any test that’s located in our tests folder by using the run_test() method. This function is your entry point to running single tests in the ValidMind Developer Framework. It takes a test ID and runs the test associated with that ID. For our custom tests, the test ID will be the namespace specified when registering the provider, followed by the path to the test file relative to the tests folder. For example, the Confusion Matrix test we created earlier will have the test ID my_test_provider.ConfusionMatrix. You could organize the tests in subfolders, say classification and regression, and the test ID for the Confusion Matrix test would then be my_test_provider.classification.ConfusionMatrix.\nLet’s go ahead and run some of our tests. But first, let’s setup a dataset and model to run against.\n\n\nSet up the model and dataset\nFirst let’s setup a an example model and dataset to run our custom metic against. Since this is a Confusion Matrix, we will use the Customer Churn dataset that ValidMind provides and train a simple XGBoost model.\n\nimport xgboost as xgb\nfrom validmind.datasets.classification import customer_churn\n\nraw_df = customer_churn.load_data()\ntrain_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n\nx_train = train_df.drop(customer_churn.target_column, axis=1)\ny_train = train_df[customer_churn.target_column]\nx_val = validation_df.drop(customer_churn.target_column, axis=1)\ny_val = validation_df[customer_churn.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nEasy enough! Now we have a model and dataset setup and trained. One last thing to do is bring the dataset and model into the ValidMind framework:\n\n# for now, we'll just use the test dataset\nvm_raw_ds = vm.init_dataset(\n    dataset=raw_df,\n    target_column=customer_churn.target_column,\n    input_id=\"raw_dataset\",\n)\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    target_column=customer_churn.target_column,\n    input_id=\"train_dataset\",\n)\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    target_column=customer_churn.target_column,\n    input_id=\"test_dataset\",\n)\n\nvm_model = vm.init_model(model, input_id=\"model\")\n# link the model to the datasets\nvm_train_ds.assign_predictions(model=vm_model)\nvm_test_ds.assign_predictions(model=vm_model)\n\n\n\n\nRun the tests\nNow that we have our model and dataset setup, we can run our custom tests against them. Let’s go ahead and run the Confusion Matrix test.\n\nfrom validmind.tests import run_test\n\nresult = run_test(\"my_test_provider.ConfusionMatrix\", inputs={\"dataset\": vm_test_ds, \"model\": vm_model})\n\nYou should see the output of the test above. If you want to learn more about how test functions are run and get turned into test results, check out the Implement custom tests notebook.\nLet’s go ahead and log the result to the ValidMind platform.\n\nresult.log()\n\n\n\n\nAdd custom metrics to model documentation\nNow that the result has been logged to the platform, you can add it to your model documentation. This will add the result where you specify but it also will add the test to the template so it gets run anytime you run_documentation_tests(). To do this, go to the documentation page of the model you connected to above and navigate to the Model Development -&gt; Model Evaluation section. Then hover between any existing content block to reveal the + button as shown in the screenshot below.\n\n\n\nscreenshot showing insert button for test-driven blocks\n\n\nNow click on the + button and select the Test-Driven Block option. This will open a dialog where you can select Metric as the type of test and the My Test Provider Confusion Matrix from the list of available metrics. You can preview the result and then click Insert Block to add it to the documentation.\n\n\n\nscreenshot showing how to insert a test-driven block\n\n\nThe test should match the result you see above.\n\n\n\nRun, save, and add other tests\nLet’s take the same steps for all of our other custom tests.\n\nrun_test(\"my_test_provider.ComplexOutput\", inputs={\"dataset\": vm_test_ds, \"model\": vm_model}).log()\nrun_test(\"my_test_provider.ExternalAPI\").log()\nrun_test(\"my_test_provider.Hyperparameters\", inputs={\"model\": vm_model}).log()\nrun_test(\"my_test_provider.Image\").log()\nrun_test(\"my_test_provider.ParameterExample\", params={\n    \"plot_title\": \"Test Provider Plot\",\n    \"x_col\": \"petal_width\",\n    \"y_col\": \"petal_length\",\n}).log()\n\nNow that they are all saved, follow the same steps above to add them to the model documentation. Once thats done, we will run the documentation tests to see all of them run automatically.\n\n\n\n\nVerify that preview_template() now loads the tests from the test providers\nNow that we have added the tests to the model documentation, we can run the preview_template() method to see the tests run automatically.\nFirst, let’s reload the connection to the model to get the updated documentation template.\n\nvm.reload()\n\nNow, run preview_template() and verify that the tests you added are included in the proper section.\n\nvm.preview_template()\n\n\n\n\nRun the documentation tests\nNow we can run the documentation tests as normal. This should include all of our custom tests which will be loaded from our test provider folder.\n\ntest_config = customer_churn.get_demo_test_config()\nsuite_results = vm.run_documentation_tests(config=test_config)"
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#conclusion",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#conclusion",
    "title": "Integrate external test providers",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we showed how to integrate custom tests into the ValidMind Developer Framework. We created a custom test provider that loads tests from a folder and then ran the tests against a model and dataset. We then added the tests to the model documentation and ran the documentation tests to see all of the tests run automatically. This is a powerful concept that allows you to create, organize and, most importantly, share custom tests with other model developers."
  },
  {
    "objectID": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#next-steps",
    "href": "notebooks/code_samples/custom_tests/integrate_external_test_providers.html#next-steps",
    "title": "Integrate external test providers",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "",
    "text": "Document a natural language processing (NLP) model using ValidMind to summarize financial news, based on a dataset of just over 300,000 unique news articles written by journalists at CNN and the Daily Mail.\nThis interactive notebook shows you how to set up the ValidMind Developer Framework, initialize the client library, and load the dataset, followed by running the model validation tests provided by the framework to quickly generate documentation about the data and model."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#about-validmind",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#before-you-begin",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#before-you-begin",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#install-the-client-library",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#initialize-the-client-library",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select NLP-based Text Summarization as the template and Marketing/Sales - Analytics as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nHelper functions\nLet’s define the following functions to help visualize datasets with long text fields:\n\nimport textwrap\n\nfrom IPython.display import display, HTML\nfrom tabulate import tabulate\n\n\ndef _format_cell_text(text, width=50):\n    \"\"\"Private function to format a cell's text.\"\"\"\n    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n\n\ndef _format_dataframe_for_tabulate(df):\n    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n    df_out = df.copy()\n\n    # Format all string columns\n    for column in df_out.columns:\n        if (\n            df_out[column].dtype == object\n        ):  # Check if column is of type object (likely strings)\n            df_out[column] = df_out[column].apply(_format_cell_text)\n    return df_out\n\n\ndef _dataframe_to_html_table(df):\n    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n    headers = df.columns.tolist()\n    table_data = df.values.tolist()\n    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n\n\ndef display_formatted_dataframe(df, num_rows=None):\n    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n    if num_rows is not None:\n        df = df.head(num_rows)\n    formatted_df = _format_dataframe_for_tabulate(df)\n    html_table = _dataframe_to_html_table(formatted_df)\n    display(HTML(html_table))\n\n\n\nLoad the dataset\nThe CNN Dailymail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail (https://huggingface.co/datasets/cnn_dailymail). The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./datasets/cnn_dailymail_100_with_predictions.csv\")\ndisplay_formatted_dataframe(df, num_rows=5)\n\n\nvm_raw_ds = vm.init_dataset(\n    dataset=df,\n    input_id=\"raw_dataset\",\n    text_column=\"article\",\n    target_column=\"highlights\",\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#nlp-data-quality-tests",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#nlp-data-quality-tests",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "NLP data quality tests",
    "text": "NLP data quality tests\nBefore we proceed with the analysis, it’s crucial to ensure the quality of our NLP data. We can run the data_preparation section of the template to validate the data’s integrity and suitability:\n\ntext_data_test_plan = vm.run_documentation_tests(\n    section=\"data_preparation\", inputs={\"dataset\": vm_raw_ds}\n)\n\n\nfrom transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\nsummarizer_model = pipeline(\n    task=\"summarization\",\n    model=model,\n    tokenizer=tokenizer,\n    min_length=0,\n    max_length=60,\n    truncation=True,\n)  # Note: We specify cache_dir to use predownloaded models.\n\n\nvm_test_ds = vm.init_dataset(\n    dataset=df,\n    input_id=\"test_dataset\",\n    text_column=\"article\",\n    target_column=\"highlights\",\n)\n\n\nvm_model = vm.init_model(\n    summarizer_model,\n)\n\n# Assign model predictions to the test dataset\nvm_test_ds.assign_predictions(vm_model, prediction_column=\"t5_prediction\")\n\n\nRun model validation tests\nIt’s possible to run a subset of tests on the documentation template by passing a section parameter to run_documentation_tests(). Let’s run only the tests that evaluate the model’s overall performance, including summarization metrics, by selecting the model_development section of the template:\n\nsummarization_results = vm.run_documentation_tests(\n    section=\"model_development\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#next-steps",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_summarization_demo.html#next-steps",
    "title": "Summarization of financial data using Hugging Face NLP models",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation in the ValidMind Platform UI:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your model documentation that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at Get started with the ValidMind Developer Framework."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "",
    "text": "Document a natural language processing (NLP) model using the ValidMind Developer Framework after performing a sentiment analysis of financial news data using several different Hugging Face transformers.\nThis notebook provides an introduction for model developers on how to document a natural language processing (NLP) model using the ValidMind Developer Framework. It shows you how to set up the ValidMind Developer Framework, initialize the client library, and load the dataset, followed by performing a sentiment analysis of financial news data using several different Hugging Face transformers. As part of the process, the notebook runs various tests to quickly generate documentation about the data and model."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#about-validmind",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on documentation projects. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#before-you-begin",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#before-you-begin",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#install-the-client-library",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#initialize-the-client-library",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select NLP-based Text Classification as the template and Marketing/Sales - Analytics as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nGet your sample dataset ready for analysis\nTo perform the sentiment analysis for financial news we’re going to load a local copy of this dataset: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\nThis dataset contains two columns, Sentiment and Sentence. The sentiment can be negative, neutral or positive.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./datasets/sentiments_with_predictions.csv\")"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#nlp-data-quality-tests",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#nlp-data-quality-tests",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "NLP data quality tests",
    "text": "NLP data quality tests\nBefore we proceed with the analysis, it’s crucial to ensure the quality of our NLP data. We can run the “data preparation” section of the template to validate the raw dataset’s integrity and suitability.\n\nvm_raw_ds = vm.init_dataset(\n    dataset=df,\n    input_id=\"raw_dataset\",\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\ntext_data_test_plan = vm.run_documentation_tests(\n    section=\"data_preparation\", inputs={\"dataset\": vm_raw_ds}\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#hugging-face-transformers",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#hugging-face-transformers",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "Hugging Face transformers",
    "text": "Hugging Face transformers\n\nHugging Face: FinancialBERT for Sentiment Analysis\nLet’s now explore integrating and testing FinancialBERT (https://huggingface.co/ahmedrachid/FinancialBERT-Sentiment-Analysis ), a model designed specifically for sentiment analysis in the financial domain:\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"ahmedrachid/FinancialBERT-Sentiment-Analysis\", num_labels=3\n)\ntokenizer = BertTokenizer.from_pretrained(\n    \"ahmedrachid/FinancialBERT-Sentiment-Analysis\"\n)\nhfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n\nInitialize the ValidMind dataset\n\n# Load a test dataset with 100 rows only\nvm_test_ds = vm.init_dataset(\n    dataset=df,\n    input_id=\"test_dataset\",\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\n\n\nInitialize the ValidMind model\nWhen initializing a ValidMind model, we pre-calculate predictions on the test dataset. This operation can take a long time for large datasets.\n\nvm_model = vm.init_model(\n    hfmodel,\n)\n\n# Assign model predictions to the test dataset\nvm_test_ds.assign_predictions(vm_model, prediction_column=\"finbert_prediction\")\n\n\n\nRun model validation tests\nIt’s possible to run a subset of tests on the documentation template by passing a section parameter to run_documentation_tests(). Let’s run the tests that correspond to model validation only:\n\nfull_suite = vm.run_documentation_tests(\n    section=\"model_development\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#next-steps",
    "href": "notebooks/code_samples/NLP_and_LLM/hugging_face_integration_demo.html#next-steps",
    "title": "Sentiment analysis of financial data using Hugging Face NLP models",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation in the ValidMind Platform UI:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your model documentation that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at Get started with the ValidMind Developer Framework."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "",
    "text": "Run and document prompt validation tests for a large language model (LLM) specialized in sentiment analysis for financial news.\nThis interactive notebook shows you how to set up the ValidMind Developer Framework, initialize the client library, and use a specific prompt template for analyzing the sentiment of given sentences. Prompt validation covers the initialization of a test dataset and the creation of a foundational model using ValidMind’s framework, followed by the execution of a test suite specifically designed for prompt validation. The notebook also includes example data to test the model’s ability to correctly identify sentiment as positive, negative, or neutral."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#contents",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#contents",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nPreview the documentation template\n\nGet ready to run the analysis\n\nGet your sample dataset ready for analysis\n\n\nPerform the prompt validation\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#about-validmind",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#install-the-client-library",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#initialize-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#initialize-the-client-library",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select LLM-based Text Classification as the template and Marketing/Sales - Analytics as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\n\nGet ready to run the analysis\nImport the ValidMind FoundationModel and Prompt classes needed for the sentiment analysis later on:\n\nfrom validmind.models import FoundationModel, Prompt\n\nCheck your access to the OpenAI API:\n\nimport os\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nfrom openai import OpenAI\n\nmodel = OpenAI()\n\n\ndef call_model(prompt):\n    return (\n        model.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        .choices[0]\n        .message.content\n    )\n\nSet the prompt guidelines for the sentiment analysis:\n\nprompt_template = \"\"\"\nYou are an AI with expertise in sentiment analysis, particularly in the context of financial news.\nYour task is to analyze the sentiment of a specific sentence provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the sentence.\n\nSentence to Analyze:\n```\n{Sentence}\n```\n\nPlease respond with the sentiment of the sentence denoted by one of either 'positive', 'negative', or 'neutral'.\nPlease respond only with the sentiment enum value. Do not include any other text in your response.\n\nNote: Ensure that your analysis is based on the content of the sentence and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"Sentence\"]\n\n\n\n\nGet your sample dataset ready for analysis\nTo perform the sentiment analysis for financial news we’re going to load a local copy of this dataset: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news.\nThis dataset contains two columns, Sentiment and Sentence. The sentiment can be negative, neutral or positive.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./datasets/sentiments.csv\")\n\ndf_test = df[:10].reset_index(drop=True)\ndf_test"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#perform-the-prompt-validation",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#perform-the-prompt-validation",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "Perform the prompt validation",
    "text": "Perform the prompt validation\nFirst, use the ValidMind Developer Framework to initialize the dataset and model objects necessary for documentation. The ValidMind predict_fn function allows the model to be tested and evaluated in a standardized manner:\n\nvm_test_ds = vm.init_dataset(\n    dataset=df_test,\n    input_id=\"test_dataset\",\n    text_column=\"Sentence\",\n    target_column=\"Sentiment\",\n)\n\nvm_model = vm.init_model(\n    FoundationModel(\n        predict_fn=call_model,\n        prompt=Prompt(\n            template=prompt_template,\n            variables=prompt_variables,\n        ),\n    ),\n    input_id=\"gpt_35_model\",\n)\n\n# Assign model predictions to the test dataset\nvm_test_ds.assign_predictions(vm_model)\n\nNext, use the ValidMind Developer Framework to run validation tests on the model. These tests evaluate various aspects of the prompts, including bias, clarity, conciseness, delimitation, negative instruction, and specificity.\nEach test is explained in detail, highlighting its purpose, test mechanism, and the importance of the specific aspect being evaluated. The tests are graded on a scale from 1 to 10, with a predetermined threshold, and the explanations for each test include a score, threshold, and a pass/fail determination.\n\ntest_suite_results = vm.run_test_suite(\n    \"prompt_validation\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)\n\nHere, most of the tests pass but the test for conciseness needs further attention, as it fails the threshold. This test is designed to evaluate the brevity and succinctness of prompts provided to a large language model (LLM).\nThe test matters, because a concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#next-steps",
    "href": "notebooks/code_samples/NLP_and_LLM/prompt_validation_demo.html#next-steps",
    "title": "Prompt validation for large language models (LLMs)",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html",
    "title": "Summarization of financial data using a large language model (LLM)",
    "section": "",
    "text": "Document a large language model (LLM) using the ValidMind Developer Framework. The use case is a summarization of financial news based on a dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\nThis interactive notebook shows you how to set up the ValidMind Developer Framework, initialize the client library, and load the dataset, followed by running the model validation tests provided by the framework to quickly generate documentation about the data and model."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#before-you-begin",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#before-you-begin",
    "title": "Summarization of financial data using a large language model (LLM)",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nThis notebook requires an OpenAI API secret key to run. If you don’t have one, visit API keys on OpenAI’s site to create a new key for yourself. Note that API usage charges may apply.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#about-validmind",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#about-validmind",
    "title": "Summarization of financial data using a large language model (LLM)",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, we recommend going through the following resources first:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#install-the-client-library",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#install-the-client-library",
    "title": "Summarization of financial data using a large language model (LLM)",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind\n\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()\n\n\n\nHelper functions\nLet’s define the following functions to help visualize datasets with long text fields:\n\nimport textwrap\n\nfrom IPython.display import display, HTML\nfrom tabulate import tabulate\n\n\ndef _format_cell_text(text, width=50):\n    \"\"\"Private function to format a cell's text.\"\"\"\n    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n\n\ndef _format_dataframe_for_tabulate(df):\n    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n    df_out = df.copy()\n\n    # Format all string columns\n    for column in df_out.columns:\n        # Check if column is of type object (likely strings)\n        if df_out[column].dtype == object:\n            df_out[column] = df_out[column].apply(_format_cell_text)\n    return df_out\n\n\ndef _dataframe_to_html_table(df):\n    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n    headers = df.columns.tolist()\n    table_data = df.values.tolist()\n    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n\n\ndef display_formatted_dataframe(df, num_rows=None):\n    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n    if num_rows is not None:\n        df = df.head(num_rows)\n    formatted_df = _format_dataframe_for_tabulate(df)\n    html_table = _dataframe_to_html_table(formatted_df)\n    display(HTML(html_table))\n\n\n\nLoad the dataset\nThe CNN Dailymail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail (https://huggingface.co/datasets/cnn_dailymail). The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"./datasets/cnn_dailymail_100_with_predictions.csv\")\ndisplay_formatted_dataframe(df, num_rows=5)\n\n\n\nGet ready to run the analysis\nImport the ValidMind FoundationModel and Prompt classes needed for the sentiment analysis later on:\n\nfrom validmind.models import FoundationModel, Prompt\n\nCheck your access to the OpenAI API:\n\nimport os\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nif os.getenv(\"OPENAI_API_KEY\") is None:\n    raise Exception(\"OPENAI_API_KEY not found\")\n\n\nfrom openai import OpenAI\n\nmodel = OpenAI()\n\n\ndef call_model(prompt):\n    return (\n        model.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        .choices[0]\n        .message.content\n    )\n\nSet the prompt guidelines for the sentiment analysis:\n\nprompt_template = \"\"\"\nYou are an AI with expertise in summarizing financial news.\nYour task is to provide a concise summary of the specific news article provided below.\nBefore proceeding, take a moment to understand the context and nuances of the financial terminology used in the article.\n\nArticle to Summarize:\n\n```\n{article}\n```\n\nPlease respond with a concise summary of the article's main points.\nEnsure that your summary is based on the content of the article and not on external information or assumptions.\n\"\"\".strip()\n\nprompt_variables = [\"article\"]\n\n\nvm_test_ds = vm.init_dataset(\n    dataset=df,\n    input_id=\"test_dataset\",\n    text_column=\"article\",\n    target_column=\"highlights\",\n)\n\nvm_model = vm.init_model(\n    FoundationModel(\n        predict_fn=call_model,\n        prompt=Prompt(\n            template=prompt_template,\n            variables=prompt_variables,\n        ),\n    ),\n    input_id=\"gpt_35_model\",\n)\n\n# Assign model predictions to the test dataset\nvm_test_ds.assign_predictions(vm_model, prediction_column=\"gpt_35_prediction\")\n\n\n\nRun model validation tests\nIt’s possible to run a subset of tests on the documentation template by passing a section parameter to run_documentation_tests(). Let’s run the tests that evaluate the model’s overall performance (including summarization metrics), by selecting the “model development” section of the template:\n\nsummarization_results = vm.run_documentation_tests(\n    section=\"model_development\",\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n    },\n)"
  },
  {
    "objectID": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#next-steps",
    "href": "notebooks/code_samples/NLP_and_LLM/foundation_models_summarization_demo.html#next-steps",
    "title": "Summarization of financial data using a large language model (LLM)",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the prompt validation test results as part of your model documentation in the ValidMind Platform UI:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand 2. Data Preparation or 3. Model Development to review all test results.\n\nWhat you can see now is a more easily consumable version of the prompt validation testing you just performed, along with other parts of your model documentation that still need to be completed.\nIf you want to learn more about where you are in the model documentation process, take a look at Get started with the ValidMind Developer Framework."
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "",
    "text": "Use the California Housing Price Prediction sample dataset from Sklearn to train a simple regression model and document that model with the ValidMind Developer Framework.\nAs part of the notebook, you will learn how to train a sample model while exploring how the documentation process works:"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#about-validmind",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#about-validmind",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind’s platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\nIf this is your first time trying out ValidMind, you can make use of the following resources alongside this notebook:\n\nGet started — The basics, including key concepts, and how our products work\nGet started with the ValidMind Developer Framework — The path for developers, more code samples, and our developer reference"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#before-you-begin",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#before-you-begin",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Before you begin",
    "text": "Before you begin\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules."
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#install-the-client-library",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#install-the-client-library",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-client-library",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-client-library",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Binary classification as the template and Marketing/Sales - Attrition/Churn Management as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-python-environment",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#initialize-the-python-environment",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\nNext, let’s import the necessary libraries and set up your Python environment for data analysis:\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n%matplotlib inline\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#load-the-sample-dataset",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#load-the-sample-dataset",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nTo be able to use a sample dataset, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#load-the-sample-dataset-1",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#load-the-sample-dataset-1",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.regression import california_housing as demo_dataset\n\nprint(f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}\")\n\nraw_df = demo_dataset.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#document-the-model",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#document-the-model",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\n\nPrepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\nInitialize RandomForestRegressor regressor: Creates an RandomForestRegressor object with random state set to 0.\nSet evaluation metrics: Specifies metrics for model evaluation as “errors” and “r2”.\nFit the model: Trains the model on x_train and y_train using the validation set (x_val, y_val). Verbose output is disabled.\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nHere we create two regression models so that the performance of the model can be compared through ValidMind test suite.\n\nscale = False\nif scale:\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_val = scaler.fit_transform(x_val)\n\nmodel = RandomForestRegressor(random_state=0)\nmodel.fit(x_train, y_train)\ns1 = model.score(x_train, y_train)\ns2 = model.score(x_val, y_val)\nprint(\"R² of Support Vector Regressor on training set: {:.3f}\".format(s1))\nprint(\"R² of Support Vector Regressor on test set: {:.3f}\".format(s2))\n\nmodel_1 = GradientBoostingRegressor(random_state=0, max_depth=4)\nmodel_1.fit(x_train, y_train)\nmodel1_s1 = model_1.score(x_train, y_train)\nmodel1_s2 = model_1.score(x_val, y_val)\nprint(\n    \"R² of Support Gradient Boosting Regressor on training set: {:.3f}\".format(\n        model1_s1\n    )\n)\nprint(\"R² of Support Gradient Boosting Regressor on test set: {:.3f}\".format(model1_s2))\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_raw_dataset = vm.init_dataset(\n    dataset=raw_df,\n    input_id=\"raw_dataset\",\n    target_column=demo_dataset.target_column,\n)\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df, input_id=\"train_dataset\", target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df, input_id=\"test_dataset\", target_column=demo_dataset.target_column\n)\n\n\n\nInitialize a model object\nAdditionally, you need to initialize a ValidMind model objects (vm_model and vm_model_1) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    input_id=\"random_forest_regressor\",\n)\nvm_model_1 = vm.init_model(\n    model_1,\n    input_id=\"gradient_boosting_regressor\",\n)\n\n\n\nAssign predictions to the datasets\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n\nvm_train_ds.assign_predictions(\n    model=vm_model,\n)\nvm_train_ds.assign_predictions(\n    model=vm_model_1,\n)\nvm_test_ds.assign_predictions(\n    model=vm_model,\n)\nvm_test_ds.assign_predictions(\n    model=vm_model_1,\n)\n\n\n\nRun the full suite of tests\nThis is where it all comes together: you are now ready to run the documentation tests for the model as defined by the documentation template you looked at earlier.\nThe vm.run_documentation_tests function finds and runs every tests specified in the test suites and then uploads all the documentation and test artifacts that get generated to the ValidMind AI Risk Platform.\nThe function takes two arguments:\n\ndataset: The data to be tested, specified as vm_dataset.\nmodel: The candidate model to be used for testing, specified as vm_model. -models: The list of models that can be compare with candidate model.\n\nThe variable full_suite then holds the result of these tests.\n\nfull_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"datasets\": (vm_train_ds, vm_test_ds),\n        \"model\": vm_model,\n        \"models\":[vm_model_1]\n    }\n)"
  },
  {
    "objectID": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#next-steps",
    "href": "notebooks/code_samples/regression/quickstart_regression_full_suite.html#next-steps",
    "title": "Document a California Housing Price Prediction regression model",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way: view the test results as part of your model documentation right in the ValidMind Platform UI:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand the following sections and take a look around:\n\n2. Data Preparation\n3. Model Development\n\n\nWhat you can see now is a much more easily consumable version of the documentation, including the results of the tests you just performed, along with other parts of your model documentation that still need to be completed. There is a wealth of information that gets uploaded when you run the full test suite, so take a closer look around, especially at test results that might need attention (hint: some of the tests in 2.1 Data description look like they need some attention).\nIf you want to learn more about where you are in the model documentation process, take a look at Get started with the ValidMind Developer Framework."
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html",
    "href": "notebooks/archive/configure_test_parameters.html",
    "title": "Configure test parameters",
    "section": "",
    "text": "Each ValidMind test or metric comes with default parameters and values that are configurable for your specific use case.\nValidMind provides a comprehensive suite of default metrics out of the box to evaluate and document your models and datasets. Typically, documenting a specific model involves configuring these tests and metrics with a config to suit your use case.\nThis interactive notebook provides a step-by-step guide for configuring parameters for a test or set of tests within a specific section of your model documentation, and documenting a simple classification model using ValidMind with a bank customer churn dataset. The bank customer churn dataset used here is from Kaggle."
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#contents",
    "href": "notebooks/archive/configure_test_parameters.html#contents",
    "title": "Configure test parameters",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the Python environment\n\nInitialize the client library\n\nPreview the model’s documentation template\n\n\nLoad the demo dataset\n\nDocumenting the model\n\nPrepocess the raw dataset\n\n\nInitialize the ValidMind datasets\n\nAssign predictions to the datasets\n\nRun the template documentation suite\n\nConfiguration of parameters for model diagnosis tests\n\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#about-validmind",
    "href": "notebooks/archive/configure_test_parameters.html#about-validmind",
    "title": "Configure test parameters",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\n\n\n\n\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now!\n\n\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#install-the-client-library",
    "href": "notebooks/archive/configure_test_parameters.html#install-the-client-library",
    "title": "Configure test parameters",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#initialize-the-python-environment",
    "href": "notebooks/archive/configure_test_parameters.html#initialize-the-python-environment",
    "title": "Configure test parameters",
    "section": "Initialize the Python environment",
    "text": "Initialize the Python environment\n\nimport xgboost as xgb\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#initialize-the-client-library",
    "href": "notebooks/archive/configure_test_parameters.html#initialize-the-client-library",
    "title": "Configure test parameters",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Binary classification as the template and Marketing/Sales - Attrition/Churn Management as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\"\n)\n\n\n\nPreview the model’s documentation template\nAll models are assigned a documentation template when registered. The template defines a list of sections that are used to document the model. Each section can contain any number of rich text and test driven blocks that populate the documentation. Test driven blocks are populated by running tests against the model.\nWe can preview the model documentation template for this model by running the following code:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#load-the-demo-dataset",
    "href": "notebooks/archive/configure_test_parameters.html#load-the-demo-dataset",
    "title": "Configure test parameters",
    "section": "Load the demo dataset",
    "text": "Load the demo dataset\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\nraw_df = demo_dataset.load_data()\n\n\n\nDocumenting the model\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\n\nPrepocess the raw dataset\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\nclass_labels — an optional value to map predicted classes to class labels\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_dataset = vm.init_dataset(\n    dataset=raw_df,\n    input_id=\"raw_dataset\",\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    input_id=\"train_dataset\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    input_id=\"test_dataset\",\n    target_column=demo_dataset.target_column\n)\n\nWe also initialize a model object using vm.init_model():\n\nvm_model = vm.init_model(\n    model,\n    input_id=\"model\"\n)\n\n\n\n\nAssign predictions to the datasets\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n\nvm_train_ds.assign_predictions(\n    model=vm_model,\n    prediction_values=list(model.predict(x_train))\n)\nvm_test_ds.assign_predictions(\n    model=vm_model,\n    prediction_values=list(model.predict(x_val))\n)\n\n\n\nRun the template documentation suite\nWe are now ready to run the model’s documentation tests as defined in its template. The following function runs every test in the template and sends all documentation artifacts to the ValidMind platform.\n\nfull_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"datasets\": (vm_train_ds, vm_test_ds),\n        \"model\": vm_model\n    },\n    section=\"model_diagnosis\",\n)\n\n\n\n\nConfiguration of parameters for model diagnosis tests\nEach test has its default parameters and their values depending on the use case you are trying to solve. ValidMind’s developer framework exposes these parameters at the user level so that they can be adjusted based on requirements.\nThe config can be applied to a specific test to override the default configuration parameters.\nThe format of the config is:\nconfig = {\n    \"&lt;test1_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n     \"&lt;test2_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n}\nUsers can input the configuration to run_documentation_tests() and run_test_suite() using config, allowing fine-tuning the suite according to the specific configuration requirements.\n\nconfig = {\n    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n        \"params\": {\n            \"cut_off_percentage\": 3,\n            \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n        },\n    },\n    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n        \"params\": {\n            \"features_columns\": [\"Age\", \"Balance\"],\n            \"accuracy_gap_threshold\": 85,\n        },\n    },\n    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n        \"params\": {\n            \"features_columns\": [\"Balance\", \"Tenure\"],\n            \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n            \"accuracy_decay_threshold\": 4,\n        },\n    }\n}\n\nfull_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"datasets\": (vm_train_ds, vm_test_ds),\n        \"model\": vm_model\n    },\n    section=\"model_diagnosis\",\n)"
  },
  {
    "objectID": "notebooks/archive/configure_test_parameters.html#next-steps",
    "href": "notebooks/archive/configure_test_parameters.html#next-steps",
    "title": "Configure test parameters",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html",
    "href": "notebooks/how_to/explore_tests.html",
    "title": "Explore tests",
    "section": "",
    "text": "View and learn more about the tests available in the ValidMind Developer Framework, including code examples and usage of key functions.\nIn this notebook, we’ll dive deep into the utilities available for viewing and understanding the various tests that ValidMind provides through the tests module. Whether you’re just getting started or looking for advanced tips, you’ll find clear examples and explanations to assist you every step of the way.\nBefore we go into the details, let’s import the describe_test and list_tests functions from the validmind.tests module. These are the two functions that can be used to easily filter through tests and view details for individual tests.\nfrom validmind.tests import (\n    describe_test,\n    list_tests,\n    list_task_types,\n    list_tags,\n    list_tasks_and_tags,\n)"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#contents",
    "href": "notebooks/how_to/explore_tests.html#contents",
    "title": "Explore tests",
    "section": "Contents",
    "text": "Contents\n\nListing All Tests\nUnderstanding Tags and Task Types\nSearching for Specific Tests using tags and task_types\nDelving into Test Details with describe_test\nNext steps\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#listing-all-tests",
    "href": "notebooks/how_to/explore_tests.html#listing-all-tests",
    "title": "Explore tests",
    "section": "Listing All Tests",
    "text": "Listing All Tests\nThe list_tests function provides a convenient way to retrieve all available tests in the validmind.tests module. When invoked without any parameters, it returns a pandas DataFrame containing detailed information about each test.\n\nlist_tests()\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nThresholdTest\nBias\nEvaluates bias in a Large Language Model based on the order and distribution of exemplars in a prompt....\nvalidmind.prompt_validation.Bias\n\n\nThresholdTest\nClarity\nEvaluates and scores the clarity of prompts in a Large Language Model based on specified guidelines....\nvalidmind.prompt_validation.Clarity\n\n\nThresholdTest\nSpecificity\nEvaluates and scores the specificity of prompts provided to a Large Language Model (LLM), based on clarity,...\nvalidmind.prompt_validation.Specificity\n\n\nThresholdTest\nRobustness\nAssesses the robustness of prompts provided to a Large Language Model under varying conditions and contexts....\nvalidmind.prompt_validation.Robustness\n\n\nThresholdTest\nNegative Instruction\nEvaluates and grades the use of affirmative, proactive language over negative instructions in LLM prompts....\nvalidmind.prompt_validation.NegativeInstruction\n\n\nThresholdTest\nConciseness\nAnalyzes and grades the conciseness of prompts provided to a Large Language Model....\nvalidmind.prompt_validation.Conciseness\n\n\nThresholdTest\nDelimitation\nEvaluates the proper use of delimiters in prompts provided to Large Language Models....\nvalidmind.prompt_validation.Delimitation\n\n\nMetric\nBert Score\nEvaluates text generation models' performance by calculating precision, recall, and F1 score based on BERT...\nvalidmind.model_validation.BertScore\n\n\nMetric\nRegard Score\n**Purpose:**...\nvalidmind.model_validation.RegardScore\n\n\nMetric\nBleu Score\nAssesses translation quality by comparing machine-translated sentences with human-translated ones using BLEU score....\nvalidmind.model_validation.BleuScore\n\n\nMetric\nContextual Recall\nEvaluates a Natural Language Generation model's ability to generate contextually relevant and factually correct...\nvalidmind.model_validation.ContextualRecall\n\n\nMetric\nRegard Histogram\n**Purpose:**...\nvalidmind.model_validation.RegardHistogram\n\n\nMetric\nToxicity Histogram\n**Purpose:**...\nvalidmind.model_validation.ToxicityHistogram\n\n\nMetric\nRouge Metrics\nEvaluates the quality of machine-generated text using various ROUGE metrics, and visualizes the results....\nvalidmind.model_validation.RougeMetrics\n\n\nMetric\nModel Metadata\nExtracts and summarizes critical metadata from a machine learning model instance for comprehensive analysis....\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nBert Score Aggregate\nEvaluates the aggregate performance of text generation models by computing the average precision, recall,...\nvalidmind.model_validation.BertScoreAggregate\n\n\nMetric\nCluster Size Distribution\nCompares and visualizes the distribution of cluster sizes in model predictions and actual data for assessing...\nvalidmind.model_validation.ClusterSizeDistribution\n\n\nMetric\nToken Disparity\nAssess and visualize token count disparity between model's predicted and actual dataset....\nvalidmind.model_validation.TokenDisparity\n\n\nMetric\nToxicity Score\n**Purpose:**...\nvalidmind.model_validation.ToxicityScore\n\n\nMetric\nRouge Metrics Aggregate\nEvaluates the average quality of machine-generated text using various ROUGE metrics and visualizes the aggregated results....\nvalidmind.model_validation.RougeMetricsAggregate\n\n\nMetric\nEmbeddings Visualization D\nVisualizes 2D representation of text embeddings generated by a model using t-SNE technique....\nvalidmind.model_validation.embeddings.EmbeddingsVisualization2D\n\n\nThresholdTest\nStability Analysis Random Noise\nEvaluate robustness of embeddings models to random noise introduced by using...\nvalidmind.model_validation.embeddings.StabilityAnalysisRandomNoise\n\n\nMetric\nCosine Similarity Distribution\nAssesses the similarity between predicted text embeddings from a model using a Cosine Similarity distribution...\nvalidmind.model_validation.embeddings.CosineSimilarityDistribution\n\n\nThresholdTest\nStability Analysis Translation\nEvaluate robustness of embeddings models to noise introduced by translating...\nvalidmind.model_validation.embeddings.StabilityAnalysisTranslation\n\n\nMetric\nCluster Distribution\nAssesses the distribution of text embeddings across clusters produced by a model using KMeans clustering....\nvalidmind.model_validation.embeddings.ClusterDistribution\n\n\nThresholdTest\nStability Analysis\nBase class for embeddings stability analysis tests\nvalidmind.model_validation.embeddings.StabilityAnalysis\n\n\nThresholdTest\nStability Analysis Keyword\nEvaluate robustness of embeddings models to keyword swaps on the test dataset...\nvalidmind.model_validation.embeddings.StabilityAnalysisKeyword\n\n\nThresholdTest\nStability Analysis Synonyms\nEvaluates the stability of text embeddings models when words in test data are replaced by their synonyms randomly....\nvalidmind.model_validation.embeddings.StabilityAnalysisSynonyms\n\n\nMetric\nDescriptive Analytics\nEvaluates statistical properties of text embeddings in an ML model via mean, median, and standard deviation...\nvalidmind.model_validation.embeddings.DescriptiveAnalytics\n\n\nMetric\nRegression Models Performance Comparison\nCompares and evaluates the performance of multiple regression models using five different metrics: MAE, MSE, RMSE,...\nvalidmind.model_validation.sklearn.RegressionModelsPerformanceComparison\n\n\nMetric\nAdjusted Mutual Information\nEvaluates clustering model performance by measuring mutual information between true and predicted labels, adjusting...\nvalidmind.model_validation.sklearn.AdjustedMutualInformation\n\n\nMetric\nSilhouette Plot\nCalculates and visualizes Silhouette Score, assessing degree of data point suitability to its cluster in ML models....\nvalidmind.model_validation.sklearn.SilhouettePlot\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nAdjusted Rand Index\nMeasures the similarity between two data clusters using the Adjusted Rand Index (ARI) metric in clustering machine...\nvalidmind.model_validation.sklearn.AdjustedRandIndex\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nHomogeneity Score\nAssesses clustering homogeneity by comparing true and predicted labels, scoring from 0 (heterogeneous) to 1...\nvalidmind.model_validation.sklearn.HomogeneityScore\n\n\nMetric\nCompleteness Score\nEvaluates a clustering model's capacity to categorize instances from a single class into the same cluster....\nvalidmind.model_validation.sklearn.CompletenessScore\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nCluster Performance Metrics\nEvaluates the performance of clustering machine learning models using multiple established metrics....\nvalidmind.model_validation.sklearn.ClusterPerformanceMetrics\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nMetric\nFowlkes Mallows Score\nEvaluates the similarity between predicted and actual cluster assignments in a model using the Fowlkes-Mallows...\nvalidmind.model_validation.sklearn.FowlkesMallowsScore\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nCluster Cosine Similarity\nMeasures the intra-cluster similarity of a clustering model using cosine similarity....\nvalidmind.model_validation.sklearn.ClusterCosineSimilarity\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nMetric\nV Measure\nEvaluates homogeneity and completeness of a clustering model using the V Measure Score....\nvalidmind.model_validation.sklearn.VMeasure\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nMetric\nRegression Square\n**Purpose**: The purpose of the RegressionR2Square Metric test is to measure the overall goodness-of-fit of a...\nvalidmind.model_validation.sklearn.RegressionR2Square\n\n\nMetric\nRegression Errors\n**Purpose**: This metric is used to measure the performance of a regression model. It gauges the model's accuracy...\nvalidmind.model_validation.sklearn.RegressionErrors\n\n\nMetric\nCluster Performance\nEvaluates and compares a clustering model's performance on training and testing datasets using multiple defined...\nvalidmind.model_validation.sklearn.ClusterPerformance\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nHyper Parameters Tuning\nExerts exhaustive grid search to identify optimal hyperparameters for the model, improving performance....\nvalidmind.model_validation.sklearn.HyperParametersTuning\n\n\nMetric\nK Means Clusters Optimization\nOptimizes the number of clusters in K-means models using Elbow and Silhouette methods....\nvalidmind.model_validation.sklearn.KMeansClustersOptimization\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegression Models Coeffs\nCompares feature importance by evaluating and contrasting coefficients of different regression models....\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs\n\n\nMetric\nBox Pierce\nDetects autocorrelation in time-series data through the Box-Pierce test to validate model performance....\nvalidmind.model_validation.statsmodels.BoxPierce\n\n\nMetric\nRegression Coeffs Plot\nVisualizes regression coefficients with 95% confidence intervals to assess predictor variables' impact on response...\nvalidmind.model_validation.statsmodels.RegressionCoeffsPlot\n\n\nMetric\nRegression Model Sensitivity Plot\nTests the sensitivity of a regression model to variations in independent variables by applying shocks and...\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\nMetric\nRegression Models Performance\nEvaluates and compares regression models' performance using R-squared, Adjusted R-squared, and MSE metrics....\nvalidmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nMetric\nZivot Andrews Arch\nEvaluates the order of integration and stationarity of time series data using Zivot-Andrews unit root test....\nvalidmind.model_validation.statsmodels.ZivotAndrewsArch\n\n\nMetric\nRegression Model Outsample Comparison\nComputes MSE and RMSE for multiple regression models using out-of-sample test to assess model's prediction accuracy...\nvalidmind.model_validation.statsmodels.RegressionModelOutsampleComparison\n\n\nMetric\nRegression Model Forecast Plot Levels\nCompares and visualizes forecasted and actual values of regression models on both raw and transformed datasets....\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nPD Rating Class Plot\nAssesses and visualizes credit risk distribution across different rating classes within a dataset via default...\nvalidmind.model_validation.statsmodels.PDRatingClassPlot\n\n\nMetric\nScorecard Histogram\nCreates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model....\nvalidmind.model_validation.statsmodels.ScorecardHistogram\n\n\nMetric\nFeature Importance And Significance\nEvaluates and visualizes the statistical significance and feature importance using regression and decision tree...\nvalidmind.model_validation.statsmodels.FeatureImportanceAndSignificance\n\n\nMetric\nL Jung Box\nAssesses autocorrelations in dataset features by performing a Ljung-Box test on each feature....\nvalidmind.model_validation.statsmodels.LJungBox\n\n\nMetric\nLogistic Reg Prediction Histogram\nGenerates and visualizes histograms of the Probability of Default predictions for both positive and negative...\nvalidmind.model_validation.statsmodels.LogisticRegPredictionHistogram\n\n\nMetric\nJarque Bera\nAssesses normality of dataset features in an ML model using the Jarque-Bera test....\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nPhillips Perron Arch\nExecutes Phillips-Perron test to assess the stationarity of time series data in each ML model feature....\nvalidmind.model_validation.statsmodels.PhillipsPerronArch\n\n\nMetric\nKolmogorov Smirnov\nExecutes a feature-wise Kolmogorov-Smirnov test to evaluate alignment with normal distribution in datasets....\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nResiduals Visual Inspection\nProvides a comprehensive visual analysis of residuals for regression models utilizing various plot types....\nvalidmind.model_validation.statsmodels.ResidualsVisualInspection\n\n\nMetric\nShapiro Wilk\nEvaluates feature-wise normality of training data using the Shapiro-Wilk test....\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nScorecard Bucket Histogram\nEvaluates and visualizes distribution of risk categories in a classification model's scores, useful in credit risk...\nvalidmind.model_validation.statsmodels.ScorecardBucketHistogram\n\n\nMetric\nRegression Model In Sample Comparison\nEvaluates and compares in-sample performance of multiple regression models using R-Squared, Adjusted R-Squared,...\nvalidmind.model_validation.statsmodels.RegressionModelInSampleComparison\n\n\nMetric\nRegression Feature Significance\nAssesses and visualizes the statistical significance of features in a set of regression models....\nvalidmind.model_validation.statsmodels.RegressionFeatureSignificance\n\n\nMetric\nRegression Model Summary\nEvaluates regression model performance using metrics including R-Squared, Adjusted R-Squared, MSE, and RMSE....\nvalidmind.model_validation.statsmodels.RegressionModelSummary\n\n\nMetric\nKPSS\nExecutes KPSS unit root test to validate stationarity of time-series data in machine learning model....\nvalidmind.model_validation.statsmodels.KPSS\n\n\nMetric\nLilliefors\nAssesses the normality of feature distributions in an ML model's training dataset using the Lilliefors test....\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nLogistic Reg Cumulative Prob\nVisualizes cumulative probabilities of positive and negative classes for both training and testing in logistic...\nvalidmind.model_validation.statsmodels.LogisticRegCumulativeProb\n\n\nMetric\nRuns Test\nExecutes Runs Test on ML model to detect non-random patterns in output data sequence....\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nScorecard Probabilities Histogram\nEvaluates risk classification of a model by visualizing the distribution of default probability across score...\nvalidmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram\n\n\nMetric\nDFGLS Arch\nExecutes Dickey-Fuller GLS metric to determine order of integration and check stationarity in time series data....\nvalidmind.model_validation.statsmodels.DFGLSArch\n\n\nMetric\nAuto ARIMA\nEvaluates ARIMA models for time-series forecasting, ranking them using Bayesian and Akaike Information Criteria....\nvalidmind.model_validation.statsmodels.AutoARIMA\n\n\nThresholdTest\nADF Test\nAssesses the stationarity of time series data using the Augmented Dickey-Fuller (ADF) test....\nvalidmind.model_validation.statsmodels.ADFTest\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\nMetric\nRegression Model Forecast Plot\nGenerates plots to visually compare the forecasted outcomes of one or more regression models against actual...\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlot\n\n\nMetric\nADF\nAssesses the stationarity of a time series dataset using the Augmented Dickey-Fuller (ADF) test....\nvalidmind.model_validation.statsmodels.ADF\n\n\nMetric\nDurbin Watson Test\nAssesses autocorrelation in time series data features using the Durbin-Watson statistic....\nvalidmind.model_validation.statsmodels.DurbinWatsonTest\n\n\nMetric\nMissing Values Risk\nAssesses and quantifies the risk related to missing values in a dataset used for training an ML model....\nvalidmind.data_validation.MissingValuesRisk\n\n\nMetric\nIQR Outliers Table\nDetermines and summarizes outliers in numerical features using Interquartile Range method....\nvalidmind.data_validation.IQROutliersTable\n\n\nMetric\nBivariate Features Bar Plots\nGenerates visual bar plots to analyze the relationship between paired features within categorical data in the model....\nvalidmind.data_validation.BivariateFeaturesBarPlots\n\n\nThresholdTest\nSkewness\nEvaluates the skewness of numerical data in a machine learning model and checks if it falls below a set maximum...\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nTests dataset for duplicate entries, ensuring model reliability via data quality verification....\nvalidmind.data_validation.Duplicates\n\n\nMetric\nMissing Values Bar Plot\nCreates a bar plot showcasing the percentage of missing values in each column of the dataset with risk...\nvalidmind.data_validation.MissingValuesBarPlot\n\n\nMetric\nDataset Description\nProvides comprehensive analysis and statistical summaries of each field in a machine learning model's dataset....\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatter Plot\nCreates a scatter plot matrix to visually analyze feature relationships, patterns, and outliers in a dataset....\nvalidmind.data_validation.ScatterPlot\n\n\nThresholdTest\nTime Series Outliers\nIdentifies and visualizes outliers in time-series data using z-score method....\nvalidmind.data_validation.TimeSeriesOutliers\n\n\nMetric\nTabular Categorical Bar Plots\nGenerates and visualizes bar plots for each category in categorical features to evaluate dataset's composition....\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nAuto Stationarity\nAutomates Augmented Dickey-Fuller test to assess stationarity across multiple time series in a DataFrame....\nvalidmind.data_validation.AutoStationarity\n\n\nMetric\nDescriptive Statistics\nPerforms a detailed descriptive statistical analysis of both numerical and categorical data within a model's...\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nANOVA One Way Table\nApplies one-way ANOVA (Analysis of Variance) to identify statistically significant numerical features in the...\nvalidmind.data_validation.ANOVAOneWayTable\n\n\nMetric\nTarget Rate Bar Plots\nGenerates bar plots visualizing the default rates of categorical features for a classification machine learning...\nvalidmind.data_validation.TargetRateBarPlots\n\n\nMetric\nPearson Correlation Matrix\nEvaluates linear dependency between numerical variables in a dataset via a Pearson Correlation coefficient heat map....\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nFeature Target Correlation Plot\nVisualizes the correlation between input features and model's target output in a color-coded horizontal bar plot....\nvalidmind.data_validation.FeatureTargetCorrelationPlot\n\n\nMetric\nTabular Numerical Histograms\nGenerates histograms for each numerical feature in a dataset to provide visual insights into data distribution and...\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nMetric\nIsolation Forest Outliers\nDetects outliers in a dataset using the Isolation Forest algorithm and visualizes results through scatter plots....\nvalidmind.data_validation.IsolationForestOutliers\n\n\nMetric\nChi Squared Features Table\nExecutes Chi-Squared test for each categorical feature against a target column to assess significant association....\nvalidmind.data_validation.ChiSquaredFeaturesTable\n\n\nThresholdTest\nHigh Cardinality\nAssesses the number of unique values in categorical columns to detect high cardinality and potential overfitting....\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissing Values\nEvaluates dataset quality by ensuring missing value ratio across all features does not exceed a set threshold....\nvalidmind.data_validation.MissingValues\n\n\nMetric\nDefault Ratesby Risk Band Plot\nGenerates a bar plot showcasing the distribution of default rates across different risk bands in a dataset....\nvalidmind.data_validation.DefaultRatesbyRiskBandPlot\n\n\nMetric\nRolling Stats Plot\nThis test evaluates the stationarity of time series data by plotting its rolling mean and standard deviation....\nvalidmind.data_validation.RollingStatsPlot\n\n\nMetric\nTabular Description Tables\nSummarizes key descriptive statistics for numerical, categorical, and datetime variables in a dataset....\nvalidmind.data_validation.TabularDescriptionTables\n\n\nMetric\nAuto MA\nAutomatically selects the optimal Moving Average (MA) order for each variable in a time series dataset based on...\nvalidmind.data_validation.AutoMA\n\n\nThresholdTest\nUnique Rows\nVerifies the diversity of the dataset by ensuring that the count of unique rows exceeds a prescribed threshold....\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nToo Many Zero Values\nIdentifies numerical columns in a dataset that contain an excessive number of zero values, defined by a threshold...\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHigh Pearson Correlation\nIdentifies highly correlated feature pairs in a dataset suggesting feature redundancy or multicollinearity....\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nAC Fand PACF Plot\nAnalyzes time series data using Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to...\nvalidmind.data_validation.ACFandPACFPlot\n\n\nMetric\nBivariate Histograms\nGenerates bivariate histograms for paired features, aiding in visual inspection of categorical variables'...\nvalidmind.data_validation.BivariateHistograms\n\n\nMetric\nWOE Bin Table\nCalculates and assesses the Weight of Evidence (WoE) and Information Value (IV) of each feature in a ML model....\nvalidmind.data_validation.WOEBinTable\n\n\nMetric\nHeatmap Feature Correlations\nCreates a heatmap to visually represent correlation patterns between pairs of numerical features in a dataset....\nvalidmind.data_validation.HeatmapFeatureCorrelations\n\n\nThresholdTest\nTime Series Frequency\nEvaluates consistency of time series data frequency and generates a frequency plot....\nvalidmind.data_validation.TimeSeriesFrequency\n\n\nMetric\nDataset Split\nEvaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML...\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nSpread Plot\nVisualizes the spread relationship between pairs of time-series variables in a dataset, thereby aiding in...\nvalidmind.data_validation.SpreadPlot\n\n\nMetric\nTime Series Line Plot\nGenerates and analyses time-series data through line plots revealing trends, patterns, anomalies over time....\nvalidmind.data_validation.TimeSeriesLinePlot\n\n\nMetric\nPi T Credit Scores Histogram\nGenerates a histogram visualization for observed and predicted credit default scores....\nvalidmind.data_validation.PiTCreditScoresHistogram\n\n\nMetric\nAuto Seasonality\nAutomatically identifies and quantifies optimal seasonality in time series data to improve forecasting model...\nvalidmind.data_validation.AutoSeasonality\n\n\nMetric\nBivariate Scatter Plots\nGenerates bivariate scatterplots to visually inspect relationships between pairs of predictor variables in machine...\nvalidmind.data_validation.BivariateScatterPlots\n\n\nMetric\nEngle Granger Coint\nValidates co-integration in pairs of time series data using the Engle-Granger test and classifies them as...\nvalidmind.data_validation.EngleGrangerCoint\n\n\nThresholdTest\nTime Series Missing Values\nValidates time-series data quality by confirming the count of missing values is below a certain threshold....\nvalidmind.data_validation.TimeSeriesMissingValues\n\n\nMetric\nTime Series Histogram\nVisualizes distribution of time-series data using histograms and Kernel Density Estimation (KDE) lines....\nvalidmind.data_validation.TimeSeriesHistogram\n\n\nMetric\nLagged Correlation Heatmap\nAssesses and visualizes correlation between target variable and lagged independent variables in a time-series...\nvalidmind.data_validation.LaggedCorrelationHeatmap\n\n\nMetric\nSeasonal Decompose\nDecomposes dataset features into observed, trend, seasonal, and residual components to identify patterns and...\nvalidmind.data_validation.SeasonalDecompose\n\n\nMetric\nWOE Bin Plots\nGenerates visualizations of Weight of Evidence (WoE) and Information Value (IV) for understanding predictive power...\nvalidmind.data_validation.WOEBinPlots\n\n\nThresholdTest\nClass Imbalance\nEvaluates and quantifies class distribution imbalance in a dataset used by a machine learning model....\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nIQR Outliers Bar Plot\nVisualizes outlier distribution across percentiles in numerical data using Interquartile Range (IQR) method....\nvalidmind.data_validation.IQROutliersBarPlot\n\n\nMetric\nPi TPD Histogram\nAssesses credit risk prediction accuracy of a model by comparing actual and predicted defaults at a chosen point in...\nvalidmind.data_validation.PiTPDHistogram\n\n\nMetric\nAuto AR\nAutomatically identifies the optimal Autoregressive (AR) order for a time series using BIC and AIC criteria....\nvalidmind.data_validation.AutoAR\n\n\nMetric\nTabular Date Time Histograms\nGenerates histograms to provide graphical insight into the distribution of time intervals in model's datetime data....\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\nMetric\nPunctuations\nAnalyzes and visualizes the frequency distribution of punctuation usage in a given text dataset....\nvalidmind.data_validation.nlp.Punctuations\n\n\nMetric\nCommon Words\nIdentifies and visualizes the 40 most frequent non-stopwords in a specified text column within a dataset....\nvalidmind.data_validation.nlp.CommonWords\n\n\nThresholdTest\nHashtags\nAssesses hashtag frequency in a text column, highlighting usage trends and potential dataset bias or spam....\nvalidmind.data_validation.nlp.Hashtags\n\n\nThresholdTest\nMentions\nCalculates and visualizes frequencies of '@' prefixed mentions in a text-based dataset for NLP model analysis....\nvalidmind.data_validation.nlp.Mentions\n\n\nMetric\nText Description\nPerforms comprehensive textual analysis on a dataset using NLTK, evaluating various parameters and generating...\nvalidmind.data_validation.nlp.TextDescription\n\n\nThresholdTest\nStop Words\nEvaluates and visualizes the frequency of English stop words in a text dataset against a defined threshold....\nvalidmind.data_validation.nlp.StopWords"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#understanding-tags-and-task-types",
    "href": "notebooks/how_to/explore_tests.html#understanding-tags-and-task-types",
    "title": "Explore tests",
    "section": "Understanding Tags and Task Types",
    "text": "Understanding Tags and Task Types\nEffectively using ValidMind’s tests involves a deep understanding of its ‘tags’ and ‘task types’. Here’s a breakdown:\n\nTask Types: Represent the kind of modeling task associated with a test. For instance:\n\nclassification: Works with Classification Models and Datasets\nregression: Works with Regression Models and Datasets\ntext classification: Works with Text Classification Models and Datasets\ntext summarization: Works with Text Summarization Models and Datasets\n\nTags: Free-form descriptors providing more details about the test, what data and models the test is compatible with and what category the test falls into etc. Some examples include:\n\nllm: Tests that work with Large Language Models\nnlp: Tests relevant for natural language processing.\nbinary_classification: Tests for binary classification tasks.\nforecasting: Tests for forecasting and time-series analysis.\ntabular_data: Tests for tabular data like CSVs and Excel spreadsheets.\n\n\nYou can use the functions list_task_types() and list_tags() to view all the task_types and tags used for classifying all the tests available in the developer framework:\n\nlist_task_types()\n\n['feature_extraction',\n 'regression',\n 'classification',\n 'clustering',\n 'text_summarization',\n 'text_classification']\n\n\n\nlist_tags()\n\n['regard_histogram',\n 'llm',\n 'feature_importance',\n 'anomaly_detection',\n 'kmeans',\n 'credit_risk',\n 'time_series_data',\n 'model_comparison',\n 'tabular_data',\n 'numerical_data',\n 'model_interpretation',\n 'visualization',\n 'multiclass_classification',\n 'statsmodels',\n 'correlation',\n 'statistical_test',\n 'model_diagnosis',\n 'categorical_data',\n 'forecasting',\n 'unit_root_test',\n 'frequency_analysis',\n 'binary_classification',\n 'model_metadata',\n 'senstivity_analysis',\n 'toxicity_histogram',\n 'few_shot',\n 'data_quality',\n 'data_distribution',\n 'logistic_regression',\n 'text_embeddings',\n 'text_data',\n 'nlp',\n 'zero_shot',\n 'risk_analysis',\n 'regard_score',\n 'model_selection',\n 'model_performance',\n 'seasonality',\n 'sklearn',\n 'stationarity',\n 'toxicity_line_plot']\n\n\nIf you want to see which tags correspond to which task type, you can use the function list_tasks_and_tags():\n\nlist_tasks_and_tags()\n\n\n\n\n\n\nTask Type\nTags\n\n\n\n\ntext_classification\nregard_histogram, llm, feature_importance, time_series_data, model_comparison, tabular_data, multiclass_classification, visualization, model_diagnosis, frequency_analysis, binary_classification, model_metadata, toxicity_histogram, few_shot, text_data, nlp, zero_shot, regard_score, model_performance, sklearn, toxicity_line_plot\n\n\ntext_summarization\nnlp, zero_shot, llm, regard_histogram, regard_score, model_metadata, time_series_data, tabular_data, toxicity_histogram, few_shot, visualization, frequency_analysis, toxicity_line_plot, text_data\n\n\nclassification\nfeature_importance, anomaly_detection, credit_risk, time_series_data, model_comparison, tabular_data, numerical_data, multiclass_classification, visualization, statsmodels, correlation, statistical_test, model_diagnosis, categorical_data, binary_classification, model_metadata, data_quality, data_distribution, logistic_regression, text_data, risk_analysis, model_performance, sklearn\n\n\nregression\nfeature_importance, time_series_data, model_comparison, tabular_data, numerical_data, model_interpretation, visualization, statsmodels, correlation, statistical_test, categorical_data, forecasting, unit_root_test, model_metadata, senstivity_analysis, data_quality, data_distribution, text_data, risk_analysis, model_selection, model_performance, seasonality, sklearn, stationarity\n\n\nclustering\nkmeans, model_performance, sklearn\n\n\nfeature_extraction\nllm, visualization, text_embeddings, text_data"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#searching-for-specific-tests-using-tags-and-task_types",
    "href": "notebooks/how_to/explore_tests.html#searching-for-specific-tests-using-tags-and-task_types",
    "title": "Explore tests",
    "section": "Searching for Specific Tests using tags and task_types",
    "text": "Searching for Specific Tests using tags and task_types\nWhile listing all tests is valuable, there are times when you need to narrow down your search. The list_tests function offers filter, task, and tags parameters to assist in this.\nIf you’re targeting a specific test or tests that match a particular task type, the filter parameter comes in handy. For example, to list tests that are compatible with ‘sklearn’ models:\n\nlist_tests(filter=\"sklearn\")\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nAdjusted Rand Index\nMeasures the similarity between two data clusters using the Adjusted Rand Index (ARI) metric in clustering machine...\nvalidmind.model_validation.sklearn.AdjustedRandIndex\n\n\nMetric\nK Means Clusters Optimization\nOptimizes the number of clusters in K-means models using Elbow and Silhouette methods....\nvalidmind.model_validation.sklearn.KMeansClustersOptimization\n\n\nMetric\nHomogeneity Score\nAssesses clustering homogeneity by comparing true and predicted labels, scoring from 0 (heterogeneous) to 1...\nvalidmind.model_validation.sklearn.HomogeneityScore\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nMetric\nCluster Performance\nEvaluates and compares a clustering model's performance on training and testing datasets using multiple defined...\nvalidmind.model_validation.sklearn.ClusterPerformance\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nSilhouette Plot\nCalculates and visualizes Silhouette Score, assessing degree of data point suitability to its cluster in ML models....\nvalidmind.model_validation.sklearn.SilhouettePlot\n\n\nMetric\nRegression Errors\n**Purpose**: This metric is used to measure the performance of a regression model. It gauges the model's accuracy...\nvalidmind.model_validation.sklearn.RegressionErrors\n\n\nMetric\nHyper Parameters Tuning\nExerts exhaustive grid search to identify optimal hyperparameters for the model, improving performance....\nvalidmind.model_validation.sklearn.HyperParametersTuning\n\n\nMetric\nAdjusted Mutual Information\nEvaluates clustering model performance by measuring mutual information between true and predicted labels, adjusting...\nvalidmind.model_validation.sklearn.AdjustedMutualInformation\n\n\nMetric\nCompleteness Score\nEvaluates a clustering model's capacity to categorize instances from a single class into the same cluster....\nvalidmind.model_validation.sklearn.CompletenessScore\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegression Square\n**Purpose**: The purpose of the RegressionR2Square Metric test is to measure the overall goodness-of-fit of a...\nvalidmind.model_validation.sklearn.RegressionR2Square\n\n\nMetric\nV Measure\nEvaluates homogeneity and completeness of a clustering model using the V Measure Score....\nvalidmind.model_validation.sklearn.VMeasure\n\n\nMetric\nRegression Models Performance Comparison\nCompares and evaluates the performance of multiple regression models using five different metrics: MAE, MSE, RMSE,...\nvalidmind.model_validation.sklearn.RegressionModelsPerformanceComparison\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nCluster Cosine Similarity\nMeasures the intra-cluster similarity of a clustering model using cosine similarity....\nvalidmind.model_validation.sklearn.ClusterCosineSimilarity\n\n\nMetric\nCluster Size Distribution\nCompares and visualizes the distribution of cluster sizes in model predictions and actual data for assessing...\nvalidmind.model_validation.ClusterSizeDistribution\n\n\nMetric\nFowlkes Mallows Score\nEvaluates the similarity between predicted and actual cluster assignments in a model using the Fowlkes-Mallows...\nvalidmind.model_validation.sklearn.FowlkesMallowsScore\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nMetric\nCluster Performance Metrics\nEvaluates the performance of clustering machine learning models using multiple established metrics....\nvalidmind.model_validation.sklearn.ClusterPerformanceMetrics\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\n\n\n\nThe task parameter is designed for pinpointing tests that align with a specific task type. For instance, to find tests tailored for ‘classification’ tasks:\n\nlist_tests(task=\"classification\")\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nModel Metadata\nExtracts and summarizes critical metadata from a machine learning model instance for comprehensive analysis....\nvalidmind.model_validation.ModelMetadata\n\n\nThresholdTest\nRobustness Diagnosis\nEvaluates the robustness of a machine learning model by injecting Gaussian noise to input data and measuring...\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAP Global Importance\nEvaluates and visualizes global feature importance using SHAP values for model explanation and risk identification....\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nThresholdTest\nOverfit Diagnosis\nDetects and visualizes overfit regions in an ML model by comparing performance on training and test datasets....\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutation Feature Importance\nAssesses the significance of each feature in a model by evaluating the impact on model performance when feature...\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimum ROCAUC Score\nValidates model by checking if the ROC AUC score meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifier Performance\nEvaluates performance of binary or multiclass classification models using precision, recall, F1-Score, accuracy,...\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimum Score\nEvaluates if the model's F1 score on the validation set meets a predefined minimum threshold....\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nHyper Parameters Tuning\nExerts exhaustive grid search to identify optimal hyperparameters for the model, improving performance....\nvalidmind.model_validation.sklearn.HyperParametersTuning\n\n\nMetric\nModels Performance Comparison\nEvaluates and compares the performance of multiple Machine Learning models using various metrics like accuracy,...\nvalidmind.model_validation.sklearn.ModelsPerformanceComparison\n\n\nThresholdTest\nWeakspots Diagnosis\nIdentifies and visualizes weak spots in a machine learning model's performance across various sections of the...\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulation Stability Index\nEvaluates the Population Stability Index (PSI) to quantify the stability of an ML model's predictions across...\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimum Accuracy\nChecks if the model's prediction accuracy meets or surpasses a specified threshold....\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nPD Rating Class Plot\nAssesses and visualizes credit risk distribution across different rating classes within a dataset via default...\nvalidmind.model_validation.statsmodels.PDRatingClassPlot\n\n\nMetric\nScorecard Histogram\nCreates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model....\nvalidmind.model_validation.statsmodels.ScorecardHistogram\n\n\nMetric\nLogistic Reg Prediction Histogram\nGenerates and visualizes histograms of the Probability of Default predictions for both positive and negative...\nvalidmind.model_validation.statsmodels.LogisticRegPredictionHistogram\n\n\nMetric\nJarque Bera\nAssesses normality of dataset features in an ML model using the Jarque-Bera test....\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nKolmogorov Smirnov\nExecutes a feature-wise Kolmogorov-Smirnov test to evaluate alignment with normal distribution in datasets....\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nShapiro Wilk\nEvaluates feature-wise normality of training data using the Shapiro-Wilk test....\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nScorecard Bucket Histogram\nEvaluates and visualizes distribution of risk categories in a classification model's scores, useful in credit risk...\nvalidmind.model_validation.statsmodels.ScorecardBucketHistogram\n\n\nMetric\nLilliefors\nAssesses the normality of feature distributions in an ML model's training dataset using the Lilliefors test....\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nLogistic Reg Cumulative Prob\nVisualizes cumulative probabilities of positive and negative classes for both training and testing in logistic...\nvalidmind.model_validation.statsmodels.LogisticRegCumulativeProb\n\n\nMetric\nRuns Test\nExecutes Runs Test on ML model to detect non-random patterns in output data sequence....\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nScorecard Probabilities Histogram\nEvaluates risk classification of a model by visualizing the distribution of default probability across score...\nvalidmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\nMetric\nMissing Values Risk\nAssesses and quantifies the risk related to missing values in a dataset used for training an ML model....\nvalidmind.data_validation.MissingValuesRisk\n\n\nMetric\nIQR Outliers Table\nDetermines and summarizes outliers in numerical features using Interquartile Range method....\nvalidmind.data_validation.IQROutliersTable\n\n\nMetric\nBivariate Features Bar Plots\nGenerates visual bar plots to analyze the relationship between paired features within categorical data in the model....\nvalidmind.data_validation.BivariateFeaturesBarPlots\n\n\nThresholdTest\nSkewness\nEvaluates the skewness of numerical data in a machine learning model and checks if it falls below a set maximum...\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nTests dataset for duplicate entries, ensuring model reliability via data quality verification....\nvalidmind.data_validation.Duplicates\n\n\nMetric\nMissing Values Bar Plot\nCreates a bar plot showcasing the percentage of missing values in each column of the dataset with risk...\nvalidmind.data_validation.MissingValuesBarPlot\n\n\nMetric\nDataset Description\nProvides comprehensive analysis and statistical summaries of each field in a machine learning model's dataset....\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatter Plot\nCreates a scatter plot matrix to visually analyze feature relationships, patterns, and outliers in a dataset....\nvalidmind.data_validation.ScatterPlot\n\n\nMetric\nTabular Categorical Bar Plots\nGenerates and visualizes bar plots for each category in categorical features to evaluate dataset's composition....\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nDescriptive Statistics\nPerforms a detailed descriptive statistical analysis of both numerical and categorical data within a model's...\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nANOVA One Way Table\nApplies one-way ANOVA (Analysis of Variance) to identify statistically significant numerical features in the...\nvalidmind.data_validation.ANOVAOneWayTable\n\n\nMetric\nTarget Rate Bar Plots\nGenerates bar plots visualizing the default rates of categorical features for a classification machine learning...\nvalidmind.data_validation.TargetRateBarPlots\n\n\nMetric\nPearson Correlation Matrix\nEvaluates linear dependency between numerical variables in a dataset via a Pearson Correlation coefficient heat map....\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nFeature Target Correlation Plot\nVisualizes the correlation between input features and model's target output in a color-coded horizontal bar plot....\nvalidmind.data_validation.FeatureTargetCorrelationPlot\n\n\nMetric\nTabular Numerical Histograms\nGenerates histograms for each numerical feature in a dataset to provide visual insights into data distribution and...\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nMetric\nIsolation Forest Outliers\nDetects outliers in a dataset using the Isolation Forest algorithm and visualizes results through scatter plots....\nvalidmind.data_validation.IsolationForestOutliers\n\n\nMetric\nChi Squared Features Table\nExecutes Chi-Squared test for each categorical feature against a target column to assess significant association....\nvalidmind.data_validation.ChiSquaredFeaturesTable\n\n\nThresholdTest\nHigh Cardinality\nAssesses the number of unique values in categorical columns to detect high cardinality and potential overfitting....\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissing Values\nEvaluates dataset quality by ensuring missing value ratio across all features does not exceed a set threshold....\nvalidmind.data_validation.MissingValues\n\n\nMetric\nDefault Ratesby Risk Band Plot\nGenerates a bar plot showcasing the distribution of default rates across different risk bands in a dataset....\nvalidmind.data_validation.DefaultRatesbyRiskBandPlot\n\n\nMetric\nTabular Description Tables\nSummarizes key descriptive statistics for numerical, categorical, and datetime variables in a dataset....\nvalidmind.data_validation.TabularDescriptionTables\n\n\nThresholdTest\nUnique Rows\nVerifies the diversity of the dataset by ensuring that the count of unique rows exceeds a prescribed threshold....\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nToo Many Zero Values\nIdentifies numerical columns in a dataset that contain an excessive number of zero values, defined by a threshold...\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHigh Pearson Correlation\nIdentifies highly correlated feature pairs in a dataset suggesting feature redundancy or multicollinearity....\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nBivariate Histograms\nGenerates bivariate histograms for paired features, aiding in visual inspection of categorical variables'...\nvalidmind.data_validation.BivariateHistograms\n\n\nMetric\nWOE Bin Table\nCalculates and assesses the Weight of Evidence (WoE) and Information Value (IV) of each feature in a ML model....\nvalidmind.data_validation.WOEBinTable\n\n\nMetric\nHeatmap Feature Correlations\nCreates a heatmap to visually represent correlation patterns between pairs of numerical features in a dataset....\nvalidmind.data_validation.HeatmapFeatureCorrelations\n\n\nMetric\nDataset Split\nEvaluates and visualizes the distribution proportions among training, testing, and validation datasets of an ML...\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nPi T Credit Scores Histogram\nGenerates a histogram visualization for observed and predicted credit default scores....\nvalidmind.data_validation.PiTCreditScoresHistogram\n\n\nMetric\nBivariate Scatter Plots\nGenerates bivariate scatterplots to visually inspect relationships between pairs of predictor variables in machine...\nvalidmind.data_validation.BivariateScatterPlots\n\n\nMetric\nWOE Bin Plots\nGenerates visualizations of Weight of Evidence (WoE) and Information Value (IV) for understanding predictive power...\nvalidmind.data_validation.WOEBinPlots\n\n\nThresholdTest\nClass Imbalance\nEvaluates and quantifies class distribution imbalance in a dataset used by a machine learning model....\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nIQR Outliers Bar Plot\nVisualizes outlier distribution across percentiles in numerical data using Interquartile Range (IQR) method....\nvalidmind.data_validation.IQROutliersBarPlot\n\n\nMetric\nPi TPD Histogram\nAssesses credit risk prediction accuracy of a model by comparing actual and predicted defaults at a chosen point in...\nvalidmind.data_validation.PiTPDHistogram\n\n\nMetric\nTabular Date Time Histograms\nGenerates histograms to provide graphical insight into the distribution of time intervals in model's datetime data....\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\n\n\n\nThe tags parameter facilitates searching tests by their tags. For instance, if you’re interested in only tests associated designed for model_performance that produce a plot (denoted by the visualization tag)\n\nlist_tests(tags=[\"model_performance\", \"visualization\"])\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nMetric\nLog Regression Confusion Matrix\nGenerates a confusion matrix for logistic regression model performance, utilizing thresholded probabilities for...\nvalidmind.model_validation.statsmodels.LogRegressionConfusionMatrix\n\n\nMetric\nGINI Table\nEvaluates classification model performance using AUC, GINI, and KS metrics for training and test datasets....\nvalidmind.model_validation.statsmodels.GINITable\n\n\n\n\n\nThe above parameters can be combined to create complex queries. For instance, to find tests that are compatible with ‘sklearn’ models, designed for ‘classification’ tasks, and produce a plot:\n\nlist_tests(\n    tags=[\"model_performance\", \"visualization\", \"sklearn\"], task=\"classification\"\n)\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nConfusion Matrix\nEvaluates and visually represents the classification ML model's predictive performance using a Confusion Matrix...\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nPrecision Recall Curve\nEvaluates the precision-recall trade-off for binary classification models and visualizes the Precision-Recall curve....\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nROC Curve\nEvaluates binary classification model performance by generating and plotting the Receiver Operating Characteristic...\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTraining Test Degradation\nTests if model performance degradation between training and test datasets exceeds a predefined threshold....\nvalidmind.model_validation.sklearn.TrainingTestDegradation"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#delving-into-test-details-with-describe_test",
    "href": "notebooks/how_to/explore_tests.html#delving-into-test-details-with-describe_test",
    "title": "Explore tests",
    "section": "Delving into Test Details with describe_test",
    "text": "Delving into Test Details with describe_test\nAfter identifying a set of potential tests, you might want to explore the specifics of an individual test. The describe_test function provides a deep dive into the details of a test. It reveals the test name, description, ID, test type, and required inputs. Below, we showcase how to describe a test using its ID:\n\ndescribe_test(\"validmind.model_validation.sklearn.OverfitDiagnosis\")"
  },
  {
    "objectID": "notebooks/how_to/explore_tests.html#next-steps",
    "href": "notebooks/how_to/explore_tests.html#next-steps",
    "title": "Explore tests",
    "section": "Next steps",
    "text": "Next steps\nBy harnessing the functionalities presented in this guide, you should be able to easily list and filter through all of ValidMind’s available tests and find those you are interested in running against your model and/or dataset. The next step is to take the IDs of the tests you’d like to run and either create a test suite for reuse or just run them directly to try them out. See the other notebooks for a tutorial on how to do both.\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html",
    "title": "Run documentation tests with custom configurations",
    "section": "",
    "text": "When running documentation tests, you can configure inputs and parameters for individual tests by passing a config as a parameter.\nAs a model developer, configuring individual tests is useful in various models development scenarios. For instance, based on a use case, a model might require changing inputs and/or parameters for certain tests. The run_documentation_tests() function allows you to directly configure tests through config, thus giving you flexibility to run tests according to your use case.\nThis interactive notebook includes the code required to load the demo dataset, preprocess the raw dataset, train a model for testing, initialize ValidMind objects, and run documentation tests with custom configurations."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#contents",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#contents",
    "title": "Run documentation tests with custom configurations",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nPreview the documentation template\n\n\nLoad the sample dataset\n\nDocument the model\n\nPrepocess the raw dataset\n\nTrain a model for testing\n\nInitialize ValidMind objects\n\nInitialize ValidMind model object\n\nInitialize the ValidMind datasets\n\nRun predictions through assign_predictions interface\n\n\nRun documentation tests\n\nPreview config\n\nUpdating config\n\nRun documentation tests\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#about-validmind",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#about-validmind",
    "title": "Run documentation tests with custom configurations",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#install-the-client-library",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#install-the-client-library",
    "title": "Run documentation tests with custom configurations",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#initialize-the-client-library",
    "title": "Run documentation tests with custom configurations",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#load-the-sample-dataset",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#load-the-sample-dataset",
    "title": "Run documentation tests with custom configurations",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n)\n\nraw_df = demo_dataset.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#document-the-model",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#document-the-model",
    "title": "Run documentation tests with custom configurations",
    "section": "Document the model",
    "text": "Document the model\nAs part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#prepocess-the-raw-dataset",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#prepocess-the-raw-dataset",
    "title": "Run documentation tests with custom configurations",
    "section": "Prepocess the raw dataset",
    "text": "Prepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\nInitialize XGBoost classifier: Creates an XGBClassifier object with early stopping rounds set to 10.\nSet evaluation metrics: Specifies metrics for model evaluation as “error,” “logloss,” and “auc.”\nFit the model: Trains the model on x_train and y_train using the validation set (x_val, y_val). Verbose output is disabled.\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#train-a-model-for-testing",
    "title": "Run documentation tests with custom configurations",
    "section": "Train a model for testing",
    "text": "Train a model for testing\nWe train a simple customer churn model for our test.\n\nimport xgboost\n%matplotlib inline\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nxgb = xgboost.XGBClassifier(early_stopping_rounds=10)\nxgb.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nxgb.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#initialize-validmind-objects",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#initialize-validmind-objects",
    "title": "Run documentation tests with custom configurations",
    "section": "Initialize ValidMind objects",
    "text": "Initialize ValidMind objects\n\n\nInitialize ValidMind model object\nBefore you can run tests, you must first initialize a ValidMind model object using the init_model function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\nmodel — the model that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\n\n\nvm_model_xgb = vm.init_model(\n    xgb,\n    input_id=\"xgb\",\n)\n\n\n\n\nInitialize the ValidMind datasets\nSimilarly, initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\nclass_labels — an optional value to map predicted classes to class labels\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_raw_ds = vm.init_dataset(\n    input_id=\"raw_dataset\",\n    dataset=raw_df,\n    target_column=demo_dataset.target_column,\n)\n\nfeature_columns = [\n    \"CreditScore\",\n    \"Gender\",\n    \"Age\",\n    \"Tenure\",\n    \"Balance\",\n    \"NumOfProducts\",\n    \"HasCrCard\",\n    \"IsActiveMember\",\n    \"EstimatedSalary\",\n    \"Geography_France\",\n    \"Geography_Germany\",\n    \"Geography_Spain\",\n]\n\nvm_train_ds = vm.init_dataset(\n    input_id=\"train_dataset\",\n    dataset=train_df,\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns,\n)\n\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset\",\n    dataset=test_df,\n    target_column=demo_dataset.target_column,\n    feature_columns=feature_columns,\n)\n\n\n\n\nRun predictions through assign_predictions interface\nWe can use assign_predictions() to run and assign model predictions to our training and test datasets:\n\nvm_train_ds.assign_predictions(model=vm_model_xgb)\nvm_test_ds.assign_predictions(model=vm_model_xgb)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#run-documentation-tests",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#run-documentation-tests",
    "title": "Run documentation tests with custom configurations",
    "section": "Run documentation tests",
    "text": "Run documentation tests\n\n\nPreview config\nYou can preview the default config for the documentation template using the vm.get_test_suite().get_default_config() interface.\n\nimport json\n\nproject_test_suite = vm.get_test_suite()\nconfig = project_test_suite.get_default_config()\nprint(\"Suite Config: \\n\", json.dumps(config, indent=2))\n\n\n\n\nUpdating config\nThe test configuration can be updated to fit with your use case and requirements\n\nconfig = {\n    \"validmind.data_validation.DatasetSplit\": {\n        \"inputs\": {\"datasets\": (vm_train_ds, vm_test_ds)},\n    },\n    \"validmind.model_validation.sklearn.PopulationStabilityIndex\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"datasets\": (vm_train_ds, vm_test_ds)},\n    },\n    \"validmind.model_validation.sklearn.ConfusionMatrix\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_train_ds},\n    },\n    \"validmind.model_validation.sklearn.ClassifierPerformance:out_of_sample\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.PrecisionRecallCurve\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.ROCCurve\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.TrainingTestDegradation\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"datasets\": (vm_train_ds, vm_test_ds)},\n    },\n    \"validmind.model_validation.sklearn.MinimumAccuracy\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.MinimumF1Score\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.MinimumROCAUCScore\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.PermutationFeatureImportance\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.SHAPGlobalImportance\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"dataset\": vm_test_ds},\n    },\n    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"datasets\": (vm_train_ds, vm_test_ds)},\n    },\n    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"datasets\": (vm_train_ds, vm_test_ds)},\n    },\n    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n        \"inputs\": {\"model\": vm_model_xgb, \"datasets\": (vm_train_ds, vm_test_ds)},\n    },\n}\n\n\n\n\nRun documentation tests\nYou can now run all documentation tests and pass an extra config parameter that overrides input and parameter configuration for the tests specified in the object.\n\nfull_suite = vm.run_documentation_tests(\n    inputs={\n        \"dataset\": vm_raw_ds,\n        \"model\": vm_model_xgb,\n    },\n    config=config,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_tests_with_config.html#next-steps",
    "href": "notebooks/how_to/run_documentation_tests_with_config.html#next-steps",
    "title": "Run documentation tests with custom configurations",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html",
    "title": "Run dataset based tests",
    "section": "",
    "text": "Use the Validmind Developer Framework’s run_test function to run built-in or custom tests that take any dataset or model as input. These tests generate outputs in the form of text, tables, and images that get populated in model documentation.\nYou’ll learn how to:\nWe recommended that you first complete the Explore tests notebook, to understand the basics of how to find and describe all the available tests in the developer framework before moving on to this advanced guide.\nThis interactive notebook provides a step-by-step guide for listing and filtering available tests, building a sample dataset, initializing the required ValidMind objects, running the test, and then logging the results to ValidMind."
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#contents",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#contents",
    "title": "Run dataset based tests",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nList and filter available tests\n\nCreate a sample dataset\n\nInitialize a ValidMind dataset\n\nRun tests with the dataset\n\nRun a test that accepts parameters\n\n\n\nAdd test results to documentation\n\nNext steps\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#about-validmind",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#about-validmind",
    "title": "Run dataset based tests",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: The classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#install-the-client-library",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#install-the-client-library",
    "title": "Run dataset based tests",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#initialize-the-client-library",
    "title": "Run dataset based tests",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details below and click Continue. (Need more help?)\n\nDocumentation template: Binary Classification\nUse case: Marketing - Attrition/Churn Management\n\nYou can fill in the other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)"
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#list-and-filter-available-tests",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#list-and-filter-available-tests",
    "title": "Run dataset based tests",
    "section": "List and filter available tests",
    "text": "List and filter available tests\nBefore we run a test, let’s find a suitable metric for this demonstration. Let’s assume you want to generate the pearson correlation matrix for a dataset. A Pearson correlation matrix is a table that shows the Pearson correlation coefficients between several variables.\nIn the Explore tests notebook, we learned how to pass a filter to the list_tests function. We’ll do the same here to find the test ID for the pearson correlation matrix:\n\nvm.tests.list_tests(filter=\"PearsonCorrelationMatrix\")\n\nFrom the output, you can see that the test ID for the pearson correlation matrix is validmind.data_validation.PearsonCorrelationMatrix. The describe_test function gives you more information about the test, including its Required Inputs:\n\ntest_id = \"validmind.data_validation.PearsonCorrelationMatrix\"\nvm.tests.describe_test(test_id)\n\nSince this test requires a dataset, it should throw an error if you were to run it without passing a dataset input:\n\ntry:\n    vm.tests.run_test(test_id)\nexcept Exception as e:\n    print(e)"
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#create-a-sample-dataset",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#create-a-sample-dataset",
    "title": "Run dataset based tests",
    "section": "Create a sample dataset",
    "text": "Create a sample dataset\nNow, let’s build a sample dataset so you can generate its pearson correlation matrix. The sklearn make_classification function can generate a random dataset for testing:\n\nimport pandas as pd\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=10000,\n    n_features=10,\n    weights=[0.1],\n    random_state=42,\n)\nX.shape\ny.shape\n\ndf = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\ndf[\"target\"] = y\ndf.head()\n\n\n\nInitialize a ValidMind dataset\nValidMind dataset objects provide a wrapper to any type of dataset (NumPy, Pandas, Polars, etc.) so that tests can run transparently regardless of the underlying library. A VM dataset object can be created using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset: The raw dataset that you want to provide as input to tests.\ninput_id: A unique identifier that allows tracking what inputs are used when running each individual test.\ntarget_column: A required argument if tests require access to true values. This is the name of the target column in the dataset.\n\nBelow you can see how to initialize a VM dataset for the sample df you created previously:\n\nvm_dataset = vm.init_dataset(\n    df,\n    input_id=\"my_demo_dataset\",\n    target_column=\"target\",\n)\n\n\n\n\nRun tests with the dataset\nYou can now call run_test with the new vm_dataset object as input:\n\nresult = vm.tests.run_test(\n    test_id,\n    inputs={\"dataset\": vm_dataset},\n)\n\nThis dataset can also be used for any other test that requires a dataset input.\n\nLet’s try to find a class imbalance to understand the distribution of the target column in the dataset.\nClass imbalance is a common problem in machine learning, particularly in classification tasks, where the number of instances (or data points) in each class isn’t evenly distributed across the available categories.\n\nWe’ll use list_tests again to showcase how to filter tests for tabular data:\n\nsorted(vm.tests.list_tags())\n\n\nvm.tests.list_tests(tags=[\"binary_classification\", \"tabular_data\"])\n\n\n\nRun a test that accepts parameters\nThe test ID for the class imbalance test is validmind.data_validation.ClassImbalance. If you describe this test you will find that it also accepts some parameters:\n\nvm.tests.describe_test(\"validmind.data_validation.ClassImbalance\")\n\nThe min_percent_threshold will allow you configure the threshold for an acceptable class imbalance.\n\nLet’s run the test without any parameters to see its output using a default value for the threshold.\nWe also call the log method on the result to send the results of the tests to the ValidMind platform.\n\n\nresult = vm.tests.run_test(\n    \"validmind.data_validation.ClassImbalance\",\n    inputs={\"dataset\": vm_dataset},\n)\n\nresult.log()\n\nThis test passes the pass-fail criteria with the default threshold of 10%.\n\nLet’s try to run the test with a threshold of 20% to see if it fails. Notice the use of the ‘custom_threshold’ result_id in the test ID.\nThis allows you to submit individual results for the same test to the platform, as we’ll see in the next section.\n\n\nresult = vm.tests.run_test(\n    \"validmind.data_validation.ClassImbalance:custom_threshold\",\n    inputs={\"dataset\": vm_dataset},\n    params={\"min_percent_threshold\": 20},\n)\n\nresult.log()"
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#add-test-results-to-documentation",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#add-test-results-to-documentation",
    "title": "Run dataset based tests",
    "section": "Add test results to documentation",
    "text": "Add test results to documentation\nThe previous result shows that the test didn’t pass the threshold of 20% for class imbalance. With these results logged, you can now add them to your model documentation:\n\nIn the Platform UI, go to the Documentation page for the model you registered earlier.\nExpand the 2.1. Data Description section.\nHover between any existing content blocks to reveal the + button.\nClick on the + button and choose Test-Driven Block. This will open a dialog where you can select:\n\nType: Threshold Test\nThreshold Test: Class Imbalance Custom Threshold Test\n\n\nYou can preview the result and then click Insert Block in the bottom-right corner to add it to the documentation."
  },
  {
    "objectID": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#next-steps",
    "href": "notebooks/how_to/run_tests/1_run_dataset_based_tests.html#next-steps",
    "title": "Run dataset based tests",
    "section": "Next steps",
    "text": "Next steps\nWhile you can look at the results of this test suite right in the notebook where you ran the code, there is a better way — expand the rest of your documentation in the platform UI and take a look around.\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready.\n\n\nDiscover more learning resources\nIn an upcoming companion notebook, you’ll learn how to run tests that require a dataset and model as inputs. This will allow you to generate documentation for model evaluation metrics such as ROC-AUC, F1 score, etc. for your model.\nWe also offer many other interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html",
    "href": "notebooks/how_to/run_documentation_sections.html",
    "title": "Run individual documentation sections",
    "section": "",
    "text": "For targeted testing, you can run tests on individual sections or specific groups of sections in your model documentation.\nAs a model developer, running individual documentation sections is useful in various development scenarios. For instance, when updates are made to a model, often only certain parts of the documentation require revision. The run_documentation_tests() function allows you to directly test only these affected sections, thus saving you time and ensuring that the documentation accurately reflects the latest changes.\nThis interactive notebook includes the code required to load the demo dataset, preprocess the raw dataset, train a model for testing, initialize ValidMind objects, and run the data preparation, model development, and multiple documentation sections."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#contents",
    "href": "notebooks/how_to/run_documentation_sections.html#contents",
    "title": "Run individual documentation sections",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nPreview the documentation template\n\n\nLoad the Demo Dataset\n\nPrepocess the raw dataset\n\n\nTrain a model for testing\n\nInitialize ValidMind objects\n\nAssign predictions to the datasets\n\n\nRun the data preparation section\n\nRun the model development section\n\nRun multiple model documentation sections\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#about-validmind",
    "href": "notebooks/how_to/run_documentation_sections.html#about-validmind",
    "title": "Run individual documentation sections",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#install-the-client-library",
    "href": "notebooks/how_to/run_documentation_sections.html#install-the-client-library",
    "title": "Run individual documentation sections",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_documentation_sections.html#initialize-the-client-library",
    "title": "Run individual documentation sections",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details, making sure to select Binary classification as the template and Marketing/Sales - Attrition/Churn Management as the use case, and click Continue. (Need more help?)\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\n%matplotlib inline\n\nimport xgboost as xgb\n\n\n\nPreview the documentation template\nA template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_documentation_sections.html#load-the-demo-dataset",
    "title": "Run individual documentation sections",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import taiwan_credit like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nPrepocess the raw dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_documentation_sections.html#train-a-model-for-testing",
    "title": "Run individual documentation sections",
    "section": "Train a model for testing",
    "text": "Train a model for testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#initialize-validmind-objects",
    "href": "notebooks/how_to/run_documentation_sections.html#initialize-validmind-objects",
    "title": "Run individual documentation sections",
    "section": "Initialize ValidMind objects",
    "text": "Initialize ValidMind objects\nWe initize the objects required to run test suites using the ValidMind framework.\n\nvm_dataset = vm.init_dataset(\n    input_id=\"raw_dataset\",\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels,\n)\n\nvm_train_ds = vm.init_dataset(\n    input_id=\"train_dataset\",\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column,\n)\n\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset\",\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column,\n)\n\nvm_model = vm.init_model(model, input_id=\"model\")\n\n\n\nAssign predictions to the datasets\nWe can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n\nvm_train_ds.assign_predictions(\n    model=vm_model,\n)\nvm_test_ds.assign_predictions(\n    model=vm_model,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#run-the-data-preparation-section",
    "href": "notebooks/how_to/run_documentation_sections.html#run-the-data-preparation-section",
    "title": "Run individual documentation sections",
    "section": "Run the data preparation section",
    "text": "Run the data preparation section\nIn this section, we focus on running the tests within the data preparation section of the model documentation. After running this function, only the tests associated with this section will be executed, and the corresponding section in the model documentation will be updated.\n\nresults = vm.run_documentation_tests(\n    section=\"data_preparation\",\n    inputs={\n        \"dataset\": vm_dataset,\n    },\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#run-the-model-development-section",
    "href": "notebooks/how_to/run_documentation_sections.html#run-the-model-development-section",
    "title": "Run individual documentation sections",
    "section": "Run the model development section",
    "text": "Run the model development section\nIn this section, we focus on running the tests within the model development section of the model documentation. After running this function, only the tests associated with this section will be executed, and the corresponding section in the model documentation will be updated.\n\nresults = vm.run_documentation_tests(\n    section=\"model_development\",\n    inputs={\n        \"dataset\": vm_train_ds,\n        \"model\": vm_model,\n        \"datasets\": (vm_train_ds, vm_test_ds),\n    },\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#run-multiple-model-documentation-sections",
    "href": "notebooks/how_to/run_documentation_sections.html#run-multiple-model-documentation-sections",
    "title": "Run individual documentation sections",
    "section": "Run multiple model documentation sections",
    "text": "Run multiple model documentation sections\nThis section demonstrates how you can execute both the data preparation and model development sections using run_documentation_tests(). After running this function, the tests associated with both sections will be executed, and their corresponding model documentation sections updated.\n\nresults = vm.run_documentation_tests(\n    section=[\"model_development\", \"model_diagnosis\"],\n    inputs={\n        \"dataset\": vm_test_ds,\n        \"model\": vm_model,\n        \"datasets\": (vm_train_ds, vm_test_ds),\n    },\n)"
  },
  {
    "objectID": "notebooks/how_to/run_documentation_sections.html#next-steps",
    "href": "notebooks/how_to/run_documentation_sections.html#next-steps",
    "title": "Run individual documentation sections",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html",
    "title": "Run tests with multiple datasets",
    "section": "",
    "text": "To support running tests that require more than one dataset, ValidMind provides a mechanim that allows you to pass multiple datasets as inputs.\nTo ensure a model generalizes well to new, unseen data, it’s common to use separate datasets for training, validation, and testing, with each set serving to check the model’s performance at different stages of development. Additionally, since models often encounter data from various sources that might differ in distribution, quality, or type, using multiple datasets in testing can simulate this diversity and better prepare the model for deployment.\nThis interactive notebook includes the code required to load the demo dataset, preprocess the raw dataset and train a model for testing, initialize ValidMind objects, and run a test that requires multiple datasets."
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#contents",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#contents",
    "title": "Run tests with multiple datasets",
    "section": "Contents",
    "text": "Contents\n\nAbout ValidMind\n\nBefore you begin\n\nNew to ValidMind?\n\nKey concepts\n\n\nInstall the client library\n\nInitialize the client library\n\nPreview the documentation template\n\n\nLoad the sample dataset\n\nPrepocess the raw dataset\n\nTrain models for testing\n\nInitialize ValidMind objects\n\nInitialize the ValidMind model\n\nInitialize the ValidMind datasets\n\n\nRun a test that requires multiple datasets\n\nRun predictions and link with the model\n\nRun test\n\n\nNext steps\n\nWork with your model documentation\n\nDiscover more learning resources"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#about-validmind",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#about-validmind",
    "title": "Run tests with multiple datasets",
    "section": "About ValidMind",
    "text": "About ValidMind\nValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\nYou use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n\n\nBefore you begin\nThis notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\nIf you encounter errors due to missing modules in your Python environment, install the modules with pip install, and then re-run the notebook. For more help, refer to Installing Python Modules.\n\n\n\nNew to ValidMind?\nIf you haven’t already seen our Get started with the ValidMind Developer Framework, we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n\nFor access to all features available in this notebook, create a free ValidMind account.\nSigning up is FREE — Sign up now\n\n\n\n\nKey concepts\nModel documentation: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\nDocumentation template: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\nTests: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\nMetrics: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\nCustom metrics: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\nInputs: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n\nmodel: A single model that has been initialized in ValidMind with vm.init_model().\ndataset: Single dataset that has been initialized in ValidMind with vm.init_dataset().\nmodels: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\ndatasets: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this example for more information.\n\nParameters: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\nOutputs: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\nTest suites: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\nExample: the classifier_full_suite test suite runs tests from the tabular_dataset and classifier test suites to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#install-the-client-library",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#install-the-client-library",
    "title": "Run tests with multiple datasets",
    "section": "Install the client library",
    "text": "Install the client library\nThe client library provides Python support for the ValidMind Developer Framework. To install it:\n\n%pip install -q validmind"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#initialize-the-client-library",
    "title": "Run tests with multiple datasets",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nValidMind generates a unique code snippet for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\nGet your code snippet:\n\nIn a browser, log into the Platform UI.\nIn the left sidebar, navigate to Model Inventory and click + Register new model.\nEnter the model details and click Continue. (Need more help?)\nFor example, to register a model for use with this notebook, select:\n\nDocumentation template: Binary classification\nUse case: Marketing/Sales - Attrition/Churn Management\n\nYou can fill in other options according to your preference.\nGo to Getting Started and click Copy snippet to clipboard.\n\nNext, replace this placeholder with your own code snippet:\n\n# Replace with your code snippet\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"...\",\n    api_secret=\"...\",\n    project=\"...\",\n)\n\n\n\nPreview the documentation template\nA template predefines sections for your documentation project and provides a general outline to follow, making the documentation process much easier.\nYou will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the vm.preview_template() function from the ValidMind library and note the empty sections:\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#load-the-sample-dataset",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#load-the-sample-dataset",
    "title": "Run tests with multiple datasets",
    "section": "Load the sample dataset",
    "text": "Load the sample dataset\nThe sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas DataFrame, a two-dimensional tabular data structure that makes use of rows and columns:\n\n# Import the sample dataset from the library\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\nprint(\n    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{demo_dataset.target_column}' \\n\\t• Class labels: {demo_dataset.class_labels}\"\n)\n\nraw_df = demo_dataset.load_data()\nraw_df.head()"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#prepocess-the-raw-dataset",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#prepocess-the-raw-dataset",
    "title": "Run tests with multiple datasets",
    "section": "Prepocess the raw dataset",
    "text": "Prepocess the raw dataset\nPreprocessing performs a number of operations to get ready for the subsequent steps:\n\nPreprocess the data: Splits the DataFrame (df) into multiple datasets (train_df, validation_df, and test_df) using demo_dataset.preprocess to simplify preprocessing.\nSeparate features and targets: Drops the target column to create feature sets (x_train, x_val) and target sets (y_train, y_val).\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(raw_df)\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#train-models-for-testing",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#train-models-for-testing",
    "title": "Run tests with multiple datasets",
    "section": "Train models for testing",
    "text": "Train models for testing\nInitialize XGBoost and Logistic Regression Classifiers\n\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost\n\n%matplotlib inline\n\nxgb = xgboost.XGBClassifier(early_stopping_rounds=10)\nxgb.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nxgb.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#initialize-validmind-objects",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#initialize-validmind-objects",
    "title": "Run tests with multiple datasets",
    "section": "Initialize ValidMind objects",
    "text": "Initialize ValidMind objects\n\n\nInitialize the ValidMind model\n\nvm_model_xgb = vm.init_model(\n    xgb,\n    input_id=\"xgb\",\n)\n\n\n\n\nInitialize the ValidMind datasets\nBefore you can run tests, you must first initialize a ValidMind dataset object using the init_dataset function from the ValidMind (vm) module.\nThis function takes a number of arguments:\n\ndataset — the raw dataset that you want to provide as input to tests\ninput_id - a unique identifier that allows tracking what inputs are used when running each individual test\ntarget_column — a required argument if tests require access to true values. This is the name of the target column in the dataset\nclass_labels — an optional value to map predicted classes to class labels\n\nWith all datasets ready, you can now initialize the raw, training and test datasets (raw_df, train_df and test_df) created earlier into their own dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    input_id=\"train_dataset\",\n    dataset=train_df,\n    target_column=demo_dataset.target_column,\n)\nvm_test_ds = vm.init_dataset(\n    input_id=\"test_dataset\", dataset=test_df, target_column=demo_dataset.target_column\n)"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#run-a-test-that-requires-multiple-datasets",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#run-a-test-that-requires-multiple-datasets",
    "title": "Run tests with multiple datasets",
    "section": "Run a test that requires multiple datasets",
    "text": "Run a test that requires multiple datasets\nWe are going to show the following in next two blocks:\n\nAssign predictions for vm_train_ds and vm_test_ds\nRun RobustnessDiagnosis which is one example test that takes two input datasets\n\n\n\nRun predictions and link with the model\n\nvm_train_ds.assign_predictions(model=vm_model_xgb)\nvm_test_ds.assign_predictions(model=vm_model_xgb)\n\n\n\n\nRun test\n\nvm.tests.run_test(\n    \"validmind.model_validation.sklearn.RobustnessDiagnosis\",\n    inputs={\"datasets\": (vm_train_ds, vm_test_ds), \"model\": vm_model_xgb},\n)"
  },
  {
    "objectID": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#next-steps",
    "href": "notebooks/how_to/run_tests_that_require_multiple_datasets.html#next-steps",
    "title": "Run tests with multiple datasets",
    "section": "Next steps",
    "text": "Next steps\nYou can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\n\n\nWork with your model documentation\n\nFrom the Model Inventory in the ValidMind Platform UI, go to the model you registered earlier.\nClick and expand the Model Development section.\n\nWhat you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it’s ready. Learn more …\n\n\n\nDiscover more learning resources\nWe offer many interactive notebooks to help you document models:\n\nRun tests & test suites\nCode samples\n\nOr, visit our documentation to learn more about ValidMind."
  },
  {
    "objectID": "training/training-for-model-validators.html#preview-ai-based-documentation-assistant",
    "href": "training/training-for-model-validators.html#preview-ai-based-documentation-assistant",
    "title": "Training for Model Validators",
    "section": "PREVIEW: AI-based documentation assistant",
    "text": "PREVIEW: AI-based documentation assistant\nUse the ValidMind AI tool to auto-generate initial report content based on model metadata and to gain insight into test results."
  },
  {
    "objectID": "training/training-for-model-validators.html#preview-ai-documentation-assistant",
    "href": "training/training-for-model-validators.html#preview-ai-documentation-assistant",
    "title": "Training for Model Validators",
    "section": "PREVIEW: AI documentation assistant",
    "text": "PREVIEW: AI documentation assistant\n\n\nUse the ValidMind AI assistant to auto-generate initial report content based on model metadata and to gain insight into test results. Click Generate Text with AI pops up a modal showing the AI content generation:\n\n\nHere, you can choose to Accept Text or Try Again:"
  },
  {
    "objectID": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-documentation-and-provide-approvals-on-the-validmind-platform.",
    "href": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-documentation-and-provide-approvals-on-the-validmind-platform.",
    "title": "Training for Model Validators",
    "section": "Learning objective: Enable model validators to review documentation and provide approvals on the ValidMind platform.",
    "text": "Learning objective: Enable model validators to review documentation and provide approvals on the ValidMind platform."
  },
  {
    "objectID": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-and-approve-documentation-and-provide-approvals-on-the-validmind-platform.",
    "href": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-and-approve-documentation-and-provide-approvals-on-the-validmind-platform.",
    "title": "Training for Model Validators",
    "section": "Learning objective: Enable model validators to review and approve documentation and provide approvals on the ValidMind platform.",
    "text": "Learning objective: Enable model validators to review and approve documentation and provide approvals on the ValidMind platform."
  },
  {
    "objectID": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-and-approve-documentation-and-on-the-validmind-platform.",
    "href": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-and-approve-documentation-and-on-the-validmind-platform.",
    "title": "Training for Model Validators",
    "section": "Learning objective: Enable model validators to review and approve documentation and on the ValidMind platform.",
    "text": "Learning objective: Enable model validators to review and approve documentation and on the ValidMind platform."
  },
  {
    "objectID": "training/training-for-model-developers.html#learning-objective-enable-model-developers-to-submit-new-model-use-cases-on-the-validmind-platform-and-to-generate-edit-model-documentation-using-the-developer-framework.-follows-our-new-introduction-for-model-developers-notebook.",
    "href": "training/training-for-model-developers.html#learning-objective-enable-model-developers-to-submit-new-model-use-cases-on-the-validmind-platform-and-to-generate-edit-model-documentation-using-the-developer-framework.-follows-our-new-introduction-for-model-developers-notebook.",
    "title": "Training for Model Developers",
    "section": "Learning objective: Enable model developers to submit new model use cases on the ValidMind platform and to generate & edit model documentation using the developer framework. Follows our new introduction for model developers notebook.",
    "text": "Learning objective: Enable model developers to submit new model use cases on the ValidMind platform and to generate & edit model documentation using the developer framework. Follows our new introduction for model developers notebook."
  },
  {
    "objectID": "training/training-for-administrators.html#learning-objective-learn-how-to-onboard-your-organization-manage-users-and-roles-and-manage-permissions-for-specific-roles.",
    "href": "training/training-for-administrators.html#learning-objective-learn-how-to-onboard-your-organization-manage-users-and-roles-and-manage-permissions-for-specific-roles.",
    "title": "Training for Administrators",
    "section": "Learning objective: Learn how to onboard your organization, manage users and roles, and manage permissions for specific roles.",
    "text": "Learning objective: Learn how to onboard your organization, manage users and roles, and manage permissions for specific roles."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#learning-objective-track-and-manage-model-risk-and-explore-workflows-and-reports.",
    "href": "training/training-for-model-risk-governance.html#learning-objective-track-and-manage-model-risk-and-explore-workflows-and-reports.",
    "title": "Training for Model Risk Governance",
    "section": "Learning objective: Track and manage model risk, and explore workflows and reports.",
    "text": "Learning objective: Track and manage model risk, and explore workflows and reports."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#learn-to-use-the-validmind-platform-to-effectively-monitor-and-control-model-risks.-understand-how-to-customize-workflows-for-the-validation-process-and-ensure-compliance.-explore-reports-to-facilitate-better-decision-making-and-risk-mitigation.",
    "href": "training/training-for-model-risk-governance.html#learn-to-use-the-validmind-platform-to-effectively-monitor-and-control-model-risks.-understand-how-to-customize-workflows-for-the-validation-process-and-ensure-compliance.-explore-reports-to-facilitate-better-decision-making-and-risk-mitigation.",
    "title": "Training for Model Risk Governance",
    "section": "Learn to use the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation.",
    "text": "Learn to use the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation."
  },
  {
    "objectID": "training/training-for-administrators.html#prerequisites",
    "href": "training/training-for-administrators.html#prerequisites",
    "title": "Training for Administrators",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo try out this training module, you need to have been onboarded as the first user onto the ValidMind training environment.\nLog into the ValidMind Platform UI to check your access:\n\n\n\nLog in\n\n\n\nBe sure to return to this page afterwards."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#learlearn-to-use-the-validmind-platform-to-effectively-monitor-and-control-model-risks.-understand-how-to-customize-workflows-for-the-validation-process-and-ensure-compliance.-explore-reports-to-facilitate-better-decision-making-and-risk-mitigation.",
    "href": "training/training-for-model-risk-governance.html#learlearn-to-use-the-validmind-platform-to-effectively-monitor-and-control-model-risks.-understand-how-to-customize-workflows-for-the-validation-process-and-ensure-compliance.-explore-reports-to-facilitate-better-decision-making-and-risk-mitigation.",
    "title": "Training for Model Risk Governance",
    "section": "LearLearn to use the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation.",
    "text": "LearLearn to use the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#learning-objective-learn-to-use-the-validmind-platform-to-effectively-monitor-and-control-model-risks.-understand-how-to-customize-workflows-for-the-validation-process-and-ensure-compliance.-explore-reports-to-facilitate-better-decision-making-and-risk-mitigation.",
    "href": "training/training-for-model-risk-governance.html#learning-objective-learn-to-use-the-validmind-platform-to-effectively-monitor-and-control-model-risks.-understand-how-to-customize-workflows-for-the-validation-process-and-ensure-compliance.-explore-reports-to-facilitate-better-decision-making-and-risk-mitigation.",
    "title": "Training for Model Risk Governance",
    "section": "Learning objective: Learn to use the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation.",
    "text": "Learning objective: Learn to use the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation."
  },
  {
    "objectID": "guide/review-model-documentation.html",
    "href": "guide/review-model-documentation.html",
    "title": "Review model documentation",
    "section": "",
    "text": "You review the model documentation provided by a model developer as part of the formal validation process."
  },
  {
    "objectID": "guide/review-model-documentation.html#prerequisites",
    "href": "guide/review-model-documentation.html#prerequisites",
    "title": "Review model documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou hold the Validator role\nA model developer has submitted model documentation for review"
  },
  {
    "objectID": "guide/review-model-documentation.html#key-concepts",
    "href": "guide/review-model-documentation.html#key-concepts",
    "title": "Review model documentation",
    "section": "Key concepts",
    "text": "Key concepts\n\nEach section of your model documentation should address critical aspects of the model’s lifecycle, from conceptualization and data preparation through development and ongoing management. This comprehensive documentation approach is essential for ensuring the model’s reliability, relevance, and compliance with business and regulatory standards.\n\n\n\nconceptual soundness\n\nEstablishes the foundation of the model, covering the model overview, intended use and business use case, regulatory requirements, model limitations, and the rationale behind the model selection. It emphasizes the model’s purpose, scope, and constraints, which are crucial for stakeholders to understand the model’s applicability and limitations.\n\ndata preparation\n\nDetails the data description, including dataset summary, data quality tests, descriptive statistics, correlations and interactions, and feature selection and engineering. It provides transparency into the data used for model training, ensuring that the model is built on a solid and relevant dataset.\n\n\n\n\n\nmodel development\n\nDiscusses the model training, evaluation, explainability, interpretability, and diagnosis, including model weak spots, overfit regions, and robustness. This section is vital for understanding how the model was developed, how it performs, and its areas of strength and weakness.\n\nmonitoring and governance\n\nFocuses on the model’s ongoing monitoring plan, implementation, and governance plan. It outlines strategies for maintaining the model’s performance over time and ensuring that it remains compliant with regulatory requirements and ethical standards."
  },
  {
    "objectID": "guide/review-model-documentation.html#document-overview",
    "href": "guide/review-model-documentation.html#document-overview",
    "title": "Review model documentation",
    "section": "Document Overview",
    "text": "Document Overview\n\nLog in to the ValidMind UI.\nIn the left sidebar, click Model Inventory.\nSelect a model by clicking on it or find your model by applying a filter or searching for it.\nIn the left sidebar that appears for your model, click Documentation.\nThis is the Document Overview. This page shows a section-by-section outline of your project’s documentation, as well as summaries of:\n\nAny unresolved conversations\nThe number of model findings\nThe completion status for your model’s documentation\n\n\n\nUnresolved conversations\nHere on this documentation overview page, hover over the  # icon to see a preview of the unresolved conversation associated with the documentation section.\n\n\n\n\n\n\nResolve conversations directly from this popup, or click to expand the conversation.\n\n\n\n\n\nModel findings\nHere on this documentation overview page, hover over over the  # icon to see a preview of model findings associated with the documentation section.\n\n\n\n\n\n\nClick to expand the finding.\n\n\n\n\n\nRecent activity\nAt the bottom of the Document Overview page, you will also see quick view of any documentation activity.\n\n\n\n\n\n\nClick on  See all Activity to be taken to the Model Activity page."
  },
  {
    "objectID": "guide/review-model-documentation.html#whats-next",
    "href": "guide/review-model-documentation.html#whats-next",
    "title": "Review model documentation",
    "section": "What’s next",
    "text": "What’s next"
  },
  {
    "objectID": "training/training-for-model-validators.html#the-validmind-platform-ui",
    "href": "training/training-for-model-validators.html#the-validmind-platform-ui",
    "title": "Training for Model Validators",
    "section": "The ValidMind Platform UI",
    "text": "The ValidMind Platform UI\n\nTBD"
  },
  {
    "objectID": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-and-approve-documentation-on-the-validmind-platform.",
    "href": "training/training-for-model-validators.html#learning-objective-enable-model-validators-to-review-and-approve-documentation-on-the-validmind-platform.",
    "title": "Training for Model Validators",
    "section": "Learning objective: Enable model validators to review and approve documentation on the ValidMind platform.",
    "text": "Learning objective: Enable model validators to review and approve documentation on the ValidMind platform."
  },
  {
    "objectID": "training/training-for-model-validators.html#section",
    "href": "training/training-for-model-validators.html#section",
    "title": "Training for Model Validators",
    "section": "",
    "text": "Discover Model Inventory → pick a model | Documentation → Validation Report | Model Findings → Reports"
  },
  {
    "objectID": "training/training-for-model-validators.html#tbd",
    "href": "training/training-for-model-validators.html#tbd",
    "title": "Training for Model Validators",
    "section": "TBD",
    "text": "TBD\nEnable model validators to review and approve documentation on the ValidMind platform."
  },
  {
    "objectID": "training/training-for-model-validators.html#learning-objective",
    "href": "training/training-for-model-validators.html#learning-objective",
    "title": "Training for Model Validators",
    "section": "Learning objective",
    "text": "Learning objective\nEnable model validators to review and approve documentation on the ValidMind platform."
  },
  {
    "objectID": "training/training-for-model-developers.html#learning-objective",
    "href": "training/training-for-model-developers.html#learning-objective",
    "title": "Training for Model Developers",
    "section": "Learning objective",
    "text": "Learning objective\nEnable model developers to submit new model use cases on the ValidMind platform and to generate & edit model documentation using the developer framework. Follows our new introduction for model developers notebook. {.center}"
  },
  {
    "objectID": "training/training-for-administrators.html#learning-objective",
    "href": "training/training-for-administrators.html#learning-objective",
    "title": "Training for Administrators",
    "section": "Learning objective",
    "text": "Learning objective\nOnboard your organization, manage users and roles, and manage permissions for specific roles."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#learning-objective",
    "href": "training/training-for-model-risk-governance.html#learning-objective",
    "title": "Training for Model Risk Governance",
    "section": "Learning objective",
    "text": "Learning objective\nUse the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation."
  },
  {
    "objectID": "training/training-for-model-risk-governance.html#learning-objectives",
    "href": "training/training-for-model-risk-governance.html#learning-objectives",
    "title": "Training for Model Risk Governance",
    "section": "Learning objectives",
    "text": "Learning objectives\nUse the ValidMind platform to effectively monitor and control model risks. Understand how to customize workflows for the validation process and ensure compliance. Explore reports to facilitate better decision-making and risk mitigation."
  },
  {
    "objectID": "training/training-for-model-validators.html#learning-objectives",
    "href": "training/training-for-model-validators.html#learning-objectives",
    "title": "Training for Model Validators",
    "section": "Learning objectives",
    "text": "Learning objectives\nEnable model validators to review and approve documentation on the ValidMind platform. Collaborate with model developers."
  },
  {
    "objectID": "training/training-for-model-validators.html#the-validmind-platform-ui-background-imagevalidmind-platform-ui.png",
    "href": "training/training-for-model-validators.html#the-validmind-platform-ui-background-imagevalidmind-platform-ui.png",
    "title": "Training for Model Validators",
    "section": "The ValidMind Platform UI {background-image=“validmind-platform-ui.png”}",
    "text": "The ValidMind Platform UI {background-image=“validmind-platform-ui.png”}\nTBD"
  },
  {
    "objectID": "training/training-for-model-validators.html#the-validmind-platform-ui-background-iframehttpsapp.prod.validmind.ai-background-interactive",
    "href": "training/training-for-model-validators.html#the-validmind-platform-ui-background-iframehttpsapp.prod.validmind.ai-background-interactive",
    "title": "Training for Model Validators",
    "section": "The ValidMind Platform UI {background-iframe=“https://app.prod.validmind.ai/” background-interactive}",
    "text": "The ValidMind Platform UI {background-iframe=“https://app.prod.validmind.ai/” background-interactive}\n\nTBD"
  },
  {
    "objectID": "training/training-for-model-validators.html#background-iframehttpsapp.prod.validmind.ai-background-interactivetrue-background-size-50",
    "href": "training/training-for-model-validators.html#background-iframehttpsapp.prod.validmind.ai-background-interactivetrue-background-size-50",
    "title": "Training for Model Validators",
    "section": "{background-iframe=“https://app.prod.validmind.ai/” background-interactive=“true”; background-size= 50%}",
    "text": "{background-iframe=“https://app.prod.validmind.ai/” background-interactive=“true”; background-size= 50%}\n\n\n\n\n\n\n\n\nTip\n\n\nAll users associated with a model, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "training/training-for-model-validators.html#background-iframehttpsapp.prod.validmind.ai-background-interactivetrue-background-size50",
    "href": "training/training-for-model-validators.html#background-iframehttpsapp.prod.validmind.ai-background-interactivetrue-background-size50",
    "title": "Training for Model Validators",
    "section": "{background-iframe=“https://app.prod.validmind.ai/” background-interactive=“true”; background-size=“50%”}",
    "text": "{background-iframe=“https://app.prod.validmind.ai/” background-interactive=“true”; background-size=“50%”}\n\n\n\n\n\n\n\n\nTip\n\n\nAll users associated with a model, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "training/training-for-model-validators.html#this-is-the-validmind-plaform-ui.-explore-the-model-inventory-model",
    "href": "training/training-for-model-validators.html#this-is-the-validmind-plaform-ui.-explore-the-model-inventory-model",
    "title": "Training for Model Validators",
    "section": "This is the ValidMind Plaform UI. Explore the Model Inventory, Model",
    "text": "This is the ValidMind Plaform UI. Explore the Model Inventory, Model"
  },
  {
    "objectID": "training/training-for-model-validators.html#submit-track-approvals",
    "href": "training/training-for-model-validators.html#submit-track-approvals",
    "title": "Training for Model Validators",
    "section": "4. Submit & track approvals",
    "text": "4. Submit & track approvals\nSubmit validation reports for review through the ValidMind platform, monitor the approval status, and address any feedback or required changes.\n\n\n\nShow me how"
  },
  {
    "objectID": "training/training-for-model-validators.html#submit-for-review-track-approvals",
    "href": "training/training-for-model-validators.html#submit-for-review-track-approvals",
    "title": "Training for Model Validators",
    "section": "4. Submit for review & track approvals",
    "text": "4. Submit for review & track approvals\nSubmit validation reports for review through the ValidMind platform, monitor the approval status, and address any feedback or required changes.\n\n\n\nShow me how"
  },
  {
    "objectID": "training/training-for-model-validators.html#submit-for-review-approval",
    "href": "training/training-for-model-validators.html#submit-for-review-approval",
    "title": "Training for Model Validators",
    "section": "4. Submit for review & approval",
    "text": "4. Submit for review & approval\nSubmit validation reports for review through the ValidMind platform, monitor the approval status, and address any feedback or required changes.\n\n\n\nShow me how"
  }
]