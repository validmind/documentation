[
  {
    "objectID": "guide/getstartedvalidator.html",
    "href": "guide/getstartedvalidator.html",
    "title": "Get started as a validator",
    "section": "",
    "text": "The model validator is responsible for assessing the accuracy and appropriateness of the model. They are typically independent of the Model Developer and are responsible for ensuring that the model is fit-for-purpose, accurate, and consistent with the business requirements. They also ensure that the assumptions and limitations of the model are understood and documented."
  },
  {
    "objectID": "guide/getstartedvalidator.html#prerequisites",
    "href": "guide/getstartedvalidator.html#prerequisites",
    "title": "Get started as a validator",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/getstartedvalidator.html#steps",
    "href": "guide/getstartedvalidator.html#steps",
    "title": "Get started as a validator",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/getstartedvalidator.html#troubleshooting",
    "href": "guide/getstartedvalidator.html#troubleshooting",
    "title": "Get started as a validator",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/getstartedvalidator.html#conclusion",
    "href": "guide/getstartedvalidator.html#conclusion",
    "title": "Get started as a validator",
    "section": "Conclusion",
    "text": "Conclusion\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind Platform Overview",
    "section": "",
    "text": "ValidMind is a solution for model risk management (MRM) that enables financial institutions to automate model documentation and validation. We provide:"
  },
  {
    "objectID": "guide/overview.html#product-components",
    "href": "guide/overview.html#product-components",
    "title": "ValidMind Platform Overview",
    "section": "Product Components",
    "text": "Product Components\nValidMind consists of two major product components: the developer framework and the cloud-based SaaS platform:\n\n\n\nThe ValidMind Platform\n\n\n\nValidMind Developer framework\nValidMind’s Developer Framework is collection of standalone libraries in Python and R that you set up into their existing development environment. The Developer Framework requires access to the data sources where datasets (training, test, and other applicable datasets) and trained model files are stored to run model documentation and validation tests, and to ValidMind’s SaaS platform to populate the model documentation and validation report.\nOur Developer Framework automatically generates documentation for algorithmic models and proactively identifies potential risks for both traditional and machine-learning models, thus enabling effective model validation according to compliance and regulatory requirements, while saving teams hundreds of hours of work writing documents today.\n\n\nValidMind SaaS Platform\nThe SaaS platform implements a multi-tenant architecture to host its SaaS components (application, APIs, databases, and other internal services). Connection to ValidMind’s SaaS platform is done via AWS PrivateLink/Azure Private Link. This network configuration allows you to consume the ValidMind API services and access ValidMind’s SaaS application as if they were local services to their network.\nAWS PrivateLink/Azure Private Link provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public Internet. Additionally, ValidMind only collects metadata used to generate documentation and present test results for the model developer and validation teams."
  },
  {
    "objectID": "guide/overview.html#features",
    "href": "guide/overview.html#features",
    "title": "ValidMind Platform Overview",
    "section": "Features",
    "text": "Features\n\nA cloud-based, flexible platform\n\nEasy to integrate, configure, and scale according to your requirements. ValidMind is designed to be platform-independent, seamlessly integrating into existing model development workflows without causing disruptions. The platform is also easy to use, with an intuitive interface that allows users to view, edit, and manage their model documentation and validation reports with ease.\n\nAutomated documentation and testing\n\nAdapts to your existing model development practices. It includes automated documentation and testing capabilities, making it easier for model developers to generate model documentation and identify potential areas of risks in their models.\n\nEasy-to-use and intuitive\n\nEnable feedback capture & workflows for model risk management. The platform promotes more efficient collaboration and communication between 1st and 2nd line teams, enabling model developers to easily capture and incorporate feedback from model validation teams to ensure more effective model documentation and validation activities.\n\n\n\nDocumentation-as-Code\nValidMind enables model developers to automatically generate documentation snippets throughout the model development process using the ValidMind developer dramework. These documentation snippets can be added to the code and model artifacts as comments or other forms of annotations, and they serve as a way of capturing the design decisions, assumptions, and evaluation metrics of the model at different stages of development.\nBy using the Developer Framework, model developers are able to ensure that the documentation is created continuously throughout the development process, rather than leaving it until the end or omitting important decisions from the documentation after the fact. This approach makes it easier to keep track of evidence through the model development process, to explain the rationale behind design decisions, and to share the knowledge with the model validation team who need to be able to understand the model.\nThe framework also provides tools for automatically generating more comprehensive documentation from these snippets or annotations which are then surfaced in our SaaS platform UI for consumption by the model validation team and other users.\n\n\nCompliance by design\nPre-configured model-based templates ensure documentation is always complete Templates ensure consistent model validation & testing approach Tests can be configured to adapt to specific validation requirements for each use case .\nValidMind’s approach emphasizes the importance of compliance with regulations and requirements set by you’ model validation teams in the design and development of model use cases. To achieve this, the ValidMind Framework provides pre-configured model-based templates that are designed to ensure that the documentation is always complete. These templates include predefined sections for capturing information about the model inputs and outputs, architecture, and performance metrics, as well as the validation and testing procedures used to evaluate the model.\nUsing these templates helps to ensure a consistent approach to model validation and testing, as all models are documented using the same structure and guidelines. This makes it easier for model validators and other stakeholders to review the models and understand how they were developed and tested.\nIn addition, the tests can be configured to adapt to specific validation requirements for each use case. This means that the testing procedures can be customized to meet the needs of each customer’s existing business processes. For example, some use cases may require more rigorous testing than others, and the tests can be adjusted accordingly to ensure that the model meets the required standards.\n\n\nProactively evaluate model risks\nRun dozens of configurable automated tests to evaluate data quality, model outcomes, robustness, explainability, and more\nOur ValidMind Framework offers dozens of configurable automated tests that can be run to evaluate various aspects of model performance. These tests cover a range of factors, including data quality, model outcomes, robustness, explainability, and more.\nBy running these automated tests, developers can identify potential issues or risks with the model, such as biases in the training data or lack of robustness to changes in the input data. This information can then be used to make improvements to the model or adjust the testing procedures to ensure that the model meets the required standards.\nThe tests can also be configured to suit the specific requirements of the model and the intended use case, allowing developers to tailor the testing procedures to their needs. For example, if the model is intended to be used in a high-risk application, more rigorous testing and additional model validation may be necessary to ensure that the model is robust and reliable.\n\n\nMade for financial services and insurance industries\nDeveloped specifically to address the risks of banking & insurance model use cases and model risk regulation\nWe understand that your industries have unique requirements and challenges when it comes to model development, and the ValidMind framework has been developed specifically to address these needs.\nOne key aspect is addressing the risks associated with the use of machine learning models in banking and insurance use cases, which require a higher degree of accuracy and reliability.Developed specifically to address the risks of banking & insurance model use cases and model risk regulation By helping you reduce the risk of errors and non-compliance in production, the ValidMind Platform increases the trust that can be placed in your models and the teams that develop and validate them.\nIn addition to addressing the specific risks associated with these industries, the framework has also been designed to help you comply with relevant regulations or guidance governing model risk, such as SR 11-7: Guidance on Model Risk Management. We understand that failure to comply with such guidance can result in costly fines and reputational damage, and that compliance with these regulations is critical for financial services and insurance companies."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind Platform Overview",
    "section": "Related Topics",
    "text": "Related Topics\nGet started with the ValidMind platform"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guide",
    "section": "",
    "text": "How-to instructions for common user tasks."
  },
  {
    "objectID": "guide/jupyternotebooks.html",
    "href": "guide/jupyternotebooks.html",
    "title": "Jupyter notebooks",
    "section": "",
    "text": "Our Jupyter notebooks are designed to showcase the capabilities and features of the ValidMind platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nOur notebooks cover a variety of use cases, ranging from a basic introductory one fpr data analysis and visualization techniques to more advanced machine learning algorithms and data science workflows for model risk management."
  },
  {
    "objectID": "guide/releasenotes.html",
    "href": "guide/releasenotes.html",
    "title": "Release notes",
    "section": "",
    "text": "(Future curated release highlights)"
  },
  {
    "objectID": "guide/releasenotes.html#changelog",
    "href": "guide/releasenotes.html#changelog",
    "title": "Release notes",
    "section": "Changelog",
    "text": "Changelog\n(Future generated changelog)"
  },
  {
    "objectID": "guide/releasenotes.html#bug-fixes",
    "href": "guide/releasenotes.html#bug-fixes",
    "title": "Release notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n(Future bugs we fixed)"
  },
  {
    "objectID": "guide/releasenotes.html#known-issues",
    "href": "guide/releasenotes.html#known-issues",
    "title": "Release notes",
    "section": "Known issues",
    "text": "Known issues\n(Future known issues our users should hear about)"
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Reference content for our Jupyter notebooks and developer framework."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support page is designed to provide you with quick and easy access to the resources you need to troubleshoot technical issues, find answers to frequently asked questions, and get the most out of our ValidMind platform."
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? You can also email support to get more help."
  },
  {
    "objectID": "guide/support.html#troubleshooting",
    "href": "guide/support.html#troubleshooting",
    "title": "Support",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n(Future troubleshooting content.)"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\ncom.amazonaws.vpce.us-west-2.vpce-svc-0b956fa3e03afa538\nprivate.prod.vm.validmind.ai"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the developer framework.\n\n\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#troubleshooting",
    "href": "guide/configure-aws-privatelink.html#troubleshooting",
    "title": "Configure AWS PrivateLink",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/glossary.html",
    "href": "guide/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "developer framework\n\nValidMind’s set of tools, guidelines, and standards designed to provide developers with a structured approach to documenting risk models. It includes our Python library, APIs, and other software components that simplify the model development process.\n\nfirst line of defense\n\nResponsible for developing risk models. Includes the model owners, developers, and users who ensure that the models are developed and documented appropriately and that associated risks are identified, assessed, and managed effectively. The first line of defense is accountable for implementing effective model risk management practices at the business unit level and plays a critical role in identifying and mitigating potential risks associated with models.\n\nmodel risk management (MRM)\n\nThe process of identifying, assessing, and controlling risks associated with models used in financial institutions. Governance and risk management functions oversee the MRM process and ensure compliance with policies.\n\nmodel risk\n\nThe potential for financial loss, incorrect decisions or unintended consequences resulting from errors or inaccuracies in financial models.\n\nsecond line of defense\n\nResponsible for validating and challenging the risk models developed and used by the first line of defense. Ensures compliance with applicable regulations and guidelines, and monitors the effectiveness of the first line’s model risk management (MRM) practices and provides guidance to enhance the first line’s understanding of model risk management.\n\nvalidation, model validation\n\nThe process of evaluating and testing models to ensure that they are accurate, reliable, and effective with the help of our platform.\n\nmodel governance\n\nThe policies, procedures, and controls put in place by financial institutions to manage and monitor the use of models.\n\nmonitoring\n\nThe process of identifying model performance issues, detecting model drift, and ensuring the continued accuracy and relevance of the model."
  },
  {
    "objectID": "guide/about.html",
    "href": "guide/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "guide/getstarteddeveloper.html",
    "href": "guide/getstarteddeveloper.html",
    "title": "Get started as a model developer",
    "section": "",
    "text": "The model developer is responsible for developing, implementing, and maintaining the model. They are typically subject matter experts in the domain of the model and are responsible for ensuring the model is fit-for-purpose, accurate, and aligned with the business requirements."
  },
  {
    "objectID": "guide/getstarteddeveloper.html#prerequisites",
    "href": "guide/getstarteddeveloper.html#prerequisites",
    "title": "Get started as a model developer",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]\nMake sure Python and support for Jupyter notebooks is installed on your machine."
  },
  {
    "objectID": "guide/getstarteddeveloper.html#steps",
    "href": "guide/getstarteddeveloper.html#steps",
    "title": "Get started as a model developer",
    "section": "Steps",
    "text": "Steps\n\nInstall the ZebraGiraffe library via pip:\n\npip install ZebraGiraffe\n\nIn your Jupyter notebook, add the following code to import ZebraGiraffe:\n\nimport ZebraGiraffe"
  },
  {
    "objectID": "guide/getstarteddeveloper.html#troubleshooting",
    "href": "guide/getstarteddeveloper.html#troubleshooting",
    "title": "Get started as a model developer",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/getstarteddeveloper.html#conclusion",
    "href": "guide/getstarteddeveloper.html#conclusion",
    "title": "Get started as a model developer",
    "section": "Conclusion",
    "text": "Conclusion\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/developerframework.html",
    "href": "guide/developerframework.html",
    "title": "Developer framework",
    "section": "",
    "text": "The ValidMind developer framework allows model developers and validators to automatically document different aspects of the model development lifecycle.\nThis Python library provides the following high level features:"
  },
  {
    "objectID": "guide/developerframework.html#installing-the-client-library",
    "href": "guide/developerframework.html#installing-the-client-library",
    "title": "Developer framework",
    "section": "Installing the client library",
    "text": "Installing the client library\nThe Python library can be installed with the following command:\npip install validmind"
  },
  {
    "objectID": "guide/developerframework.html#initializing-the-client-library",
    "href": "guide/developerframework.html#initializing-the-client-library",
    "title": "Developer framework",
    "section": "Initializing the client library",
    "text": "Initializing the client library\nEvery validation project has a project identifier that allows the client library to associate documentation and tests with the appropriate project. In order to initialize the client, we need to provide the following arguments:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nLocation of the ValidMind API.\n\n\napi_key\nAccount API key.\n\n\napi_secret\nAccount Secret key.\n\n\nproject\nThe project identifier.\n\n\n\nThe following code snippet shows how to initialize a ValidMind client instance:\nimport validmind as vm\n\nvm.init(\n  api_host = \"<API_HOST>\",\n  api_key = \"<API_KEY>\",\n  api_secret = \"<API_SECRET>\",\n  project = \"<PROJECT_ID>\"\n)"
  },
  {
    "objectID": "guide/getstartedauditor.html",
    "href": "guide/getstartedauditor.html",
    "title": "ValidMind",
    "section": "",
    "text": "title: “Get started as an auditor”\nThe internal or external auditor is responsible for evaluating the effectiveness of the model risk management process. They are typically independent of the Model Developer and Model Validator and are responsible for ensuring that the model risk management process is well-designed, operating effectively, and meeting regulatory requirements. They provide an objective assessment of the model risk management process to senior management and the board of directors."
  },
  {
    "objectID": "guide/getstartedauditor.html#prerequisites",
    "href": "guide/getstartedauditor.html#prerequisites",
    "title": "ValidMind",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/getstartedauditor.html#steps",
    "href": "guide/getstartedauditor.html#steps",
    "title": "ValidMind",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/getstartedauditor.html#troubleshooting",
    "href": "guide/getstartedauditor.html#troubleshooting",
    "title": "ValidMind",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/getstartedauditor.html#conclusion",
    "href": "guide/getstartedauditor.html#conclusion",
    "title": "ValidMind",
    "section": "Conclusion",
    "text": "Conclusion\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/getstarted.html",
    "href": "guide/getstarted.html",
    "title": "Get started with the ValidMind platform",
    "section": "",
    "text": "Get started overview for how model developers and model validators collaborate."
  },
  {
    "objectID": "validmind/api.html",
    "href": "validmind/api.html",
    "title": "ValidMind",
    "section": "",
    "text": "Python Library API\nMain entrypoint to the ValidMind Python Library\n\nvalidmind.init(project, api_key=None, api_secret=None, api_host=None)\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\n\nParameters\n\nproject (str) – The project CUID\napi_key (str, optional) – The API key. Defaults to None.\napi_secret (str, optional) – The API secret. Defaults to None.\napi_host (str, optional) – The API host. Defaults to None.\n\nRaises\nValueError – If the API key and secret are not provided\nReturns\nTrue if the ping was successful\nReturn type\nbool\n\n\n\nvalidmind.init_dataset(dataset, type=‘training’, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\nRaises\nValueError – If the dataset type is not supported\nReturns\nA VM Dataset instance\nReturn type\nvm.vm.Dataset\n\n\n\nvalidmind.init_model(model)\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained sklearn model\nRaises\nValueError – If the model type is not supported\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.run_test_plan(test_plan_name, send=True, **kwargs)\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘sklearn_classifier’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises\nValueError – If the test plan name is not found or if there is an error initializing the test plan\nReturns\nA dictionary of test results\nReturn type\ndict\n\n\n\nvalidmind.log_dataset(vm_dataset)\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\nvm_dataset (validmind.VMDataset) – A VM dataset object\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options (dict, optional) – Additional dataset options for analysis. Defaults to None.\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset. Defaults to None.\nfeatures (list, optional) – Optional. A list of features metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nThe VMDataset object\nReturn type\nvalidmind.VMDataset\n\n\n\nvalidmind.log_figure(data_or_path, key, metadata, run_cuid=None)\nLogs a figure\n\nParameters\n\ndata_or_path (str or matplotlib.figure.Figure) – The path of the image or the data of the plot\nkey (str) – Identifier of the figure\nmetadata (dict) – Python data structure\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metadata(content_id, text=None, extra_json=None)\nLogs free-form metadata to ValidMind API.\n\nParameters\n\ncontent_id (str) – Unique content identifier for the metadata\ntext (str, optional) – Free-form text to assign to the metadata. Defaults to None.\nextra_json (dict, optional) – Free-form key-value pairs to assign to the metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metrics(metrics, run_cuid=None)\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics (list) – A list of Metric objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_model(vm_model)\nLogs model metadata and hyperparameters to ValidMind API.\n\nParameters\nvm_model (validmind.VMModel) – A VM model object\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_test_results(results, run_cuid=None, dataset_type=‘training’)\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults (list) – A list of TestResults objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nclass validmind.Dataset(raw_dataset: object, fields: list, variables: list, sample: list, shape: dict, correlation_matrix: object | None = None, correlations: dict | None = None, type: str | None = None, options: dict | None = None, statistics: dict | None = None, targets: dict | None = None, target_column: str = ’’, class_labels: dict | None = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: object | None = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nvariables(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.DatasetTargets(target_column: str, description: str | None = None, class_labels: dict | None = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.Figure(key: str, metadata: dict, figure: object, extras: dict | None = None)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.Metric(test_context: TestContext, params: dict | None = None, result: TestPlanMetricResult | None = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.Model(attributes: ModelAttributes | None = None, task: str | None = None, subtask: str | None = None, params: dict | None = None, model_id: str = ‘main’, model: object | None = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nclassmethod is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.ModelAttributes(architecture: str | None = None, framework: str | None = None, framework_version: str | None = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.TestResult(*, test_name: str | None = None, column: str | None = None, passed: bool | None = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: str | Non )\n\n\ncolumn(: str | Non )\n\n\npassed(: bool | Non )\n\n\nvalues(: dic )\n\n\n\nclass validmind.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.ThresholdTest(test_context: TestContext, params: dict | None = None, test_results: TestResults | None = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/vm_models.html",
    "href": "validmind/vm_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Models\nModels entrypoint\n\nclass validmind.vm_models.Dataset(raw_dataset: object, fields: list, variables: list, sample: list, shape: dict, correlation_matrix: object | None = None, correlations: dict | None = None, type: str | None = None, options: dict | None = None, statistics: dict | None = None, targets: dict | None = None, target_column: str = ’’, class_labels: dict | None = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: object | None = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nvariables(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.vm_models.DatasetTargets(target_column: str, description: str | None = None, class_labels: dict | None = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.vm_models.Figure(key: str, metadata: dict, figure: object, extras: dict | None = None)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Metric(test_context: TestContext, params: dict | None = None, result: TestPlanMetricResult | None = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.vm_models.MetricResult(type: str, scope: str, key: dict, value: dict | list | DataFrame, value_formatter: str | None = None)\nBases: object\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\ntype(: st )\n\n\nscope(: st )\n\n\nkey(: dic )\n\n\nvalue(: dict | list | DataFram )\n\n\nvalue_formatter(: str | Non _ = Non_ )\n\n\nserialize()\nSerializes the Metric to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Model(attributes: ModelAttributes | None = None, task: str | None = None, subtask: str | None = None, params: dict | None = None, model_id: str = ‘main’, model: object | None = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nclassmethod is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.vm_models.ModelAttributes(architecture: str | None = None, framework: str | None = None, framework_version: str | None = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.vm_models.TestContext(dataset: Dataset | None = None, model: Model | None = None, train_ds: Dataset | None = None, test_ds: Dataset | None = None, y_train_predict: object | None = None, y_test_predict: object | None = None)\nBases: object\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as model predictions, shared dataset metrics, etc.\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\n\nclass validmind.vm_models.TestContextUtils()\nBases: object\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\ntest_context(: TestContex )\n\n\nproperty dataset()\n\n\nproperty model()\n\n\nproperty train_ds()\n\n\nproperty test_ds()\n\n\nproperty y_train_predict()\n\n\nproperty y_test_predict()\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\nproperty df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame\n\n\n\nclass validmind.vm_models.TestPlan(config: {} = None, test_context: TestContext = None, dataset: Dataset = None, model: Model = None, train_ds: Dataset = None, test_ds: Dataset = None, pbar: tqdm = None)\nBases: object\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\nname(: ClassVar[str )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\ntests(: ClassVar[List[object] _ = [_ )\n\n\ntest_plans(: ClassVar[List[object] _ = [_ )\n\n\nresults(: ClassVar[List[TestPlanResult] _ = [_ )\n\n\nconfig(: { _ = Non_ )\n\n\ntest_context(: TestContex _ = Non_ )\n\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\npbar(: tqd _ = Non_ )\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\nrun(send=True)\nRuns the test plan\n\n\nlog_results()\nLogs the results of the test plan to ValidMind\nThis method will be called after the test plan has been run and all results have been collected. This method will log the results to ValidMind.\n\n\nsummarize()\nSummarizes the results of the test plan\nThis method will be called after the test plan has been run and all results have been logged to ValidMind. It will summarize the results of the test plan by creating an html table with the results of each test. This html table will be displayed in an VS Code, Jupyter or other notebook environment.\n\n\n\nclass validmind.vm_models.TestPlanDatasetResult(dataset: Dataset | None = None)\nBases: TestPlanResult\nResult wrapper for datasets that run as part of a test plan\n\ndataset(: Datase _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanMetricResult(figures: List[Figure] | None = None, metric: MetricResult | None = None)\nBases: TestPlanResult\nResult wrapper for metrics that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\nmetric(: MetricResult | Non _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanModelResult(model: Model | None = None)\nBases: TestPlanResult\nResult wrapper for models that run as part of a test plan\n\nmodel(: Mode _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanTestResult(test_results: TestResults | None = None)\nBases: TestPlanResult\nResult wrapper for test results produced by the tests that run as part of a test plan\n\ntest_results(: TestResult _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestResult(*, test_name: str | None = None, column: str | None = None, passed: bool | None = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: str | Non )\n\n\ncolumn(: str | Non )\n\n\npassed(: bool | Non )\n\n\nvalues(: dic )\n\n\n\nclass validmind.vm_models.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.vm_models.ThresholdTest(test_context: TestContext, params: dict | None = None, test_results: TestResults | None = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/model_validation_tests_sklearn.html",
    "href": "validmind/model_validation_tests_sklearn.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions models trained with sklearn or that provide a sklearn-like API\n\n\n\nBases: Metric\nAccuracy Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nCharacteristic Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculates PSI for each of the dataset features\n\n\n\n\n\n\n\nBases: Metric\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nF1 Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPermutation Feature Importance\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPrecision Recall Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nPrecision Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nRecall Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nROC AUC Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nROC Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: TestContextUtils\nSHAP Global Importance. Custom metric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBases: Metric\nPopulation Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the accuracy score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the F1 score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the ROC AUC score is above a threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the training set metrics are better than the test set metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "validmind/readme.html",
    "href": "validmind/readme.html",
    "title": "ValidMind",
    "section": "",
    "text": "pip install validmind\n\n\npip install validmind[r-support]\n\n\n\n\n\n\n\n\nEnsure you have poetry installed: https://python-poetry.org/\nAfter cloning this repo run:\n\npoetry shell\npoetry install\n\n\n\nIf you want to use the R support that is provided by the ValidMind Developer Framework, you must have R installed on your machine. You can download R from https://cran.r-project.org/. If you are on a Mac, you can install R using Homebrew:\nbrew install r\nOnce you have R installed, you can install the r-support extra to install the necessary dependencies for R by running:\npoetry install --extras r-support\n\n\n\nMake sure you bump the package version before merging a PR with the following command:\nmake version tag=prerelease\nThe value of tag corresponds to one of the options provided by Poetry: https://python-poetry.org/docs/cli/#version\n\n\n\n\nIf you want to integate the validmind package to your development environment, you must build the package wheel first, since we have not pushed the package to a public PyPI repository yet. Steps:\n\nRun make build to build a new Python package for the developer framework\nThis will create a new wheel file in the dist folder\nRun pip install <path-to-wheel> to install the newly built package in your environment\n\n\n\n\nAPI documentation can be generated in Markdown or HTML format. Our documentation pipeline uses Markdown documentation before generating the final HTML assets for the documentation site.\nFor local testing, HTML docs can be generated with Sphinx. Note that the output template is different since the documentation pipeline uses the source Markdown files for the final HTML output.\nMarkdown and HTML docs can be generated with the following commands:\n# Navigate to the docs folder\ncd docs/\n\n# Generate Markdown docs\nmake markdown\n\n# Generate HTML docs\nmake html\nThe resulting markdown and html under docs/_build folders will contain the generated documentation.\n\n\n\n\n\nIf you run into an error related to the ValidMind wheel, try:\npoetry add wheel\npoetry update wheel\npoetry install\nIf there are lightgbm errors partway through, run remove lightgbm, followed by poetry update wheel and poetry install."
  },
  {
    "objectID": "validmind/test_plans.html",
    "href": "validmind/test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "Test Plans entry point\n\n\nReturns a list of all available test plans\n\n\n\nReturns a list of all available tests\n\n\n\nReturns the test plan by name\n\n\n\nReturns a description of the test plan\n\n\n\nTest plan for sklearn classifier models\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan for sklearn classifier metrics\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models that includes both metrics and validation tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest plan for tabular datasets\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for generic tabular datasets"
  },
  {
    "objectID": "validmind/index.html",
    "href": "validmind/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Developer Framework\n\nValidMind Python Client\n\nInstallation\nContributing to ValidMind Developer Framework\nIntegrating the ValidMind Developer Framework to your development environment\nGenerating Docs\nKnown Issues\n\nPython Library API\nCore Library Tests\n\nData Validation Tests\n\nCore Library Tests\n\nModel Validation Tests for SKLearn-Compatible Models\n\nTest Plans\n\nlist_plans()\nlist_tests()\nget_by_name()\ndescribe_plan()\nTest Plans for SKLearn-Compatible Classifiers\nTest Plans for Tabular Datasets\n\nValidMind Models"
  },
  {
    "objectID": "validmind/data_validation_tests.html",
    "href": "validmind/data_validation_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions for any Pandas-compatible datasets\n\n\n\nBases: TestContextUtils\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadat is necessary to initialize dataset object that can be related to different metrics and test results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function\n\n\n\n\nBases: Metric\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the minority class does not represent more than a threshold of the total number of examples\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of duplicates is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique values in a column is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the Pearson correlation between two columns is less than a threshold\nInspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the skewness of a column is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique rows is greater than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\n\n\n\nBases: ThresholdTest\nTest that the number of zeros is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Find all the information you need to use our platform for model risk management (MRM)\n\n\n\n    \n    \n    Column Cards\n    \n\n\n    \n        \n            \n                Model developers\n                Collect, manage, and automate your model documentation and testing with our developer framework.\n                Start\n            \n            \n                Model validators\n                Review and evaluate models and model documentation to ensure it complies with organizational and regulatory requirements.\n                Start"
  },
  {
    "objectID": "notebooks/lending_club.html",
    "href": "notebooks/lending_club.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\ndf = pd.read_pickle(\"notebooks/datasets/_temp/df_loans_cleaned.pickle\")\n\ntargets = vm.DatasetTargets(\n    target_column=\"loan_status\",\n    class_labels={\n        \"Fully Paid\": \"Fully Paid\",\n        \"Charged Off\": \"Charged Off\",\n    }\n)\n\nvm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nTrue\n\n\n\nresults = vm.run_dataset_tests(df, target_column=\"loan_status\", dataset_type=\"training\", vm_dataset=vm_dataset, send=True)\n\nRunning data quality tests for \"training\" dataset...\n\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74.72it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\n\n\n\n\n\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest                 Passed      # Passed    # Errors    % Passed\n-------------------  --------  ----------  ----------  ----------\nclass_imbalance      True               1           0         100\nduplicates           False              0           1           0\ncardinality          False             14           7     66.6667\nmissing              False             25          53     32.0513\npearson_correlation  False              0          10           0\nskewness             False              3           6     33.3333\nzeros                False              1           3          25\n\n\n\n\ntrain_ds, val_ds = train_test_split(df, test_size=0.20)\n\nx_train = train_ds.drop(\"loan_status\", axis=1)\nx_val = val_ds.drop(\"loan_status\", axis=1)\ny_train = train_ds.loc[:, \"loan_status\"].astype(str)\ny_val = val_ds.loc[:, \"loan_status\"].astype(str)\n\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\nxgb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = xgb_model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nvm.log_model(xgb_model)"
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the library code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local library code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\n# Use api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\" if you want to connect to the dev environment\nvm.init(project=\"cl1jyv16o000809lg98gi9tie\")\n\nTrue\n\n\n\n\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\nBefore we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n\n\n\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured content_id can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the Model Overview section for a project, they can use model_overview as the content_id:\nvm.log_metadata(\"model_overview\", text=\"Testing\")\nThe text argument accepts Markdown formatted text as we’ll see in the cell below. The documentation used for this model has been taken from the Kaggle dataset.\n\nmodel_overview = \"\"\"\nTesting writing metadata from the framework\nThe ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`\n\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nTrue\n\n\nThe dashboard should now display the Model Overview section with the text we have provided from the library:\n\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nWe can now initialize the TabularDataset test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\n\n        \n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Surname', passed=False, values={'n_distinct': 2616, 'p_distinct': 0.327}), TestResult(test_name=None, column='Geography', passed=True, values={'n_distinct': 3, 'p_distinct': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389458}]})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CustomerId', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Geography', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'skewness': -0.005920679739677088}), TestResult(test_name=None, column='CustomerId', passed=True, values={'skewness': 0.010032280260684402}), TestResult(test_name=None, column='CreditScore', passed=True, values={'skewness': -0.06195161237091896}), TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.0245221429799511}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.007692043774702702}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111804}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077728})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='CustomerId', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_unique': 2616, 'p_unique': 0.327}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_unique': 452, 'p_unique': 0.0565}), TestResult(test_name=None, column='Geography', passed=True, values={'n_unique': 3, 'p_unique': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                           Name                        Description                                   \n\n\nsklearn_classifier_metrics   SKLearnClassifierMetrics    Test plan for sklearn classifier metrics      \nsklearn_classifier_validationSKLearnClassifierPerformanceTest plan for sklearn classifier models       \nsklearn_classifier           SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                               \ntabular_dataset              TabularDataset              Test plan for generic tabular datasets        \ntabular_dataset_description  TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                               \ntabular_data_quality         TabularDataQuality          Test plan for data quality on tabular datasets\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                       Name                        Description                                                               \n\n\nMetric       dataset_correlations     DatasetCorrelations         Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                           \nMetric       dataset_description      DatasetDescription          Collects a set of descriptive statistics for a dataset                    \nCustom Test  dataset_metadata         DatasetMetadata             Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadat is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                           \nThresholdTestclass_imbalance          ClassImbalanceTest          Test that the minority class does not represent more than a threshold\n    of the total number of examples                                                                           \nThresholdTestduplicates               DuplicatesTest              Test that the number of duplicates is less than a threshold               \nThresholdTestcardinality              HighCardinalityTest         Test that the number of unique values in a column is less than a threshold\nThresholdTestpearson_correlation      HighPearsonCorrelationTest  Test that the Pearson correlation between two columns is less than a threshold\n\n    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           \nThresholdTestmissing                  MissingValuesTest           Test that the number of missing values is less than a threshold           \nThresholdTestskewness                 SkewnessTest                Test that the skewness of a column is less than a threshold               \nThresholdTestunique                   UniqueRowsTest              Test that the number of unique rows is greater than a threshold           \nThresholdTestzeros                    ZerosTest                   Test that the number of zeros is less than a threshold                    \nMetric       accuracy                 AccuracyScore               Accuracy Score                                                            \nMetric       csi                      CharacteristicStabilityIndexCharacteristic Stability Index between two datasets                       \nMetric       confusion_matrix         ConfusionMatrix             Confusion Matrix                                                          \nMetric       f1_score                 F1Score                     F1 Score                                                                  \nMetric       pfi                      PermutationFeatureImportancePermutation Feature Importance                                            \nMetric       psi                      PopulationStabilityIndex    Population Stability Index between two datasets                           \nMetric       pr_curve                 PrecisionRecallCurve        Precision Recall Curve                                                    \nMetric       precision                PrecisionScore              Precision Score                                                           \nMetric       roc_auc                  ROCAUCScore                 ROC AUC Score                                                             \nMetric       roc_curve                ROCCurve                    ROC Curve                                                                 \nMetric       recall                   RecallScore                 Recall Score                                                              \nCustom Test  shap                     SHAPGlobalImportance        SHAP Global Importance. Custom metric                                     \nThresholdTestaccuracy_score           AccuracyTest                Test that the accuracy score is above a threshold.                        \nThresholdTestf1_score                 F1ScoreTest                 Test that the F1 score is above a threshold.                              \nThresholdTestroc_auc_score            ROCAUCScoreTest             Test that the ROC AUC score is above a threshold.                         \nThresholdTesttraining_test_degradationTrainingTestDegradationTest Test that the training set metrics are better than the test set metrics.  \n\n\n\n\nOnce the TabularDataset test plan has finished running, we can view the results in the ValidMind dashboard:\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.86375\n\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe can now run the SKLearnClassifier test plan:\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nntree_limit is deprecated, use `iteration_range` or model slicing instead.\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n        \n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.84875\n                \n            \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.5739436619718309\n                \n            \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7244444444444444\n                \n            \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.4752186588921283\n                \n            \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7129474360490873\n                \n            \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'Gender': 8.3e-05, 'Age': 0.000525, 'Tenure': 0.000832, 'Balance': 0.00077, 'NumOfProducts': 0.000195, 'HasCrCard': 0.000231, 'IsActiveMember': 0.0, 'EstimatedSalary': 0.000272, 'Geography_France': 0.0, 'Geography_Germany': 0.000301, 'Geography_Spain': 0.000286}\n                \n            \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        {self.metric.key}\n                    \n                    \n                        Metric Type\n                        {self.metric.type}\n                    \n                    \n                        Metric Scope\n                        {self.metric.scope}\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial  new  percent_new       psi\nbin                                                      \n1       2547         0.530625  828     0.517500  0.000329\n2        904         0.188333  295     0.184375  0.000084\n3        352         0.073333  132     0.082500  0.001080\n4        209         0.043542   69     0.043125  0.000004\n5        166         0.034583   50     0.031250  0.000338\n6        132         0.027500   46     0.028750  0.000056\n7        124         0.025833   47     0.029375  0.000455\n8         99         0.020625   35     0.021875  0.000074\n9         92         0.019167   42     0.026250  0.002228\n10       175         0.036458   56     0.035000  0.000060\n                \n            \n            \n        \n        \n        \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\n\n        \n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.70875, 'threshold': 0.7})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.1795774647887324, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.5051316128224218, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=True, values={'test_score': 0.70875, 'train_score': 0.72875, 'degradation': 0.027444253859348223}), TestResult(test_name='precision', column=None, passed=False, values={'test_score': 0.22666666666666666, 'train_score': 0.2148626817447496, 'degradation': -0.054937343358395944}), TestResult(test_name='recall', column=None, passed=False, values={'test_score': 0.14868804664723032, 'train_score': 0.14014752370916755, 'degradation': -0.06093952081369594}), TestResult(test_name='f1', column=None, passed=False, values={'test_score': 0.1795774647887324, 'train_score': 0.16964285714285715, 'degradation': -0.05856189770200149})]"
  },
  {
    "objectID": "notebooks/library_intro_demos.html",
    "href": "notebooks/library_intro_demos.html",
    "title": "ValidMind",
    "section": "",
    "text": "The ValidMind Python client allows model developers and validators to automatically document different aspects of the model development lifecycle.\nFor modelers, the client provides the following high level features:\n\nLog qualitative data about the model’s conceptual soundness\nLog information about datasets and models\nLog training and evaluation metrics about datasets and models\nRun data quality checks\nRun model evaluation tests\n\nFor validators, the client also provides (TBD) the ability to effectively challenge the model’s performance according to its objective, use case and specific project’s requirements.\n\n\n\nThis notebook and the ValidMind client must be executed on an environment running Python >= 3.8.\n\n\n\n\nWhile we finish the process of making the library publicly accessible pip, it can be installed with the following command that will direct pip to the S3 bucket that contains the latest version of the client.\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv('./env')\n\nTrue\n\n\n\n\n\n\nBefore we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n\nNavigate to the dashboard and click on the “Create new Project” button\nProvide a name and description for the project\nSelect a model use case\nFor modeling objective, we only support automated documentation of Binary Clasification models at the moment\n\nAfter creating the project you will be provided with client library setup instructions. We have provided similar instructions below.\n\n\nEvery validation project in the ValidMind dashboard has an associated project identifier. In order to initialize the client, we need to provide the following arguments:\n\nproject: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for /projects/cl1jyvh2c000909lg1rk0a0zb the project identifier is cl1jyvh2c000909lg1rk0a0zb\napi_host: Location of the ValidMind API. This value is already set on this notebook.\napi_key: Account API key. This can be found in the settings page in the ValidMind dashboard\napi_secret: Account Secret key. Also found in the settings page in the ValidMind dashboard\n\n\n# Lookup your own project id\n# project='cla6walda00001wl6pdzagu9v'\nproject='clar3ppjg000f1gmikrfmkld6'\n\nWe can now initialize the client library with the vm.init function:\n\nimport validmind as vm\n\nvm.init(\n    project=project\n)\n\nTrue\n\n\n\n# Necessary imports for training our demo models\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nAs of version 0.8.x of the client library, the following logging and testing functions are available:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlog_metadata\nLogs free-form metadata text for a given content ID in the model documentation\n\n\nlog_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics\n\n\nrun_dataset_tests\nRuns dataset quality tests on the input dataset\n\n\nanalyze_dataset\nAnalyzes a dataset, computes summary statistics and runs data quality tests. This function combines log_dataset and run_dataset_tests\n\n\nlog_model\nLogs information about a model’s framework, architecture, target objective and training parameters\n\n\nlog_training_metrics\nExtracts and logs training metrics from a pre-trained model\n\n\nevaluate_model\nExtracts metadata and metrics from a train model instances and runs model evaluation tests according to the model objective, use case and specific validation requirements. This function combines log_model, log_training_metrics and an additional set of preconfigured model evaluation tests\n\n\n\nIn the example model training code in this notebook, we will demonstrate each of the documented client library functions.\n\n\nLogs free-form metadata text for a given content ID in the model documentation.\nArguments:\n\ncontent_id: Content ID of the model documentation. This is a unique identifier generated by the ValidMind dashboard. See available content_ids in the model training section below\ntext: Free-form text to be logged. A text template can be specified in combination with extra_json (see below)\nextra_json: (TBD support for this) JSON object containing variables to be substituted in the text template\n\n\n\n\nAnalyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n\nDescriptive statistics for numerical and categorical columns\nHistograms and value counts for summarizing distribution of values\nPearson correlation matrix for numerical columns\nCorelation plots for top 15 correlated features\n\nAdditionally, it will run a collection of data quality tests such as:\n\nClass imbalance test on target column\nDuplicate rows and duplicates based on primary key\nHigh cardinality test on categorical columns\nMissing values\nHighly correlated column pairs\nSkewness test\nZeros test (columns with too many zeros)\n\nArguments:\n\ndataset: Input dataset. Only Pandas DataFrames are supported at the moment\ndataset_type: Type of dataset, e.g. training, test, validation. Value needs to be set to training for now\ntargets: vm.DatasetTargets describing the label column and its values\nfeatures: Optional list of properties to specify for some features in the dataset\n\nReturns:\n\nresults: List of data quality test results\n\n\n\n\nLogs the following information about a model:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\nModel performance metrics from training, validation and test dataset\n\nAdditionally, this function runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n\nAccuracy score\nPrecision score\nRecall score\nF1 score\nROC AUC score\nROC AUC curve\nConfusion matrix\nPrecision Recall curve\nPermutation feature importance\nSHAP global importance\n\nArguments:\n\nmodel: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\ntrain_set: Training dataset tuple (x_train, y_train)\nval_set: Validation dataset tuple (x_val, y_val)\ntest_set: Test dataset tuple (x_test, y_test)\n\n\n\n\n\nWe’ll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n# Bank Customer Churn Dataset\nchurn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n\n# Health Insurance Cross-Sell Dataset\ninsurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")\n\n\nchurn_dataset2 = pd.read_csv(\"https://gist.githubusercontent.com/mehdi0501/5b9e64b51ed3bbddbe8f018fc7caf626/raw/ee9b21e5f5308299eb5f4d9dd251bc1b9c5ecc85/churn_test.csv\")\n\n\nchurn_dataset2.head()\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      Surname\n      CreditScore\n      Geography\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      0\n      1\n      15634602\n      Hargrave\n      619\n      France\n      Female\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n    \n    \n      1\n      2\n      15647311\n      Hill\n      608\n      Spain\n      Female\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n    \n    \n      2\n      3\n      15619304\n      Onio\n      502\n      France\n      Female\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n    \n    \n      3\n      4\n      15701354\n      Boni\n      699\n      France\n      Female\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n    \n    \n      4\n      5\n      15737888\n      Mitchell\n      850\n      Spain\n      Female\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n    \n  \n\n\n\n\n\nchurn_dataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        8000 non-null   int64  \n 1   CustomerId       8000 non-null   int64  \n 2   Surname          8000 non-null   object \n 3   CreditScore      8000 non-null   int64  \n 4   Geography        8000 non-null   object \n 5   Gender           8000 non-null   object \n 6   Age              8000 non-null   int64  \n 7   Tenure           8000 non-null   int64  \n 8   Balance          8000 non-null   float64\n 9   NumOfProducts    8000 non-null   int64  \n 10  HasCrCard        8000 non-null   int64  \n 11  IsActiveMember   8000 non-null   int64  \n 12  EstimatedSalary  8000 non-null   float64\n 13  Exited           8000 non-null   int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 875.1+ KB\n\n\n\nchurn_dataset.describe()\n\n\n\nBefore we start logging information about our dataset, we’d want to send metadata to ValidMind about the model’s conceptual soundness, for example. Model developers have the option to directly populate parts of the dashboard documentation using special content_ids. The following is the list of content_ids supported at the moment:\n\n\n\n\n\n\n\nContent ID\nPopulates Section\n\n\n\n\nmodel_overview\nConceptual Soundness -> Model Overview\n\n\nmodel_selection\nConceptual Soundness -> Model Selection\n\n\nbusiness_case\nConceptual Soundness -> Intended Use and Business Use Case\n\n\nfeature_selection\nData Preparation -> Feature Selection and Engineering\n\n\ngovernance_plan\nMonitoring and Governance -> Governance Plan\n\n\nmonitoring_implementation\nMonitoring and Governance -> Monitoring Implementation\n\n\nmonitoring_plan\nMonitoring and Governance -> Monitoring Plan\n\n\n\nIn the following log_metadata example, we will populate the Model Overview section in the dashboard:\n\nmodel_overview = \"\"\"\nWe aim to accomplish the following for this study:\n\n- Identify and visualize which factors contribute to customer churn\n- Build a prediction model that will perform the following:\n  - Classify if a customer is going to churn or not\n  - Preferably and based on model performance, choose a model that will attach a probability\n  to the churn to make it easier for customer service to target low hanging fruits in their\n  efforts to prevent churn\n\"\"\"\n\nvm.log_metadata(content_id=\"model_overview\", text=model_overview)\n\nSuccessfully logged metadata\n\n\nTrue\n\n\nWe can now go to Project Overview -> Documentation -> Model Overview and verify this content has been populated on the dashboard.\n\n\n\nAfter loading the dataset, we can log metadata and summary statistics, and run data quality checks for it using analyze_dataset. Note that the analyze_dataset function expects a targets definition. Additional information about columns can be provided with the features argument.\n\nchurn_targets = vm.DatasetTargets(\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nchurn_features = [\n    {\n        \"id\": \"RowNumber\",\n        \"type_options\": {\n            \"primary_key\": True,\n        }\n    }\n]\n\nanalyze_results = vm.analyze_dataset(\n    dataset=churn_dataset,\n    dataset_type=\"training\",\n    targets=churn_targets,\n    features=churn_features\n)\n\nAnalyzing dataset...\nPandas dataset detected.\nInferring dataset types...\nPreparing in-memory dataset copy...\nCalculating field statistics...\nCalculating feature correlations...\nGenerating correlation plots...\nSuccessfully logged dataset metadata and statistics.\nRunning data quality tests...\nRunning data quality tests for \"training\" dataset...\n\nPreparing dataset for tests...\nPreparing in-memory dataset copy...\n\n\n100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest             Passed      # Passed    # Errors    % Passed\n---------------  --------  ----------  ----------  ----------\nclass_imbalance  True               1           0         100\nduplicates       True               2           0         100\ncardinality      False              6           1     85.7143\nmissing          True              14           0         100\nskewness         False              6           1     85.7143\nzeros            False              0           2           0\n\n\n\nAfter running analyze_dataset, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\nDashboard -> Project Overview -> Documentation -> Data Description\n\n\n\nWe are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance.\n\ndef preprocess_churn_dataset(df):\n    # Drop columns with no correlation to target\n    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n    # Encode binary features\n    genders = {\"Male\": 0, \"Female\": 1}\n    df.replace({\"Gender\": genders}, inplace=True)\n\n    # Encode categorical features\n    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n    df.drop(\"Geography\", axis=1, inplace=True)\n\n    return df\n\n\npreprocessed_churn = preprocess_churn_dataset(churn_dataset)\n\n\ndef train_val_test_split_dataset(df):\n    train_df, test_df = train_test_split(df, test_size=0.20)\n\n    # This guarantees a 60/20/20 split\n    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n    # For training\n    x_train = train_ds.drop(\"Exited\", axis=1)\n    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n    x_val = val_ds.drop(\"Exited\", axis=1)\n    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n    # For testing\n    x_test = test_df.drop(\"Exited\", axis=1)\n    y_test = test_df.loc[:, \"Exited\"].astype(int)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)\n\n\ndef train_churn_dataset(x_train, y_train, x_val, y_val):\n    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n\n    xgb_model.set_params(\n        eval_metric=[\"error\", \"logloss\", \"auc\"],\n    )    \n\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=False,\n    )\n    return xgb_model\n\n\nxgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)\n\n\ndef model_accuracy(model, x, y):\n    y_pred = model.predict_proba(x)[:, -1]\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y, predictions)\n\n    print(f\"Accuracy: {accuracy}\")    \n\n\nmodel_accuracy(xgb_model, x_val, y_val)\n\n\n\n\nFinally, after training our model, we can log its model parameters, collect performance metrics and run model evaluation tests on it using evaluate_model:\n\neval_results = vm.evaluate_model(\n    xgb_model,\n    train_set=(x_train, y_train),\n    val_set=(x_val, y_val),\n    test_set=(x_test, y_test)\n)\n\nAfter running evaluate_model, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability"
  },
  {
    "objectID": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "href": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "title": "ValidMind",
    "section": "",
    "text": "Introduction\n\nExecutive Summary\nBeing able to make accurate and timely estimates of future claims is a fundamental task for actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on understanding future claims, with mortality being one of the central issues facing a life insurer.\nIn this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation.\n\n\nOverview of Mortality Case Study\n\n Case Study Data \nOur dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\nFor the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old.\nMore details on this dataset can be found in Section 2 of the data report https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf\n\n\n Case Study Model \nFor the case study in this paper, we used the statsmodel’s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\nThe  response variable used in this case study is the number of deaths. Policies exposed was used as a weight in the model. We also tried to fit the mortality rate, which is number of deaths/ policies exposed using Gaussian distribution with log link, that can be found in the Appendix\nThe features used in the mortality model are:\n\nAttained Age – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\nDuration – the number of years (starting with a value of one) the policyholder has had the policy.\nSmoking Status – if the policyholder is considered a smoker or not.\nPreferred Class – an underwriting structure used by insurers to classify and price policyholders. Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\nGender – A categorical feature in the model with two levels, male and female.\nGuaranteed Term Period – the length of the policy at issue during which the premium will remain constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\nFace_Amount_Band\nObservation Year\n\n\n\n\n\nSet Up\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport sklearn \nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport os\nimport xgboost as xgb\n\nFirst, let’s download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file.\n\n# directly curl from the SOA website and unzip\n! echo Working Directory = $(pwd)\n! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n! echo \"Done\"\n\nWorking Directory = /Users/andres/code/validmind-sdk/notebooks/insurance_mortality\nData folder already exists\nFile already exists\nDone\n\n\nSecond, sample 5% from the giant file. Another 10 minutes or so the first time you run it :)\n\n#sample 5% and save it out to a sample file\nif not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n    p = 0.05\n    random.seed(42)\n    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv', \n                        skiprows = lambda i: i>0 and random.random() >p)\n    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)\n\n\n\nEDA\n\n# load sample file \nsample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n                               'Face_Amount_Band', 'Preferred_Class', \n                               'Number_Of_Deaths','Policies_Exposed', \n                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator', \n                               'Expected_Death_QX2015VBT_by_Policy',\n                               'Issue_Age', 'Issue_Year'])\n\n# target variable\nsample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n\nsample_df.head()\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Gender\n      Smoker_Status\n      Insurance_Plan\n      Issue_Age\n      Duration\n      Attained_Age\n      Face_Amount_Band\n      Issue_Year\n      Preferred_Class\n      SOA_Anticipated_Level_Term_Period\n      SOA_Guaranteed_Level_Term_Period\n      SOA_Post_level_Term_Indicator\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      0\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      4.882191\n      0.001074\n      0.0\n    \n    \n      1\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      500000-999999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      25.795943\n      0.006449\n      0.0\n    \n    \n      2\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      1.117809\n      0.000134\n      0.0\n    \n    \n      3\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      250000-499999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      70.098636\n      0.009814\n      0.0\n    \n    \n      4\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      4\n      3\n      50000-99999\n      2006\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      493.523281\n      0.034547\n      0.0\n    \n  \n\n\n\n\n\n# filter pipeline\nsample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n               & (sample_df.Smoker_Status != 'Unknown') \n               & (sample_df.Insurance_Plan == ' Term')\n               & (-sample_df.Preferred_Class.isna())\n               & (sample_df.Attained_Age >= 18)\n               & (sample_df.Issue_Year >= 1980)\n               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n               & (sample_df.mort < 1)]\n\nprint(f'Count: {sample_df.shape[0]}')\nprint()\n\n# describe data\nsample_df.describe()\n\nCount: 307233\n\n\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Issue_Age\n      Duration\n      Attained_Age\n      Issue_Year\n      Preferred_Class\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      count\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      3.072330e+05\n      307233.000000\n    \n    \n      mean\n      2014.084001\n      42.248505\n      7.951434\n      49.199939\n      2006.640537\n      2.035013\n      0.018514\n      12.504679\n      1.932158e-02\n      0.001627\n    \n    \n      std\n      1.413654\n      12.777574\n      4.793230\n      13.340539\n      4.888334\n      0.962332\n      0.147063\n      29.112019\n      5.412559e-02\n      0.023061\n    \n    \n      min\n      2012.000000\n      18.000000\n      1.000000\n      18.000000\n      1984.000000\n      1.000000\n      0.000000\n      0.002732\n      1.918000e-07\n      0.000000\n    \n    \n      25%\n      2013.000000\n      32.000000\n      4.000000\n      39.000000\n      2003.000000\n      1.000000\n      0.000000\n      0.838356\n      7.766577e-04\n      0.000000\n    \n    \n      50%\n      2014.000000\n      42.000000\n      7.000000\n      49.000000\n      2007.000000\n      2.000000\n      0.000000\n      2.612022\n      3.316641e-03\n      0.000000\n    \n    \n      75%\n      2015.000000\n      52.000000\n      12.000000\n      59.000000\n      2011.000000\n      3.000000\n      0.000000\n      10.680379\n      1.470165e-02\n      0.000000\n    \n    \n      max\n      2016.000000\n      84.000000\n      30.000000\n      91.000000\n      2016.000000\n      4.000000\n      6.000000\n      655.938021\n      2.827005e+00\n      0.981233\n    \n  \n\n\n\n\n\n# Encode categorical variables\ncat_vars = ['Observation_Year', \n     'Gender', \n     'Smoker_Status',\n     'Face_Amount_Band', \n     'Preferred_Class',\n     'SOA_Anticipated_Level_Term_Period']\n\nonehot = preprocessing.OneHotEncoder()\nresults = onehot.fit_transform(sample_df[cat_vars]).toarray()\ncat_vars_encoded = list(onehot.get_feature_names_out())\nsample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n\n\n# categorical variables\nface_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\nterm_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\nfig, ax = plt.subplots(4,2, figsize = (20,30))\nax = ax.flatten()\nfor i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n       'Face_Amount_Band', 'Preferred_Class',\n       'SOA_Guaranteed_Level_Term_Period']):\n    if column == 'Face_Amount_Band':\n        order = face_amount_order\n    elif column == 'SOA_Guaranteed_Level_Term_Period':\n        order = term_period_order\n    else:\n        order = None\n    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\nplt.show()\n\n\n\n\n\n# age and duration variables\nfig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n\nsns.histplot(x = sample_df['Duration'], ax = ax[1])\nplt.show()\n\n\n\n\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n# log mort by Attained Age\n\ndef stratify(field):\n    fig, ax = plt.subplots(figsize = (7,3))\n    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n    plt.show()\n\nstratify('Smoker_Status')\nstratify('Preferred_Class')\nstratify('Gender')\nstratify('Observation_Year')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\nTrain/test split\nFirst we split the data into 80% for training and 20% for testing.\nIn this context because we don’t really need to do hyperparameter tuning so it’s not necessary to create a validation set.\n\n# create training (80%), validation (5%) and test set (15%)\nrandom_seed = 0\ntrain_df = sample_df.sample(frac = 0.8, random_state = random_seed)\ntest_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n\n# add constant variable\ntrain_df['Const'] = 1\ntest_df['Const'] = 1\n \nprint(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')\n\nTrain size: 245786, test size: 61447\n\n\n\ntrain_df.to_csv('train_df.csv', index = False)\ntest_df.to_csv('test_df.csv', index = False)\n\n\n\nGLM modeling 101\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, \\(μ\\), of the distribution depends on the independent variables, X, through\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\nwhere:\n\n\\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on \\(X\\)\n\\(Xβ\\) is the linear predictor, a linear combination of unknown parameters \\(β\\)\n\\(g\\) is the link function.\n\n\n\nModel 1: Poisson distribution with log link on count\n Target Variable  = [Number_Of_Deaths]\n Input Variables  = [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\nAs the  target variable is a count measure, we will fit GLM with Poisson distribution and log link.\nThe target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to https://en.wikipedia.org/wiki/Poisson_regression\n\nmodel1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n                                       + Attained_Age + Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres1 = model1.fit()\nres1.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:   Number_Of_Deaths   No. Observations:     245786   \n\n\n  Model:                  GLM         Df Residuals:       3076911.54 \n\n\n  Model Family:         Poisson       Df Model:                 26   \n\n\n  Link Function:          log         Scale:                 1.0000  \n\n\n  Method:                IRLS         Log-Likelihood:     -7.1471e+05\n\n\n  Date:            Mon, 05 Dec 2022   Deviance:           9.8740e+05 \n\n\n  Time:                22:28:25       Pearson chi2:        3.17e+06  \n\n\n  No. Iterations:         24          Pseudo R-squ. (CS):   0.6540   \n\n\n  Covariance Type:     nonrobust                                     \n\n\n\n\n                                                               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                                                    -9.2794     0.158   -58.838  0.000    -9.589    -8.970\n\n\n  C(Observation_Year)[T.2013]                                  -0.0545     0.007    -8.190  0.000    -0.067    -0.041\n\n\n  C(Observation_Year)[T.2014]                                  -0.0051     0.006    -0.789  0.430    -0.018     0.008\n\n\n  C(Observation_Year)[T.2015]                                  -0.1405     0.007   -20.705  0.000    -0.154    -0.127\n\n\n  C(Observation_Year)[T.2016]                                  -0.0813     0.007   -12.377  0.000    -0.094    -0.068\n\n\n  C(Gender)[T.Male]                                             0.3527     0.005    74.784  0.000     0.343     0.362\n\n\n  C(Smoker_Status)[T.Smoker]                                    1.0350     0.015    67.166  0.000     1.005     1.065\n\n\n  C(Face_Amount_Band)[T.   10000-24999]                        -0.7187     0.118    -6.104  0.000    -0.949    -0.488\n\n\n  C(Face_Amount_Band)[T.   25000-49999]                        -0.7632     0.117    -6.500  0.000    -0.993    -0.533\n\n\n  C(Face_Amount_Band)[T.   50000-99999]                        -0.9776     0.117    -8.372  0.000    -1.206    -0.749\n\n\n  C(Face_Amount_Band)[T.  100000-249999]                       -1.6819     0.116   -14.452  0.000    -1.910    -1.454\n\n\n  C(Face_Amount_Band)[T.  250000-499999]                       -2.0061     0.116   -17.222  0.000    -2.234    -1.778\n\n\n  C(Face_Amount_Band)[T.  500000-999999]                       -2.0428     0.117   -17.521  0.000    -2.271    -1.814\n\n\n  C(Face_Amount_Band)[T. 1000000-2499999]                      -2.0690     0.117   -17.721  0.000    -2.298    -1.840\n\n\n  C(Face_Amount_Band)[T. 2500000-4999999]                      -2.0173     0.138   -14.656  0.000    -2.287    -1.747\n\n\n  C(Face_Amount_Band)[T. 5000000-9999999]                      -2.0177     0.229    -8.795  0.000    -2.467    -1.568\n\n\n  C(Face_Amount_Band)[T.10000000+]                            -23.7738  1.48e+04    -0.002  0.999 -2.89e+04  2.89e+04\n\n\n  C(Preferred_Class)[T.2.0]                                     0.4593     0.005    94.004  0.000     0.450     0.469\n\n\n  C(Preferred_Class)[T.3.0]                                     0.4168     0.007    60.272  0.000     0.403     0.430\n\n\n  C(Preferred_Class)[T.4.0]                                     0.5337     0.011    48.013  0.000     0.512     0.555\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.10 yr anticipated]    -0.1692     0.105    -1.607  0.108    -0.376     0.037\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.15 yr anticipated]    -0.2569     0.105    -2.438  0.015    -0.463    -0.050\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.20 yr anticipated]    -0.4042     0.105    -3.844  0.000    -0.610    -0.198\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.25 yr anticipated]     0.0217     0.106     0.205  0.838    -0.186     0.229\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.30 yr anticipated]    -0.2437     0.105    -2.314  0.021    -0.450    -0.037\n\n\n  Attained_Age                                                  0.0739     0.000   254.173  0.000     0.073     0.075\n\n\n  Duration                                                      0.0497     0.001    92.131  0.000     0.049     0.051\n\n\n\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nres1.save('res1.pkl')\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nloaded = sm.load('res1.pkl')\n\n\nfitted = loaded.model.fit()\n\n\nfitted.predict(train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nfitted.params[\"Intercept\"]\n\n-9.279412567322963\n\n\nFirst, we show the lift chart that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we’re only look at the high-level average for each decile.\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat1'] = res1.predict(exog = train_df)\ntrain_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\ntest_df['mort_hat1'] = res1.predict(exog = test_df)\ntest_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n\n# groupby and aggregate by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\ntemp\n\n# lift chart \nfig, ax = plt.subplots(figsize = (7,3))\ntemp.plot(ax = ax)\nplt.title('Actual vs predicted mortality rate by deciles')\nplt.show() \n\nSecond, we can plot the partial dependency chart between the log mortality rate and key covariates like Attained Age or Duration to see more granular comparisons between actual vs predicted.\nWe can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well.\n\ndef pdp(df, agg_field, title, predict_col = 'death_hat1'):\n    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n    plt.legend(['actual','predicted']) \n    plt.xlabel(agg_field)\n    plt.ylabel('log_mort')\n    plt.title(title)\n    plt.show()\n    \npdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\npdp(train_df, 'Duration', 'How well does the model fit the train set')\n\n\npdp(test_df, 'Attained_Age', 'How well does the model fit the test set')\npdp(test_df, 'Duration', 'How well does the model fit the test set')\n\nThird, we look at Prediction Error by taking the difference between the Number Of Deaths (actual) and Predicted Number of Deaths and then normalized by Policies Exposed. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\nagg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\nagg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Testing error')\nplt.show()\n\n\n\n\nValidation\n\n1. Goodness of Fit\n\n Pseudo R-squared \nIn linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\nFor GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n\n\\(R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}\\)\n\n\nres1.pseudo_rsquared()\n\n\n\n Deviance \nThe (total) deviance for a model M with estimates \\({\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}\\), based on a dataset y, may be constructed by its likelihood as:\n\n\\({\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}\\)\n\nHere \\(\\hat \\theta_0\\) denotes the fitted values of the parameters in the model M, while \\(\\hat \\theta_s\\) denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y.\nIn large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\nHere we divided the deviance by the residual degree of freedom and observed a ratio much smaller than 1\n\nres1.deviance/res1.df_resid\n\n\n\n Pearson Statistic and dispersion \nSimilar to deviance test, the Pearson Statistic is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit.\nAdditionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. Overdispersion means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X.\nHere we divided the pearson statistic by the residual degree of freedom and observed a value very close to 1\n\nres1.pearson_chi2/res1.df_resid\n\n\n\n\n2. Feature importance\n\nConfidence intervals and p-values \nConfidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available.\n\nres1.summary()\n\nFrom the summary, we can see that all of the features other than SOA_Anticipated_Level_Term_Period are significant as all p-values are < 5%.\nDirectionally, the coeficients for the main features like Gender, Smoking Status, Attained_Age or Duration are all aligned with our intuition and the EDA charts that we created previously:\n\nMortality rate for Male is higher than Female\nMortality rate for Smoker is higher than non-Smoker\nMortality rate is higher as age is higher\nMortality rate is higher as duration is longer\n\n\n\n\n3. Main Effects\nWe want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given.\n\ndef pdp2(df, x, hue, predict_col = 'death_hat1'):\n    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (6,3))\n    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n    plt.xlabel(x)\n    plt.ylabel('log_mort')\n    plt.title(f'Log mortality by {x} and {hue}')\n    plt.show()\n    \npdp2(train_df, 'Attained_Age', 'Gender')\npdp2(train_df, 'Duration', 'Gender')\npdp2(train_df, 'Attained_Age', 'Smoker_Status')\npdp2(train_df, 'Duration', 'Smoker_Status')\npdp2(train_df, 'Attained_Age', 'Preferred_Class')\npdp2(train_df, 'Duration', 'Preferred_Class')\n\nWe can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\nAdditionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it’s almost constant regardless of ages.\n\n\n4. Interaction Effects\nOne of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features.\nHere we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions.\n\n Model 2: Poisson distribution with log link on Death Count with interactions \n\nmodel2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat2'] = res2.predict(exog = train_df)\ntrain_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\ntest_df['mort_hat2'] = res2.predict(exog = test_df)\ntest_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n\nres2.summary()\n\n\n\n Compared to the vanilla model \nFirst, pearson and deviance are reasonable\n\nprint(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n\nprint(f'deviance/df = {res2.deviance/res2.df_resid}')\n\nCompared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features.\n\nprint(f'AIC for Model 1 - No interaction: {res1.aic}')\nprint(f'AIC for Model 2 - With interactions: {res2.aic}')\n\nSide note on definition of AIC:  A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value.\n\n\n\n5. Correlated Features\nFor GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\nThis is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let’s show it again below. We don’t see severe correlation between any two features that requires dropping one from the feature set.\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\nplt.show()\n\n\n\n\nConclusion\nIn this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model. - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. - We checked for any necessary inclusion of interactions and handling of correlated features.\nApparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics.\n\n\nAppendix\n\nModel 1 not using formula\nThis is the explicit setup where we don’t lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis.\n\n# Target Variable\nY = ['Number_Of_Deaths']\n\n# Predictors (aka Input Variables)\nX = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const'] \n\n# Our choice for Link function is the Gaussian distribution for the nature of death frequency\nmodel = sm.GLM(endog = train_df[Y], \n               exog = train_df[X], \n               family=sm.families.Poisson(sm.families.links.log()),\n               freq_weights = train_df['Policies_Exposed'],\n               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres = model.fit()\nres.summary()\n\n\n\nModel 3: Gaussian distribution with log link on mortality rate\nThis is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. Pseudo R-squared is far worse than Model 1\n\nmodel2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration', \n                 data = train_df,\n                 family=sm.families.Gaussian(link = sm.families.links.log()),\n                 freq_weights = train_df['Policies_Exposed'])\nres2 = model2.fit()\nres2.summary()\n\n\n\nModel 4: XGBoost\nIn this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.\nNote that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time.\n\nX = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\nY = ['mort']#['Number_Of_Deaths']\n\nX_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\nfor x in X_cat:\n    train_df[x] = train_df[x].astype(\"category\")\n    test_df[x] = test_df[x].astype('category')\n\n\n# create model instance\nbst = xgb.XGBRegressor(n_estimators=50, \n                   max_depth=4, \n                   learning_rate=0.5, \n                   objective='count:poisson', \n                   enable_categorical = True, \n                   tree_method = 'approx', \n                   booster = 'gbtree', \n                   verbosity = 1)\n\n# fit model\nbst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n\n# make predictions\npreds = bst.predict(test_df[X])\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat4'] = bst.predict(train_df[X])\ntrain_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\ntest_df['mort_hat4'] = bst.predict(test_df[X])\ntest_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']\n\nLift chart does not show too much of a difference from Model 1\n\n# lift chart by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n\n# groupby and aggregate\nfig, ax = plt.subplots(figsize = (7,3))\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\ntemp.plot(ax = ax)\nplt.title('Actual vs Predicted by deciles')\nplt.show()\n\nPlotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better.\n\n# partial dependence plots\npdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\npdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\npdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\npdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')\n\nLooking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates.\n\npdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\npdp2(train_df, 'Duration', 'Gender','death_hat4')\npdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\npdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')\n\n\n\nCompare Model 1, Model 2 and Model 4\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\ntrain_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\ntrain_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n\nagg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\n# plt.ylim(0,1)\n# plt.xlim(30,85)\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\ntest_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\ntest_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n\nagg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\nplt.ylabel('Error')\n# plt.ylim(0,1)\n# plt.xlim(30,85)\na = plt.title('Testing Error')\nplt.show()\n\n\nres1.save('mortality_model.pickle')"
  },
  {
    "objectID": "notebooks/lending_club_regression.html",
    "href": "notebooks/lending_club_regression.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_rows = None\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n# vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/loan_data_2007_2014_preprocessed.csv\")\n\n# targets = vm.DatasetTargets(\n#     target_column=\"loan_status\",\n#     class_labels={\n#         \"Fully Paid\": \"Fully Paid\",\n#         \"Charged Off\": \"Charged Off\",\n#     }\n# )\n\n# vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nColumns (21,49) have mixed types.Specify dtype option on import or set low_memory=False.\n\n\n\nloan_data_defaults = df[df['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n\n\nloan_data_defaults.shape\n\n(43236, 209)\n\n\n\nloan_data_defaults['mths_since_last_delinq'].fillna(0, inplace=True)\nloan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nloan_data_defaults['CCF'].describe()\n\ncount    43236.000000\nmean         0.735952\nstd          0.200742\nmin          0.000438\n25%          0.632088\n50%          0.789908\n75%          0.888543\nmax          1.000000\nName: CCF, dtype: float64\n\n\n\nplt.hist(loan_data_defaults['CCF'], bins = 100)\n\n(array([   3.,   17.,   16.,   44.,   16.,   13.,   71.,   26.,    7.,\n          63.,   67.,   17.,   60.,   90.,   23.,   55.,   82.,   42.,\n          47.,  123.,   82.,   70.,  122.,   86.,   89.,  110.,  117.,\n         111.,  122.,  120.,  135.,  141.,  154.,  146.,  160.,  175.,\n         152.,  187.,  202.,  174.,  204.,  208.,  210.,  211.,  241.,\n         264.,  281.,  224.,  308.,  267.,  287.,  296.,  340.,  274.,\n         365.,  370.,  392.,  364.,  393.,  419.,  411.,  429.,  445.,\n         497.,  481.,  478.,  569.,  568.,  599.,  618.,  727.,  691.,\n         626.,  805.,  804.,  776.,  881.,  851.,  916.,  934.,  925.,\n        1078.,  933., 1218., 1041., 1082., 1336., 1040., 1374., 1073.,\n        1406., 1287.,  952., 1414.,  795., 1320.,  578.,  949.,  343.,\n         531.]),\n array([4.3800000e-04, 1.0433620e-02, 2.0429240e-02, 3.0424860e-02,\n        4.0420480e-02, 5.0416100e-02, 6.0411720e-02, 7.0407340e-02,\n        8.0402960e-02, 9.0398580e-02, 1.0039420e-01, 1.1038982e-01,\n        1.2038544e-01, 1.3038106e-01, 1.4037668e-01, 1.5037230e-01,\n        1.6036792e-01, 1.7036354e-01, 1.8035916e-01, 1.9035478e-01,\n        2.0035040e-01, 2.1034602e-01, 2.2034164e-01, 2.3033726e-01,\n        2.4033288e-01, 2.5032850e-01, 2.6032412e-01, 2.7031974e-01,\n        2.8031536e-01, 2.9031098e-01, 3.0030660e-01, 3.1030222e-01,\n        3.2029784e-01, 3.3029346e-01, 3.4028908e-01, 3.5028470e-01,\n        3.6028032e-01, 3.7027594e-01, 3.8027156e-01, 3.9026718e-01,\n        4.0026280e-01, 4.1025842e-01, 4.2025404e-01, 4.3024966e-01,\n        4.4024528e-01, 4.5024090e-01, 4.6023652e-01, 4.7023214e-01,\n        4.8022776e-01, 4.9022338e-01, 5.0021900e-01, 5.1021462e-01,\n        5.2021024e-01, 5.3020586e-01, 5.4020148e-01, 5.5019710e-01,\n        5.6019272e-01, 5.7018834e-01, 5.8018396e-01, 5.9017958e-01,\n        6.0017520e-01, 6.1017082e-01, 6.2016644e-01, 6.3016206e-01,\n        6.4015768e-01, 6.5015330e-01, 6.6014892e-01, 6.7014454e-01,\n        6.8014016e-01, 6.9013578e-01, 7.0013140e-01, 7.1012702e-01,\n        7.2012264e-01, 7.3011826e-01, 7.4011388e-01, 7.5010950e-01,\n        7.6010512e-01, 7.7010074e-01, 7.8009636e-01, 7.9009198e-01,\n        8.0008760e-01, 8.1008322e-01, 8.2007884e-01, 8.3007446e-01,\n        8.4007008e-01, 8.5006570e-01, 8.6006132e-01, 8.7005694e-01,\n        8.8005256e-01, 8.9004818e-01, 9.0004380e-01, 9.1003942e-01,\n        9.2003504e-01, 9.3003066e-01, 9.4002628e-01, 9.5002190e-01,\n        9.6001752e-01, 9.7001314e-01, 9.8000876e-01, 9.9000438e-01,\n        1.0000000e+00]),\n <BarContainer object of 100 artists>)\n\n\n\n\n\n\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n\n\nfeatures_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\n\n\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \n\n\nead_inputs_train = ead_inputs_train[features_all]\n\n\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport numpy as np\nimport scipy.stats as stat\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1, positive=False):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ / se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self\n\n\nreg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n\n\nLinearRegression()\n\n\n\nfeature_name = ead_inputs_train.columns.values\n\n\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_ead.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table\n\n\n\n\n\n  \n    \n      \n      Feature name\n      Coefficients\n      p_values\n    \n  \n  \n    \n      0\n      Intercept\n      1.109746e+00\n      NaN\n    \n    \n      1\n      grade:A\n      -3.030033e-01\n      0.000000e+00\n    \n    \n      2\n      grade:B\n      -2.364277e-01\n      0.000000e+00\n    \n    \n      3\n      grade:C\n      -1.720232e-01\n      0.000000e+00\n    \n    \n      4\n      grade:D\n      -1.198470e-01\n      0.000000e+00\n    \n    \n      5\n      grade:E\n      -6.768713e-02\n      0.000000e+00\n    \n    \n      6\n      grade:F\n      -2.045907e-02\n      4.428795e-03\n    \n    \n      7\n      home_ownership:MORTGAGE\n      -6.343341e-03\n      2.632464e-03\n    \n    \n      8\n      home_ownership:NONE\n      -5.539064e-03\n      9.318931e-01\n    \n    \n      9\n      home_ownership:OTHER\n      -2.426052e-03\n      9.335820e-01\n    \n    \n      10\n      home_ownership:OWN\n      -1.619582e-03\n      6.366112e-01\n    \n    \n      11\n      verification_status:Not Verified\n      5.339510e-05\n      9.828295e-01\n    \n    \n      12\n      verification_status:Source Verified\n      8.967822e-03\n      7.828941e-05\n    \n    \n      13\n      purpose:car\n      7.904787e-04\n      9.330252e-01\n    \n    \n      14\n      purpose:debt_consolidation\n      1.264922e-02\n      5.898438e-07\n    \n    \n      15\n      purpose:educational\n      9.643587e-02\n      1.801025e-06\n    \n    \n      16\n      purpose:home_improvement\n      1.923044e-02\n      4.873543e-05\n    \n    \n      17\n      purpose:house\n      1.607120e-02\n      1.653651e-01\n    \n    \n      18\n      purpose:major_purchase\n      2.984917e-02\n      2.197793e-05\n    \n    \n      19\n      purpose:medical\n      3.962479e-02\n      5.238263e-06\n    \n    \n      20\n      purpose:moving\n      4.577630e-02\n      2.987383e-06\n    \n    \n      21\n      purpose:other\n      3.706744e-02\n      0.000000e+00\n    \n    \n      22\n      purpose:renewable_energy\n      7.212969e-02\n      8.889877e-03\n    \n    \n      23\n      purpose:small_business\n      5.128674e-02\n      0.000000e+00\n    \n    \n      24\n      purpose:vacation\n      1.874863e-02\n      1.152702e-01\n    \n    \n      25\n      purpose:wedding\n      4.350522e-02\n      2.032121e-04\n    \n    \n      26\n      initial_list_status:w\n      1.318126e-02\n      6.115181e-09\n    \n    \n      27\n      term_int\n      4.551882e-03\n      0.000000e+00\n    \n    \n      28\n      emp_length_int\n      -1.591478e-03\n      4.404626e-10\n    \n    \n      29\n      mths_since_issue_d\n      -4.305274e-03\n      0.000000e+00\n    \n    \n      30\n      mths_since_earliest_cr_line\n      -3.634030e-05\n      2.742071e-03\n    \n    \n      31\n      funded_amnt\n      2.212126e-06\n      7.225181e-03\n    \n    \n      32\n      int_rate\n      -1.172652e-02\n      0.000000e+00\n    \n    \n      33\n      installment\n      -6.865607e-05\n      7.429261e-03\n    \n    \n      34\n      annual_inc\n      5.021817e-09\n      8.574696e-01\n    \n    \n      35\n      dti\n      2.832769e-04\n      3.632507e-02\n    \n    \n      36\n      delinq_2yrs\n      4.833234e-04\n      6.946456e-01\n    \n    \n      37\n      inq_last_6mths\n      1.131678e-02\n      0.000000e+00\n    \n    \n      38\n      mths_since_last_delinq\n      -1.965980e-04\n      3.220434e-06\n    \n    \n      39\n      mths_since_last_record\n      -5.085639e-05\n      3.291896e-01\n    \n    \n      40\n      open_acc\n      -2.142130e-03\n      4.218847e-15\n    \n    \n      41\n      pub_rec\n      6.782062e-03\n      4.252750e-02\n    \n    \n      42\n      total_acc\n      4.518110e-04\n      1.902931e-04\n    \n    \n      43\n      acc_now_delinq\n      9.974801e-03\n      5.012787e-01\n    \n    \n      44\n      total_rev_hi_lim\n      2.166527e-07\n      8.196014e-05\n    \n  \n\n\n\n\n\nead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\n\n\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\ny_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\nead_targets_test_temp = ead_targets_test\n\n\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.000000\n      0.530654\n    \n    \n      0\n      0.530654\n      1.000000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.736013\n    \n    \n      std\n      0.105194\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.161088\n    \n  \n\n\n\n\n\ny_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735992\n    \n    \n      std\n      0.105127\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.000000\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead)\n\n0.0291749760949319\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead)\n\n0.2822776667644732\n\n\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.set_params(\n    booster='gblinear',\n    eval_metric=mean_squared_error,\n)\n\nXGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, predictor=None, random_state=None,\n             reg_alpha=None, reg_lambda=None, ...)\n\n\n\nxgb_model.fit(ead_inputs_train, ead_targets_train, eval_set=[(ead_inputs_train, ead_targets_train), (ead_inputs_test, ead_targets_test)],)\n\n[0] validation_0-rmse:0.18787   validation_0-mean_squared_error:0.03529 validation_1-rmse:0.18834   validation_1-mean_squared_error:0.03547\n[1] validation_0-rmse:0.18491   validation_0-mean_squared_error:0.03419 validation_1-rmse:0.18507   validation_1-mean_squared_error:0.03425\n[2] validation_0-rmse:0.18372   validation_0-mean_squared_error:0.03375 validation_1-rmse:0.18379   validation_1-mean_squared_error:0.03378\n[3] validation_0-rmse:0.18299   validation_0-mean_squared_error:0.03348 validation_1-rmse:0.18312   validation_1-mean_squared_error:0.03353\n[4] validation_0-rmse:0.18238   validation_0-mean_squared_error:0.03326 validation_1-rmse:0.18246   validation_1-mean_squared_error:0.03329\n[5] validation_0-rmse:0.18177   validation_0-mean_squared_error:0.03304 validation_1-rmse:0.18202   validation_1-mean_squared_error:0.03313\n[6] validation_0-rmse:0.18120   validation_0-mean_squared_error:0.03283 validation_1-rmse:0.18152   validation_1-mean_squared_error:0.03295\n[7] validation_0-rmse:0.18075   validation_0-mean_squared_error:0.03267 validation_1-rmse:0.18116   validation_1-mean_squared_error:0.03282\n[8] validation_0-rmse:0.18032   validation_0-mean_squared_error:0.03251 validation_1-rmse:0.18073   validation_1-mean_squared_error:0.03266\n[9] validation_0-rmse:0.17989   validation_0-mean_squared_error:0.03236 validation_1-rmse:0.18038   validation_1-mean_squared_error:0.03254\n[10]    validation_0-rmse:0.17948   validation_0-mean_squared_error:0.03221 validation_1-rmse:0.17992   validation_1-mean_squared_error:0.03237\n[11]    validation_0-rmse:0.17909   validation_0-mean_squared_error:0.03207 validation_1-rmse:0.17954   validation_1-mean_squared_error:0.03224\n[12]    validation_0-rmse:0.17873   validation_0-mean_squared_error:0.03195 validation_1-rmse:0.17920   validation_1-mean_squared_error:0.03211\n[13]    validation_0-rmse:0.17841   validation_0-mean_squared_error:0.03183 validation_1-rmse:0.17896   validation_1-mean_squared_error:0.03202\n[14]    validation_0-rmse:0.17809   validation_0-mean_squared_error:0.03172 validation_1-rmse:0.17858   validation_1-mean_squared_error:0.03189\n[15]    validation_0-rmse:0.17779   validation_0-mean_squared_error:0.03161 validation_1-rmse:0.17825   validation_1-mean_squared_error:0.03177\n[16]    validation_0-rmse:0.17751   validation_0-mean_squared_error:0.03151 validation_1-rmse:0.17789   validation_1-mean_squared_error:0.03164\n[17]    validation_0-rmse:0.17717   validation_0-mean_squared_error:0.03139 validation_1-rmse:0.17761   validation_1-mean_squared_error:0.03155\n[18]    validation_0-rmse:0.17689   validation_0-mean_squared_error:0.03129 validation_1-rmse:0.17730   validation_1-mean_squared_error:0.03144\n[19]    validation_0-rmse:0.17664   validation_0-mean_squared_error:0.03120 validation_1-rmse:0.17706   validation_1-mean_squared_error:0.03135\n[20]    validation_0-rmse:0.17641   validation_0-mean_squared_error:0.03112 validation_1-rmse:0.17678   validation_1-mean_squared_error:0.03125\n[21]    validation_0-rmse:0.17619   validation_0-mean_squared_error:0.03104 validation_1-rmse:0.17656   validation_1-mean_squared_error:0.03117\n[22]    validation_0-rmse:0.17598   validation_0-mean_squared_error:0.03097 validation_1-rmse:0.17634   validation_1-mean_squared_error:0.03110\n[23]    validation_0-rmse:0.17580   validation_0-mean_squared_error:0.03090 validation_1-rmse:0.17614   validation_1-mean_squared_error:0.03102\n[24]    validation_0-rmse:0.17560   validation_0-mean_squared_error:0.03083 validation_1-rmse:0.17592   validation_1-mean_squared_error:0.03095\n[25]    validation_0-rmse:0.17542   validation_0-mean_squared_error:0.03077 validation_1-rmse:0.17574   validation_1-mean_squared_error:0.03088\n[26]    validation_0-rmse:0.17525   validation_0-mean_squared_error:0.03071 validation_1-rmse:0.17553   validation_1-mean_squared_error:0.03081\n[27]    validation_0-rmse:0.17507   validation_0-mean_squared_error:0.03065 validation_1-rmse:0.17533   validation_1-mean_squared_error:0.03074\n[28]    validation_0-rmse:0.17491   validation_0-mean_squared_error:0.03060 validation_1-rmse:0.17515   validation_1-mean_squared_error:0.03068\n[29]    validation_0-rmse:0.17481   validation_0-mean_squared_error:0.03056 validation_1-rmse:0.17505   validation_1-mean_squared_error:0.03064\n[30]    validation_0-rmse:0.17469   validation_0-mean_squared_error:0.03052 validation_1-rmse:0.17487   validation_1-mean_squared_error:0.03058\n[31]    validation_0-rmse:0.17455   validation_0-mean_squared_error:0.03047 validation_1-rmse:0.17470   validation_1-mean_squared_error:0.03052\n[32]    validation_0-rmse:0.17443   validation_0-mean_squared_error:0.03043 validation_1-rmse:0.17451   validation_1-mean_squared_error:0.03045\n[33]    validation_0-rmse:0.17431   validation_0-mean_squared_error:0.03038 validation_1-rmse:0.17443   validation_1-mean_squared_error:0.03043\n[34]    validation_0-rmse:0.17420   validation_0-mean_squared_error:0.03035 validation_1-rmse:0.17426   validation_1-mean_squared_error:0.03037\n[35]    validation_0-rmse:0.17412   validation_0-mean_squared_error:0.03032 validation_1-rmse:0.17418   validation_1-mean_squared_error:0.03034\n[36]    validation_0-rmse:0.17403   validation_0-mean_squared_error:0.03029 validation_1-rmse:0.17399   validation_1-mean_squared_error:0.03027\n[37]    validation_0-rmse:0.17393   validation_0-mean_squared_error:0.03025 validation_1-rmse:0.17388   validation_1-mean_squared_error:0.03024\n[38]    validation_0-rmse:0.17386   validation_0-mean_squared_error:0.03023 validation_1-rmse:0.17376   validation_1-mean_squared_error:0.03019\n[39]    validation_0-rmse:0.17377   validation_0-mean_squared_error:0.03020 validation_1-rmse:0.17370   validation_1-mean_squared_error:0.03017\n[40]    validation_0-rmse:0.17370   validation_0-mean_squared_error:0.03017 validation_1-rmse:0.17363   validation_1-mean_squared_error:0.03015\n[41]    validation_0-rmse:0.17363   validation_0-mean_squared_error:0.03015 validation_1-rmse:0.17358   validation_1-mean_squared_error:0.03013\n[42]    validation_0-rmse:0.17357   validation_0-mean_squared_error:0.03012 validation_1-rmse:0.17350   validation_1-mean_squared_error:0.03010\n[43]    validation_0-rmse:0.17350   validation_0-mean_squared_error:0.03010 validation_1-rmse:0.17346   validation_1-mean_squared_error:0.03009\n[44]    validation_0-rmse:0.17345   validation_0-mean_squared_error:0.03009 validation_1-rmse:0.17340   validation_1-mean_squared_error:0.03007\n[45]    validation_0-rmse:0.17339   validation_0-mean_squared_error:0.03007 validation_1-rmse:0.17334   validation_1-mean_squared_error:0.03005\n[46]    validation_0-rmse:0.17335   validation_0-mean_squared_error:0.03005 validation_1-rmse:0.17327   validation_1-mean_squared_error:0.03002\n[47]    validation_0-rmse:0.17329   validation_0-mean_squared_error:0.03003 validation_1-rmse:0.17320   validation_1-mean_squared_error:0.03000\n[48]    validation_0-rmse:0.17325   validation_0-mean_squared_error:0.03001 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[49]    validation_0-rmse:0.17321   validation_0-mean_squared_error:0.03000 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[50]    validation_0-rmse:0.17316   validation_0-mean_squared_error:0.02998 validation_1-rmse:0.17304   validation_1-mean_squared_error:0.02994\n[51]    validation_0-rmse:0.17312   validation_0-mean_squared_error:0.02997 validation_1-rmse:0.17302   validation_1-mean_squared_error:0.02994\n[52]    validation_0-rmse:0.17309   validation_0-mean_squared_error:0.02996 validation_1-rmse:0.17299   validation_1-mean_squared_error:0.02993\n[53]    validation_0-rmse:0.17305   validation_0-mean_squared_error:0.02995 validation_1-rmse:0.17293   validation_1-mean_squared_error:0.02991\n[54]    validation_0-rmse:0.17301   validation_0-mean_squared_error:0.02993 validation_1-rmse:0.17290   validation_1-mean_squared_error:0.02989\n[55]    validation_0-rmse:0.17298   validation_0-mean_squared_error:0.02992 validation_1-rmse:0.17285   validation_1-mean_squared_error:0.02988\n[56]    validation_0-rmse:0.17296   validation_0-mean_squared_error:0.02991 validation_1-rmse:0.17278   validation_1-mean_squared_error:0.02985\n[57]    validation_0-rmse:0.17292   validation_0-mean_squared_error:0.02990 validation_1-rmse:0.17276   validation_1-mean_squared_error:0.02985\n[58]    validation_0-rmse:0.17289   validation_0-mean_squared_error:0.02989 validation_1-rmse:0.17273   validation_1-mean_squared_error:0.02984\n[59]    validation_0-rmse:0.17287   validation_0-mean_squared_error:0.02988 validation_1-rmse:0.17269   validation_1-mean_squared_error:0.02982\n[60]    validation_0-rmse:0.17284   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17267   validation_1-mean_squared_error:0.02982\n[61]    validation_0-rmse:0.17282   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17264   validation_1-mean_squared_error:0.02981\n[62]    validation_0-rmse:0.17280   validation_0-mean_squared_error:0.02986 validation_1-rmse:0.17262   validation_1-mean_squared_error:0.02980\n[63]    validation_0-rmse:0.17278   validation_0-mean_squared_error:0.02985 validation_1-rmse:0.17257   validation_1-mean_squared_error:0.02978\n[64]    validation_0-rmse:0.17275   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[65]    validation_0-rmse:0.17274   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[66]    validation_0-rmse:0.17272   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17254   validation_1-mean_squared_error:0.02977\n[67]    validation_0-rmse:0.17270   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17249   validation_1-mean_squared_error:0.02975\n[68]    validation_0-rmse:0.17268   validation_0-mean_squared_error:0.02982 validation_1-rmse:0.17248   validation_1-mean_squared_error:0.02975\n[69]    validation_0-rmse:0.17267   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17246   validation_1-mean_squared_error:0.02974\n[70]    validation_0-rmse:0.17265   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17244   validation_1-mean_squared_error:0.02974\n[71]    validation_0-rmse:0.17264   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17240   validation_1-mean_squared_error:0.02972\n[72]    validation_0-rmse:0.17262   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17239   validation_1-mean_squared_error:0.02972\n[73]    validation_0-rmse:0.17261   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17237   validation_1-mean_squared_error:0.02971\n[74]    validation_0-rmse:0.17259   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17236   validation_1-mean_squared_error:0.02971\n[75]    validation_0-rmse:0.17258   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17234   validation_1-mean_squared_error:0.02970\n[76]    validation_0-rmse:0.17257   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17232   validation_1-mean_squared_error:0.02969\n[77]    validation_0-rmse:0.17256   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17230   validation_1-mean_squared_error:0.02969\n[78]    validation_0-rmse:0.17255   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17228   validation_1-mean_squared_error:0.02968\n[79]    validation_0-rmse:0.17254   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17226   validation_1-mean_squared_error:0.02967\n[80]    validation_0-rmse:0.17253   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17224   validation_1-mean_squared_error:0.02967\n[81]    validation_0-rmse:0.17252   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17223   validation_1-mean_squared_error:0.02966\n[82]    validation_0-rmse:0.17251   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17222   validation_1-mean_squared_error:0.02966\n[83]    validation_0-rmse:0.17250   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[84]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[85]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[86]    validation_0-rmse:0.17248   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[87]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17218   validation_1-mean_squared_error:0.02965\n[88]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[89]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[90]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[91]    validation_0-rmse:0.17245   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[92]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17215   validation_1-mean_squared_error:0.02964\n[93]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17214   validation_1-mean_squared_error:0.02963\n[94]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17213   validation_1-mean_squared_error:0.02963\n[95]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[96]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17212   validation_1-mean_squared_error:0.02962\n[97]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[98]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17210   validation_1-mean_squared_error:0.02962\n[99]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02972 validation_1-rmse:0.17209   validation_1-mean_squared_error:0.02962\n\n\nXGBRegressor(base_score=0.5, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=-1, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.5, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=0,\n             num_parallel_tree=None, predictor=None, random_state=0,\n             reg_alpha=0, reg_lambda=0, ...)\n\n\n\ny_hat_test_ead_xgb = xgb_model.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead_xgb)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.00000\n      0.52144\n    \n    \n      0\n      0.52144\n      1.00000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead_xgb)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead_xgb).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735745\n    \n    \n      std\n      0.101577\n    \n    \n      min\n      0.408254\n    \n    \n      25%\n      0.664853\n    \n    \n      50%\n      0.728506\n    \n    \n      75%\n      0.811329\n    \n    \n      max\n      1.310113\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead_xgb)\n\n0.029612655435575855\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead_xgb)\n\n0.2715104861315287"
  },
  {
    "objectID": "notebooks/explore_x_train_lc.html",
    "href": "notebooks/explore_x_train_lc.html",
    "title": "ValidMind",
    "section": "",
    "text": "Explore x to train LC\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# PD Model\nvm.init(project=\"cl1jyvh2c000909lg1rk0a0zb\")\n\nTrue\n\n\n\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport scipy\n\n\ndef jeffreys_test(p: float, n: int = 0, d: int = 0) -> float:\n    \"\"\"\n    Perform a test that the test probability, p, is consistent with the observed number of \n    successes, d, from a number of trials, n.\n\n    This uses the Jeffrey's posterior probability, which is the Beta distribution with shape\n    parameters a = d + 1/2 and b = n - d + 1/2. The result is the one sided p-value representing the \n    probability that the test probability, p, is greater than the true probability.\n\n    :param p: the test probability to be compared to the number of successes given n trials\n    :param n: the number of trials\n    :param d: the number of successes [optional, default = 0]\n\n    :return p-value: one sided p-value of the test\n    \"\"\"\n    return scipy.stats.beta.cdf(p, d + 0.5, n - d + 0.5)\n\n\ndef update_result(s, d, n, dr, p, pval, out = 'Yet to decide'):\n    return ({'Segment': s,\n            'Defaults': d,\n            'Observations': n,\n            'Default Rate': dr,\n            'Calibrated PD': p,\n            'P-value': pval, \n            'Outcome': out})\n\n\ndef calculate_and_return(df = pd.DataFrame, cal_pd = {}, pool = None, obs = 'observed', threshold = 0.9):\n    \"\"\"\n    Take the input dataframe, analyse & clean, seprate poolwise.\n    Calculate the jeffreys statistic\n    \"\"\"\n    \n    result = pd.DataFrame(columns = ['Segment', 'Defaults', 'Observations', 'Default Rate', 'Calibrated PD', 'P-value', 'Outcome'])\n    \n    n = len(df[obs])\n    d = np.sum(df[obs])\n    dr = np.round(d/n,2)\n    p = cal_pd['Model']\n    pval = np.round(jeffreys_test(p, n, d),4)\n    if pval>=threshold:\n        out = 'Satisfactory'\n    else:\n        out = 'Not Satisfactory'\n    \n    result = result.append(update_result('Model', d, n, dr, p, pval, out), ignore_index = True)\n    \n    if pool != None:\n        samples = df.groupby(pool)\n        \n        for sample in samples:\n            n = len(sample[1][obs])\n            d = np.sum(sample[1][obs])\n            dr = np.round(d/n,2)\n            p = cal_pd[sample[0]]\n            pval = np.round(jeffreys_test(p, n, d),4)\n            \n            if pval>=threshold:\n                out = 'Satisfactory'\n            else:\n                out = 'Not Satisfactory'\n            \n            result = result.append(update_result(sample[0], d, n, dr, p, pval, out), ignore_index = True)\n            \n    return result\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/x_train_lc.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      28000.0\n      7.12\n      10\n      125000.0\n      15.97\n      0.0\n      26\n      725.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      11200.0\n      10.99\n      2\n      80600.0\n      15.93\n      0.0\n      15\n      670.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      2\n      14000.0\n      15.10\n      6\n      83000.0\n      18.17\n      0.0\n      13\n      660.0\n      1.0\n      76.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      12725.0\n      12.12\n      6\n      71300.0\n      29.70\n      0.0\n      13\n      675.0\n      2.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      7200.0\n      15.31\n      1\n      25000.0\n      32.98\n      0.0\n      8\n      700.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ndf[\"acc_now_delinq\"].value_counts()\n\n0.0    59802\n1.0      187\n2.0        7\n3.0        3\n5.0        1\nName: acc_now_delinq, dtype: int64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 60.0 MB\n\n\n\ntest_df = pd.read_csv(\"./notebooks/datasets/_temp/x_test_lc.csv\")\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      15500.0\n      8.90\n      10\n      100000.0\n      0.74\n      0.0\n      14\n      715.0\n      3.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      10800.0\n      11.67\n      10\n      68000.0\n      15.44\n      1.0\n      20\n      670.0\n      1.0\n      8.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      2\n      15850.0\n      15.10\n      2\n      36000.0\n      26.50\n      0.0\n      31\n      720.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      16000.0\n      15.31\n      2\n      80000.0\n      24.54\n      1.0\n      12\n      705.0\n      0.0\n      21.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      4\n      14000.0\n      12.12\n      10\n      90000.0\n      14.07\n      0.0\n      14\n      695.0\n      1.0\n      44.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ntest_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 20.0 MB\n\n\n\nmodel = joblib.load(\"./notebooks/datasets/_temp/lc_model.pickle\")\n\n\nsegments = [\n    {\n        \"name\": \"Grade\",\n        \"segments\": [\n            {\"name\": \"Grade A\", \"query\": \"grade_A == 1\"},\n            {\"name\": \"Grade B\", \"query\": \"grade_B == 1\"},\n            {\"name\": \"Grade C\", \"query\": \"grade_C == 1\"},\n            {\"name\": \"Grade D\", \"query\": \"grade_D == 1\"},\n            {\"name\": \"Grade E\", \"query\": \"grade_E == 1\"},\n            {\"name\": \"Grade F\", \"query\": \"grade_F == 1\"},\n            {\"name\": \"Grade G\", \"query\": \"grade_G == 1\"},\n        ]\n    },\n    {\n        \"name\": \"Delinquency\",\n        \"segments\": [\n            {\"name\": \"Delinquency: None\", \"query\": \"acc_now_delinq == 0\"},\n            {\"name\": \"Delinquency: 1 Account\", \"query\": \"acc_now_delinq == 1\"},\n            {\"name\": \"Delinquency: 2 Accounts\", \"query\": \"acc_now_delinq == 2\"},\n        ]\n    }\n]\n\n\ndef get_calibrated_pds(df, model, segments):\n    model_preds = model.predict_proba(df)[:, 1]\n    model_class_preds = (model_preds > 0.5).astype(int)\n\n    pds = {\"Model\": model_class_preds.sum() / len(model_class_preds)}\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            total_pds = class_pred.sum()\n            segment_pd = total_pds / len(class_pred)\n\n            pds[segment[\"name\"]] = segment_pd\n    return pds\n\n\ncalibrated_pds = get_calibrated_pds(df, model, segments)\ncalibrated_pds\n\n{'Model': 0.027933333333333334,\n 'Grade A': 0.0022715539494062983,\n 'Grade B': 0.007202947160059383,\n 'Grade C': 0.014655226404459197,\n 'Grade D': 0.043563336766220394,\n 'Grade E': 0.10736266241167085,\n 'Grade F': 0.1781818181818182,\n 'Grade G': 0.24396135265700483,\n 'Delinquency: None': 0.027791712651750778,\n 'Delinquency: 1 Account': 0.06951871657754011,\n 'Delinquency: 2 Accounts': 0.14285714285714285}\n\n\n\ndef process_observations(df, model, segments):\n    test_input = pd.DataFrame(columns = ['Segment', 'Observed'])\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            # Concat to test_input by adding all rows of class_pred and segment as a single value\n            test_input = pd.concat([test_input, pd.DataFrame({'Segment': segment[\"name\"], 'Observed': class_pred})], ignore_index=True)\n\n    return test_input\n\n\nobservations = process_observations(test_df, model, segments)\n\n\nresults = calculate_and_return(\n    observations,\n    cal_pd=calibrated_pds,\n    pool = 'Segment',\n    obs=\n    'Observed',\n    threshold = 0.85\n)\nresults\n\n\n\n\n\n  \n    \n      \n      Segment\n      Defaults\n      Observations\n      Default Rate\n      Calibrated PD\n      P-value\n      Outcome\n    \n  \n  \n    \n      0\n      Model\n      708\n      39999\n      0.02\n      0.027933\n      1.0000\n      Satisfactory\n    \n    \n      1\n      Delinquency: 1 Account\n      3\n      54\n      0.06\n      0.069519\n      0.6307\n      Not Satisfactory\n    \n    \n      2\n      Delinquency: 2 Accounts\n      0\n      6\n      0.00\n      0.142857\n      0.8352\n      Not Satisfactory\n    \n    \n      3\n      Delinquency: None\n      351\n      19939\n      0.02\n      0.027792\n      1.0000\n      Satisfactory\n    \n    \n      4\n      Grade A\n      2\n      3341\n      0.00\n      0.002272\n      0.9904\n      Satisfactory\n    \n    \n      5\n      Grade B\n      13\n      6023\n      0.00\n      0.007203\n      1.0000\n      Satisfactory\n    \n    \n      6\n      Grade C\n      24\n      5318\n      0.00\n      0.014655\n      1.0000\n      Satisfactory\n    \n    \n      7\n      Grade D\n      84\n      3133\n      0.03\n      0.043563\n      1.0000\n      Satisfactory\n    \n    \n      8\n      Grade E\n      120\n      1509\n      0.08\n      0.107363\n      0.9999\n      Satisfactory\n    \n    \n      9\n      Grade F\n      82\n      537\n      0.15\n      0.178182\n      0.9408\n      Satisfactory\n    \n    \n      10\n      Grade G\n      29\n      139\n      0.21\n      0.243961\n      0.8338\n      Not Satisfactory\n    \n  \n\n\n\n\n\n\nSend results to ValidMind\n\n# Test passed only if all values for 'Outcome' are 'Satisfactory'\npassed = results['Outcome'].all() == 'Satisfactory'\npassed\n\nFalse\n\n\n\n# Build a vm.TestResult object for each row in the results dataframe\ntest_results = []\nfor index, row in results.iterrows():\n    test_results.append(vm.TestResult(\n        passed=row['Outcome'] == 'Satisfactory',\n        values={\n            'segment': row['Segment'],\n            'defaults': row['Defaults'],\n            'observations': row['Observations'],\n            'default_rate': row['Default Rate'],\n            'calibrated_pd': row['Calibrated PD'],\n            'p_value': row['P-value']\n        }\n    ))\n\n\njeffreys_params = {\n    \"threshold\": 0.85\n}\n\njeffreys_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"jeffreys_test\",\n    params=jeffreys_params,\n    passed=passed,\n    results=test_results,\n)\n\n\nvm.log_test_results([\n    jeffreys_test_result\n])\n\nSuccessfully logged test results for test: jeffreys_test\n\n\nTrue"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-demo.html",
    "href": "notebooks/r_demo/r-ecm-demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "We want to be able to load R models using Python and describe, test and evaluate them using the ValidMind framework like we do with Python models. This notebook demonstrates how we can load R models either from an RDS file or by building the model in R directly in the notebook with the rpy2 package. Either way, we can then use the ValidMind framework to run a TestPlan designed for the model (in this case, a simple ECM model).\n\n# lets import the required libraries\nimport os\nimport tempfile\n\nimport pandas as pd\nimport rpy2.robjects as robjects\nfrom IPython.display import display_png\nfrom PIL import Image as PILImage\nfrom rpy2.robjects.packages import importr\n\n# import the R packages\ntidyverse = importr(\"tidyverse\")\nbroom = importr(\"broom\")\ngraphics = importr(\"graphics\")\ngrdevices = importr(\"grDevices\")\n\n\n# Load the RDS model we created earlier (in r-ecm-model notebook)\n# alternatively, the model could be recreated in rpy2 from scratch\nr_model = robjects.r[\"readRDS\"](\"r-ecm-model.rds\")\n\n\n# lets run summary on the model\n# in pure R, this would be: `summary(model)`\n# for this, however we want to get a string representation of the summary\n# so we can use it in python\nsummary = robjects.r[\"summary\"](r_model)\nsummary_str = str(summary)\nprint(summary_str)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\n\n\n# now lets something similar to run tidy, augment, and glance\n# in pure R, this would be: `tidy(model)`, `augment(model)`, `glance(model)`\n# however, we want to end up with a pandas dataframe containing the data in the Tibble created by these functions\ntidy = robjects.r[\"tidy\"](r_model)\ntidy_df = pd.DataFrame(robjects.conversion.rpy2py(tidy))\n\naugment = robjects.r[\"augment\"](r_model)\naugment_df = pd.DataFrame(robjects.conversion.rpy2py(augment))\n\nglance = robjects.r[\"glance\"](r_model)\nglance_df = pd.DataFrame(robjects.conversion.rpy2py(glance))\n\n# lets display the dataframes\ndisplay(tidy_df)\ndisplay(augment_df)\ndisplay(glance_df)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n    \n  \n  \n    \n      0\n      (Intercept)\n      deltaCorpProfits\n      deltaFedFundsRate\n      deltaUnempRate\n      CorpProfitsLag1\n      FedFundsRateLag1\n      UnempRateLag1\n      yLag1\n    \n    \n      1\n      -0.416021\n      0.011985\n      -0.123162\n      -1.484146\n      0.002708\n      0.065564\n      -0.053275\n      -0.033703\n    \n    \n      2\n      0.823671\n      0.002033\n      0.154749\n      0.43898\n      0.000826\n      0.049471\n      0.104092\n      0.019268\n    \n    \n      3\n      -0.505082\n      5.894868\n      -0.795883\n      -3.380892\n      3.27874\n      1.325304\n      -0.51181\n      -1.74917\n    \n    \n      4\n      0.614155\n      0.0\n      0.42721\n      0.000896\n      0.001265\n      0.186849\n      0.609448\n      0.082066\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      168\n      169\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n    \n  \n  \n    \n      0\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      ...\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n      178\n      179\n    \n    \n      1\n      -0.023333\n      0.0\n      0.126667\n      0.04\n      0.013333\n      0.063333\n      -0.05\n      -0.11\n      0.046667\n      -0.066667\n      ...\n      3.668281\n      4.628594\n      3.689168\n      2.55649\n      3.484343\n      1.330312\n      3.180822\n      2.360396\n      -3.238249\n      0.48375\n    \n    \n      2\n      4.012\n      2.183\n      4.194\n      1.068\n      3.195\n      6.352\n      11.655\n      3.034\n      1.692\n      4.836\n      ...\n      27.453\n      44.735\n      -79.877\n      86.058\n      49.698\n      15.633\n      -122.484\n      64.793\n      -58.055\n      -191.198\n    \n    \n      3\n      0.91\n      -0.723333\n      -1.21\n      0.76\n      0.44\n      0.403333\n      1.393333\n      1.28\n      2.743333\n      -0.563333\n      ...\n      -0.033333\n      0.003333\n      -0.013333\n      0.02\n      -0.003333\n      0.01\n      0.01\n      0.013333\n      0.013333\n      0.023333\n    \n    \n      4\n      0.133333\n      -0.1\n      -0.166667\n      -0.066667\n      -0.133333\n      -0.2\n      -0.433333\n      0.0\n      -0.133333\n      -0.033333\n      ...\n      -0.3\n      -0.3\n      -0.266667\n      -0.433333\n      -0.133333\n      -0.4\n      -0.133333\n      -0.166667\n      -0.3\n      -0.1\n    \n    \n      5\n      59.168\n      63.18\n      65.363\n      69.557\n      70.625\n      73.82\n      80.172\n      91.827\n      94.861\n      96.553\n      ...\n      1658.148\n      1685.601\n      1730.336\n      1650.459\n      1736.517\n      1786.215\n      1801.848\n      1679.364\n      1744.157\n      1686.102\n    \n    \n      6\n      4.563333\n      5.473333\n      4.75\n      3.54\n      4.3\n      4.74\n      5.143333\n      6.536667\n      7.816667\n      10.56\n      ...\n      0.116667\n      0.083333\n      0.086667\n      0.073333\n      0.093333\n      0.09\n      0.1\n      0.11\n      0.123333\n      0.136667\n    \n    \n      7\n      5.9\n      6.033333\n      5.933333\n      5.766667\n      5.7\n      5.566667\n      5.366667\n      4.933333\n      4.933333\n      4.8\n      ...\n      7.533333\n      7.233333\n      6.933333\n      6.666667\n      6.233333\n      6.1\n      5.7\n      5.566667\n      5.4\n      5.1\n    \n    \n      8\n      1.136667\n      1.113333\n      1.113333\n      1.24\n      1.28\n      1.293333\n      1.356667\n      1.306667\n      1.196667\n      1.243333\n      ...\n      68.969531\n      72.637812\n      77.266406\n      80.955574\n      83.512063\n      86.996406\n      88.326719\n      91.507541\n      93.867937\n      90.629688\n    \n    \n      9\n      -0.571137\n      0.018616\n      0.165415\n      -0.326461\n      -0.10769\n      0.07776\n      0.417852\n      -0.166964\n      -0.069546\n      0.416952\n      ...\n      2.133892\n      2.301005\n      0.741401\n      2.646158\n      1.939253\n      1.94912\n      -0.082572\n      1.779979\n      0.611129\n      -1.313894\n    \n    \n      10\n      0.547803\n      -0.018616\n      -0.038748\n      0.366461\n      0.121023\n      -0.014427\n      -0.467852\n      0.056964\n      0.116212\n      -0.483619\n      ...\n      1.534389\n      2.327588\n      2.947767\n      -0.089668\n      1.54509\n      -0.618807\n      3.263394\n      0.580416\n      -3.849378\n      1.797644\n    \n    \n      11\n      0.031876\n      0.026309\n      0.043736\n      0.032399\n      0.025897\n      0.024937\n      0.033407\n      0.029378\n      0.063377\n      0.031915\n      ...\n      0.035826\n      0.039586\n      0.050582\n      0.068121\n      0.053507\n      0.061846\n      0.075844\n      0.088754\n      0.087292\n      0.120518\n    \n    \n      12\n      1.715996\n      1.71653\n      1.716528\n      1.716291\n      1.716505\n      1.71653\n      1.71614\n      1.716525\n      1.716506\n      1.716114\n      ...\n      1.712317\n      1.70678\n      1.700683\n      1.716516\n      1.712178\n      1.715827\n      1.696552\n      1.715893\n      1.688317\n      1.710186\n    \n    \n      13\n      0.000436\n      0.0\n      0.000003\n      0.000198\n      0.000017\n      0.0\n      0.000334\n      0.000004\n      0.000042\n      0.00034\n      ...\n      0.003872\n      0.009922\n      0.020808\n      0.000027\n      0.006085\n      0.001148\n      0.040359\n      0.001537\n      0.066262\n      0.021487\n    \n    \n      14\n      0.325303\n      -0.011023\n      -0.023152\n      0.217675\n      0.071646\n      -0.008537\n      -0.278046\n      0.033784\n      0.070162\n      -0.287194\n      ...\n      0.913035\n      1.387735\n      1.76764\n      -0.054274\n      0.92795\n      -0.373291\n      1.983474\n      0.355264\n      -2.354259\n      1.120004\n    \n  \n\n15 rows × 178 columns\n\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      2.919125e-01\n    \n    \n      1\n      2.627560e-01\n    \n    \n      2\n      1.711475e+00\n    \n    \n      3\n      1.001190e+01\n    \n    \n      4\n      1.885342e-10\n    \n    \n      5\n      7.000000e+00\n    \n    \n      6\n      -3.441276e+02\n    \n    \n      7\n      7.062553e+02\n    \n    \n      8\n      7.348913e+02\n    \n    \n      9\n      4.979547e+02\n    \n    \n      10\n      1.700000e+02\n    \n    \n      11\n      1.780000e+02\n    \n  \n\n\n\n\n\n# finally, lets plot the model and somehow get the plots into python\n# in pure R, this would be: `plot(model)`\n# for this, however we want to get a png image of the plots\n\n# first of all, lets get a temporary file path that we can use to save the image\ntemp_file = tempfile.NamedTemporaryFile(suffix=\".png\")\n\n# now lets save the image to the temporary file using grDevices package\ngrdevices.png(temp_file.name, width=1200, height=800)\ngraphics.par(mfrow=robjects.IntVector([2, 2]))\nrobjects.r[\"plot\"](r_model) # creates 4 plots that will be combined into one image\ngrdevices.dev_off()\n\n# now we split the image into the 4 plots\nimage = PILImage.open(temp_file.name)\nwidth, height = image.size\nplot_width = width / 2\nplot_height = height / 2\nplots = [\n    image.crop((0, 0, plot_width, plot_height)),\n    image.crop((plot_width, 0, width, plot_height)),\n    image.crop((0, plot_height, plot_width, height)),\n    image.crop((plot_width, plot_height, width, height))\n]\n\n# display the plots\nfor plot in plots:\n    display_png(plot)\n\n# and finally, lets delete the temporary file\nos.remove(temp_file.name)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html",
    "href": "notebooks/r_demo/r-ecm-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "Install R with Homebrew on macOS:\nbrew install r"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "href": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "title": "ValidMind",
    "section": "Step 1: Load the Required Libraries",
    "text": "Step 1: Load the Required Libraries\nWe will start by loading the necessary libraries that we will use in this notebook. Here, we will use the ecm package to build the ECM model.\n\nlibrary(ecm)\nlibrary(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "href": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "title": "ValidMind",
    "section": "Step 2: Load the Data",
    "text": "Step 2: Load the Data\nNext, we will load the data that we will use to build the ECM model. For this example, we will use ecm to predict Wilshire 5000 index based on corporate profits, Federal Reserve funds rate, and unemployment rate\n\n# Load the data\ndata(Wilshire)\n# Use 2015-12-01 and earlier data to build models\ntrn <- Wilshire[Wilshire$date<='2015-12-01',]"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 3: Build the ECM Model",
    "text": "Step 3: Build the ECM Model\nNow, we will build the ECM model using the ecm package.\n\n# Assume all predictors are needed in the equilibrium and transient terms of ecm.\nxeq <- xtr <- trn[c('CorpProfits', 'FedFundsRate', 'UnempRate')]\nmodel <- ecm(trn$Wilshire5000, xeq, xtr, includeIntercept=TRUE)\n\nsummary(model)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\nsummary(model)$coefficients\n\n\n\nA matrix: 8 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits 0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate-1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1 0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1 0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1-0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1-0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\ntidy(model)\n\n\n\nA tibble: 8 × 5\n\n    termestimatestd.errorstatisticp.value\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    (Intercept)      -0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits  0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate   -1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1   0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1  0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1    -0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1            -0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\naugment(model)\n\n\n\nA tibble: 178 × 15\n\n    .rownamesdydeltaCorpProfitsdeltaFedFundsRatedeltaUnempRateCorpProfitsLag1FedFundsRateLag1UnempRateLag1yLag1.fitted.resid.hat.sigma.cooksd.std.resid\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    2 -0.023333333  4.012 0.91000000 0.13333333 59.168 4.5633335.9000001.1366667-0.57113659 0.5478032600.031876201.7159964.355351e-04 0.325303481\n    3  0.000000000  2.183-0.72333333-0.10000000 63.180 5.4733336.0333331.1133333 0.01861637-0.0186163730.026308501.7165304.104050e-07-0.011023359\n    4  0.126666667  4.194-1.21000000-0.16666667 65.363 4.7500005.9333331.1133333 0.16541470-0.0387480300.043736251.7165283.064464e-06-0.023152097\n    5  0.040000000  1.068 0.76000000-0.06666667 69.557 3.5400005.7666671.2400000-0.32646091 0.3664609080.032399111.7162911.983193e-04 0.217675258\n    6  0.013333333  3.195 0.44000000-0.13333333 70.625 4.3000005.7000001.2800000-0.10768956 0.1210228920.025897371.7165051.705888e-05 0.071646457\n    7  0.063333333  6.352 0.40333333-0.20000000 73.820 4.7400005.5666671.2933333 0.07776005-0.0144267170.024936681.7165302.329577e-07-0.008536516\n    8 -0.050000000 11.655 1.39333333-0.43333333 80.172 5.1433335.3666671.3566667 0.41785218-0.4678521810.033407001.7161403.339924e-04-0.278045827\n    9 -0.110000000  3.034 1.28000000 0.00000000 91.827 6.5366674.9333331.3066667-0.16696419 0.0569641920.029377801.7165254.318091e-06 0.033783635\n    10 0.046666667  1.692 2.74333333-0.13333333 94.861 7.8166674.9333331.1966667-0.06954559 0.1162122590.063376581.7165064.163627e-05 0.070161519\n    11-0.066666667  4.836-0.56333333-0.03333333 96.55310.5600004.8000001.2433333 0.41695185-0.4836185150.031915461.7161143.398987e-04-0.287194305\n    12-0.046666667  6.127-0.67333333 0.36666667101.389 9.9966674.7666671.1766667-0.16750282 0.1208361570.028032821.7165051.848956e-05 0.071614448\n    13-0.110000000  2.535 1.92666667 0.06666667107.516 9.3233335.1333331.1300000-0.13104919 0.0210491890.043999891.7165309.102812e-07 0.012578705\n    14-0.186666667  5.197 0.84000000 0.43333333110.05111.2500005.2000001.0200000-0.37615628 0.1894896130.039931241.7164666.638164e-05 0.112996186\n    15 0.003333333-13.096-2.74333333 0.96666667115.24812.0900005.6333330.8333333-0.89326786 0.8966011900.077122691.7150293.106433e-03 0.545326840\n    16 0.140000000-12.925-3.04333333 1.66666667102.152 9.3466676.6000000.8366667-2.16010566 2.3001056600.149084931.7057814.648641e-02 1.456915037\n    17 0.143333333  4.811-0.88333333 0.60000000 89.227 6.3033338.2666670.9766667-0.95851326 1.1018465940.045981031.7143362.617436e-03 0.659131663\n    18-0.043333333 16.609 0.74000000-0.40000000 94.038 5.4200008.8666671.1200000 0.38541881-0.4287521470.039334411.7162013.343566e-04-0.255593460\n    19 0.040000000  7.948-0.74666667-0.16666667110.647 6.1600008.4666671.0766667 0.23467281-0.1946728100.029288701.7164635.026893e-05-0.115448901\n    20 0.160000000  8.331-0.58666667-0.56666667118.595 5.4133338.3000001.1166667 0.79331290-0.6333128990.047063801.7158058.870848e-04-0.379066991\n    21 0.023333333  3.825 0.37000000-0.16666667126.926 4.8266677.7333331.2766667 0.03671404-0.0133807080.022620991.7165301.809310e-07-0.007908191\n    22 0.036666667  2.177 0.08666667 0.16666667130.751 5.1966677.5666671.3000000-0.40014906 0.4368157300.023399561.7161941.997742e-04 0.258267261\n    23 0.026666667  0.171-0.41000000 0.03333333132.928 5.2833337.7333331.3366667-0.16367336 0.1903400260.021229221.7164673.426111e-05 0.112413682\n    24-0.010000000 10.848-0.21333333-0.26666667133.099 4.8733337.7666671.3633333 0.35622331-0.3662233060.024874771.7162941.497267e-04-0.216693223\n    25 0.006666667  8.840 0.49666667-0.36666667143.947 4.6600007.5000001.3533333 0.42305580-0.4163891310.024524301.7162241.906916e-04-0.246331926\n    26 0.006666667  6.448 0.66333333-0.23333333152.787 5.1566677.1333331.3600000 0.25178222-0.2451155550.019335271.7164255.154897e-05-0.144623924\n    27-0.013333333  1.004 0.69333333-0.23333333159.235 5.8200006.9000001.3666667 0.25599483-0.2693281660.017280481.7164035.538973e-05-0.158743710\n    28-0.040000000  5.396 0.24333333-0.33333333160.239 6.5133336.6666671.3533333 0.57352801-0.6135280130.018415451.7158703.070187e-04-0.361826262\n    29 0.153333333 18.499 0.52666667-0.33333333165.635 6.7566676.3333331.3133333 0.74534680-0.5920134700.018631561.7159152.893460e-04-0.349176568\n    30 0.126666667  6.325 0.81666667 0.03333333184.134 7.2833336.0000001.4666667 0.11674323 0.0099234380.017495501.7165307.616416e-08 0.005849577\n    31-0.116666667 11.520 1.48333333-0.13333333190.459 8.1000006.0333331.5933333 0.40888139-0.5255480600.025206401.7160423.126651e-04-0.311018089\n    ⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n    150 -3.93984375 -40.796-0.146666667 0.666666671229.6192.086666675.33333351.19484-0.41969615-3.520147600.049995301.6939002.929332e-02-2.11021907\n    151-13.21515625-517.379-1.433333333 0.866666671188.8231.940000006.00000047.25500-6.29287534-6.922280910.450834881.5589003.056885e+00-5.45792159\n    152 -3.41295850 372.812-0.323333333 1.40000000 671.4440.506666676.86666734.03984 2.35246941-5.765427910.316976911.6304949.638047e-01-4.07608658\n    153  3.81120999  76.362-0.003333333 1.033333331044.2560.183333338.26666730.62689 0.33287856 3.478331430.119783231.6926747.982295e-02 2.16623560\n    154  4.38612351 152.571-0.023333333 0.333333331120.6180.180000009.30000034.43810 2.31067811 2.075445400.070595941.7085241.502320e-02 1.25787816\n    155  3.78281250 101.178-0.036666667 0.300000001273.1890.156666679.63333338.82422 1.99182565 1.790986850.063379231.7106189.889477e-03 1.08128468\n    156  1.98280482  75.820 0.013333333-0.100000001374.3670.120000009.93333342.60703 2.40347852-0.420673710.052818351.7162094.446089e-04-0.25255634\n    157  0.99476711 -13.432 0.060000000-0.200000001450.1870.133333339.83333344.58984 1.62109998-0.626332870.054187501.7158161.014071e-03-0.37629827\n    158 -1.36429067  62.316-0.006666667-0.166666671436.7550.193333339.63333345.58460 2.43239779-3.796688470.048396961.6902203.287650e-02-2.27408451\n    159  5.05359375  -4.611 0.000000000 0.033333331499.0710.186666679.46666744.22031 1.55575748 3.497836270.056815211.6940253.334564e-02 2.10441132\n    160  4.37222278-144.215-0.030000000-0.466666671494.4600.186666679.50000049.27391 0.44374298 3.928479800.090026541.6870467.160301e-02 2.40624425\n    161  0.91371224  72.900-0.063333333 0.033333331350.2450.156666679.03333353.64613 1.79302659-0.879314350.043377471.7151371.564013e-03-0.52529517\n    162 -3.86734127   7.241-0.010000000-0.066666671423.1450.093333339.06666754.55984 1.30859223-5.175933500.039141481.6677824.846909e-02-3.08523686\n    163 -0.01948413  76.687-0.010000000-0.366666671430.3860.083333339.00000050.69250 2.73901212-2.758496250.042936741.7027721.522165e-02-1.64752351\n    164  5.57569380 220.314 0.030000000-0.366666671507.0730.073333338.63333350.67302 4.68268067 0.893013130.098256501.7150064.112255e-03 0.54947240\n    165  0.03144905 -72.595 0.050000000-0.066666671727.3870.103333338.26666756.24871 1.15447874-1.123029690.061144801.7142143.733480e-03-0.67720706\n    166  2.23761905  31.842-0.010000000-0.166666671654.7920.153333338.20000056.28016 2.37120363-0.133584590.041488471.7164993.438860e-05-0.07972366\n    167  1.14496416 -22.685 0.016666667-0.233333331686.6340.143333338.03333358.51778 1.83236442-0.687400260.042991361.7156799.465378e-04-0.41056442\n    168  4.96225806  15.537-0.016666667-0.066666671663.9490.160000007.80000059.66274 1.96072774 3.001530320.035880771.7003481.484071e-02 1.78610468\n    169  4.34453125 -21.338-0.026666667-0.200000001679.4860.143333337.73333364.62500 1.59517181 2.749359440.035211341.7029731.220254e-02 1.63547900\n    170  3.66828125  27.453-0.033333333-0.300000001658.1480.116666677.53333368.96953 2.13389190 1.534389350.035826041.7123173.871937e-03 0.91303500\n    171  4.62859375  44.735 0.003333333-0.300000001685.6010.083333337.23333372.63781 2.30100541 2.327588340.039586121.7067809.922189e-03 1.38773488\n    172  3.68916752 -79.877-0.013333333-0.266666671730.3360.086666676.93333377.26641 0.74140097 2.947766550.050581811.7006832.080820e-02 1.76764015\n    173  2.55648972  86.058 0.020000000-0.433333331650.4590.073333336.66666780.95557 2.64615821-0.089668480.068120881.7165162.691596e-05-0.05427372\n    174  3.48434276  49.698-0.003333333-0.133333331736.5170.093333336.23333383.51206 1.93925279 1.545089970.053506951.7121786.084879e-03 0.92795006\n    175  1.33031250  15.633 0.010000000-0.400000001786.2150.090000006.10000086.99641 1.94911984-0.618807340.061846411.7158271.148277e-03-0.37329144\n    176  3.18082223-122.484 0.010000000-0.133333331801.8480.100000005.70000088.32672-0.08257207 3.263394300.075844431.6965524.035912e-02 1.98347420\n    177  2.36039552  64.793 0.013333333-0.166666671679.3640.110000005.56666791.50754 1.77997920 0.580416330.088753711.7158931.536609e-03 0.35526406\n    178 -3.23824901 -58.055 0.013333333-0.300000001744.1570.123333335.40000093.86794 0.61112886-3.849377870.087292381.6883176.626180e-02-2.35425908\n    179  0.48375000-191.198 0.023333333-0.100000001686.1020.136666675.10000090.62969-1.31389361 1.797643610.120518221.7101862.148698e-02 1.12000437\n\n\n\n\n\nglance(model)\n\n\n\nA tibble: 1 × 12\n\n    r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobs\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><int><int>\n\n\n    0.29191250.2627561.71147510.01191.885342e-107-344.1276706.2553734.8913497.9547170178\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(model)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 4: Save the ECM Model",
    "text": "Step 4: Save the ECM Model\nFinally, we will save the ECM model to a file so that we can use it later.\n\n# save the model to an RDS file\nsaveRDS(model, 'r-ecm-model.rds')"
  },
  {
    "objectID": "notebooks/r_demo/rpy2.html",
    "href": "notebooks/r_demo/rpy2.html",
    "title": "ValidMind",
    "section": "",
    "text": "from rpy2.robjects.packages import importr\n\n\nbase = importr('base')\n\n\nfrom rpy2.robjects.packages import importr\n\ntidyr = importr('tidyr')\nggplot2 = importr('ggplot2')\npurrr = importr('purrr')\nprintr = importr('printr')\npROC = importr('pROC') \nROCR = importr('ROCR') \ncaret = importr('caret')\ncar = importr('car')\nrpart = importr('rpart')\nrpart_plot = importr('rpart.plot')\n\n\nfrom rpy2.robjects import r\n\ndata = r['read.csv']('./datasets/bank_customer_churn.csv', stringsAsFactors = True)\n\n\nr['str'](data)\n\n'data.frame':   8000 obs. of  14 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : Factor w/ 2616 levels \"Abazu\",\"Abbie\",..: 1002 1060 1832 258 1634 485 156 1793 1032 970 ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : Factor w/ 3 levels \"France\",\"Germany\",..: 1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n<rpy2.rinterface_lib.sexp.NULLType object at 0x103d3c740> [RTYPES.NILSXP]\n\n\n\nr('''\n    knitr::kable(sapply(data, function(x) sum(is.na(x))), col.names = c(\"Missing Value Count\"))\n''')\n\n\n\n        StrVector with 10 elements.\n        \n        \n          \n          \n            \n            '|       ...\n            \n          \n            \n            '|:------...\n            \n          \n            \n            '|...    ...\n            \n          \n            \n            ...\n            \n          \n            \n            '|envir  ...\n            \n          \n            \n            '|overwri...\n            \n          \n            \n            '|       ...\n            \n          \n          \n        \n        \n        \n\n\n\nr(\"\"\"\n    # plot box plot\n    data[, names(data) %in% c('Age', 'Balance', 'CreditScore', 'EstimatedSalary')] %>%\n    gather() %>%\n    ggplot(aes(value)) +\n        facet_wrap(~ key, scales = \"free\") +\n        geom_boxplot() +\n        theme(axis.text.x = element_text(size = 7, angle=90), axis.text.y = element_text(size = 7))\n\"\"\")\n\nR[write to console]: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable\n\n\n\nRRuntimeError: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable"
  },
  {
    "objectID": "notebooks/r_demo/r-python.html",
    "href": "notebooks/r_demo/r-python.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nfrom pypmml import Model\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = Model.fromFile('./prune_dt.pmml')\n\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel.predict({\n    \"CreditScore\": 0.64,\n    \"Geography\": 0,\n    \"Gender\": 0,\n    \"Age\": 0.51936320,\n    \"Tenure\": 2,\n    \"Balance\": 0.9118043,\n    \"NumOfProducts\": 1,\n    \"HasCrCard\": 1,\n    \"IsActiveMember\": 1,\n    \"EstimatedSalary\": 0.506734893\n})\n\n\nmodel.inputNames"
  },
  {
    "objectID": "notebooks/send_custom_metrics.html",
    "href": "notebooks/send_custom_metrics.html",
    "title": "ValidMind",
    "section": "",
    "text": "Send custom metrics\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\n# Send custom metrics to the API. Depending on the metric's type, scope and key, they will\n# be displayed in the ValidMind dashboard according to the template defined by the validator\n\n# TODO: document the values allowed for type, scope and key\naccuracy_metric = vm.Metric(\n    type=\"evaluation\",\n    scope=\"test\",\n    key=\"accuracy\",\n    value=[0.666]\n)\n\n# This metric won't show up in the UI because there's no component defined for it\nmy_metric = vm.Metric(\n    type=\"evaluation\",\n    scope=\"test\",\n    key=\"my_custom_metric\",\n    value=[0.1]\n)\n\n\nvm.log_metrics([accuracy_metric, my_metric])\n\nSuccessfully logged metrics\n\n\nTrue\n\n\n\ncustom_params = {\n    \"min_percent_threshold\": 0.5\n}\n\ncustom_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"accuracy_score\",\n    params=custom_params,\n    passed=False,\n    results=[\n        vm.TestResult(\n            passed=False,\n            values={\n                \"score\": 0.15,\n                \"threshold\": custom_params[\"min_percent_threshold\"],\n            },\n        )\n    ],\n)\n\n\nvm.log_test_results([custom_test_result])\n\nSuccessfully logged test results for test: accuracy_score\n\n\nTrue"
  },
  {
    "objectID": "notebooks/test_plan_summmary.html",
    "href": "notebooks/test_plan_summmary.html",
    "title": "ValidMind",
    "section": "",
    "text": "import validmind as vm\n\nvm.init(\n    api_host = \"http://localhost:3000/api/v1/tracking\",\n    api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n    api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n    project = \"cleytvf7i0000w5oo2a9ygkmi\"\n)\n\nTrue\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n                                                                                                                                                                                                                 \n\n\n\nResults for tabular_dataset_description Test Plan:Logged the following dataset to the ValidMind platform:\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        training\n                    \n                \n            \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for tabular_data_quality Test Plan:Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Surname', passed=False, values={'n_distinct': 2616, 'p_distinct': 0.327}), TestResult(test_name=None, column='Geography', passed=True, values={'n_distinct': 3, 'p_distinct': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389458}]})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CustomerId', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Geography', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'skewness': -0.005920679739677088}), TestResult(test_name=None, column='CustomerId', passed=True, values={'skewness': 0.010032280260684402}), TestResult(test_name=None, column='CreditScore', passed=True, values={'skewness': -0.06195161237091896}), TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.0245221429799511}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.007692043774702702}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111804}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077728})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='CustomerId', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_unique': 2616, 'p_unique': 0.327}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_unique': 452, 'p_unique': 0.0565}), TestResult(test_name=None, column='Geography', passed=True, values={'n_unique': 3, 'p_unique': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.853125\n\n\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning SHAPGlobalImportance: shap:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 12/13 [00:00<00:00, 33.23it/s]ntree_limit is deprecated, use `iteration_range` or model slicing instead.\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n                                                                                                                                                                                                                 \n\n\nResults for sklearn_classifier_metrics Test Plan:Logged the following model to the ValidMind platform:\n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.879375\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.6053169734151329\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.783068783068783\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.49333333333333335\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7308974358974359\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'Gender': 1.4e-05, 'Age': 0.000816, 'Tenure': 0.000625, 'Balance': 0.000977, 'NumOfProducts': 2.6e-05, 'HasCrCard': 2e-06, 'IsActiveMember': 6.3e-05, 'EstimatedSalary': 0.000863, 'Geography_France': 0.00021, 'Geography_Germany': 2.9e-05, 'Geography_Spain': 0.000131}\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial  new  percent_new       psi\nbin                                                      \n1       2607         0.543125  858        0.536  0.000088\n2        748         0.155833  253        0.158  0.000033\n3        398         0.082917  148        0.092  0.001048\n4        267         0.055625   97        0.061  0.000430\n5        177         0.036875   58        0.036  0.000011\n6        118         0.024583   48        0.030  0.001079\n7        101         0.021042   24        0.015  0.002045\n8         89         0.018542   31        0.019  0.000037\n9        109         0.022708   24        0.015  0.003197\n10       186         0.038750   59        0.037  0.000093\n                \n            \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for sklearn_classifier_validation Test Plan:Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.733125, 'threshold': 0.7})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.12678936605316973, 'threshold': 0.5})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.49089743589743584, 'threshold': 0.5})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=False, values={'test_score': 0.733125, 'train_score': 0.7170833333333333, 'degradation': -0.022370714700755467}), TestResult(test_name='precision', column=None, passed=True, values={'test_score': 0.164021164021164, 'train_score': 0.18407960199004975, 'degradation': 0.10896610896610899}), TestResult(test_name='recall', column=None, passed=True, values={'test_score': 0.10333333333333333, 'train_score': 0.11361310133060389, 'degradation': 0.09048048048048046}), TestResult(test_name='f1', column=None, passed=True, values={'test_score': 0.12678936605316973, 'train_score': 0.14050632911392405, 'degradation': 0.09762523259455776})]"
  },
  {
    "objectID": "notebooks/log_image.html",
    "href": "notebooks/log_image.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialize ValidMind SDK\nimport validmind as vm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(os.getcwd())\n\n/Users/panchicore/www/validmind/validmind-sdk\n\n\n\n\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0\n\n\n\n\n\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  },
  {
    "objectID": "guide/overview.html#benefits",
    "href": "guide/overview.html#benefits",
    "title": "ValidMind Platform Overview",
    "section": "Benefits",
    "text": "Benefits\n\nDocumentation-as-code\nValidMind enables model developers to automatically generate documentation snippets throughout the model development process using the ValidMind developer dramework. These documentation snippets can be added to the code and model artifacts as comments or other forms of annotations, and they serve as a way of capturing the design decisions, assumptions, and evaluation metrics of the model at different stages of development.\nBy using the Developer Framework, model developers are able to ensure that the documentation is created continuously throughout the development process, rather than leaving it until the end or omitting important decisions from the documentation after the fact. This approach makes it easier to keep track of evidence through the model development process, to explain the rationale behind design decisions, and to share the knowledge with the model validation team who need to be able to understand the model.\nThe framework also provides tools for automatically generating more comprehensive documentation from these snippets or annotations which are then surfaced in our SaaS platform UI for consumption by the model validation team and other users.\n\n\nCompliance by design\nPre-configured model-based templates ensure documentation is always complete Templates ensure consistent model validation & testing approach Tests can be configured to adapt to specific validation requirements for each use case .\nValidMind’s approach emphasizes the importance of compliance with regulations and requirements set by you’ model validation teams in the design and development of model use cases. To achieve this, the ValidMind Framework provides pre-configured model-based templates that are designed to ensure that the documentation is always complete. These templates include predefined sections for capturing information about the model inputs and outputs, architecture, and performance metrics, as well as the validation and testing procedures used to evaluate the model.\nUsing these templates helps to ensure a consistent approach to model validation and testing, as all models are documented using the same structure and guidelines. This makes it easier for model validators and other stakeholders to review the models and understand how they were developed and tested.\nIn addition, the tests can be configured to adapt to specific validation requirements for each use case. This means that the testing procedures can be customized to meet the needs of each customer’s existing business processes. For example, some use cases may require more rigorous testing than others, and the tests can be adjusted accordingly to ensure that the model meets the required standards.\n\n\nProactively evaluate model risks\nRun dozens of configurable automated tests to evaluate data quality, model outcomes, robustness, explainability, and more\nOur ValidMind Framework offers dozens of configurable automated tests that can be run to evaluate various aspects of model performance. These tests cover a range of factors, including data quality, model outcomes, robustness, explainability, and more.\nBy running these automated tests, developers can identify potential issues or risks with the model, such as biases in the training data or lack of robustness to changes in the input data. This information can then be used to make improvements to the model or adjust the testing procedures to ensure that the model meets the required standards.\nThe tests can also be configured to suit the specific requirements of the model and the intended use case, allowing developers to tailor the testing procedures to their needs. For example, if the model is intended to be used in a high-risk application, more rigorous testing and additional model validation may be necessary to ensure that the model is robust and reliable.\n\n\nMade for financial services and insurance industries\nDeveloped specifically to address the risks of banking & insurance model use cases and model risk regulation\nWe understand that your industries have unique requirements and challenges when it comes to model development, and the ValidMind framework has been developed specifically to address these needs.\nOne key aspect is addressing the risks associated with the use of machine learning models in banking and insurance use cases, which require a higher degree of accuracy and reliability.Developed specifically to address the risks of banking & insurance model use cases and model risk regulation By helping you reduce the risk of errors and non-compliance in production, the ValidMind Platform increases the trust that can be placed in your models and the teams that develop and validate them.\nIn addition to addressing the specific risks associated with these industries, the framework has also been designed to help you comply with relevant regulations or guidance governing model risk, such as SR 11-7: Guidance on Model Risk Management. We understand that failure to comply with such guidance can result in costly fines and reputational damage, and that compliance with these regulations is critical for financial services and insurance companies. ## Related Topics\nGet started with the ValidMind platform"
  },
  {
    "objectID": "guide/overview.html#section",
    "href": "guide/overview.html#section",
    "title": "ValidMind Platform Overview",
    "section": "",
    "text": "ValidMind seamlessly integrates into existing model development workflows without causing disruptions. The platform has an intuitive interface that allows users to view, edit, and manage their model documentation and validation reports. ValidMind’s approach emphasizes the importance of compliance with regulations and requirements set by the model validation teams in the design and development of model use cases.\nThe ValidMind Framework offers dozens of configurable automated tests that can be run to evaluate various aspects of model performance, including data quality, model outcomes, robustness, and explainability. These tests can identify potential issues or risks with the model, such as biases in the training data or lack of robustness to changes in the input data, allowing developers to make improvements to the model or adjust the testing procedures to ensure that the model meets the required standards.\nValidMind is specifically developed to address the risks associated with the use of machine learning models in banking and insurance use cases, which require a higher degree of accuracy and reliability. By helping organizations reduce the risk of errors and non-compliance in production, the ValidMind Platform increases the trust that can be placed in the models and the teams that develop and validate them. The ValidMind platform contains two major product components: the Developer Framework and the Software-as-a-Service (SaaS) platform.\n\nA cloud-based, flexible platform\n\nEasy to integrate, configure, and scale according to your requirements. ValidMind is designed to be platform-independent, seamlessly integrating into existing model development workflows without causing disruptions. The platform is also easy to use, with an intuitive interface that allows users to view, edit, and manage their model documentation and validation reports with ease.\n\nAutomated documentation and testing\n\nAdapts to your existing model development practices. It includes automated documentation and testing capabilities, making it easier for model developers to generate model documentation and identify potential areas of risks in their models.\n\nEasy-to-use and intuitive\n\nEnable feedback capture & workflows for model risk management. The platform promotes more efficient collaboration and communication between 1st and 2nd line teams, enabling model developers to easily capture and incorporate feedback from model validation teams to ensure more effective model documentation and validation activities."
  },
  {
    "objectID": "guide/overview.html#at-a-glanc",
    "href": "guide/overview.html#at-a-glanc",
    "title": "ValidMind Platform Overview",
    "section": "At a glanc",
    "text": "At a glanc\nOur solution stands out in the industry for several reasons:\n\nA cloud-based, flexible SaaS platform\n\nWe offer a platform that lets you integrate, configure, and scale your model lifecycle management workflows with ease. ValidMind seamlessly adapts to your existing model development and validation practices, and is designed to be platform-independent, allowing you to use it with your existing tools and environments.\n\n\nValidMind’s modern SaaS-based platform simplifies and improves the customer’s model risk management workflow. The platform is user-friendly and accessible, making it easy for teams to collaborate and manage their models efficiently. Additionally, the platform is customizable, allowing organizations to tailor their MRM workflows to their specific needs.\n\nAutomated documentation and testing\n\nOur platform helps you generate documentation and identify potential risks with ease. With our automated documentation and testing capabilities, you can streamline your workflow and ensure that your models are accurate, reliable, and secure.\n\n\nthe automation of model developer documentation and model validation processes is a unique feature that sets ValidMind apart from other MRM solutions. This automation streamlines the model development and validation process, reducing the risk of errors and improving the accuracy of the models. By automating these processes, ValidMind allows organizations to focus on higher-value tasks such as analyzing and interpreting the model output.\n\nEasy-to-use and intuitive\n\nThe platform facilitates efficient collaboration and communication between first and second-line teams, allowing model developers to seamlessly capture and incorporate feedback from model validation teams. This ensures more effective model documentation and validation activities, leading to better model performance and increased confidence in model outputs.\n\n\nWhether you work with machine learning models or other data-driven models, ValidMind provides a unified interface that simplifies your documentation and validation processes. You can easily view, edit, and manage your model documentation and validation reports, making it simple to track your progress and collaborate with your team.\nthe quality of the output is improved due to the rigorous validation process facilitated by ValidMind’s platform. By automating the model validation process, ValidMind reduces the likelihood of human error, and ensures that models are thoroughly checked and tested before deployment. This results in higher-quality output and a reduced risk of model failure."
  }
]