[
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This tutorial shows Model Developers on how to use and configure the Developer Framework and the MRM ValidMind Platform. The following steps will guide you to automatically document and test Time Series Forecasting models:\n\nStep 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Run Data Validation Test Suite on Raw Data\nStep 4: Preprocess Data\nStep 5: Run Data Validation Test Suite on Processed Data\nStep 6: Load Pre-Trained Models\nStep 7: Run Model Validation Test Suite on Models"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#introduction",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#introduction",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This tutorial shows Model Developers on how to use and configure the Developer Framework and the MRM ValidMind Platform. The following steps will guide you to automatically document and test Time Series Forecasting models:\n\nStep 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Run Data Validation Test Suite on Raw Data\nStep 4: Preprocess Data\nStep 5: Run Data Validation Test Suite on Processed Data\nStep 6: Load Pre-Trained Models\nStep 7: Run Model Validation Test Suite on Models"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-1-connect-notebook-to-validmind-project",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-1-connect-notebook-to-validmind-project",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 1: Connect Notebook to ValidMind Project",
    "text": "Step 1: Connect Notebook to ValidMind Project\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\nImport Libraries\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n\n\nConnect Notebook to ValidMind Project\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n  project = \"cli4fylps0000s5y6oi5z06xy\"\n)\n  \n\n2023-06-07 14:29:41,383 - INFO - api_client - Connected to ValidMind. Project: [3] FRED Loan Rates Model - Periodic Review (cli4fylps0000s5y6oi5z06xy)\n\n\n\n\nExplore Test Suites, Test Plans and Tests\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\nbinary_classifier_model_validation\nBinaryClassifierModelValidation\nTest suite for binary classification models.\nbinary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\ntabular_dataset_description, tabular_data_quality\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast, time_series_sensitivity\n\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n\n\nID\nName\nDescription\n\n\n\n\nbinary_classifier_metrics\nBinaryClassifierMetrics\nTest plan for sklearn classifier metrics\n\n\nbinary_classifier_validation\nBinaryClassifierPerformance\nTest plan for sklearn classifier models\n\n\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nTest plan for sklearn classifier model diagnosis tests\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\ntabular_data_quality\nTabularDataQuality\nTest plan for data quality on tabular datasets\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest plan for data quality on time series datasets\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nTest plan to perform time series univariate analysis.\n\n\ntime_series_multivariate\nTimeSeriesMultivariate\nTest plan to perform time series multivariate analysis.\n\n\ntime_series_forecast\nTimeSeriesForecast\nTest plan to perform time series forecast tests.\n\n\ntime_series_sensitivity\nTimeSeriesSensitivity\nTest plan to perform time series forecast tests.\n\n\nregression_model_description\nRegressionModelDescription\nTest plan for performance metric of regression model of statsmodels library\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\nvm.tests.list_tests()\n\n\n\n\n\n\nTest Type\nID\nName\nDescription\n\n\n\n\nMetric\nacf_pacf_plot\nACFandPACFPlot\nPlots ACF and PACF for a given time series dataset.\n\n\nMetric\nauto_ar\nAutoAR\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\nMetric\nauto_ma\nAutoMA\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\nMetric\nauto_seasonality\nAutoSeasonality\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\nMetric\nauto_stationarity\nAutoStationarity\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\nMetric\nclassifier_in_sample_performance\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\n\n\nMetric\nclassifier_out_of_sample_performance\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\n\n\nMetric\nconfusion_matrix\nConfusionMatrix\nConfusion Matrix\n\n\nMetric\ndataset_correlations\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\nMetric\ndataset_description\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\n\n\nMetric\ndataset_split\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\nMetric\ndescriptive_statistics\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\nMetric\nengle_granger_coint\nEngleGrangerCoint\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\nMetric\nlagged_correlation_heatmap\nLaggedCorrelationHeatmap\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\nMetric\nmodel_metadata\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\nMetric\npearson_correlation_matrix\nPearsonCorrelationMatrix\nExtracts the Pearson correlation coefficient for all pairs of numerical variables in the dataset. This metric is useful to identify highly correlated variables that can be removed from the dataset to reduce dimensionality.\n\n\nMetric\npfi\nPermutationFeatureImportance\nPermutation Feature Importance\n\n\nMetric\npsi\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\n\n\nMetric\npr_curve\nPrecisionRecallCurve\nPrecision Recall Curve\n\n\nMetric\nroc_curve\nROCCurve\nROC Curve\n\n\nMetric\nregression_forecast_plot_levels\nRegressionModelForecastPlotLevels\nThis metric creates a plot of forecast vs observed for each model in the list.\n\n\nMetric\nregression_sensitivity_plot\nRegressionModelSensitivityPlot\nThis metric performs sensitivity analysis applying shocks to one variable at a time.\n\n\nMetric\nregression_models_coefficients\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\n\n\nMetric\nregression_models_performance\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\n\n\nMetric\nrolling_stats_plot\nRollingStatsPlot\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\nMetric\nshap\nSHAPGlobalImportance\nSHAP Global Importance\n\n\nMetric\nscatter_plot\nScatterPlot\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\nMetric\nseasonal_decompose\nSeasonalDecompose\nCalculates seasonal_decompose metric for each of the dataset features\n\n\nMetric\nspread_plot\nSpreadPlot\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\nMetric\ntime_series_histogram\nTimeSeriesHistogram\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\nMetric\ntime_series_line_plot\nTimeSeriesLinePlot\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\nNone\ndataset_metadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\nThresholdTest\nclass_imbalance\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\nThresholdTest\nduplicates\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\nThresholdTest\ncardinality\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\nThresholdTest\npearson_correlation\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\nThresholdTest\naccuracy_score\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nf1_score\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nroc_auc_score\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nmissing\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\nThresholdTest\noverfit_regions\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\nThresholdTest\nrobustness\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\n\n\nThresholdTest\nskewness\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\nThresholdTest\ntime_series_frequency\nTimeSeriesFrequency\nTest that detect frequencies in the data\n\n\nThresholdTest\ntime_series_missing_values\nTimeSeriesMissingValues\nTest that the number of missing values is less than a threshold\n\n\nThresholdTest\ntime_series_outliers\nTimeSeriesOutliers\nTest that find outliers for time series data using the z-score method\n\n\nThresholdTest\nzeros\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\n\n\nThresholdTest\ntraining_test_degradation\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\nThresholdTest\nunique\nUniqueRows\nTest that the number of unique rows is greater than a threshold\n\n\nThresholdTest\nweak_spots\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-2-import-raw-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-2-import-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 2: Import Raw Data",
    "text": "Step 2: Import Raw Data\n\nImport FRED Dataset\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\ndf = demo_dataset.load_data()\ndf.tail(10)\n\n\n\n\n\n\n\n\nMORTGAGE30US\nFEDFUNDS\nGS10\nUNRATE\n\n\nDATE\n\n\n\n\n\n\n\n\n2023-03-02\n6.65\nNaN\nNaN\nNaN\n\n\n2023-03-09\n6.73\nNaN\nNaN\nNaN\n\n\n2023-03-16\n6.60\nNaN\nNaN\nNaN\n\n\n2023-03-23\n6.42\nNaN\nNaN\nNaN\n\n\n2023-03-30\n6.32\nNaN\nNaN\nNaN\n\n\n2023-04-01\nNaN\nNaN\n3.46\nNaN\n\n\n2023-04-06\n6.28\nNaN\nNaN\nNaN\n\n\n2023-04-13\n6.27\nNaN\nNaN\nNaN\n\n\n2023-04-20\n6.39\nNaN\nNaN\nNaN\n\n\n2023-04-27\n6.43\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-3-run-data-validation-test-suite-on-raw-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-3-run-data-validation-test-suite-on-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 3: Run Data Validation Test Suite on Raw Data",
    "text": "Step 3: Run Data Validation Test Suite on Raw Data\n\nExplore the Time Series Dataset Test Suite\n\nvm.test_suites.describe_suite(\"time_series_dataset\")\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\n\n\n\n\n\nExplore Test Plans\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest plan for data quality on time series datasets\ndataset\nTimeSeriesOutliers (ThresholdTest), TimeSeriesMissingValues (ThresholdTest), TimeSeriesFrequency (ThresholdTest)\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nTest plan to perform time series univariate analysis.\ndataset\nTimeSeriesLinePlot (Metric), TimeSeriesHistogram (Metric), ACFandPACFPlot (Metric), SeasonalDecompose (Metric), AutoSeasonality (Metric), AutoStationarity (Metric), RollingStatsPlot (Metric), AutoAR (Metric), AutoMA (Metric)\n\n\n\n\n\n\nConnect Raw Dataset to ValidMind Platform\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nRun Time Series Dataset Test Suite on Raw Dataset\n\nconfig={\n    \n    # TIME SERIES DATA QUALITY PARAMS\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    },\n    \n    # TIME SERIES UNIVARIATE PARAMS \n    \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n     \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive'\n    },\n     \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 2\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 2\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS \n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)\n\n\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nNo frequency could be inferred for variable 'MORTGAGE30US'. Skipping seasonal decomposition and plots for this variable.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\n\n\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-4-preprocess-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-4-preprocess-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 4: Preprocess Data",
    "text": "Step 4: Preprocess Data\n\nHandle Frequencies, Missing Values and Stationairty\n\n# Sample frequencies to Monthly\nresampled_df = df.resample(\"MS\").last()\n\n# Remove all missing values\nnona_df = resampled_df.dropna()\n\n# Take the first different across all variables\npreprocessed_df = nona_df.diff().dropna()"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-5-run-data-validation-test-suite-on-processed-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-5-run-data-validation-test-suite-on-processed-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 5: Run Data Validation Test Suite on Processed Data",
    "text": "Step 5: Run Data Validation Test Suite on Processed Data\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-6-load-pre-trained-models",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-6-load-pre-trained-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 6: Load Pre-Trained Models",
    "text": "Step 6: Load Pre-Trained Models\n\nLoad Pre-Trained Models\n\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\n\n\nConnect a List of Models To the ValidMind Platform\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n\nlist_of_models = [vm_model_A, vm_model_B]"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-7-run-model-validation-test-suite-on-models",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-7-run-model-validation-test-suite-on-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 7: Run Model Validation Test Suite on Models",
    "text": "Step 7: Run Model Validation Test Suite on Models\n\nExplore the Time Series Model Validation Test Suite\n\nvm.test_suites.describe_test_suite(\"time_series_model_validation\")\n\n\n\nExplore Test Plans\n\nvm.test_plans.describe_plan(\"regression_model_description\")\n\n\nvm.test_plans.describe_plan(\"regression_models_evaluation\")\n\n\nvm.test_plans.describe_plan(\"time_series_forecast\")\n\n\nvm.test_plans.describe_plan(\"time_series_sensitivity\")\n\n\n\nRun Model Validation Test Suite on a List of Models\n\nconfig= {\n    \"regression_forecast_plot_levels\": {\n        \"transformation\": \"integrate\",\n    },\n    \"regression_sensitivity_plot\": {\n        \"transformation\": \"integrate\",\n        \"shocks\": [0.3],\n    }\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model = vm_model_B,\n    models = list_of_models,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html",
    "href": "notebooks/how_to/implementing_custom_tests.html",
    "title": "Implementing Custom Tests",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv .env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"...\"\n)"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#implementing-custom-metrics-and-threshold-tests",
    "href": "notebooks/how_to/implementing_custom_tests.html#implementing-custom-metrics-and-threshold-tests",
    "title": "Implementing Custom Tests",
    "section": "Implementing custom metrics and threshold tests",
    "text": "Implementing custom metrics and threshold tests\nCustom metrics allow you to extend the default set of metrics provided by ValidMind and provide full flexibility for documenting any type of model or use case. Metrics and threshold tests are similar in that they both provide a way to evaluate a model. The difference is that metrics capture any arbitrary set of values that measure a behavior in a dataset(s) or model(s), while threshold tests are Boolean tests that evaluate whether a behavior passes or fails a set of criteria.\n\nDocumentation components of a metric and threshold test\nA metric is composed of the following documentation elements:\n\nTitle\nDescription\nResults Table(s)\nPlot(s)\n\nA threshold test is composed of the following documentation elements:\n\nTitle\nDescription\nTest Parameters\nResults Table(s)\nPlot(s)\n\n\n\nMetric class signature\nIn order to implement a custom metric or threshold test, you must create a class that inherits from the Metric or ThresholdTest class. The class signatures below show the different methods that need to be implemented in order to provide the required documentation elements:\n@dataclass\nclass ExampleMetric(Metric):\n    name = \"mean_of_values\"\n\n    # Markdown compatible description of the metric\n    def description(self):\n\n    # Code to compute the metric and cache its results and Figures\n    def run(self):\n\n    # Code to build a list of ResultSummaries that form the results tables\n    def summary(self, metric_values):\nWe’ll now implement a sample metric to illustrate their different documentation components.\n\n\nImplementing a custom metric\nThe following example shows how to implement a custom metric that calculates the mean of a list of numbers.\n\nBasic metric implementation\nAt its most basic, a metric implementation requires a run() method that computes the metric and caches its results and Figures. The run() method is called by the ValidMind client when the metric is executed. The run() should return any value that can be serialized to JSON.\nIn the example below we also provide a simple description for the metric:\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        \n        return self.cache_results(mean)\n\n\n\nTesting the custom metric\nWe should run a metric first without running an entire test plan and test its behavior.\nThe only requirement to run a metric is build a TestContext object and pass it to the metric initializer. Test context objects allow metrics and tests to access data inside their class methods in a predictable way. By default, ValidMind provides support for the following special keys in a test context objects:\n\ndataset\nmodel\nmodels\n\nWhen a test context object is build with one of these keys, the corresponding value is automatically added to the object as an attribute. For example, if you build a test context object with the dataset key, you can access the dataset inside the metric’s run() method as self.dataset. We’ll illustrate this in detail in the next section.\nIn our simple example, we don’t need to pass any arguments to the TestContext initializer.\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\nYou can also inspect the results of the metric by accessing the result variable:\n\nmean_metric.result.show()\n\n\n\n\nAdd a summary() method to the custom metric\nThe summary() method is used to build a ResultSummary object that can display the results of our test as a list of one or more summray tables. The ResultSummary class takes a results argument that is a list of ResultTable objects.\nEach ResultTable object is composed of a data and metadata attribute. The data attribute is any valid Pandas tabular DataFrame and metadata is a ResultTableMetadata instance that takes title as the table description.\n\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def summary(self, metric_value):\n        # Create a dataframe structure that can be rendered as a table\n        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n\n        return ResultSummary(\n            results=[\n                ResultTable(\n                    data=simple_df,\n                    metadata=ResultTableMetadata(title=\"Example Table\"),\n                ),                \n            ]\n        )        \n        \n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        return self.cache_results(mean)\n\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\n\nmean_metric.result.show()\n\n\n\nAdd figures to a metric\nYou can also add figures to a metric by passing a figures list to cache_results(). Each figure is a Figure object that takes the following arguments:\n\nfor_object: The name of the object that the figure is for. Usually defaults to self\nfigure: A Matplotlib or Plotly figure object\nkey: A unique key for the figure\n\nThe developer framework uses for_object and key to link figures to the corresponding metric or test.\n\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom validmind.vm_models import Figure, Metric, ResultSummary, ResultTable, ResultTableMetadata\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def summary(self, metric_value):\n        # Create a dataframe structure that can be rendered as a table\n        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n\n        return ResultSummary(\n            results=[\n                ResultTable(\n                    data=simple_df,\n                    metadata=ResultTableMetadata(title=\"Example Table\"),\n                ),\n            ]\n        )        \n\n        \n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(\n            for_object=self,\n            key=self.key,\n            figure=fig\n        )\n\n        return self.cache_results(mean, figures=[figure])\n\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\n\nmean_metric.result.show()\n\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MeanMetric]\n\nmy_custom_test_plan = MyCustomTestPlan(config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n})\nresults = my_custom_test_plan.run()"
  },
  {
    "objectID": "notebooks/how_to/listing-and-loading-tests.html",
    "href": "notebooks/how_to/listing-and-loading-tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "from validmind.tests import list_tests, load_test, describe_test\n\nlist_tests()\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAPGlobalImportance\nSHAP Global Importance\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusionMatrix\nConfusion Matrix\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nThresholdTest\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutationFeatureImportance\nPermutation Feature Importance\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecisionRecallCurve\nPrecision Recall Curve\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifierPerformance\nTest that outputs the performance of the model on the training or test data.\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROCCurve\nROC Curve\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nThresholdTest\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques.\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs\n\n\nMetric\nBoxPierce\nThe Box-Pierce test is a statistical test used to determine whether a given set of data has autocorrelations that are different from zero.\nvalidmind.model_validation.statsmodels.BoxPierce\n\n\nMetric\nRegressionModelSensitivityPlot\nThis metric performs sensitivity analysis applying shocks to one variable at a time.\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\nMetric\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\nvalidmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nMetric\nZivotAndrewsArch\nZivot-Andrews unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.ZivotAndrewsArch\n\n\nMetric\nRegressionModelOutsampleComparison\nTest that evaluates the performance of different regression models on a separate test dataset that was not used to train the models.\nvalidmind.model_validation.statsmodels.RegressionModelOutsampleComparison\n\n\nMetric\nRegressionModelForecastPlotLevels\nThis metric creates a plot of forecast vs observed for each model in the list.\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\n\n\nMetric\nLJungBox\nThe Ljung-Box test is a statistical test used to determine whether a given set of data has autocorrelations that are different from zero.\nvalidmind.model_validation.statsmodels.LJungBox\n\n\nMetric\nJarqueBera\nThe Jarque-Bera test is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nPhillipsPerronArch\nPhillips-Perron (PP) unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.PhillipsPerronArch\n\n\nMetric\nKolmogorovSmirnov\nThe Kolmogorov-Smirnov metric is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nResidualsVisualInspection\nLog plots for visual inspection of residuals\nvalidmind.model_validation.statsmodels.ResidualsVisualInspection\n\n\nMetric\nShapiroWilk\nThe Shapiro-Wilk test is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nRegressionModelInsampleComparison\nTest that output the comparison of stats library regression models.\nvalidmind.model_validation.statsmodels.RegressionModelInsampleComparison\n\n\nMetric\nRegressionModelSummary\nTest that output the summary of regression models of statsmodel library.\nvalidmind.model_validation.statsmodels.RegressionModelSummary\n\n\nMetric\nKPSS\nKwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.KPSS\n\n\nMetric\nLilliefors\nThe Lilliefors test is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nRunsTest\nThe runs test is a statistical test used to determine whether a given set of data has runs of positive and negative values that are longer than expected under the null hypothesis of randomness.\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nDFGLSArch\nDickey-Fuller GLS unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.DFGLSArch\n\n\nMetric\nAutoARIMA\nAutomatically fits multiple ARIMA models for each variable and ranks them by BIC and AIC.\nvalidmind.model_validation.statsmodels.AutoARIMA\n\n\nThresholdTest\nADFTest\nAugmented Dickey-Fuller Metric for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.ADFTest\n\n\nMetric\nRegressionModelForecastPlot\nThis metric creates a plot of forecast vs observed for each model in the list.\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlot\n\n\nMetric\nADF\nAugmented Dickey-Fuller unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.ADF\n\n\nMetric\nDurbinWatsonTest\nThe Durbin-Watson Metric is a statistical test that can be used to detect autocorrelation in a time series.\nvalidmind.model_validation.statsmodels.DurbinWatsonTest\n\n\nThresholdTest\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\nvalidmind.data_validation.Duplicates\n\n\nMetric\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatterPlot\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\nvalidmind.data_validation.ScatterPlot\n\n\nThresholdTest\nTimeSeriesOutliers\nTest that find outliers for time series data using the z-score method\nvalidmind.data_validation.TimeSeriesOutliers\n\n\nMetric\nTabularCategoricalBarPlots\nGenerates a visual analysis of categorical data by plotting bar plots. The input dataset can have multiple categorical variables if necessary. In this case, we produce a separate plot for each categorical variable.\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nAutoStationarity\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\nvalidmind.data_validation.AutoStationarity\n\n\nMetric\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nPearsonCorrelationMatrix\nExtracts the Pearson correlation coefficient for all pairs of numerical variables in the dataset. This metric is useful to identify highly correlated variables that can be removed from the dataset to reduce dimensionality.\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nTabularNumericalHistograms\nGenerates a visual analysis of numerical data by plotting the histogram. The input dataset can have multiple numerical variables if necessary. In this case, we produce a separate plot for each numerical variable.\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nThresholdTest\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\nvalidmind.data_validation.MissingValues\n\n\nMetric\nRollingStatsPlot\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\nvalidmind.data_validation.RollingStatsPlot\n\n\nMetric\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\nvalidmind.data_validation.DatasetCorrelations\n\n\nMetric\nTabularDescriptionTables\nCollects a set of descriptive statistics for a tabular dataset, for numerical, categorical and datetime variables.\nvalidmind.data_validation.TabularDescriptionTables\n\n\nMetric\nAutoMA\nAutomatically detects the MA order of a time series using both BIC and AIC.\nvalidmind.data_validation.AutoMA\n\n\nThresholdTest\nUniqueRows\nTest that the number of unique rows is greater than a threshold\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nACFandPACFPlot\nPlots ACF and PACF for a given time series dataset.\nvalidmind.data_validation.ACFandPACFPlot\n\n\nThresholdTest\nTimeSeriesFrequency\nTest that detect frequencies in the data\nvalidmind.data_validation.TimeSeriesFrequency\n\n\nMetric\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nSpreadPlot\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\nvalidmind.data_validation.SpreadPlot\n\n\nMetric\nTimeSeriesLinePlot\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\nvalidmind.data_validation.TimeSeriesLinePlot\n\n\nMetric\nAutoSeasonality\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\nvalidmind.data_validation.AutoSeasonality\n\n\nMetric\nEngleGrangerCoint\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\nvalidmind.data_validation.EngleGrangerCoint\n\n\nThresholdTest\nTimeSeriesMissingValues\nTest that the number of missing values is less than a threshold\nvalidmind.data_validation.TimeSeriesMissingValues\n\n\nDatasetMetadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\nvalidmind.data_validation.DatasetMetadata\n\n\nMetric\nTimeSeriesHistogram\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\nvalidmind.data_validation.TimeSeriesHistogram\n\n\nMetric\nLaggedCorrelationHeatmap\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\nvalidmind.data_validation.LaggedCorrelationHeatmap\n\n\nMetric\nSeasonalDecompose\nCalculates seasonal_decompose metric for each of the dataset features\nvalidmind.data_validation.SeasonalDecompose\n\n\nThresholdTest\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nAutoAR\nAutomatically detects the AR order of a time series using both BIC and AIC.\nvalidmind.data_validation.AutoAR\n\n\nMetric\nTabularDateTimeHistograms\nGenerates a visual analysis of datetime data by plotting histograms of differences between consecutive dates. The input dataset can have multiple datetime variables if necessary. In this case, we produce a separate plot for each datetime variable.\nvalidmind.data_validation.TabularDateTimeHistograms\n\n\n\n\n\n\nlist_tests(filter=\"model_validation.sklearn\")\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAPGlobalImportance\nSHAP Global Importance\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusionMatrix\nConfusion Matrix\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nThresholdTest\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutationFeatureImportance\nPermutation Feature Importance\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecisionRecallCurve\nPrecision Recall Curve\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifierPerformance\nTest that outputs the performance of the model on the training or test data.\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROCCurve\nROC Curve\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nThresholdTest\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques.\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\n\n\n\n\nlist_tests(pretty=False)\n\n['validmind.model_validation.ModelMetadata',\n 'validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance',\n 'validmind.model_validation.sklearn.RobustnessDiagnosis',\n 'validmind.model_validation.sklearn.SHAPGlobalImportance',\n 'validmind.model_validation.sklearn.ConfusionMatrix',\n 'validmind.model_validation.sklearn.ClassifierInSamplePerformance',\n 'validmind.model_validation.sklearn.OverfitDiagnosis',\n 'validmind.model_validation.sklearn.PermutationFeatureImportance',\n 'validmind.model_validation.sklearn.MinimumROCAUCScore',\n 'validmind.model_validation.sklearn.PrecisionRecallCurve',\n 'validmind.model_validation.sklearn.ClassifierPerformance',\n 'validmind.model_validation.sklearn.MinimumF1Score',\n 'validmind.model_validation.sklearn.ROCCurve',\n 'validmind.model_validation.sklearn.TrainingTestDegradation',\n 'validmind.model_validation.sklearn.WeakspotsDiagnosis',\n 'validmind.model_validation.sklearn.PopulationStabilityIndex',\n 'validmind.model_validation.sklearn.MinimumAccuracy',\n 'validmind.model_validation.statsmodels.RegressionModelsCoeffs',\n 'validmind.model_validation.statsmodels.BoxPierce',\n 'validmind.model_validation.statsmodels.RegressionModelSensitivityPlot',\n 'validmind.model_validation.statsmodels.RegressionModelsPerformance',\n 'validmind.model_validation.statsmodels.ZivotAndrewsArch',\n 'validmind.model_validation.statsmodels.RegressionModelOutsampleComparison',\n 'validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels',\n 'validmind.model_validation.statsmodels.LJungBox',\n 'validmind.model_validation.statsmodels.JarqueBera',\n 'validmind.model_validation.statsmodels.PhillipsPerronArch',\n 'validmind.model_validation.statsmodels.KolmogorovSmirnov',\n 'validmind.model_validation.statsmodels.ResidualsVisualInspection',\n 'validmind.model_validation.statsmodels.ShapiroWilk',\n 'validmind.model_validation.statsmodels.RegressionModelInsampleComparison',\n 'validmind.model_validation.statsmodels.RegressionModelSummary',\n 'validmind.model_validation.statsmodels.KPSS',\n 'validmind.model_validation.statsmodels.Lilliefors',\n 'validmind.model_validation.statsmodels.RunsTest',\n 'validmind.model_validation.statsmodels.DFGLSArch',\n 'validmind.model_validation.statsmodels.AutoARIMA',\n 'validmind.model_validation.statsmodels.ADFTest',\n 'validmind.model_validation.statsmodels.RegressionModelForecastPlot',\n 'validmind.model_validation.statsmodels.ADF',\n 'validmind.model_validation.statsmodels.DurbinWatsonTest',\n 'validmind.data_validation.Skewness',\n 'validmind.data_validation.Duplicates',\n 'validmind.data_validation.DatasetDescription',\n 'validmind.data_validation.ScatterPlot',\n 'validmind.data_validation.TimeSeriesOutliers',\n 'validmind.data_validation.TabularCategoricalBarPlots',\n 'validmind.data_validation.AutoStationarity',\n 'validmind.data_validation.DescriptiveStatistics',\n 'validmind.data_validation.PearsonCorrelationMatrix',\n 'validmind.data_validation.TabularNumericalHistograms',\n 'validmind.data_validation.HighCardinality',\n 'validmind.data_validation.MissingValues',\n 'validmind.data_validation.RollingStatsPlot',\n 'validmind.data_validation.DatasetCorrelations',\n 'validmind.data_validation.TabularDescriptionTables',\n 'validmind.data_validation.AutoMA',\n 'validmind.data_validation.UniqueRows',\n 'validmind.data_validation.TooManyZeroValues',\n 'validmind.data_validation.HighPearsonCorrelation',\n 'validmind.data_validation.ACFandPACFPlot',\n 'validmind.data_validation.TimeSeriesFrequency',\n 'validmind.data_validation.DatasetSplit',\n 'validmind.data_validation.SpreadPlot',\n 'validmind.data_validation.TimeSeriesLinePlot',\n 'validmind.data_validation.AutoSeasonality',\n 'validmind.data_validation.EngleGrangerCoint',\n 'validmind.data_validation.TimeSeriesMissingValues',\n 'validmind.data_validation.DatasetMetadata',\n 'validmind.data_validation.TimeSeriesHistogram',\n 'validmind.data_validation.LaggedCorrelationHeatmap',\n 'validmind.data_validation.SeasonalDecompose',\n 'validmind.data_validation.ClassImbalance',\n 'validmind.data_validation.AutoAR',\n 'validmind.data_validation.TabularDateTimeHistograms']\n\n\n\ndescribe_test(\"validmind.model_validation.ModelMetadata\")\n\n\n\n\n\n\nID\nTest Type\nName\nDescription\n\n\n\n\nvalidmind.model_validation.ModelMetadata\nMetric\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\n\n\n\n\ndescribe_test(\"ModelMetadata\")\n\n\n\n\n\n\nID\nTest Type\nName\nDescription\n\n\n\n\nvalidmind.model_validation.ModelMetadata\nMetric\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\n\n\n\n\ntest = load_test(\"validmind.model_validation.ModelMetadata\")\nprint(test)\nprint(f\"Test name is: {test.name}\")\nprint(test.__doc__)\nprint(test.required_context)\n\n&lt;class 'validmind.tests.model_validation.ModelMetadata.ModelMetadata'&gt;\nTest name is: model_metadata\n\n    Custom class to collect the following metadata for a model:\n    - Model architecture\n    - Model hyperparameters\n    - Model task type\n    \n['model']\n\n\n\nload_test(\"validmind.model_validation.sklearn.ConfusionMatrix\")\n\nvalidmind.tests.model_validation.sklearn.ConfusionMatrix.ConfusionMatrix\n\n\n\nload_test(\"validmind.data_validation.ClassImbalance\")\n\nvalidmind.tests.data_validation.ClassImbalance.ClassImbalance\n\n\n\nfor test in list_tests(pretty=False, filter=\"validmind.model_validation\"):\n    print(load_test(test))\n\n&lt;class 'validmind.tests.model_validation.ModelMetadata.ModelMetadata'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.ClassifierOutOfSamplePerformance.ClassifierOutOfSamplePerformance'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.RobustnessDiagnosis.RobustnessDiagnosis'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.SHAPGlobalImportance.SHAPGlobalImportance'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.ConfusionMatrix.ConfusionMatrix'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.ClassifierInSamplePerformance.ClassifierInSamplePerformance'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.OverfitDiagnosis.OverfitDiagnosis'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.PermutationFeatureImportance.PermutationFeatureImportance'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.MinimumROCAUCScore.MinimumROCAUCScore'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.PrecisionRecallCurve.PrecisionRecallCurve'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.ClassifierPerformance.ClassifierPerformance'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.MinimumF1Score.MinimumF1Score'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.ROCCurve.ROCCurve'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.TrainingTestDegradation.TrainingTestDegradation'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.WeakspotsDiagnosis.WeakspotsDiagnosis'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.PopulationStabilityIndex.PopulationStabilityIndex'&gt;\n&lt;class 'validmind.tests.model_validation.sklearn.MinimumAccuracy.MinimumAccuracy'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs.RegressionModelsCoeffs'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.BoxPierce.BoxPierce'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelSensitivityPlot.RegressionModelSensitivityPlot'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelsPerformance.RegressionModelsPerformance'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.ZivotAndrewsArch.ZivotAndrewsArch'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelOutsampleComparison.RegressionModelOutsampleComparison'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelForecastPlotLevels.RegressionModelForecastPlotLevels'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.LJungBox.LJungBox'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.JarqueBera.JarqueBera'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.PhillipsPerronArch.PhillipsPerronArch'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.KolmogorovSmirnov.KolmogorovSmirnov'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.ResidualsVisualInspection.ResidualsVisualInspection'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.ShapiroWilk.ShapiroWilk'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelInsampleComparison.RegressionModelInsampleComparison'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelSummary.RegressionModelSummary'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.KPSS.KPSS'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.Lilliefors.Lilliefors'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RunsTest.RunsTest'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.DFGLSArch.DFGLSArch'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.AutoARIMA.AutoARIMA'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.ADFTest.ADFTest'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.RegressionModelForecastPlot.RegressionModelForecastPlot'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.ADF.ADF'&gt;\n&lt;class 'validmind.tests.model_validation.statsmodels.DurbinWatsonTest.DurbinWatsonTest'&gt;\n\n\n\nfrom validmind.tests.data_validation.ClassImbalance import ClassImbalance\nClassImbalance\n\nvalidmind.tests.data_validation.ClassImbalance.ClassImbalance\n\n\n\nfrom validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\nConfusionMatrix\n\nvalidmind.tests.model_validation.sklearn.ConfusionMatrix.ConfusionMatrix"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html",
    "href": "notebooks/how_to/run_a_test_suite.html",
    "title": "Running an Individual Test Suite",
    "section": "",
    "text": "This notebook shows how to run an individual test suite"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#initialize-validmind",
    "href": "notebooks/how_to/run_a_test_suite.html#initialize-validmind",
    "title": "Running an Individual Test Suite",
    "section": "Initialize ValidMind",
    "text": "Initialize ValidMind\n\n%load_ext dotenv\n%dotenv .env\n%matplotlib inline\n\nimport validmind as vm\nimport xgboost as xgb\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "title": "Running an Individual Test Suite",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import taiwan_credit like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "href": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "title": "Running an Individual Test Suite",
    "section": "List available test suites",
    "text": "List available test suites\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\nbinary_classifier_model_validation\nBinaryClassifierModelValidation\nTest suite for binary classification models.\nbinary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\ntabular_dataset_description, tabular_data_quality\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the Data Validation Test suite",
    "text": "Run the Data Validation Test suite\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the Model Validation Test suite",
    "text": "Run the Model Validation Test suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the Raw Dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\n\nTrain a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Import and Run the Individual Test Suite",
    "text": "Import and Run the Individual Test Suite\n\nInitialize ValidMind objects\nWe initize the objects required to run test plans using the ValidMind framework\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nRun the Binary Classification Test Suite\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)"
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html",
    "href": "notebooks/how_to/run_a_template.html",
    "title": "Run a Documentation Template",
    "section": "",
    "text": "This notebook shows how to run all tests defined in a documentation template."
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#structure-of-a-documentation-template",
    "href": "notebooks/how_to/run_a_template.html#structure-of-a-documentation-template",
    "title": "Run a Documentation Template",
    "section": "Structure of a documentation template",
    "text": "Structure of a documentation template\nAll projects created in ValidMind are based on a documentation template. A documentation template is a collection of content blocks that, when rendered, produce a document that model developers can use for model validation.\nThe template structure is a simple combination of content sections where each section can have one or more content blocks. We currently support text-based content blocks (that are populated with the ValidMind UI) and test-driven content blocks. Under the hood, a template is represented as a YAML file. For more information about a template’s structure, please refer to this page."
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#previewing-a-template",
    "href": "notebooks/how_to/run_a_template.html#previewing-a-template",
    "title": "Run a Documentation Template",
    "section": "Previewing a template",
    "text": "Previewing a template\nWe can use the preview_template() function to preview the content blocks that will be populated by the developer framework. Before we do that, let’s first initialize the ValidMind client:\n\n# Load environment variables required by the developer framework\n%load_ext dotenv\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clj6bou4u0000kg8h6lxa1807\"\n)\n\n2023-06-21 16:34:17,517 - INFO - api_client - Connected to ValidMind. Project: Customer Churn Demo - 4 (clj6bou4u0000kg8h6lxa1807)\n\n\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#running-a-template",
    "href": "notebooks/how_to/run_a_template.html#running-a-template",
    "title": "Run a Documentation Template",
    "section": "Running a template",
    "text": "Running a template\nWe can use the run_template() function to run all tests defined in a documentation template. Note that each test in the template will have specific context requirements. Each context object can be passed as a function argument. For example, if a test requires a model object, we can pass it as a function argument:\nvm.run_template(model=vm_model) # vm_model is a validmind Model instance\nLet’s run through our customer churn demo to illustrate how to run a template and pass a model and dataset context to the run_template() function.\n\n# Load a demo dataset and train a simple model\nimport xgboost as xgb\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n2023-06-21 16:34:17,845 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-06-21 16:34:17,846 - INFO - dataset - Inferring dataset types...\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n2023-06-21 16:34:18,069 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-06-21 16:34:18,072 - INFO - dataset - Inferring dataset types...\n2023-06-21 16:34:18,157 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-06-21 16:34:18,159 - INFO - dataset - Inferring dataset types...\n\n\nNow we can run this project’s documentation template by passing the required dataset and model context:\n\nmodel_suite = vm.run_template(model=vm_model, dataset=vm_dataset)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "",
    "text": "This interactive notebook will guide you through documenting a model using the ValidMind Developer framework. We will use sample datasets provided by the library and train a simple classification model.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#before-you-begin",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#before-you-begin",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Before you begin",
    "text": "Before you begin\nClick File &gt; Save a copy in Drive &gt; to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#install-the-client-library",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#install-the-client-library",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind\n\nCollecting validmind\n  Downloading validmind-1.15.5-py3-none-any.whl (6.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/6.1 MB 5.9 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.64.0\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 1.7 MB/s eta 0:00:00a 0:00:01\nCollecting pandas-profiling&lt;4.0.0,&gt;=3.6.6\n  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.4/324.4 KB 5.3 MB/s eta 0:00:0000:01\nCollecting tabulate&lt;0.9.0,&gt;=0.8.9\n  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\nCollecting catboost&lt;2.0,&gt;=1.2\n  Downloading catboost-1.2-cp310-cp310-macosx_11_0_universal2.whl (25.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 4.9 MB/s eta 0:00:0000:0100:01\nCollecting kaleido==0.2.1\n  Downloading kaleido-0.2.1-py2.py3-none-macosx_10_11_x86_64.whl (85.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.2/85.2 MB 4.7 MB/s eta 0:00:0000:0100:01\nCollecting myst-parser&lt;2.0.0,&gt;=1.0.0\n  Downloading myst_parser-1.0.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.3/77.3 KB 1.9 MB/s eta 0:00:00\nCollecting pypmml&lt;0.10.0,&gt;=0.9.17\n  Downloading pypmml-0.9.17.tar.gz (14.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/14.2 MB 6.7 MB/s eta 0:00:0000:0100:01\n  Preparing metadata (setup.py) ... done\nCollecting sphinx-rtd-theme&lt;2.0.0,&gt;=1.2.0\n  Downloading sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 9.3 MB/s eta 0:00:00:00:0100:01\nCollecting statsmodels&lt;0.14.0,&gt;=0.13.5\n  Downloading statsmodels-0.13.5-cp310-cp310-macosx_10_9_x86_64.whl (9.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/9.7 MB 6.1 MB/s eta 0:00:0000:0100:01\nCollecting ipython==7.34.0\n  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 KB 5.9 MB/s eta 0:00:0000:0100:01\nCollecting shap&lt;0.42.0,&gt;=0.41.0\n  Downloading shap-0.41.0-cp310-cp310-macosx_10_9_x86_64.whl (436 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 436.5/436.5 KB 5.8 MB/s eta 0:00:00a 0:00:01\nCollecting sphinx&lt;7.0.0,&gt;=6.1.3\n  Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 8.5 MB/s eta 0:00:0000:0100:01m\nCollecting numpy==1.22.3\n  Downloading numpy-1.22.3-cp310-cp310-macosx_10_14_x86_64.whl (17.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/17.6 MB 7.4 MB/s eta 0:00:0000:0100:01\nCollecting scikit-learn&lt;2.0.0,&gt;=1.0.2\n  Downloading scikit_learn-1.3.0-cp310-cp310-macosx_10_9_x86_64.whl (10.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 9.4 MB/s eta 0:00:00:00:01:01m\nCollecting pandas==1.5.3\n  Downloading pandas-1.5.3-cp310-cp310-macosx_10_9_x86_64.whl (12.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 7.1 MB/s eta 0:00:0000:0100:01\nCollecting xgboost&lt;2.0.0,&gt;=1.5.2\n  Downloading xgboost-1.7.6-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (1.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 8.6 MB/s eta 0:00:00:00:0100:01\nCollecting click&lt;9.0.0,&gt;=8.0.4\n  Downloading click-8.1.4-py3-none-any.whl (98 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.2/98.2 KB 2.9 MB/s eta 0:00:00\nCollecting markdown&lt;4.0.0,&gt;=3.4.3\n  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 KB 2.4 MB/s eta 0:00:00\nCollecting python-dotenv&lt;0.21.0,&gt;=0.20.0\n  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\nCollecting pydantic&lt;2.0.0,&gt;=1.9.1\n  Downloading pydantic-1.10.11-cp310-cp310-macosx_10_9_x86_64.whl (2.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 8.1 MB/s eta 0:00:0000:0100:01m\nCollecting sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5\n  Downloading sphinx_markdown_builder-0.5.5-py2.py3-none-any.whl (15 kB)\nCollecting aiohttp[speedups]&lt;4.0.0,&gt;=3.8.4\n  Downloading aiohttp-3.8.4-cp310-cp310-macosx_10_9_x86_64.whl (358 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 358.1/358.1 KB 5.2 MB/s eta 0:00:00a 0:00:01\nCollecting requests&lt;3.0.0,&gt;=2.27.1\n  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 KB 1.7 MB/s eta 0:00:00\nCollecting arch&lt;6.0.0,&gt;=5.4.0\n  Downloading arch-5.6.0-cp310-cp310-macosx_10_9_x86_64.whl (894 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 894.7/894.7 KB 5.5 MB/s eta 0:00:0000:0100:01\nCollecting nltk&lt;4.0.0,&gt;=3.8.1\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 3.6 MB/s eta 0:00:0000:0100:01m\nCollecting dython&lt;0.8.0,&gt;=0.7.1\n  Downloading dython-0.7.4-py3-none-any.whl (24 kB)\nCollecting pdoc&lt;14.0.0,&gt;=13.1.1\n  Downloading pdoc-13.1.1-py3-none-any.whl (132 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.6/132.6 KB 965.9 kB/s eta 0:00:00a 0:00:01\nCollecting plotly&lt;6.0.0,&gt;=5.14.1\n  Downloading plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.5/15.5 MB 7.5 MB/s eta 0:00:0000:0100:01\nCollecting seaborn&lt;0.12.0,&gt;=0.11.2\n  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 KB 1.4 MB/s eta 0:00:0000:01\nCollecting sentry-sdk&lt;2.0.0,&gt;=1.24.0\n  Downloading sentry_sdk-1.27.1-py2.py3-none-any.whl (211 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 KB 1.6 MB/s eta 0:00:00a 0:00:01\nCollecting ipywidgets&lt;9.0.0,&gt;=8.0.6\n  Downloading ipywidgets-8.0.7-py3-none-any.whl (138 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.3/138.3 KB 2.9 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (3.0.38)\nRequirement already satisfied: backcall in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (0.1.6)\nRequirement already satisfied: setuptools&gt;=18.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython==7.34.0-&gt;validmind) (58.1.0)\nRequirement already satisfied: pygments in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (2.15.1)\nRequirement already satisfied: pexpect&gt;4.3 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (4.8.0)\nRequirement already satisfied: decorator in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (5.1.1)\nRequirement already satisfied: appnope in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (0.1.3)\nRequirement already satisfied: pickleshare in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (0.7.5)\nRequirement already satisfied: traitlets&gt;=4.2 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (5.9.0)\nRequirement already satisfied: jedi&gt;=0.16 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipython==7.34.0-&gt;validmind) (0.18.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from pandas==1.5.3-&gt;validmind) (2.8.2)\nCollecting pytz&gt;=2020.1\n  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.3/502.3 KB 4.6 MB/s eta 0:00:0000:0100:01\nCollecting frozenlist&gt;=1.1.1\n  Downloading frozenlist-1.3.3-cp310-cp310-macosx_10_9_x86_64.whl (35 kB)\nCollecting yarl&lt;2.0,&gt;=1.0\n  Downloading yarl-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl (65 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 KB 1.7 MB/s eta 0:00:00\nCollecting multidict&lt;7.0,&gt;=4.5\n  Downloading multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\nCollecting attrs&gt;=17.3.0\n  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 KB 1.6 MB/s eta 0:00:00\nCollecting async-timeout&lt;5.0,&gt;=4.0.0a3\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting aiosignal&gt;=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting charset-normalizer&lt;4.0,&gt;=2.0\n  Downloading charset_normalizer-3.1.0-cp310-cp310-macosx_10_9_x86_64.whl (124 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.8/124.8 KB 4.1 MB/s eta 0:00:00\nCollecting Brotli\n  Downloading Brotli-1.0.9-cp310-cp310-macosx_10_9_x86_64.whl (421 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 421.8/421.8 KB 6.2 MB/s eta 0:00:00a 0:00:01\nCollecting aiodns\n  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\nCollecting property-cached&gt;=1.6.4\n  Downloading property_cached-1.6.4-py2.py3-none-any.whl (7.8 kB)\nCollecting scipy&gt;=1.3\n  Downloading scipy-1.11.1-cp310-cp310-macosx_10_9_x86_64.whl (37.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.2/37.2 MB 7.3 MB/s eta 0:00:00:00:0100:01\nCollecting graphviz\n  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 KB 1.2 MB/s eta 0:00:00\nRequirement already satisfied: six in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.16.0)\nCollecting matplotlib\n  Downloading matplotlib-3.7.2-cp310-cp310-macosx_10_12_x86_64.whl (7.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 9.1 MB/s eta 0:00:00:00:0100:01\nCollecting dython&lt;0.8.0,&gt;=0.7.1\n  Downloading dython-0.7.3-py3-none-any.whl (23 kB)\n  Downloading dython-0.7.2-py3-none-any.whl (22 kB)\nRequirement already satisfied: psutil&gt;=5.9.1 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from dython&lt;0.8.0,&gt;=0.7.1-&gt;validmind) (5.9.5)\nCollecting scikit-plot&gt;=0.3.7\n  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\nCollecting widgetsnbextension~=4.0.7\n  Downloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 8.8 MB/s eta 0:00:00:00:0100:01\nCollecting jupyterlab-widgets~=3.0.7\n  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.0/215.0 KB 4.6 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: ipykernel&gt;=4.5.1 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (6.23.1)\nCollecting pyyaml\n  Downloading PyYAML-6.0-cp310-cp310-macosx_10_9_x86_64.whl (197 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 197.6/197.6 KB 2.7 MB/s eta 0:00:0000:01\nCollecting mdit-py-plugins~=0.3.4\n  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.1/52.1 KB 1.3 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 4.2 MB/s eta 0:00:00\nCollecting docutils&lt;0.20,&gt;=0.15\n  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 570.5/570.5 KB 6.7 MB/s eta 0:00:0000:0100:01\nCollecting markdown-it-py&lt;3.0.0,&gt;=1.0.0\n  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 KB 1.8 MB/s eta 0:00:00ta 0:00:01\nCollecting regex&gt;=2021.8.3\n  Downloading regex-2023.6.3-cp310-cp310-macosx_10_9_x86_64.whl (294 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 KB 6.5 MB/s eta 0:00:00a 0:00:01\nCollecting joblib\n  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 KB 5.0 MB/s eta 0:00:0000:01\nCollecting ydata-profiling\n  Downloading ydata_profiling-4.3.1-py2.py3-none-any.whl (352 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 353.0/353.0 KB 3.5 MB/s eta 0:00:0000:0100:01\nCollecting MarkupSafe\n  Downloading MarkupSafe-2.1.3-cp310-cp310-macosx_10_9_x86_64.whl (13 kB)\nCollecting tenacity&gt;=6.2.0\n  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: packaging in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from plotly&lt;6.0.0,&gt;=5.14.1-&gt;validmind) (23.1)\nCollecting typing-extensions&gt;=4.2.0\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nCollecting py4j&gt;=0.10.7\n  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 KB 4.0 MB/s eta 0:00:00a 0:00:01\nCollecting urllib3&lt;3,&gt;=1.21.1\n  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.6/123.6 KB 4.0 MB/s eta 0:00:00\nCollecting certifi&gt;=2017.4.17\n  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.0/157.0 KB 3.1 MB/s eta 0:00:0000:01\nCollecting idna&lt;4,&gt;=2.5\n  Downloading idna-3.4-py3-none-any.whl (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 KB 2.0 MB/s eta 0:00:00\nCollecting threadpoolctl&gt;=2.0.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nCollecting numba\n  Downloading numba-0.57.1-cp310-cp310-macosx_10_9_x86_64.whl (2.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 6.4 MB/s eta 0:00:0000:0100:01m\nCollecting cloudpickle\n  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nCollecting slicer==0.0.7\n  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\nCollecting sphinxcontrib-applehelp\n  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.6/120.6 KB 703.9 kB/s eta 0:00:00a 0:00:01\nCollecting sphinxcontrib-qthelp\n  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.6/90.6 KB 1.5 MB/s eta 0:00:00ta 0:00:01\nCollecting babel&gt;=2.9\n  Downloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 8.0 MB/s eta 0:00:0000:010:01m\nCollecting imagesize&gt;=1.3\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nCollecting alabaster&lt;0.8,&gt;=0.7\n  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\nCollecting snowballstemmer&gt;=2.0\n  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.0/93.0 KB 1.4 MB/s eta 0:00:00:00:01\nCollecting sphinxcontrib-serializinghtml&gt;=1.1.5\n  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.0/94.0 KB 2.3 MB/s eta 0:00:00:00:01\nCollecting sphinxcontrib-htmlhelp&gt;=2.0.0\n  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.8/99.8 KB 2.1 MB/s eta 0:00:00ta 0:00:01\nCollecting sphinxcontrib-jsmath\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nCollecting sphinxcontrib-devhelp\n  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.7/84.7 KB 2.7 MB/s eta 0:00:00\nCollecting pydash\n  Downloading pydash-7.0.5-py3-none-any.whl (109 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109.7/109.7 KB 2.3 MB/s eta 0:00:00a 0:00:01\nCollecting html2text\n  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\nCollecting yapf\n  Downloading yapf-0.40.1-py3-none-any.whl (250 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.3/250.3 KB 3.8 MB/s eta 0:00:0000:01\nCollecting unify\n  Downloading unify-0.5.tar.gz (4.4 kB)\n  Preparing metadata (setup.py) ... done\nCollecting sphinxcontrib-jquery&lt;5,&gt;=4\n  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 KB 3.2 MB/s eta 0:00:00\nCollecting docutils&lt;0.20,&gt;=0.15\n  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 570.0/570.0 KB 2.2 MB/s eta 0:00:0000:0100:01\nCollecting patsy&gt;=0.5.2\n  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 KB 5.0 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: comm&gt;=0.1.1 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (0.1.3)\nRequirement already satisfied: pyzmq&gt;=20 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (25.1.0)\nRequirement already satisfied: debugpy&gt;=1.6.5 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (1.6.7)\nRequirement already satisfied: nest-asyncio in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (1.5.6)\nRequirement already satisfied: tornado&gt;=6.1 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (6.3.2)\nRequirement already satisfied: jupyter-client&gt;=6.1.12 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (8.2.0)\nRequirement already satisfied: jupyter-core!=5.0.*,&gt;=4.12 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&lt;9.0.0,&gt;=8.0.6-&gt;validmind) (5.3.0)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from jedi&gt;=0.16-&gt;ipython==7.34.0-&gt;validmind) (0.8.3)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting fonttools&gt;=4.22.0\n  Downloading fonttools-4.40.0-cp310-cp310-macosx_10_9_x86_64.whl (2.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 8.1 MB/s eta 0:00:00:00:0100:01\nCollecting pillow&gt;=6.2.0\n  Downloading Pillow-10.0.0-cp310-cp310-macosx_10_10_x86_64.whl (3.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 7.9 MB/s eta 0:00:00:00:0100:01\nCollecting contourpy&gt;=1.0.1\n  Downloading contourpy-1.1.0-cp310-cp310-macosx_10_9_x86_64.whl (243 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 243.6/243.6 KB 6.9 MB/s eta 0:00:00\nCollecting pyparsing&lt;3.1,&gt;=2.3.1\n  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 KB 2.3 MB/s eta 0:00:00ta 0:00:01\nCollecting cycler&gt;=0.10\n  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\nCollecting kiwisolver&gt;=1.0.1\n  Downloading kiwisolver-1.4.4-cp310-cp310-macosx_10_9_x86_64.whl (65 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 KB 308.8 kB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: ptyprocess&gt;=0.5 in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from pexpect&gt;4.3-&gt;ipython==7.34.0-&gt;validmind) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/sydneysugar/Library/Python/3.10/lib/python/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==7.34.0-&gt;validmind) (0.2.6)\nCollecting pycares&gt;=4.0.0\n  Downloading pycares-4.3.0-cp310-cp310-macosx_10_9_x86_64.whl (75 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.5/75.5 KB 1.1 MB/s eta 0:00:00a 0:00:01\n\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue with the next cell."
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#initialize-the-client-library",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#initialize-the-client-library",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"API_SECRET\",\n  project = \"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#initializing-the-python-environment",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#initializing-the-python-environment",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Initializing the Python environment",
    "text": "Initializing the Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#load-the-demo-dataset",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#load-the-demo-dataset",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with: \n#from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#run-the-full-data-and-model-validation-test-suite",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#run-the-full-data-and-model-validation-test-suite",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Run the Full Data and Model Validation Test Suite",
    "text": "Run the Full Data and Model Validation Test Suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the Raw Dataset\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nWe can now initialize the training and test datasets into dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nWe also initialize a model object using vm.init_model():\n\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the Full Suite\nWe are now ready to run the test suite for binary classifier with tabular datasets. This function will run test plans on the dataset and model objects, and will document the results in the ValidMind UI.\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/markdowntest.html",
    "href": "notebooks/markdowntest.html",
    "title": "ValidMind",
    "section": "",
    "text": "here is some normal text\n\n\n\n\n\n\nHere is some special text!\n\n\n\nIt is special! so very, very special."
  },
  {
    "objectID": "notebooks/markdowntest.html#this-is-a-test-header",
    "href": "notebooks/markdowntest.html#this-is-a-test-header",
    "title": "ValidMind",
    "section": "",
    "text": "here is some normal text\n\n\n\n\n\n\nHere is some special text!\n\n\n\nIt is special! so very, very special."
  },
  {
    "objectID": "release_highlights_template.html#how-to-upgrade",
    "href": "release_highlights_template.html#how-to-upgrade",
    "title": "Release date",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases? Our documentation includes how-to instructions for the following user roles:"
  },
  {
    "objectID": "guide/next-steps.html#have-more-questions",
    "href": "guide/next-steps.html#have-more-questions",
    "title": "Next steps",
    "section": "Have more questions?",
    "text": "Have more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/next-steps.html#need-help",
    "href": "guide/next-steps.html#need-help",
    "title": "Next steps",
    "section": "Need help?",
    "text": "Need help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/next-steps.html#related-topics",
    "href": "guide/next-steps.html#related-topics",
    "title": "Next steps",
    "section": "Related topics",
    "text": "Related topics\n\nAdditional Jupyter notebooks\nIntroduction to the ValidMind Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier\n\n\n\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick Client integration and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"&lt;project-identifier&gt;\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enable their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "href": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test plan execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test plans\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test plans and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html",
    "href": "guide/create-your-first-documentation-project.html",
    "title": "Create Your First Documentation Project",
    "section": "",
    "text": "Let’s learn how to create your own documentation project. You can use this project to upload tests and documentation and then add that to the Quickstart notebook you looked at earlier."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html#steps",
    "href": "guide/create-your-first-documentation-project.html#steps",
    "title": "Create Your First Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the landing page by clicking on the ValidMind logo or Log in to the ValidMind UI.\nFrom the left sidebar, select Documentation Projects and on the page that opens, click the Create new Project button at top right of the screen.\nSelect the right options in the form:\n\n\nModel: [Quickstart] Customer Churn Model\nType: Initial Validation (selected automatically) \nProject name: Enter your preferred name\n\nClick Create Project.\nValidMind will create an empty documentation project associated with the customer churn model.\nYou can now access this project from the UI on the Documentation Projects page or by navigating to the relevant model - [Quickstart] Customer Churn Model - in the Model Inventory page.\nFrom the left sidebar, select Client Integration.\nThe page that opens provides you with the credentials for the newly created project to use with the ValidMind Developer Framework.\nLocate the project identifier, API key, and secret:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\n\n\n\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nTry this: Use the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html#whats-next",
    "href": "guide/create-your-first-documentation-project.html#whats-next",
    "title": "Create Your First Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nContinue with Upload to your documentation project to learn about how you can use the ValidMind Platform for your projects."
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "As of the current release (v1.15.4), the Developer Framework supports the following model types:\nThe following table presents an overview of libraries supported by each test plan, as well as the tests which comprise each test plan as of the current Developer Framework release."
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\n\nCheck out our Developer Framework documentation for more details on how to use our documentation and testing functions with supported models."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/license-agreement.html",
    "href": "guide/license-agreement.html",
    "title": "License agreement",
    "section": "",
    "text": "SOFTWARE LICENSE AGREEMENT\nIMPORTANT - READ CAREFULLY:\nThis software and associated media, printed materials, and “online” or electronic documentation files (the “Software”), is theproprietary information of ValidMind Inc. and its licensors (collectively, “Licensor”), and is protected under copyright and other intellectual property laws.\nNo part of this Software may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means, including, but not limited to, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of ValidMind Inc. or the respective copyright owner.\nBy installing, copying, or otherwise using the Software, the undersigned (“you”) agrees to be bound by the terms of this Software License Agreement (this “Agreement”). If you do not agree to the terms of this Agreement, do not install or use the Software.\nLICENSE GRANT. Subject to the terms and conditions of this Agreement, Licensor grants you a limited, personal, non-exclusive, non-transferable license to use the Software solely for the duration of the 4-week testing phase (the “Testing Period”) of the Software - starting on May 15th, 2023. You may install and use the Software on a single computer or device. You further agree to use the Software solely for internal testing purposes.\nOWNERSHIP. The Software is owned by Licensor and is protected by copyright laws and international copyright treaties, as well as other intellectual property laws and treaties. Licensor retains all right, title, and interest in and to the Software, including all intellectual property rights.\nRESTRICTIONS. You may not modify, adapt, translate, reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code of the Software including (without limitation) for the purpose of obtaining unauthorized access to the Software. You may not distribute, sublicense, rent, lease, or lend the Software to any third party.\nSUPPORT. Licensor may, at its discretion, provide technical support for the Software. Technical support is provided on a best-effort basis and is subject to Licensor’s support policies.\nCONFIDENTIALITY. You agree to safeguard the Software and its related materials with that degree of normal due care commensurate with reasonable standards of industrial security for the protection of trade secrets and proprietary information so that no unauthorized use is made of them and no disclosure of any part of their contents is made to anyone other than your employees, agents or consultants whose duties reasonably require such disclosure, or as necessary in the ordinary course of business. You shall make all such persons fully aware of their responsibility to fulfill your obligations under this Agreement and agree to be responsible for any breach of this Agreement by such persons. You agree to promptly notify Licensor if you obtain information as to any unauthorized possession, use or disclosure of the Software by any person or entity, and further agrees to cooperate with Licensor in protecting Licensor’s proprietary rights.\nTERMINATION. This Agreement will terminate automatically after the Testing Period, or if you fail to comply with any of the terms and conditions of this Agreement. Upon termination, you must immediately cease all use of the Software and destroy all copies of the Software in your possession.\nDISCLAIMER OF WARRANTY. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.\nLIMITATION OF LIABILITY. IN NO EVENT SHALL LICENSOR BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR IN CONNECTION WITH THE USE OR INABILITY TO USE THE SOFTWARE, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. IN NO EVENT SHALL LICENSOR’S LIABILITY EXCEED THE AMOUNT PAID BY YOU FOR THE SOFTWARE.\nINDEMNIFICATION. You agree to indemnify and hold Licensor harmless from and against all loss, cost, expense or liability (including reasonable attorney’s fees) (i) arising out of a claim by a third party against Licensor based upon your use of the Software, or (ii) related to, or associated with your customizations, updates and/or corrections to the Software. Licensor shall have no liability to you for any damage sustained by you as a result of your use of the Software, whether such damages would arise as a result of breach of contract, tort or otherwise.\nGOVERNING LAW. This Agreement shall be governed by and construed in accordance with the laws of the State of California and the federal laws of the United States of America, without giving effect to any principles of conflicts of law.\nENTIRE AGREEMENT. This Agreement constitutes the entire agreement between you and Licensor with respect to the Software and supersedes all prior or contemporaneous communications and proposals, whether oral or written, between you and Licensor.\nBy installing or using the Software, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.\nCopyright © 2023 ValidMind Inc. All rights reserved."
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nView validation guidelines"
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documetnation"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html",
    "href": "guide/review-data-streams-and-audit-trails.html",
    "title": "Review Audit Trail",
    "section": "",
    "text": "Learn how to access and use the audit trail functionality in the ValidMind Platform. This topic matters for for model developers, model validators, and auditors who are looking to track or audit all the information events associated with a specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "href": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "title": "Review Audit Trail",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#steps",
    "href": "guide/review-data-streams-and-audit-trails.html#steps",
    "title": "Review Audit Trail",
    "section": "Steps",
    "text": "Steps\n\nIn the ValidMind platform, navigate to the relevant model documentation project.\nFrom the Overview page, select Audit Trail on the left.\n\nThe table in this page shows a record of all activities generated from the Developer Framework and actions performed by users in the organization related to this specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "href": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "title": "Review Audit Trail",
    "section": "What’s Next",
    "text": "What’s Next\n\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/developer-framework.html",
    "href": "guide/developer-framework.html",
    "title": "Developers",
    "section": "",
    "text": "Geared towards model developers, this section includes information for:"
  },
  {
    "objectID": "guide/developer-framework.html#related-topics",
    "href": "guide/developer-framework.html#related-topics",
    "title": "Developers",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developer tasks related to documentation projects and collaborating with model validators and model owners, refer to our Guides."
  },
  {
    "objectID": "guide/add-a-content-block.html",
    "href": "guide/add-a-content-block.html",
    "title": "Add new content blocks to documentation",
    "section": "",
    "text": "Learn how to add a new content block to your documentation project, to write and update your model’s documentation."
  },
  {
    "objectID": "guide/add-a-content-block.html#prerequisites",
    "href": "guide/add-a-content-block.html#prerequisites",
    "title": "Add new content blocks to documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/add-a-content-block.html#steps",
    "href": "guide/add-a-content-block.html#steps",
    "title": "Add new content blocks to documentation",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project you want to edit.\nInside your documentation project, navigate to the Documentation page.\n\nIn your documentation, hover your mouse over the space where you want your new block to go until the option to insert a new block appears.\nClicking Insert Block will insert a Simple text Block, allowing you to write and edit plain text."
  },
  {
    "objectID": "guide/add-a-content-block.html#related-topics",
    "href": "guide/add-a-content-block.html#related-topics",
    "title": "Add new content blocks to documentation",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Validmind UI"
  },
  {
    "objectID": "guide/edit-templates.html",
    "href": "guide/edit-templates.html",
    "title": "Edit templates",
    "section": "",
    "text": "Learn how to edit templates that get used for model documentation or for validation reports. This topic is relevant for administrators who need to configure templates for specific use cases or where the existing templates supplied by ValidMind need to be customized.\nDocumentation templates are stored as YAML files that you edit directly in the online editor. These templates are versioned and saving a documentation template after making changes or reverting to a previous version state always creates a new version."
  },
  {
    "objectID": "guide/edit-templates.html#prerequisites",
    "href": "guide/edit-templates.html#prerequisites",
    "title": "Edit templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe template you want to edit must have been added to the ValidMind Platform already.\nIf you are not sure which template or which version of a template a documentation project is using, check the Project Overview page of your documentation project. The Documentation Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/edit-templates.html#template-schema",
    "href": "guide/edit-templates.html#template-schema",
    "title": "Edit templates",
    "section": "Template schema",
    "text": "Template schema\n Schema Docs\n\n\n\n\n\n\n\n\n  Type: object     template_id Required     root    template_idType: string Unique identifier for the template.          template_name Required     root    template_nameType: string Name of the template.          version Required     root    versionType: string Version of the template.          description     root    descriptionType: string Description of the template.          sections Required     root    sectionsType: array Documentation sections of the template.  Each item of this array must be:   root    sections    sectionType: object     id Required     root    sections    sections items    idType: string Unique identifier for the section.          title Required     root    sections    sections items    titleType: string Title of the section.          description     root    sections    sections items    descriptionType: string Description of the section.          parent_section     root    sections    sections items    parent_sectionType: string ID of the parent section.          order     root    sections    sections items    orderType: integer Order of the section in the navigation menu. By default sections are ordered alphabetically. If order is specified, sections will be ordered by the order value, and then alphabetically.          default_text     root    sections    sections items    default_textType: string Default text for the section. If set, a metadata content row will be created with this text when installing the template on a given project          index_only     root    sections    sections items    index_onlyType: boolean If true, the section will be displayed in the navigation menu, but it will not be accessible via direct link.          condensed     root    sections    sections items    condensedType: boolean If true, the section will condense all of its subsections into a single section.          guidelines     root    sections    sections items    guidelinesType: array of string Documentation or validation guidelines for the section.  Each item of this array must be:   root    sections    sections items    guidelines    guidelines itemsType: string           contents     root    sections    sections items    contentsType: array Contents to be displayed on the section.  Each item of this array must be:   root    sections    sections items    contents    section_contentsType: object Single content block of the module.      content_type Required     root    sections    sections items    contents    contents items    content_typeType: enum (of string) Default: \"metadata_text\"  Must be one of: \"metadata_text\"\"dynamic\"\"metric\"\"test\"   Examples: \"metadata_text\"\n \"test\"\n          content_id     root    sections    sections items    contents    contents items    content_idType: string ID of the content to be displayed for the given content type (text, metric, testm, etc.).   Examples: \"sample_text\"\n \"section_intro\"\n          options     root    sections    sections items    contents    contents items    optionsType: object Options for the content block.   Examples: {\n    \"default_text\": \"This is a sample text block.\"\n}\n {\n    \"metric_id\": \"metric_1\",\n    \"title\": \"Custom Title for Metric 1\"\n}\n {\n    \"test_id\": \"adf_test\"\n}\n      default_text     root    sections    sections items    contents    contents items    options    default_textType: string Default text for the content block. Only applicable for metadata_text content blocks.          title     root    sections    sections items    contents    contents items    options    titleType: string Title of the content block. Only applicable for metric and test content blocks.                       Generated using json-schema-for-humans on 2023-06-08 at 14:44:40 -0700"
  },
  {
    "objectID": "guide/edit-templates.html#steps",
    "href": "guide/edit-templates.html#steps",
    "title": "Edit templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Templates.\nSelect one of the tabs for the type of template you want to edit:\n\nDocumentation Templates\nValidation Report Templates\n\nLocate the template to edit and, at the bottom of the template card, click Edit Template.\nIn the YAML editor that opens, make your changes.\n\nUse See changes to view a side-by-side comparison of your changes with the latest version of the template.\nUse Reset changes to delete your changes and return to the latest version of the template.\n\nClick Prepare new version to save your changes.\n\nAdd a description in Version notes to track the changes that were made once the version is saved.\n\n\nAfter you have saved a new version, it becomes available for use with model documentation or validation reports."
  },
  {
    "objectID": "guide/edit-templates.html#troubleshooting",
    "href": "guide/edit-templates.html#troubleshooting",
    "title": "Edit templates",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nThe documentation template editor validates the YAML changes you make and flags any errors that it finds. If you make a change that the editor cannot parse correctly, the editor will not let you save the changes until you correct the YAML.\nCommon issues with YAML include incorrect indenting, imbalanced quotes, or missing colons between keys and values. If you run into issues with incorrect YAML, check the error message provided by the template editor, as it might provide a line and column number where the error occurs."
  },
  {
    "objectID": "guide/edit-templates.html#whats-next",
    "href": "guide/edit-templates.html#whats-next",
    "title": "Edit templates",
    "section": "What’s Next",
    "text": "What’s Next\nYou can see which version of a template is used in the Project Overview page of each documentation project."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "href": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test plans and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html",
    "href": "guide/use-test-plans-and-tests.html",
    "title": "When to use test plans and tests",
    "section": "",
    "text": "This topic provides an overview about:"
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "href": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "title": "When to use test plans and tests",
    "section": "What tests, test plans, and test suites are",
    "text": "What tests, test plans, and test suites are\n\nTests are designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\nTest plans are collections of tests which are meant to be run simultaneously to address specific aspects of the documentation. quart Example: the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind Platform.\nTest suites are collection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\nExample: the binary_classifier_full_suite test suite runs the tabular_dataset and binary_classifier test plans to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "href": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "title": "When to use test plans and tests",
    "section": "When to use ValidMind Tests, Test plans, and Test suites",
    "text": "When to use ValidMind Tests, Test plans, and Test suites\nValidMind provides many built-in tests and test plans which make it easy for a model developer to document their work at any point during the model development lifecycle when they need to validate that their work satisfies model risk management requirements.\nWhile model developers have the flexibility to decide when to use ValidMind tests, we have identified a few typical scenarios which have their own characteristics and needs:\n\nWhen you want to document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test plan.\nFor time-series datasets: use the time_series_dataset test plan.\n\nWhen you want to document and validate about your model:\n\nFor binary classification models: use the binary_classifier test plan.\nFor time series models: use the timeseries test plan.\n\nWhen you want to document a binary classification model and the relevant dataset end-to-end: use the binary_classifier_full_suite test suite."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#api-reference",
    "href": "guide/use-test-plans-and-tests.html#api-reference",
    "title": "When to use test plans and tests",
    "section": "API Reference",
    "text": "API Reference\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/view-all-test-plans.html",
    "href": "guide/view-all-test-plans.html",
    "title": "View all test plans",
    "section": "",
    "text": "Learn how to use list_plans(), list_test(), and describe_plan() methods to view and describe test plans and tests available in the Developer Framework."
  },
  {
    "objectID": "guide/view-all-test-plans.html#prerequisites",
    "href": "guide/view-all-test-plans.html#prerequisites",
    "title": "View all test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are working on an active documentation project\nYou have already installed the ValidMind client library in your developer environment"
  },
  {
    "objectID": "guide/view-all-test-plans.html#steps",
    "href": "guide/view-all-test-plans.html#steps",
    "title": "View all test plans",
    "section": "Steps",
    "text": "Steps\n\nInitialize the client library.\nUse list_plans() and list_tests() to view the list of all available test plans and tests.\nExamples:\n\nList all available test plans currently available in the the Developer Framework:\nvm.test_plans.list_plans()\nList all available individual tests currently available in the Developer Framework:\nvm.test_plans.list_tests() \n\nUse describe_testplan() to list all the tests included in a specific test plan:\nExample: The following code will list tests included in the tabular_data_quality test plan:\nvm.test_plans.describe_plan(\"tabular_data_quality\")"
  },
  {
    "objectID": "guide/view-all-test-plans.html#related-topics",
    "href": "guide/view-all-test-plans.html#related-topics",
    "title": "View all test plans",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "href": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe validation guidelines for each template can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s Next",
    "text": "What’s Next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/view-templates.html",
    "href": "guide/view-templates.html",
    "title": "View templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-templates.html#prerequisites",
    "href": "guide/view-templates.html#prerequisites",
    "title": "View templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-templates.html#steps",
    "href": "guide/view-templates.html#steps",
    "title": "View templates",
    "section": "Steps",
    "text": "Steps\n\nFrom the ValidMind Platform homepage, go to Templates on the left.\nClick on one of the available templates to view the YAML configuration file.\nIn the configuration file that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the Developer Framework that feed into the template\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTemplates can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-templates.html#related-topics",
    "href": "guide/view-templates.html#related-topics",
    "title": "View templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/developer-framework-introduction.html",
    "href": "guide/developer-framework-introduction.html",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "This page provides an introduction for:\n\nWhat the ValidMind Developer Framework is, key concepts, and what functionality it provides\nHow ValidMind documentation projects are structured"
  },
  {
    "objectID": "guide/developer-framework-introduction.html#brief-introduction",
    "href": "guide/developer-framework-introduction.html#brief-introduction",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "This page provides an introduction for:\n\nWhat the ValidMind Developer Framework is, key concepts, and what functionality it provides\nHow ValidMind documentation projects are structured"
  },
  {
    "objectID": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "href": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "What ValidMind’s Developer Framework is",
    "text": "What ValidMind’s Developer Framework is\n\nValidMind’s Python Developer Framework is a library of developer tools and methods designed to automate the documentation and validation of your models.\nThe Developer Framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python library will provide all the standard functionality without requiring your developers to rewrite any functions.\nThe Developer Framework provides a rich suite of documentation tools and test plans, from documenting descriptions of your dataset to testing your models for weak spots and overfit areas. The Developer Framework helps you automate the generation of model documentation by feeding the ValidMind platform with documentation artifacts and test results to the ValidMind platform."
  },
  {
    "objectID": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "href": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "ValidMind Documentation Project Structure",
    "text": "ValidMind Documentation Project Structure\n\n\nProject\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s MRM lifecycle will constitute a new project, and may be configured with its own templates and workflows.\n\nModel documentation\n\nProvides a comprehensive record and description of a quantitative model. This documentation should encompass all relevant information about the model in accordance with:\n\n\n\nRegulatory requirements (set by regulatory bodies)\nModel risk policies (set by an institution’s MRM team)\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\nIntended use\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model.\n\nTemplate\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results:\n\n\n\nModel documentation is populated when a user successfully executes all the tests contained in a template, thereby completing the test suite. Template placeholders get populated with content generated by the ValidMind Developer Framework.\nEssentially, our platform scans these templates, identifies all tests and systematically organizes them into a well-structured test suite.\n\nThe criteria for these templates are typically provided by your model risk management team. They can be programmatically customized to suit the unique requirements of each model use case, a task usually performed by an administrator.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html",
    "href": "guide/upload-to-documentation-project.html",
    "title": "Upload to Your Documentation Project",
    "section": "",
    "text": "You are now ready to modify the Quickstart notebook to upload to your own project that you created earlier."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html#steps",
    "href": "guide/upload-to-documentation-project.html#steps",
    "title": "Upload to Your Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nReopen the Quickstart notebook you accessed earlier.\nIn the Quickstart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you created your new documentation project:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html#whats-next",
    "href": "guide/upload-to-documentation-project.html#whats-next",
    "title": "Upload to Your Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nReady to learn more about how you can use ValidMind? Check out Next Steps."
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind overview",
    "section": "",
    "text": "ValidMind is a model risk management (MRM) solution designed for the specific needs of model developers and model validators alike. The platform automates key aspects of the MRM process, including model documentation, validation, and testing. In addition, the platform comes with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date.\nOur solution comprises two primary architectural components: the ValidMind Developer Framework and the cloud-based ValidMind MRM platform."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind overview",
    "section": "Related Topics",
    "text": "Related Topics\nReady to try out ValidMind? Try the Quickstarts."
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developers section."
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/jupyter-notebooks.html",
    "href": "guide/jupyter-notebooks.html",
    "title": "Example notebooks",
    "section": "",
    "text": "Our example notebooks are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nTry the notebooks yourself:\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time, and download notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/jupyter-notebooks.html#related-topics",
    "href": "guide/jupyter-notebooks.html#related-topics",
    "title": "Example notebooks",
    "section": "Related topics",
    "text": "Related topics\nFor an introduction to how these notebooks get used with ValidMind, take a look at the Quickstart."
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a solution designed to help simplify and automate key aspects of model risk management (MRM) activities for model developers and model validators alike. The platform helps automate model documentation, validation, and testing. In addition, the platform offers built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/get-started.html#welcome-to-validmind",
    "href": "guide/get-started.html#welcome-to-validmind",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a solution designed to help simplify and automate key aspects of model risk management (MRM) activities for model developers and model validators alike. The platform helps automate model documentation, validation, and testing. In addition, the platform offers built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/get-started.html#how-does-it-work",
    "href": "guide/get-started.html#how-does-it-work",
    "title": "Get started",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\nValidMind consists of two main products components:\n\nThe Developer Framework is a library of tools and methods designed to automate model documentation and validation. It is platform agnostic, and integrates with the model development environment.\nThe ValidMind Platform is an easy-to-use web-based UI that enables users to review and edit the documentation and test metrics generated by the Developer Framework online. It also enables collaboration and feedback capture between model developers and model validators, and offers workflow capabilities to manage the model documentation and validation process.\n\nFor more information about what ValidMind offers, check out our ValidMind overview."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "href": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "title": "Get started",
    "section": "How do I get access to ValidMind?",
    "text": "How do I get access to ValidMind?\nIf you are new to our products, you will need access: Request a trial."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nThe fastest way to explore what ValidMind can offer is with our Quickstarts, where you can:\n\nTry out our Developer Framework with a sample Jupyter notebook\nExplore the ValidMind Platform UI\n\nIf you have already tried the Quickstarts, more how-to instructions and links to our FAQs can be found under Next steps."
  },
  {
    "objectID": "guide/editions-and-features.html",
    "href": "guide/editions-and-features.html",
    "title": "Editions and features",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions-and-features.html#editions",
    "href": "guide/editions-and-features.html#editions",
    "title": "Editions and features",
    "section": "Editions",
    "text": "Editions\n\nDeveloper Edition\nThe Developer Edition is the ideal training ground for developers to play around with ValidMind’s automated model documentation and to test the robustness of our developer framework, documentation, and testing features. The Developer Edition is free, allowing developers who are new to model documentation and model risk management to build, implement, test, and maintain higher quality models and model documentation.\nThe Developer Edition is only for personal testing purposes and cannot be used as a commercial model documentation or model risk management solution.\n\n\nEssential Edition\nWith the Essential Edition, you get an advanced model risk management (MRM) solution. It offers your organization all the features and services of the Developer Edition, plus additional features tailored to the needs of larger-scale organizations.\n\n\nBusiness Critical\nProvides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. This edition encompasses all features and services of the Essential Edition but within a separate ValidMind environment, isolated from other ValidMind accounts via Virtual Private ValidMind (VPV). VPV accounts do not share resources with non-VPV accounts."
  },
  {
    "objectID": "guide/editions-and-features.html#features",
    "href": "guide/editions-and-features.html#features",
    "title": "Editions and features",
    "section": "Features",
    "text": "Features\n\n\n\n\nModel development & documentation\nDeveloper\nEssential\nBusiness Critical\n\n\n\n\nAutomated model documentation\n\n\n\n\n\nPlatform-independent developer framework\n\n\n\n\n\nOnline documentation editing\n\n\n\n\n\nAdvanced editing & readability assistance\n\n\n\n\n\nDocumentation quality measurement\n\n\n\n\n\nOffline document ingestion\n\n\n\n\n\nFeedback capture on online document\n\n\n\n\n\nDocumentation version history management\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nStandard tests & validation libraries\n\n\n\n\n\nConfigure / customize tests & validation libraries\n\n\n\n\n\nSupport for customer-provided tests\n\n\n\n\n\nDeveloper workflow management\n\n\n\n\n\nPre-configured documentation templates & boilerplates\n\n\n\n\n\nConfigurable documentation templates & boilerplates\n\n\n\n\n\nModel validation & audit\n\n\n\n\n\nModel validation report automation\n\n\n\n\n\nFindings / issues & remediation actions tracking\n\n\n\n\n\nConfigurable approval workflows\n\n\n\n\n\nMRM workflows & validation lifecycle tracking\n\n\n\n\n\nMRM resource & workflow management\n\n\n\n\n\nCentral model inventory\n\n\n\n\n\nHistorical documentation repository /documentation CMS\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nExecutive reporting\n\n\n\n\n\nPlatform integration & support\n\n\n\n\n\nData lake integration, such as Evidence Storeand monitoring data\n\n\n\n\n\nSSO integration\n\n\n\n\n\nCustomer managed encryption\n\n\n\n\n\nSupport 8/5 (one timezone)\n\n\n\n\n\nSupport 24/7 (global)\n\n\n\n\n\nPlatform deployment\n\n\n\n\n\nMulti-tenant SaaS\n\n\n\n\n\nVirtual private ValidMind (VPV)\n\n\n\n\n\nSelf-managed VPV\n\n\n\n\n\n\nContact Us\nContact Us\nContact Us"
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html",
    "href": "guide/try-developer-framework-with-docker.html",
    "title": "Try it with Docker Desktop",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with the ValidMind Docker image."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#prerequisites",
    "href": "guide/try-developer-framework-with-docker.html#prerequisites",
    "title": "Try it with Docker Desktop",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have Docker Desktop installed on your machine."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#steps",
    "href": "guide/try-developer-framework-with-docker.html#steps",
    "title": "Try it with Docker Desktop",
    "section": "Steps",
    "text": "Steps\n\nFrom the command line, pull the latest ValidMind Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nRun the ValidMind Docker image:\ndocker run -it -p 8888:8888 validmind/validmind-jupyter-demo\nAfter the command completes, you should see a message that Jupyter Server is running similar to this:\n[I 2023-05-18 21:53:06.030 ServerApp] Serving notebooks from local directory: /app\n    1 active kernel\n    Jupyter Server 2.5.0 is running at:\n    http://032c824982aa:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\n        http://127.0.0.1:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\nCopy the browser URL that starts with http://127.0.0.1:8888 from the message and paste it into a new browser tab.\nAfter JupyterLab opens in your browser, you should see a link for our Quickstart_Customer Churn_full_suite.ipynb notebook.\nDouble click the notebook to open it:"
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#whats-next",
    "href": "guide/try-developer-framework-with-docker.html#whats-next",
    "title": "Try it with Docker Desktop",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.prod.validmind.ai.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html",
    "href": "guide/release-notes-2023-may-30.html",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#release-highlights",
    "href": "guide/release-notes-2023-may-30.html#release-highlights",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#bugfixes",
    "href": "guide/release-notes-2023-may-30.html#bugfixes",
    "title": "May 30, 2023",
    "section": "Bugfixes",
    "text": "Bugfixes\n\nFixed the display alignment in certain pages of the UI.\nFixed display issues related to Helvetica Neue font not available for Windows users.\nFixed an issue preventing users to drag & drop image files directly in the online editor.\nAdjusted filters for Model Inventory and Documentation Projects search boxes."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "href": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "title": "May 30, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, refresh your browser.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html",
    "href": "guide/load-credentials-to-env-file.html",
    "title": "Load project credentials to a .env file",
    "section": "",
    "text": "Learn how to store project identifier credentials in a .env file instead of using inline credentials. This topic is relevant for model developers who want to follow best practices for security when running notebooks."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#why-is-this-recommended",
    "href": "guide/load-credentials-to-env-file.html#why-is-this-recommended",
    "title": "Load project credentials to a .env file",
    "section": "Why is this recommended?",
    "text": "Why is this recommended?\nStoring credentials in a .env file is considered a best practice for security. Embedding credentials directly within the code makes them more susceptible to accidental exposure when sharing code or collaborating on projects. Keeing project credentials in a separate file also allows for precise access control and ensures that sensitive credentials are not publically accessible."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#prerequisites",
    "href": "guide/load-credentials-to-env-file.html#prerequisites",
    "title": "Load project credentials to a .env file",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory"
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#steps",
    "href": "guide/load-credentials-to-env-file.html#steps",
    "title": "Load project credentials to a .env file",
    "section": "Steps",
    "text": "Steps\n\nCreate a new file in the same folder as your notebook and name it .env. This is a hidden file, so you may need to change your settings to view it.\nLocate the project identifier credentials for your documentation project. These credentials can be found on the Client Integration page. Copy the values from this page and paste them into your .env file in the following format:\n\n``` VM_API_PROJECT= VM_API_HOST= VM_API_KEY= VM_API_SECRET= ```\n\nInsert this code snippet above your project identifier credentials:\n\n``` %load_ext dotenv %dotenv dev.env ```\nThe updated notebook should look like this:\n``` %load_ext dotenv %dotenv .env\nimport validmind as vm\nvm.init( api_host = “http://localhost:3000/api/v1/tracking”, project = “…” ) ```\n\nRun the cell. Instead of using inline credentials, this cell will now load your project credentials from a .env file."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#related-topics",
    "href": "guide/load-credentials-to-env-file.html#related-topics",
    "title": "Load project credentials to a .env file",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the ValidMind UI\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/explore-example-documentation-project.html",
    "href": "guide/explore-example-documentation-project.html",
    "title": "Explore an Example Documentation Project",
    "section": "",
    "text": "Let’s take a look at how the Developer Framework works hand-in-hand with the ValidMind Platform and how documentation and test results get uploaded.\nThe ValidMind Platform is the central place to:"
  },
  {
    "objectID": "guide/explore-example-documentation-project.html#steps",
    "href": "guide/explore-example-documentation-project.html#steps",
    "title": "Explore an Example Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the [Quickstart] Customer Churn Model - Initial Validation and select it.\nOn the model details page that open, you can find important information about the model, such as:\n\nThe ID of the model and its specific use case\nThe owners, developers, validators, and business unit associated with the model\nThe risk tier and current version\nAnd more\n\nScroll down to Documentation Project History and select the model.\nOn the project overview page that opens, you can see what is included, such as model, project findings, recent activity, and project stakeholders, and more. In the left sidebar, you can find links to the documentation, project findings, validation report, audit trail, and client integration.\nFor this Quickstart, we will focus on the Documentation section to show you how content from the Developer Framework gets uploaded.\nNote that the model status is In Documentation. This is the status that a model starts in as part of a documentation project. You can click See workflow to look at what the full workflow is, from documentation, to validation, to review, and finally approval.\nFrom the left sidebar, select Documentation &gt; 2. Data preparation &gt; 2.1. Data description.\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically by the Developer Framework whenever a test result is above or below a certain threshold.\nFrom the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the Developer Framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nThe Documentation Guidelines in the ValidMind Insights right sidebar can tell you more about what these sections mean and help you with the task of documenting the model.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the Developer Framework, but they get added by the model developer in the Platform UI."
  },
  {
    "objectID": "guide/explore-example-documentation-project.html#whats-next",
    "href": "guide/explore-example-documentation-project.html#whats-next",
    "title": "Explore an Example Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nReady to learn about how you can use the ValidMind Platform? Continue with Create your first documentation project."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html",
    "href": "guide/try-developer-framework-with-colab.html",
    "title": "Try it with Google Colaboratory",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Google Colaboratory."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#prerequisites",
    "href": "guide/try-developer-framework-with-colab.html#prerequisites",
    "title": "Try it with Google Colaboratory",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to Google Colaboratory (Colab).\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#steps",
    "href": "guide/try-developer-framework-with-colab.html#steps",
    "title": "Try it with Google Colaboratory",
    "section": "Steps",
    "text": "Steps\n\n\n\n\n\n\n\n\nAbout our Jupyter notebooks\n\n\n\nNotebooks from ValidMind are safe to run — If you get a warning that this notebook was not authored by Google, we welcome you to inspect the notebook source.  Runtime errors — We recommend that you not use the Run all option. Run each cell individually to see what is happening in the notebook. If you do see errors, re-run the notebook cells.\n\n\n\nOpen the Quickstart notebook in Google Colaboratory: \n\nClick File &gt; Save a copy in Drive to make a copy of the Quickstart notebook so that you can modify it later.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#whats-next",
    "href": "guide/try-developer-framework-with-colab.html#whats-next",
    "title": "Try it with Google Colaboratory",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\nEmail support@validmind.com\nEmail support@validmind.com"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s Next",
    "text": "What’s Next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "href": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html",
    "href": "guide/try-developer-framework-with-jupyterhub.html",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Jupyter Hub (recommended)."
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html#steps",
    "href": "guide/try-developer-framework-with-jupyterhub.html#steps",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "Steps",
    "text": "Steps\n\nIn a web browser, go to https://jupyterhub.validmind.ai.\nClick Sign in with Auth0, enter your ValidMind email address and password credentials, and click Continue.\nIn the sidebar, double click the Quickstart_Customer Churn_full_suite.ipynb notebook:\n\nAfter the notebook opens, run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will guide you through installing the ValidMind Developer Framework, initializing the Python environment, and finally initializing the ValidMind Client Library by connecting to your own documentation project in the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the Developer Framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the Platform UI."
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html#whats-next",
    "href": "guide/try-developer-framework-with-jupyterhub.html#whats-next",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "href": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html",
    "href": "guide/release-notes-2023-jun-22.html",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the Platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our Developer Framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with Validmind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test plans. Learn more …\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model built with the PyTorch library. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. Learn more …\n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the Platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#release-highlights",
    "href": "guide/release-notes-2023-jun-22.html#release-highlights",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the Platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our Developer Framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with Validmind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test plans. Learn more …\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model built with the PyTorch library. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. Learn more …\n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the Platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#enhancements",
    "href": "guide/release-notes-2023-jun-22.html#enhancements",
    "title": "June 22, 2023",
    "section": "Enhancements",
    "text": "Enhancements\nWe revised our Quickstart guide to be more modular and to highlight that our suggested starting point with the ValidMind Developer Framework is now Jupyter Hub. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "title": "June 22, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/tutorials.html",
    "href": "guide/tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our tutorials provide a more targeted learning experience and cover specific scenarios or use cases."
  },
  {
    "objectID": "guide/tutorials.html#related-topics",
    "href": "guide/tutorials.html#related-topics",
    "title": "Tutorials",
    "section": "Related topics",
    "text": "Related topics\nBesides our tutorials, we also offer a Quickstart that walks you through the full experience from the Developer Framework to the Platform UI."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\nLocating the project identifier, API key and secret:\nOn the Client Integration page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/create-documentation-project.html#related-topics",
    "href": "guide/create-documentation-project.html#related-topics",
    "title": "Create documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nInstall and initialize the Developer Framework\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/troubleshooting.html",
    "href": "guide/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Learn how to resolve commonly encountered issues with the Developer Framework."
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-install-the-validmind-developer-framework",
    "href": "guide/troubleshooting.html#cannot-install-the-validmind-developer-framework",
    "title": "Troubleshooting",
    "section": "Cannot install the ValidMind Developer Framework",
    "text": "Cannot install the ValidMind Developer Framework\nIssue: You cannot run pip install validmind or import validmind as vm in the ValidMind Developer Framework notebooks.\nFix: Make sure you are installing the latest version of the Developer Framework by running this command:\n%pip install --upgrade validmind"
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "href": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "title": "Troubleshooting",
    "section": "Cannot initialize ValidMind client library",
    "text": "Cannot initialize ValidMind client library\nIssue: When you run vm.init(), you encounter an error message like this:\nMissingAPICredentialsError: API key and secret must be provided either as environment variables or as arguments to init.\nor\nInvalidProjectError: Invalid project ID. Please ensure that you have provided a project ID that belongs to your organization.\nFix: Make sure that you are using the correct initialization credentials for the project you are trying to connect to.\nFollow the steps in Install and initialize the Developer Framework for detailed instructions on how to integrate the Developer Framework and upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/troubleshooting.html#additional-resources",
    "href": "guide/troubleshooting.html#additional-resources",
    "title": "Troubleshooting",
    "section": "Additional resources",
    "text": "Additional resources\nCheck out our FAQ page to browse through common questions, or contact our support team for more help troubleshooting technical issues."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s Next",
    "text": "What’s Next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "Don’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "",
    "text": "Don’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#whats-next",
    "href": "guide/before-you-begin.html#whats-next",
    "title": "Before you begin",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore the Developer Framework. We recommend using Jupyter Hub."
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstarts",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework in Jupyter Hub and to explore the ValidMind Platform UI online."
  },
  {
    "objectID": "guide/quickstart.html#before-you-begin",
    "href": "guide/quickstart.html#before-you-begin",
    "title": "Quickstarts",
    "section": "Before you begin",
    "text": "Before you begin\n\n\n\n\n\n\nDon’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\nQuickstart requirements\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\nAccess to the ValidMind Platform UI\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstarts",
    "section": "Steps",
    "text": "Steps\n\nTry the Developer Framework (10 minutes)\nTry our introductory Jupyter notebook to see the Developer Framework in action.\nExplore the Platform UI (15 minutes)\nExplore our Platform UI to work with a documentation project and see the results of tests you run."
  },
  {
    "objectID": "guide/quickstart.html#next-steps",
    "href": "guide/quickstart.html#next-steps",
    "title": "Quickstarts",
    "section": "Next Steps",
    "text": "Next Steps\nReady to use ValidMind for production with your own use cases? Our documentation includes how-to instructions for the following user roles:\n\nFor platform administrators — Learn how to configure the platform, from setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your existing workflows, and more.\nFor model developers — Find information for ValidMind test plans and tests, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nAlso check the Guides for how to integrate the Developer Framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers.\n\n\nHave more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human.\n\n\nNeed help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation.\n\n\nRelated topics\n\nAdditional Jupyter notebooks\nIntroduction to the ValidMind Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "href": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html",
    "href": "guide/collaborate-on-documentation-projects.html",
    "title": "Collaborate on documentation projects",
    "section": "",
    "text": "Learn how ValidMind enhances collaboration between model validators and developers on documentation projects. This topic is relevant for model validators who want to track changes across projects, add comments, and access revision history for real-time collaboration with model developers."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#prerequisites",
    "href": "guide/collaborate-on-documentation-projects.html#prerequisites",
    "title": "Collaborate on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#commenting",
    "href": "guide/collaborate-on-documentation-projects.html#commenting",
    "title": "Collaborate on documentation projects",
    "section": "Commenting",
    "text": "Commenting\n\nPosting comments to the documentation\n\nIn any section of the model documentation, highlight the portion of text you would like to comment on, and click the Comment button in the toolbar.\nEnter your comment and click Comment.\nYou can view the comment by clicking the highlighted text. Comments will also appear in the right sidebar.\n\n\n\nResponding to an existing comment\n\nClick the highlighted text portion to view the comment thread.\nEnter your comment and click Reply.\nYou can view the comment thread by clicking the highlighted text.\n\n\n\nResolving comment threads and viewing archived comments\n\nClick the highlighted text portion to view the thread, then click  to resolve the thread.\nTo view the resolved comment thread, click the Comment archive button in the toolbar. You can view a history of all archived comments in the Comment archive.\nTo reopen a comment thread, reply to the comment thread in the Comment archive or click the Reopen button that appears next to the highlighted text portion.\n\n\n\nEditing and deleting comments\n\nClick the highlighted text portion to access the comment thread.\nTo edit a comment in the thread, click the More options icon for the corresponding comment and click Edit.\nEdit your comment and click Save.\nTo edit a comment in a resolved thread, follow the same steps but click the Comments archive button first to access the resolved thread.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#tracking-changes",
    "href": "guide/collaborate-on-documentation-projects.html#tracking-changes",
    "title": "Collaborate on documentation projects",
    "section": "Tracking changes",
    "text": "Tracking changes\n\nSuggesting a change\n\nClick the Track changes button in the toolbar to turn on suggestion mode.\nMake your changes to the documentation project. When changes tracking is enabled, other project contributers can accept or decline the suggested changes.\n\n\n\nResolving changes\n\nSuggested changes appear in green or red highlighted text, depending on if the change is adding or removing content. To accept or decline a change, click the highlighted text, then click  or . You can also reply to a suggested change.\nTo mass accept or decline suggestions, click the dropdown arror next to the Track changes button and click the desired option."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#revision-history",
    "href": "guide/collaborate-on-documentation-projects.html#revision-history",
    "title": "Collaborate on documentation projects",
    "section": "Revision history",
    "text": "Revision history\n\nSaving a version\n\nClick the Revision history button in the toolbar.\nIn the dropdown, click Save current version. Optionally, enter a version name. The default name is the date and time the latest change was made.\n\n\n\nViewing revision history\n\nClick the Revision history button in the toolbar, then click Open revision history. Here, you can view a history of all saved versions and your current version.\nTo see the the change made with each version, select the version in the right sidebar. Changes made in that version are highlighted. Hover over the highlighted content to see who made the change.\n\n\n\nRestoring a version\n\nTo restore a version, select the desired version and click Restore this version.\nThe restored version will now appear under revision history with the name: “Restored: ‘version name’”. To exit revision history without restoring a version, click Back to editing."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#real-time-collaboration",
    "href": "guide/collaborate-on-documentation-projects.html#real-time-collaboration",
    "title": "Collaborate on documentation projects",
    "section": "Real-time collaboration",
    "text": "Real-time collaboration\nUsers can simultaneously edit the documentation project, leave and respond to comments suggestions, and access revision history. Changes to the documentation project are also automatically added to ValidMind’s activity feed."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#additional-features",
    "href": "guide/collaborate-on-documentation-projects.html#additional-features",
    "title": "Collaborate on documentation projects",
    "section": "Additional features",
    "text": "Additional features\n\nSpell and grammar checker.\nMath formulas. Add math formulas to documentation by clicking the MathType button and using the toolbar, or switch to handwriting."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#related-topics",
    "href": "guide/collaborate-on-documentation-projects.html#related-topics",
    "title": "Collaborate on documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nComment on Documentation Projects"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The guide to elevating your MRM workflow",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Documentation\n                            The guide to elevating your MRM workflow\n                            Need help? Find all the information you need to use our platform for model risk management (MRM).\n                            \n                            \n                                \n                                    \n                                        \n                                            \n                                        \n                                        \n                                                                \n                                        \n                                    \n                                \n                            \n\n                        \n                    \n                \n                \n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating key aspects of the model risk management process, ValidMind is an MRM solution designed for the unique needs documentation and validation needs of model developers and validators.\n                    Model Documentation Automation\n                    MRM Lifecycle and Workflow\n                    Communication & TrackingInstructional GuidesGet Started\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Automate your model documentation and testing tasks with our Developer Framework.Collaboration for Model Developers\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and documentation to ensure they comply with organizational & regulatory requirements.Collaboration for Model Validators\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "notebooks/external_test_providers_demo.html",
    "href": "notebooks/external_test_providers_demo.html",
    "title": "Integrate an External Test Provider",
    "section": "",
    "text": "This notebook demonstrates how to use a custom test provider to be able to use custom tests with the Validmind Developer Framework. In the notebook, we load a couple different demo test providers and register them with the Validmind framework to be able to run a template that utilizes those tests.\n\n# install validmind if its not already installed\n# %pip install validmind\n\n\nUpdate your template\nWithin the project you would like to use, create a new Content Block in the documentation template and add the following code:\n      - content_type: metric\n        content_id: my_local_provider.tests.MyCustomTest\n      - content_type: metric\n        content_id: my_inline_provider.tests.MyCustomTest\nSee the section below on setting up and registering test providers for more info on how these content_id’s get mapped to the actual test providers and tests.\n\n\nSet up the Validmind connection\nget your api_key and api_secret from the Validmind UI\n\nimport validmind as vm\n\nvm.init(\n    api_host = \"http://localhost:3000/api/v1/tracking\",\n    api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n    api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n    project = \"clj1vl7l9000lahztzgtj5aqj\"\n)\n\n2023-06-20 15:08:36,290 - INFO - api_client - Connected to ValidMind. Project: External Test Provider Demo (clj1vl7l9000lahztzgtj5aqj)\n\n\n\n\nPreview the template for the current project to validate that it looks correct\n\n# we should see two custom content blocks in the template whose IDs are under the namespaces registered below (`my_inline_provider` and `my_local_provider`)\n# the ID should match the path to the `tests` directory in this repo\nvm.preview_template()\n\n\n\n\n\n\nRegister external test providers\nWe will now instantiate and register the Test Provider classes that we imported above.\nFor the Github provider, we will need to specify the org and repo to pull from as well as optionally pass a token if the repo is private.\nFor the Local Filesystem provider, we will just need to specify the root folder under which the provider class will look for tests. In this case, it is the current directory for this demo so you may have to adjust the path for your machine.\nImport the Local File System Test Provider from the validmind.tests module\n\nfrom validmind.tests import LocalTestProvider\n\nCreate an inline TestProvider Class that just returns a single test\n\nclass MySecondCustomTest(vm.vm_models.Metric):\n    def description(self):\n        return \"This is a custom test from an external test provider.\"\n\n    def run(self):\n        return self.cache_results({\"foo\": \"bar\"})\n\n    def summary(self, results):\n        return vm.vm_models.ResultSummary(\n            results=[\n                vm.vm_models.ResultTable(\n                    data=[{\"results\": results}],\n                    metadata=vm.vm_models.ResultTableMetadata(\n                        title=\"Custom Test from External Test Provider\"\n                    ),\n                )\n            ]\n        )\n\n\nclass TestProviderInline:\n    def load_test(self, test_id):\n        # ignore the test_id and just return the single test above\n        return MySecondCustomTest\n\n\n# instantiate the test provider\ninline_test_provider = TestProviderInline()\nlocal_test_provider = LocalTestProvider(root_folder=\".\")\n\n# register the test providers\nvm.tests.register_test_provider(\n    namespace=\"my_inline_provider\",\n    test_provider=inline_test_provider,\n) # validmind will now call the `TestProviderInline.load_test` method whenever it encounters a test ID that starts with `my_inline_provider`\nvm.tests.register_test_provider(\n    namespace=\"my_local_provider\",\n    test_provider=local_test_provider,\n) # validmind will now call the `LocalTestProvider.load_test` method whenever it encounters a test ID that starts with `my_local_provider`\n\n\n\nRunning the template\nNow we can run the template as usual and it will use the external test providers to load the appropriate tests.\n\nvm.run_template()"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html",
    "href": "notebooks/Introduction_Customer_Churn.html",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "",
    "text": "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#before-you-begin",
    "href": "notebooks/Introduction_Customer_Churn.html#before-you-begin",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Before you begin",
    "text": "Before you begin\nClick File &gt; Save a copy in Drive &gt; to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#install-the-client-library",
    "href": "notebooks/Introduction_Customer_Churn.html#install-the-client-library",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting validmind\n  Downloading validmind-1.11.7-py3-none-any.whl (1.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 11.9 MB/s eta 0:00:00\nCollecting arch&lt;6.0.0,&gt;=5.4.0 (from validmind)\n  Downloading arch-5.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (918 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 918.2/918.2 kB 67.3 MB/s eta 0:00:00\nCollecting catboost&lt;2.0,&gt;=1.2 (from validmind)\n  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 MB 8.2 MB/s eta 0:00:00\nRequirement already satisfied: click&lt;9.0.0,&gt;=8.0.4 in /usr/local/lib/python3.10/dist-packages (from validmind) (8.1.3)\nCollecting dython&lt;0.8.0,&gt;=0.7.1 (from validmind)\n  Downloading dython-0.7.4-py3-none-any.whl (24 kB)\nRequirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (7.34.0)\nRequirement already satisfied: markdown&lt;4.0.0,&gt;=3.4.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (3.4.3)\nCollecting myst-parser&lt;2.0.0,&gt;=1.0.0 (from validmind)\n  Downloading myst_parser-1.0.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.3/77.3 kB 8.5 MB/s eta 0:00:00\nCollecting numpy==1.22.3 (from validmind)\n  Downloading numpy-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 83.5 MB/s eta 0:00:00\nRequirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.5.3)\nCollecting pandas-profiling&lt;4.0.0,&gt;=3.6.6 (from validmind)\n  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.4/324.4 kB 24.5 MB/s eta 0:00:00\nRequirement already satisfied: pydantic&lt;2.0.0,&gt;=1.9.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.10.7)\nCollecting pypmml&lt;0.10.0,&gt;=0.9.17 (from validmind)\n  Downloading pypmml-0.9.17.tar.gz (14.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/14.2 MB 93.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nCollecting python-dotenv&lt;0.21.0,&gt;=0.20.0 (from validmind)\n  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.27.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (2.27.1)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.0.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.2.2)\nCollecting seaborn&lt;0.12.0,&gt;=0.11.2 (from validmind)\n  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 32.6 MB/s eta 0:00:00\nCollecting shap&lt;0.42.0,&gt;=0.41.0 (from validmind)\n  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 572.6/572.6 kB 53.1 MB/s eta 0:00:00\nCollecting sphinx&lt;7.0.0,&gt;=6.1.3 (from validmind)\n  Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 84.8 MB/s eta 0:00:00\nCollecting sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5 (from validmind)\n  Downloading sphinx_markdown_builder-0.5.5-py2.py3-none-any.whl (15 kB)\nCollecting sphinx-rtd-theme&lt;2.0.0,&gt;=1.2.0 (from validmind)\n  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 85.9 MB/s eta 0:00:00\nRequirement already satisfied: statsmodels&lt;0.14.0,&gt;=0.13.5 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.13.5)\nRequirement already satisfied: tabulate&lt;0.9.0,&gt;=0.8.9 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.8.10)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.64.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (4.65.0)\nRequirement already satisfied: xgboost&lt;2.0.0,&gt;=1.5.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.7.5)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (67.7.2)\nCollecting jedi&gt;=0.16 (from ipython==7.34.0-&gt;validmind)\n  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 87.6 MB/s eta 0:00:00\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (0.7.5)\nRequirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (3.0.38)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (2.14.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (4.8.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3-&gt;validmind) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3-&gt;validmind) (2022.7.1)\nRequirement already satisfied: scipy&gt;=1.3 in /usr/local/lib/python3.10/dist-packages (from arch&lt;6.0.0,&gt;=5.4.0-&gt;validmind) (1.10.1)\nCollecting property-cached&gt;=1.6.4 (from arch&lt;6.0.0,&gt;=5.4.0-&gt;validmind)\n  Downloading property_cached-1.6.4-py2.py3-none-any.whl (7.8 kB)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (0.20.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (3.7.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (5.13.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.16.0)\nINFO: pip is looking at multiple versions of dython to determine which version is compatible with other requirements. This could take a while.\nCollecting dython&lt;0.8.0,&gt;=0.7.1 (from validmind)\n  Downloading dython-0.7.3-py3-none-any.whl (23 kB)\n  Downloading dython-0.7.2-py3-none-any.whl (22 kB)\nCollecting scikit-plot&gt;=0.3.7 (from dython&lt;0.8.0,&gt;=0.7.1-&gt;validmind)\n  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\nRequirement already satisfied: psutil&gt;=5.9.1 in /usr/local/lib/python3.10/dist-packages (from dython&lt;0.8.0,&gt;=0.7.1-&gt;validmind) (5.9.5)\nRequirement already satisfied: docutils&lt;0.20,&gt;=0.15 in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (0.16)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (3.1.2)\nRequirement already satisfied: markdown-it-py&lt;3.0.0,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (2.2.0)\nCollecting mdit-py-plugins~=0.3.4 (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind)\n  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.1/52.1 kB 6.6 MB/s eta 0:00:00\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (6.0)\nCollecting ydata-profiling (from pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading ydata_profiling-4.1.2-py2.py3-none-any.whl (345 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.9/345.9 kB 39.2 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;2.0.0,&gt;=1.9.1-&gt;validmind) (4.5.0)\nRequirement already satisfied: py4j&gt;=0.10.7 in /usr/local/lib/python3.10/dist-packages (from pypmml&lt;0.10.0,&gt;=0.9.17-&gt;validmind) (0.10.9.7)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (1.26.15)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (2022.12.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (2.0.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (3.4)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.2-&gt;validmind) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.2-&gt;validmind) (3.1.0)\nRequirement already satisfied: packaging&gt;20.9 in /usr/local/lib/python3.10/dist-packages (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (23.1)\nCollecting slicer==0.0.7 (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind)\n  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (0.56.4)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (2.2.1)\nRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.4)\nRequirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.2)\nRequirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.1)\nRequirement already satisfied: sphinxcontrib-htmlhelp&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (2.0.1)\nRequirement already satisfied: sphinxcontrib-serializinghtml&gt;=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.1.5)\nRequirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.3)\nCollecting docutils&lt;0.20,&gt;=0.15 (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind)\n  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 570.5/570.5 kB 43.3 MB/s eta 0:00:00\nRequirement already satisfied: snowballstemmer&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (2.2.0)\nRequirement already satisfied: babel&gt;=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (2.12.1)\nRequirement already satisfied: alabaster&lt;0.8,&gt;=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (0.7.13)\nRequirement already satisfied: imagesize&gt;=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.4.1)\nCollecting html2text (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\nCollecting pydash (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading pydash-7.0.3-py3-none-any.whl (109 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109.5/109.5 kB 13.0 MB/s eta 0:00:00\nCollecting unify (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading unify-0.5.tar.gz (4.4 kB)\n  Preparing metadata (setup.py) ... done\nCollecting yapf (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading yapf-0.33.0-py2.py3-none-any.whl (200 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.9/200.9 kB 1.4 MB/s eta 0:00:00\nCollecting docutils&lt;0.20,&gt;=0.15 (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind)\n  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 570.0/570.0 kB 33.8 MB/s eta 0:00:00\nCollecting sphinxcontrib-jquery!=3.0.0,&gt;=2.0.0 (from sphinx-rtd-theme&lt;2.0.0,&gt;=1.2.0-&gt;validmind)\n  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 15.4 MB/s eta 0:00:00\nRequirement already satisfied: patsy&gt;=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels&lt;0.14.0,&gt;=0.13.5-&gt;validmind) (0.5.3)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;ipython==7.34.0-&gt;validmind) (0.8.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (2.1.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&lt;3.0.0,&gt;=1.0.0-&gt;myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (0.1.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (4.39.3)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (8.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (3.0.9)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;ipython==7.34.0-&gt;validmind) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==7.34.0-&gt;validmind) (0.2.6)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba-&gt;shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (0.39.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (8.2.2)\nCollecting untokenize (from unify-&gt;sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading untokenize-0.1.1.tar.gz (3.1 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: tomli&gt;=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf-&gt;sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind) (2.0.1)\nCollecting scipy&gt;=1.3 (from arch&lt;6.0.0,&gt;=5.4.0-&gt;validmind)\n  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.7/33.7 MB 43.5 MB/s eta 0:00:00\nCollecting matplotlib (from catboost&lt;2.0,&gt;=1.2-&gt;validmind)\n  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 93.9 MB/s eta 0:00:00\nCollecting visions[type_image_path]==0.7.5 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.7/102.7 kB 13.8 MB/s eta 0:00:00\nCollecting htmlmin==0.1.12 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... done\nCollecting phik&lt;0.13,&gt;=0.11.1 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading phik-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 679.5/679.5 kB 56.2 MB/s eta 0:00:00\nCollecting tqdm&lt;5.0.0,&gt;=4.64.0 (from validmind)\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 9.7 MB/s eta 0:00:00\nCollecting multimethod&lt;1.10,&gt;=1.4 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading multimethod-1.9.1-py3-none-any.whl (10 kB)\nCollecting typeguard&lt;2.14,&gt;=2.13.2 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nCollecting imagehash==4.3.1 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.5/296.5 kB 35.5 MB/s eta 0:00:00\nRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind) (1.4.1)\nRequirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind) (23.1.0)\nRequirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind) (3.1)\nCollecting tangled-up-in-unicode&gt;=0.0.4 (from visions[type_image_path]==0.7.5-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 102.3 MB/s eta 0:00:00\nBuilding wheels for collected packages: pypmml, unify, htmlmin, untokenize\n  Building wheel for pypmml (setup.py) ... done\n  Created wheel for pypmml: filename=pypmml-0.9.17-py3-none-any.whl size=14215036 sha256=b063498209ef70ccfff6e68ff2443587e5783d68ee64efa51b180fd989759977\n  Stored in directory: /root/.cache/pip/wheels/8c/74/f1/946a04acaa6de2e9df0f02739511aba5a7aac52383c52ac900\n  Building wheel for unify (setup.py) ... done\n  Created wheel for unify: filename=unify-0.5-py3-none-any.whl size=5224 sha256=e90e221dffdeb63d42f729d865afa7754af0586422ac5ec5e53e897e22e9f0cf\n  Stored in directory: /root/.cache/pip/wheels/f1/d3/32/7f86dc94d89d1775b69018f1cb94e1ff77691cd676b1d6e99a\n  Building wheel for htmlmin (setup.py) ... done\n  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27081 sha256=342eecb3263f09426ad33a7f390ace1b4947999829e532973402b9de2cac8507\n  Stored in directory: /root/.cache/pip/wheels/dd/91/29/a79cecb328d01739e64017b6fb9a1ab9d8cb1853098ec5966d\n  Building wheel for untokenize (setup.py) ... done\n  Created wheel for untokenize: filename=untokenize-0.1.1-py3-none-any.whl size=2874 sha256=7726ac802ce0b6b6f57dd42d85e9a125c96ee99d0b8e780ac96de3bc00065728\n  Stored in directory: /root/.cache/pip/wheels/dd/b6/d4/187059c19a28026b81e54afd260a63aab2e7ccddf2e05977eb\nSuccessfully built pypmml unify htmlmin untokenize\nInstalling collected packages: untokenize, htmlmin, yapf, unify, typeguard, tqdm, tangled-up-in-unicode, slicer, python-dotenv, pypmml, pydash, property-cached, numpy, multimethod, jedi, html2text, docutils, sphinx, scipy, mdit-py-plugins, visions, sphinxcontrib-jquery, sphinx-markdown-builder, myst-parser, matplotlib, imagehash, sphinx-rtd-theme, shap, seaborn, scikit-plot, phik, catboost, arch, ydata-profiling, dython, pandas-profiling, validmind\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.65.0\n    Uninstalling tqdm-4.65.0:\n      Successfully uninstalled tqdm-4.65.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.22.4\n    Uninstalling numpy-1.22.4:\n      Successfully uninstalled numpy-1.22.4\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.16\n    Uninstalling docutils-0.16:\n      Successfully uninstalled docutils-0.16\n  Attempting uninstall: sphinx\n    Found existing installation: Sphinx 3.5.4\n    Uninstalling Sphinx-3.5.4:\n      Successfully uninstalled Sphinx-3.5.4\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.10.1\n    Uninstalling scipy-1.10.1:\n      Successfully uninstalled scipy-1.10.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.1\n    Uninstalling matplotlib-3.7.1:\n      Successfully uninstalled matplotlib-3.7.1\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\nSuccessfully installed arch-5.5.0 catboost-1.2 docutils-0.18.1 dython-0.7.2 html2text-2020.1.16 htmlmin-0.1.12 imagehash-4.3.1 jedi-0.18.2 matplotlib-3.6.3 mdit-py-plugins-0.3.5 multimethod-1.9.1 myst-parser-1.0.0 numpy-1.22.3 pandas-profiling-3.6.6 phik-0.12.3 property-cached-1.6.4 pydash-7.0.3 pypmml-0.9.17 python-dotenv-0.20.0 scikit-plot-0.3.7 scipy-1.9.3 seaborn-0.11.2 shap-0.41.0 slicer-0.0.7 sphinx-6.2.1 sphinx-markdown-builder-0.5.5 sphinx-rtd-theme-1.2.0 sphinxcontrib-jquery-4.1 tangled-up-in-unicode-0.2.0 tqdm-4.64.1 typeguard-2.13.3 unify-0.5 untokenize-0.1.1 validmind-1.11.7 visions-0.7.5 yapf-0.33.0 ydata-profiling-4.1.2\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue on to the next cell."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#initialize-the-client-library",
    "href": "notebooks/Introduction_Customer_Churn.html#initialize-the-client-library",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"API_SECRET\",\n  project = \"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n  \n\n\nInitializing Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#training-an-example-model",
    "href": "notebooks/Introduction_Customer_Churn.html#training-an-example-model",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Training an Example Model",
    "text": "Training an Example Model\nWe will now train an example model that will be used to demonstrate the ValidMind Developer Framework functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\nLoading demo dataset\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nPreparing the training dataset\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\nDropping irrelevant variables\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\nEncoding categorical variables\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\nDataset preparation\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\nModel training\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nNow that we are satisfied with our model, we can begin using the ValidMind Library to generate test and document it.\n\n\nViewing all test plans available in the developer framework\nWe can find all the test plans and tests available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nList all available tests: vm.test_plans.list_tests()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_data_quality\")\n\nHere is an example:\n\nvm.test_plans.list_plans()\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\n\nRunning a data quality test plan\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\n\n\nInitialize and run the TabularDataset test plan\nWe can now initialize the TabularDataset test suite. The primary method of doing this is with the run_test_suite function from the vm module. This function takes in a test suite name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Data Preparation” section of the model documentation.\n\n\n\nRunning a model evaluation test plan\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\nInitialize VM model object and train/test datasets\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nWe can now run the BinaryClassifierModelValidation test plan:\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html",
    "href": "notebooks/how_to/run_a_test_plan.html",
    "title": "Running an Individual Test Plan",
    "section": "",
    "text": "This notebook shows how to run an individual test plan and pass custom config parameters for the tests.\n%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\ncannot find .env file\nimport validmind as vm\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_plan.html#load-the-demo-dataset",
    "title": "Running an Individual Test Plan",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the Raw Dataset\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test_plan.html#train-a-model-for-testing",
    "title": "Running an Individual Test Plan",
    "section": "Train a Model for Testing",
    "text": "Train a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#import-and-run-the-individual-test-plan",
    "href": "notebooks/how_to/run_a_test_plan.html#import-and-run-the-individual-test-plan",
    "title": "Running an Individual Test Plan",
    "section": "Import and Run the Individual Test Plan",
    "text": "Import and Run the Individual Test Plan\n\nInitialize ValidMind objects\nWe initize the objects required to run test plans using the ValidMind framework\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nList of available Test Plans\nThe interface to show list of test plans available in the ValidMind development framework\n\nvm.test_plans.list_plans()\n\n\n\n\n\n\nID\nName\nDescription\n\n\n\n\nbinary_classifier_metrics\nBinaryClassifierMetrics\nTest plan for sklearn classifier metrics\n\n\nbinary_classifier_validation\nBinaryClassifierPerformance\nTest plan for sklearn classifier models\n\n\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nTest plan for sklearn classifier model diagnosis tests\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\ntabular_data_quality\nTabularDataQuality\nTest plan for data quality on tabular datasets\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest plan for data quality on time series datasets\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nTest plan to perform time series univariate analysis.\n\n\ntime_series_multivariate\nTimeSeriesMultivariate\nTest plan to perform time series multivariate analysis.\n\n\ntime_series_forecast\nTimeSeriesForecast\nTest plan to perform time series forecast tests.\n\n\nregression_model_description\nRegressionModelDescription\nTest plan for performance metric of regression model of statsmodels library\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\nDetail of an individual Test Plan\nThe interface will get detail of a specific test plan\n\nvm.test_plans.describe_plan(\"binary_classifier_model_diagnosis\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nTest plan for sklearn classifier model diagnosis tests\nmodel\nOverfitDiagnosis (ThresholdTest), WeakspotsDiagnosis (ThresholdTest), RobustnessDiagnosis (ThresholdTest)\n\n\n\n\n\n\n\nDefine the required config parameters\nThe config can be apply to specific test to override the default configuration parameters.\nThe format of a config is:\nconfig = {\n    \"&lt;test1_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n     \"&lt;test2_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n}\nUsers can input the configuration to test plan using config, allowing fine-tuning the suite according to their specific data requirements.\n\nconfig={\n    \"overfit_regions\": {\n        \"cut_off_percentage\": 3,\n        \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n    },\n    \"weak_spots\":{\n        \"features_columns\": [\"Age\", \"Balance\"],\n        \"accuracy_gap_threshold\": 85,\n    },\n    \"robustness\":{\n        \"features_columns\": [ \"Balance\", \"Tenure\"],\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        \"accuracy_decay_threshold\": 4,\n    }\n}\n\n\n\nRun the test plan and display results\n\nmodel_diagnosis_test_plan = vm.run_test_plan(\"binary_classifier_model_diagnosis\", \n                                             model=vm_model,\n                                             config=config)\n\n\n\n\n\n\n\n\n\nAccessing the test plan results\nWe can now access all the results of the test plan, including subtest plans using test_plan.get_results().\n\ntest_plan.get_results(): With no arguments, this returns a list of all results\ntest_plan.get_results(test_id): If provided with a test id, this returns the all results that match the given test id\n\nBy default, get_results() returns a list, in case there are multiple tests with the same id.\n\nmodel_diagnosis_test_plan.get_results()\n\n[TestPlanTestResult(result_id=\"overfit_regions\", test_results),\n TestPlanTestResult(result_id=\"weak_spots\", test_results),\n TestPlanTestResult(result_id=\"robustness\", test_results)]\n\n\n\nmodel_robustness = model_diagnosis_test_plan.get_results(\"robustness\")[0]\nmodel_robustness.show()"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "",
    "text": "This notebook aim to demostrate the list of interfaces available to get details of test suites, test plans and tests"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#initialize-validmind",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#initialize-validmind",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Initialize ValidMind",
    "text": "Initialize ValidMind\n\n%load_ext dotenv\n%dotenv .env\n\nimport validmind as vm\n\nvm.init(\n    api_host = \"http://localhost:3000/api/v1/tracking\",\n    project = \"clip4v3jl00031rry7h1bu5ul\"\n)\n\n2023-06-14 09:08:29,942 - INFO - api_client - Connected to ValidMind. Project: Test Project (clip4v3jl00031rry7h1bu5ul)\n\n\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Available test suites",
    "text": "Available test suites\nThe interface will provide the list of test suites available in the ValidMind framework\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\nbinary_classifier_model_validation\nBinaryClassifierModelValidation\nTest suite for binary classification models.\nbinary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\ntabular_dataset_description, tabular_data_quality\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast, time_series_sensitivity"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#test-plans",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#test-plans",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Test plans",
    "text": "Test plans\nThe list of test plans available in a given test suite\n\nvm.test_suites.describe_suite(\"binary_classifier_full_suite\")\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\n\n\n\n\nTest plan description and list of tests\nThe list of tests avaiable in a specific test plan\n\nvm.test_plans.describe_plan(\"tabular_dataset_description\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n['dataset']\nDatasetMetadata (DatasetMetadata)\nDatasetDescription (Metric)\nDescriptiveStatistics (Metric)\nPearsonCorrelationMatrix (Metric)\nDatasetCorrelations (Metric)\n\n\n\n\n\n\n\nTest detail\n\nvm.tests.describe_test('DescriptiveStatistics')\n\n\n\n\n\n\nID\nTest Type\nName\nDescription\n\n\n\n\nvalidmind.data_validation.DescriptiveStatistics\nMetric\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\n\n\n\n\n\nDetails of test suites, test plans and tests\nThis interface provide comprehensive details of test suites, test plans and tests\n\nvm.test_suites.describe_suite(\"binary_classifier_full_suite\", verbose=True)\n\n\n\n\n\n\nTest Suite ID\nTest Suite Name\nTest Plan ID\nTest Plan Name\nTest ID\nTest Name\nTest Type\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_dataset_description\nTabularDatasetDescription\nvalidmind.data_validation.DatasetMetadata\nDatasetMetadata\nDatasetMetadata\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_dataset_description\nTabularDatasetDescription\nvalidmind.data_validation.DatasetDescription\nDatasetDescription\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_dataset_description\nTabularDatasetDescription\nvalidmind.data_validation.DescriptiveStatistics\nDescriptiveStatistics\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_dataset_description\nTabularDatasetDescription\nvalidmind.data_validation.PearsonCorrelationMatrix\nPearsonCorrelationMatrix\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_dataset_description\nTabularDatasetDescription\nvalidmind.data_validation.DatasetCorrelations\nDatasetCorrelations\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.ClassImbalance\nClassImbalance\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.Duplicates\nDuplicates\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.HighCardinality\nHighCardinality\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.HighPearsonCorrelation\nHighPearsonCorrelation\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.MissingValues\nMissingValues\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.Skewness\nSkewness\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.UniqueRows\nUniqueRows\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\ntabular_data_quality\nTabularDataQuality\nvalidmind.data_validation.TooManyZeroValues\nTooManyZeroValues\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.ModelMetadata\nModelMetadata\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.data_validation.DatasetSplit\nDatasetSplit\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.ConfusionMatrix\nConfusionMatrix\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\nClassifierInSamplePerformance\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\nClassifierOutOfSamplePerformance\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\nPermutationFeatureImportance\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\nPrecisionRecallCurve\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.ROCCurve\nROCCurve\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\nPopulationStabilityIndex\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_metrics\nBinaryClassifierMetrics\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\nSHAPGlobalImportance\nMetric\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_validation\nBinaryClassifierPerformance\nvalidmind.model_validation.sklearn.MinimumAccuracy\nMinimumAccuracy\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_validation\nBinaryClassifierPerformance\nvalidmind.model_validation.sklearn.MinimumF1Score\nMinimumF1Score\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_validation\nBinaryClassifierPerformance\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\nMinimumROCAUCScore\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_validation\nBinaryClassifierPerformance\nvalidmind.model_validation.sklearn.TrainingTestDegradation\nTrainingTestDegradation\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nvalidmind.model_validation.sklearn.OverfitDiagnosis\nOverfitDiagnosis\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\nWeakspotsDiagnosis\nThresholdTest\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\nRobustnessDiagnosis\nThresholdTest"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites-1",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites-1",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Available test suites",
    "text": "Available test suites\n\nvm.tests.list_tests()\n\n\n\n\n\n\nTest Type\nName\nDescription\nID\n\n\n\n\nMetric\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\nvalidmind.model_validation.ModelMetadata\n\n\nMetric\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\nvalidmind.model_validation.sklearn.ClassifierOutOfSamplePerformance\n\n\nThresholdTest\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\nvalidmind.model_validation.sklearn.RobustnessDiagnosis\n\n\nMetric\nSHAPGlobalImportance\nSHAP Global Importance\nvalidmind.model_validation.sklearn.SHAPGlobalImportance\n\n\nMetric\nConfusionMatrix\nConfusion Matrix\nvalidmind.model_validation.sklearn.ConfusionMatrix\n\n\nMetric\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\nvalidmind.model_validation.sklearn.ClassifierInSamplePerformance\n\n\nThresholdTest\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\nvalidmind.model_validation.sklearn.OverfitDiagnosis\n\n\nMetric\nPermutationFeatureImportance\nPermutation Feature Importance\nvalidmind.model_validation.sklearn.PermutationFeatureImportance\n\n\nThresholdTest\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumROCAUCScore\n\n\nMetric\nPrecisionRecallCurve\nPrecision Recall Curve\nvalidmind.model_validation.sklearn.PrecisionRecallCurve\n\n\nMetric\nClassifierPerformance\nTest that outputs the performance of the model on the training or test data.\nvalidmind.model_validation.sklearn.ClassifierPerformance\n\n\nThresholdTest\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumF1Score\n\n\nMetric\nROCCurve\nROC Curve\nvalidmind.model_validation.sklearn.ROCCurve\n\n\nThresholdTest\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\nvalidmind.model_validation.sklearn.TrainingTestDegradation\n\n\nThresholdTest\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques.\nvalidmind.model_validation.sklearn.WeakspotsDiagnosis\n\n\nMetric\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\nvalidmind.model_validation.sklearn.PopulationStabilityIndex\n\n\nThresholdTest\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\nvalidmind.model_validation.sklearn.MinimumAccuracy\n\n\nMetric\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\nvalidmind.model_validation.statsmodels.RegressionModelsCoeffs\n\n\nMetric\nBoxPierce\nThe Box-Pierce test is a statistical test used to determine whether a given set of data has autocorrelations that are different from zero.\nvalidmind.model_validation.statsmodels.BoxPierce\n\n\nMetric\nRegressionModelSensitivityPlot\nThis metric performs sensitivity analysis applying shocks to one variable at a time.\nvalidmind.model_validation.statsmodels.RegressionModelSensitivityPlot\n\n\nMetric\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\nvalidmind.model_validation.statsmodels.RegressionModelsPerformance\n\n\nMetric\nZivotAndrewsArch\nZivot-Andrews unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.ZivotAndrewsArch\n\n\nMetric\nRegressionModelOutsampleComparison\nTest that evaluates the performance of different regression models on a separate test dataset that was not used to train the models.\nvalidmind.model_validation.statsmodels.RegressionModelOutsampleComparison\n\n\nMetric\nRegressionModelForecastPlotLevels\nThis metric creates a plot of forecast vs observed for each model in the list.\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\n\n\nMetric\nLJungBox\nThe Ljung-Box test is a statistical test used to determine whether a given set of data has autocorrelations that are different from zero.\nvalidmind.model_validation.statsmodels.LJungBox\n\n\nMetric\nJarqueBera\nThe Jarque-Bera test is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.JarqueBera\n\n\nMetric\nPhillipsPerronArch\nPhillips-Perron (PP) unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.PhillipsPerronArch\n\n\nMetric\nKolmogorovSmirnov\nThe Kolmogorov-Smirnov metric is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.KolmogorovSmirnov\n\n\nMetric\nResidualsVisualInspection\nLog plots for visual inspection of residuals\nvalidmind.model_validation.statsmodels.ResidualsVisualInspection\n\n\nMetric\nShapiroWilk\nThe Shapiro-Wilk test is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.ShapiroWilk\n\n\nMetric\nRegressionModelInsampleComparison\nTest that output the comparison of stats library regression models.\nvalidmind.model_validation.statsmodels.RegressionModelInsampleComparison\n\n\nMetric\nRegressionModelSummary\nTest that output the summary of regression models of statsmodel library.\nvalidmind.model_validation.statsmodels.RegressionModelSummary\n\n\nMetric\nKPSS\nKwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.KPSS\n\n\nMetric\nLilliefors\nThe Lilliefors test is a statistical test used to determine whether a given set of data follows a normal distribution.\nvalidmind.model_validation.statsmodels.Lilliefors\n\n\nMetric\nRunsTest\nThe runs test is a statistical test used to determine whether a given set of data has runs of positive and negative values that are longer than expected under the null hypothesis of randomness.\nvalidmind.model_validation.statsmodels.RunsTest\n\n\nMetric\nDFGLSArch\nDickey-Fuller GLS unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.DFGLSArch\n\n\nMetric\nAutoARIMA\nAutomatically fits multiple ARIMA models for each variable and ranks them by BIC and AIC.\nvalidmind.model_validation.statsmodels.AutoARIMA\n\n\nThresholdTest\nADFTest\nAugmented Dickey-Fuller Metric for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.ADFTest\n\n\nMetric\nRegressionModelForecastPlot\nThis metric creates a plot of forecast vs observed for each model in the list.\nvalidmind.model_validation.statsmodels.RegressionModelForecastPlot\n\n\nMetric\nADF\nAugmented Dickey-Fuller unit root test for establishing the order of integration of time series\nvalidmind.model_validation.statsmodels.ADF\n\n\nMetric\nDurbinWatsonTest\nThe Durbin-Watson Metric is a statistical test that can be used to detect autocorrelation in a time series.\nvalidmind.model_validation.statsmodels.DurbinWatsonTest\n\n\nThresholdTest\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\nvalidmind.data_validation.Skewness\n\n\nThresholdTest\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\nvalidmind.data_validation.Duplicates\n\n\nMetric\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\nvalidmind.data_validation.DatasetDescription\n\n\nMetric\nScatterPlot\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\nvalidmind.data_validation.ScatterPlot\n\n\nThresholdTest\nTimeSeriesOutliers\nTest that find outliers for time series data using the z-score method\nvalidmind.data_validation.TimeSeriesOutliers\n\n\nMetric\nTabularCategoricalBarPlots\nGenerates a visual analysis of categorical data by plotting bar plots. The input dataset can have multiple categorical variables if necessary. In this case, we produce a separate plot for each categorical variable.\nvalidmind.data_validation.TabularCategoricalBarPlots\n\n\nMetric\nAutoStationarity\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\nvalidmind.data_validation.AutoStationarity\n\n\nMetric\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\nvalidmind.data_validation.DescriptiveStatistics\n\n\nMetric\nPearsonCorrelationMatrix\nExtracts the Pearson correlation coefficient for all pairs of numerical variables in the dataset. This metric is useful to identify highly correlated variables that can be removed from the dataset to reduce dimensionality.\nvalidmind.data_validation.PearsonCorrelationMatrix\n\n\nMetric\nTabularNumericalHistograms\nGenerates a visual analysis of numerical data by plotting the histogram. The input dataset can have multiple numerical variables if necessary. In this case, we produce a separate plot for each numerical variable.\nvalidmind.data_validation.TabularNumericalHistograms\n\n\nThresholdTest\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\nvalidmind.data_validation.HighCardinality\n\n\nThresholdTest\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\nvalidmind.data_validation.MissingValues\n\n\nMetric\nRollingStatsPlot\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\nvalidmind.data_validation.RollingStatsPlot\n\n\nMetric\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\nvalidmind.data_validation.DatasetCorrelations\n\n\nMetric\nTabularDescriptionTables\nCollects a set of descriptive statistics for a tabular dataset, for numerical, categorical and datetime variables.\nvalidmind.data_validation.TabularDescriptionTables\n\n\nMetric\nAutoMA\nAutomatically detects the MA order of a time series using both BIC and AIC.\nvalidmind.data_validation.AutoMA\n\n\nThresholdTest\nUniqueRows\nTest that the number of unique rows is greater than a threshold\nvalidmind.data_validation.UniqueRows\n\n\nThresholdTest\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\nvalidmind.data_validation.TooManyZeroValues\n\n\nThresholdTest\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\nvalidmind.data_validation.HighPearsonCorrelation\n\n\nMetric\nACFandPACFPlot\nPlots ACF and PACF for a given time series dataset.\nvalidmind.data_validation.ACFandPACFPlot\n\n\nThresholdTest\nTimeSeriesFrequency\nTest that detect frequencies in the data\nvalidmind.data_validation.TimeSeriesFrequency\n\n\nMetric\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\nvalidmind.data_validation.DatasetSplit\n\n\nMetric\nSpreadPlot\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\nvalidmind.data_validation.SpreadPlot\n\n\nMetric\nTimeSeriesLinePlot\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\nvalidmind.data_validation.TimeSeriesLinePlot\n\n\nMetric\nAutoSeasonality\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\nvalidmind.data_validation.AutoSeasonality\n\n\nMetric\nEngleGrangerCoint\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\nvalidmind.data_validation.EngleGrangerCoint\n\n\nThresholdTest\nTimeSeriesMissingValues\nTest that the number of missing values is less than a threshold\nvalidmind.data_validation.TimeSeriesMissingValues\n\n\nDatasetMetadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\nvalidmind.data_validation.DatasetMetadata\n\n\nMetric\nTimeSeriesHistogram\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\nvalidmind.data_validation.TimeSeriesHistogram\n\n\nMetric\nLaggedCorrelationHeatmap\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\nvalidmind.data_validation.LaggedCorrelationHeatmap\n\n\nMetric\nSeasonalDecompose\nCalculates seasonal_decompose metric for each of the dataset features\nvalidmind.data_validation.SeasonalDecompose\n\n\nThresholdTest\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\nvalidmind.data_validation.ClassImbalance\n\n\nMetric\nAutoAR\nAutomatically detects the AR order of a time series using both BIC and AIC.\nvalidmind.data_validation.AutoAR\n\n\nMetric\nTabularDateTimeHistograms\nGenerates a visual analysis of datetime data by plotting histograms of differences between consecutive dates. The input dataset can have multiple datetime variables if necessary. In this case, we produce a separate plot for each datetime variable.\nvalidmind.data_validation.TabularDateTimeHistograms"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html",
    "href": "notebooks/how_to/run_a_test.html",
    "title": "Running an Individual Test",
    "section": "",
    "text": "This notebook shows how to run individual metrics or thresholds tests.\n%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\ncannot find .env file\nimport validmind as vm\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "title": "Running an Individual Test",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the Raw Dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "title": "Running an Individual Test",
    "section": "Train a Model for Testing",
    "text": "Train a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#import-and-run-the-individual-test",
    "href": "notebooks/how_to/run_a_test.html#import-and-run-the-individual-test",
    "title": "Running an Individual Test",
    "section": "Import and Run the Individual Test",
    "text": "Import and Run the Individual Test\n\nInitialize ValidMind objects\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nImport the individual test\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.model_validation.sklearn.threshold_tests import TrainingTestDegradation\n\n\n\nPass the required context and config parameters\n\ntest_context = TestContext(model=vm_model)\nws_diagnostic = TrainingTestDegradation(test_context)\n\n\n\nRun the test\n\nws_diagnostic.run()\n\nTestPlanTestResult(result_id=\"training_test_degradation\", test_results)\n\n\n\n\nDisplay results\n\nws_diagnostic.result.show()"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_demo.html",
    "href": "notebooks/nlp/nlp_sentiment_analysis_demo.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "This notebook aims to present NLP data analysis and binary classification using the PyTorch library on COVID-19 tweets. The emphasis is on the in-depth analysis and preprocessing of the text data (tweets). In the first section, the notebook introduces the manually tagged COVID-19 tweets, which range from Highly Negative to Highly Positive, representing five distinct classes. In this Exploratory Data Analysis (EDA), these five classes will be simplified to two classes: Positive and Negative."
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_demo.html#natural-language-processing-analysis-binary-classification-using-pytorch",
    "href": "notebooks/nlp/nlp_sentiment_analysis_demo.html#natural-language-processing-analysis-binary-classification-using-pytorch",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "This notebook aims to present NLP data analysis and binary classification using the PyTorch library on COVID-19 tweets. The emphasis is on the in-depth analysis and preprocessing of the text data (tweets). In the first section, the notebook introduces the manually tagged COVID-19 tweets, which range from Highly Negative to Highly Positive, representing five distinct classes. In this Exploratory Data Analysis (EDA), these five classes will be simplified to two classes: Positive and Negative."
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_demo.html#explorary-data-analysis-of-covid-tweets-data",
    "href": "notebooks/nlp/nlp_sentiment_analysis_demo.html#explorary-data-analysis-of-covid-tweets-data",
    "title": "Sensitivity Analysis",
    "section": "1. Explorary Data Analysis of Covid tweets data",
    "text": "1. Explorary Data Analysis of Covid tweets data\n\nValidMind project connection\n\n%load_ext dotenv\n%dotenv .env\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"cliop8llc003x32rlklophmdl\"\n)\n\n\n\nLoad library\n\n%set_env PYTORCH_MPS_HIGH_WATERMARK_RATIO 0.8\n\nimport pandas as pd\nimport numpy as np\nimport os\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\ndevice = \"cpu\"\n\ntrain_model = True\n\n\n\nLoad covid-19 tweets data\n\nfrom validmind.datasets.nlp import twitter_covid_19 as demo_data\ndf = demo_data.load_data()\ndf.head(10)\n\n\n\nRun text data quality test plan\n\nvm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='OriginalTweet', target_column=\"Sentiment\")\n\n\nconfig = {\n    \"class_imbalance\":{\"min_percent_threshold\": 3}\n}\ntext_data_test_plan = vm.run_test_plan(\"text_data_quality\",\n                                       dataset=vm_ds,\n                                       config=config)"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_demo.html#preprocess-data",
    "href": "notebooks/nlp/nlp_sentiment_analysis_demo.html#preprocess-data",
    "title": "Sensitivity Analysis",
    "section": "2. Preprocess data",
    "text": "2. Preprocess data\n\nHandle class bias\nOne way to handle class bias is to merge a specific class data with related class. Here, we will copy the text and class lables in separate columns so that the original text is also there for comparison.\n\nprint(\"Original Classes:\", df.Sentiment.unique())\n\ndf['text'] = df.OriginalTweet\ndf[\"text\"] = df[\"text\"].astype(str)\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"positive\"\n    elif x == \"Extremely Negative\":\n        return \"negative\"\n    elif x == \"Negative\":\n        return \"negative\"\n    elif x ==  \"Positive\":\n        return \"positive\"\n    else:\n        return \"neutral\"\n    \ndf['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\ntarget=df['sentiment']\n\nprint(df.sentiment.value_counts(normalize= True))\nprint(\"Modified Classes:\", df.sentiment.unique())\n\n\n\nRemove neutral class\n\ndf = df[df[\"sentiment\"] != \"neutral\"]\nprint(df.sentiment.unique())\nprint(df.sentiment.value_counts(normalize= True))\nprint(df.shape)\n\n\ndf\n\n\n\nRemove urls and html links\n\n#Remove Urls and HTML links\nimport re\n\ndef remove_urls(text):\n    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\n\ndf['text']=df['text'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'&lt;.*?&gt;')\n    return html.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x:remove_html(x))\n\n\n\nConvert text to lower case\n\n# Lower casing\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['text']=df['text'].apply(lambda x:lower(x))\n\n\n\nRemove numbers\n\n# Number removal\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['text']=df['text'].apply(lambda x:remove_num(x))\n\n\n\nRemove stopwords\n\n#Remove stopwords\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['text']=df['text'].apply(lambda x:remove_stopwords(x))\n\n\n\nRemove Punctuations\n\n#Remove Punctuations\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['text']=df['text'].apply(lambda x:punct_remove(x))\n\n\n\nRemove mentions\n\n#Remove mentions \ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['text']=df['text'].apply(lambda x:remove_mention(x))\n\n\n\nRemove hashtags\n\n#Remove hashtags \n\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['text']=df['text'].apply(lambda x:remove_hash(x))\n\n\n\nRemove extra white space left while removing stuff\n\n#Remove extra white space left while removing stuff\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['text']=df['text'].apply(lambda x:remove_space(x))\n\n\ndf\n\n\n\nRun text data quality tests again\n\nvm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='text', target_column=\"sentiment\")\n\nconfig = {\n    \"class_imbalance\":{\"min_percent_threshold\": 3}\n}\ntext_data_test_plan = vm.run_test_plan(\"text_data_quality\",\n                                       dataset=vm_ds,\n                                       config=config)"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_demo.html#feature-engineering",
    "href": "notebooks/nlp/nlp_sentiment_analysis_demo.html#feature-engineering",
    "title": "Sensitivity Analysis",
    "section": "3. Feature Engineering",
    "text": "3. Feature Engineering\n\nEncoding the words\nThe embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our tweets into integers so they can be passed into the network.\nNow you’re going to encode the words with integers. Build a dictionary that maps words to integers. Later we’re going to pad our input vectors with zeros, so make sure the integers start at 1, not 0. Also, convert the tweets to integers and store the tweets in a new list called tweets_ints.\n\nText to words\n\nall_text = ' '.join(df.text)\n# create a list of words\nwords = all_text.split()\n\n\n\nBuild dictionary and map words to integers\n\n# feel free to use this import \nfrom collections import Counter\n\n## Build a dictionary that maps words to integers\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab,1)} \nvocab[1:10]\n\n\n\nTokenize each tweet\n\n## use the dict to tokenize each tweet in tweets_split\n## store the tokenized tweets in tweets_ints\ntweets_ints = []\nfor tweet in df.text:\n  tweets_ints.append([vocab_to_int[word] for word in tweet.split()])\n\n\n# stats about vocabulary\nprint('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\nprint()\n\n# print tokens in first tweet\nprint('Tokenized tweet: \\n', tweets_ints[:1])\nprint(len(tweets_ints))\n\n\n\n\nEncoding the labels\nOur labels are “positive” or “negative”. To use these labels in our network, we need to convert them to 0 and 1.\nConvert labels from positive and negative to 1 and 0, respectively, and place those in a new list, encoded_labels.\n\n# 1=positive, 0=negative label conversion\nimport numpy as np\n\nlabels_split = df.sentiment.values\nencoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\nprint(len(encoded_labels))\n\n\nPadding sequences\nTo deal with both short and very long tweets, we’ll pad or truncate all our tweets to a specific length. For tweets shorter than some seq_length, we’ll pad with 0s. For tweets longer than seq_length, we can truncate them to the first seq_length words. A good seq_length, in this case, is 200.\nDefine a function that returns an array features that contains the padded data, of a standard size, that we’ll pass to the network. * The data should come from tweet_ints, since we want to feed integers to the network. * Each row should be seq_length elements long. * For tweets longer than seq_length, use only the first seq_length words as the feature vector.\nAs a small example, if the seq_length=10 and an input tweet is:\n[117, 18, 128]\nThe resultant, padded sequence should be:\n[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\nYour final features array should be a 2D array, with as many rows as there are tweets, and as many columns as the specified seq_length.\nThis isn’t trivial and there are a bunch of ways to do this. But, if you’re going to be building your own deep learning networks, you’re going to have to get used to preparing your data.\n\ndef pad_features(tweets_ints, seq_length):\n    ''' Return features of tweet_ints, where each tweet is padded with 0's \n        or truncated to the input seq_length.\n    '''\n    ## getting the correct rows x cols shape\n    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n    \n    ## for each tweet, I grab that tweet\n    for i, row in enumerate(tweets_ints):\n      features[i, -len(row):] = np.array(row)[:seq_length]\n    \n    return features\n\n\n# Test your implementation!\n\nseq_length = 100\n\nfeatures = pad_features(tweets_ints, seq_length=seq_length)\n\n## test statements - do not change - ##\nassert len(features)==len(tweets_ints), \"Your features should have as many rows as tweets.\"\nassert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n\n# print first 10 values of the first 30 batches \nprint(features[:10,-25:])\nfeatures = features[0:len(features)-23]\nencoded_labels = encoded_labels[0:len(encoded_labels)-23] \nprint(len(features),len(encoded_labels))"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_demo.html#modeling",
    "href": "notebooks/nlp/nlp_sentiment_analysis_demo.html#modeling",
    "title": "Sensitivity Analysis",
    "section": "4. Modeling",
    "text": "4. Modeling\n\nTraining, validation, test\nWith our data in nice shape, we’ll split it into training, validation, and test sets.\nCreate the training, validation, and test sets. * You’ll need to create sets for the features and the labels, train_x and train_y, for example. * Define a split fraction, split_frac as the fraction of data to keep in the training set. Usually this is set to 0.8 or 0.9. * Whatever data is left will be split in half to create the validation and testing data.\n\nsplit_frac = 0.8\n\n## split data into training, validation, and test data (features and labels, x and y)\nsplit_idx = 25000\ntrain_x, remaining_x = features[:split_idx], features[split_idx:]\ntrain_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)* 0.53449)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\n## print out the shapes of your resultant feature data\nprint(\"\\t\\t\\tFeatures Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape),\n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n\n\n\n\nDataloaders and batching\nAfter creating training, test, and validation data, we can create DataLoaders for this data by following two steps: 1. Create a known format for accessing our data, using TensorDataset which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset. 2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\nThis is an alternative to creating a generator function for batching our data into full batches.\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x).to(device), torch.from_numpy(train_y).to(device))\nvalid_data = TensorDataset(torch.from_numpy(val_x).to(device), torch.from_numpy(val_y).to(device))\ntest_data = TensorDataset(torch.from_numpy(test_x).to(device), torch.from_numpy(test_y).to(device))\n\n# dataloaders\nbatch_size = 50\n\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n\n# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = next(dataiter)\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)\n\n\n\nSentiment network with PyTorch\nBelow is where you’ll define the network. ### Network architecture\nThe architecture for this network is shown below.\n    Input (Word Tokens)\" --&gt; \"Embedding Layer\" --&gt; \"LSTM Layer\" --&gt; \"Fully-Connected Layer\" --&gt; \"Sigmoid Activation\" --&gt; \"Output (Last Sigmoid)\";\nFirst, we’ll pass in words to an embedding layer. We need an embedding layer because we have tens of thousands of words, so we’ll need a more efficient representation for our input data than one-hot encoded vectors. You should have seen this before from the Word2Vec lesson. You can actually train an embedding with the Skip-gram Word2Vec model and use those embeddings as input, here. However, it’s good enough to just have an embedding layer and let the network learn a different embedding table on its own.\nAfter input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells. The LSTM cells will add recurrent connections to the network and give us the ability to include information about the sequence of words in the covid twitter data.\nFinally, the LSTM outputs will go to a sigmoid output layer. We’re using a sigmoid function because positive and negative = 1 and 0, respectively, and a sigmoid will output predicted, sentiment values between 0-1.\nWe don’t care about the sigmoid outputs except for the very last one; we can ignore the rest. We’ll calculate the loss by comparing the output at the last time step and the training label (pos or neg).\nThe layers are as follows: 1. An embedding layer that converts our word tokens (integers) into embeddings of a specific size. 2. An lstm layer defined by a hidden_state size and number of layers 3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size 4. A sigmoid activation layer which turns all outputs into a value 0-1; return only the last sigmoid output as the output of this network.\n\n\nThe embedding layer\nWe need to add an embedding layer because there are 53000+ words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it’s fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.\n\n\nThe LSTM layer(s)\nWe’ll create an LSTM to use in our recurrent network, which takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\nMost of the time, you’re network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships.\nComplete the __init__, forward, and init_hidden functions for the SentimentRNN model class.\nNote: init_hidden should initialize the hidden and cell state of an lstm layer to all zeros, and move those state to GPU, if available.\n\nif(lower(device) == \"gpu\"):\n    print('Training on GPU.')\nelif (lower(device) == \"mps\"):\n    print('Training on mps.')\nelse:\n    print('No GPU available, training on CPU.')\n\n\nimport torch.nn as nn\n\nclass SentimentRNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.5)\n        \n        # linear and sigmoid layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n        \n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if(lower(device) == \"gpu\"):\n          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        elif(lower(device) == \"mps\"):\n           hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        else:\n          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n    \n    def predict(self, x_data):\n\n        test_loader = DataLoader(x_data, shuffle=False, batch_size=batch_size)\n\n        # init hidden state\n        h = self.init_hidden(batch_size)\n\n        self.eval()\n        predictions = torch.empty((0), dtype=torch.float32)\n\n        # iterate over test data\n        for inputs in test_loader:\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            if(lower(device) == \"gpu\"):\n                inputs = inputs.cuda()\n            if(lower(device) == \"mps\"):\n                inputs = inputs.to(device)\n            # get predicted outputs\n            output, h = self(inputs, h)\n            \n            # convert output probabilities to predicted class (0 or 1)\n            pred = torch.round(output.squeeze())  # rounds to the nearest integer\n            \n            # compare predictions to true label\n            # correct_tensor = pred.eq(labels.float().view_as(pred))\n            if(lower(device) == \"mps\"):\n                pred = pred.cpu()\n            elif lower(device) == \"gpu\":\n                pred = pred\n            else:\n                pred = pred.cpu()\n            predictions = torch.cat((predictions, pred), 0)\n\n        return predictions.detach().numpy()\n\n\nWeights and hyper parameters\n\n# Instantiate the model w/ hyperparams\nvocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\noutput_size = 1\nembedding_dim = 400 \nhidden_dim = 256\nn_layers = 4\n\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)\n\n\n\nLoss and optimization functions\n\n# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n\nif train_model:\n    # training params\n\n    epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n\n    counter = 0\n    print_every = 100\n    clip=5 # gradient clipping\n\n    # move model to GPU, if available\n    if(lower(device) == \"gpu\"):\n        net.cuda()\n    if(lower(device) == \"mps\"):\n        net = net.to(device)\n\n    net.train()\n    # train for some number of epochs\n    for e in range(epochs):\n        # initialize hidden state\n        h = net.init_hidden(batch_size)\n\n        # batch loop\n        for inputs, labels in train_loader:\n            counter += 1\n\n            if(lower(device) == \"gpu\"):\n                inputs, labels = inputs.cuda(), labels.cuda()\n            if(lower(device) == \"mps\"):\n                inputs, labels = inputs.to(device), labels.to(device)\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            net.zero_grad()\n\n            # get the output from the model\n            output, h = net(inputs, h)\n\n            # calculate the loss and perform backprop\n            loss = criterion(output.squeeze(), labels.float())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            optimizer.step()\n\n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for inputs, labels in valid_loader:\n\n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n\n                    if(lower(device) == \"gpu\"):\n                        inputs, labels = inputs.cuda(), labels.cuda()\n                    if(lower(device) == \"mps\"):\n                        inputs, labels = inputs.to(device), labels.to(device)\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output.squeeze(), labels.float())\n\n                    val_losses.append(val_loss.item())\n\n                net.train()\n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                    \"Step: {}...\".format(counter),\n                    \"Loss: {:.6f}...\".format(loss.item()),\n                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n\n    torch.save(net, os.path.join('./model/', 'model.pt'))\n\n\nnet = torch.load(os.path.join('./model/', 'model.pt'))\nnet = net.to(device)\nnet.eval()\n\n\n\nTraning accuracy\n\ndef compute_accuracy(net, data_loader, batch_size, device, criterion):\n    # Get test data loss and accuracy\n\n    test_losses = [] # track loss\n    num_correct = 0\n\n    # init hidden state\n    h = net.init_hidden(batch_size)\n\n    net.eval()\n    # iterate over test data\n    for inputs, labels in data_loader:\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        if(lower(device) == \"gpu\"):\n            inputs, labels = inputs.cuda(), labels.cuda()\n        if(lower(device) == \"mps\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n        # get predicted outputs\n        output, h = net(inputs, h)\n        \n        # calculate loss\n        test_loss = criterion(output.squeeze(), labels.float())\n        test_losses.append(test_loss.item())\n        \n        # convert output probabilities to predicted class (0 or 1)\n        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n        \n        # compare predictions to true label\n        correct_tensor = pred.eq(labels.float().view_as(pred))\n        if(lower(device) == \"mps\"):\n            correct = np.squeeze(correct_tensor.cpu().numpy())\n        elif lower(device) == \"gpu\":\n            correct = np.squeeze(correct_tensor.numpy())\n        else:\n            correct = np.squeeze(correct_tensor.cpu().numpy())\n\n        num_correct += np.sum(correct)\n\n\n    # -- stats! -- ##\n    # avg test loss\n    avg_loss = np.mean(test_losses)\n    test_acc = num_correct/len(data_loader.dataset) * 100\n    return test_acc, avg_loss\n\n\ntrain_model = True\nif train_model:\n    training_accuracy, avg_loss  = compute_accuracy(net, train_loader, batch_size, device, criterion)\n    print(f\"Training loss: {avg_loss}\")\n    print(f\"Training accuracy: {training_accuracy}\")\n\n\n\nTest accuracy\n\nif train_model:\n    test_accuracy, avg_loss  = compute_accuracy(net, test_loader, batch_size, device, criterion)\n    print(f\"Test loss: {avg_loss}\")\n    print(f\"Test accuracy: {test_accuracy}\")\n\n\n\n\nInitialize validmind objects\n\ntrain_data = TensorDataset(torch.from_numpy(train_x[:15000]).to(device), torch.from_numpy(train_y[:15000]).to(device))\n\nvm_train_ds = vm.init_dataset(dataset=train_data, type=\"generic\")\n\n\nvm_test_ds = vm.init_dataset(dataset=test_data, type=\"generic\")\n\n\n\nvm_model = vm.init_model(net, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n\nRun model metrics test plan\n\nmodel_metrics_test_plan = vm.run_test_plan(\"binary_classifier_metrics\", \n                                             model=vm_model\n                                            )\n\n\n\nRun model validation test plan\n\n\nmodel_validation_test_plan = vm.run_test_plan(\"binary_classifier_validation\", \n                                             model=vm_model\n                                            )"
  },
  {
    "objectID": "guide/add-content-blocks.html",
    "href": "guide/add-content-blocks.html",
    "title": "Add content blocks to documentation",
    "section": "",
    "text": "Learn how to add a new content block to your documentation project, to write and update your model’s documentation."
  },
  {
    "objectID": "guide/add-content-blocks.html#prerequisites",
    "href": "guide/add-content-blocks.html#prerequisites",
    "title": "Add content blocks to documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nYou are logged into the ValidMind Platform as a model developer"
  },
  {
    "objectID": "guide/add-content-blocks.html#steps",
    "href": "guide/add-content-blocks.html#steps",
    "title": "Add content blocks to documentation",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project you want to edit.\nInside your documentation project, navigate to the Documentation page.\nSelect one of the numbered sections, such as 1.1 Model Overview.\n\nIn your documentation, hover your mouse over the space where you want your new block to go until a horizontal dashed line with a  sign appears that indicates you can insert a new block:\n\nClick  and then select one of the available options:\n\nSimple text block: Adds a new section with a blank content block. After the new content block has been added, click  to edit the contents of the section like any other.\nTest-driven block: Select one of the options:\n\nMetric: Select one of the available metrics, such as Confusion Matrix.\nThreshold test: Select one of the available threshold tests, such as Data Quality: Skewness or Model Diagnosis: Overfit Regions.\n\n\nFor test-driven blocks, a preview of the available metrics or threshold test gets shown. Click Insert module when you are ready.\n\nAfter you have completed these steps, the new content block becomes a part of your model documentation."
  },
  {
    "objectID": "guide/add-content-blocks.html#related-topics",
    "href": "guide/add-content-blocks.html#related-topics",
    "title": "Add content blocks to documentation",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Validmind UI"
  },
  {
    "objectID": "guide/add-content-blocks.html#what-are-content-blocks",
    "href": "guide/add-content-blocks.html#what-are-content-blocks",
    "title": "Add content blocks to documentation",
    "section": "What are content blocks?",
    "text": "What are content blocks?\nContent blocks provide you with sections that are part of a template. You can think of these sections as an empty canvas that you fill in with text, metrics, and test results. Multiple sections are joined to create a longer document with a table of contents that has different heading and subheading levels, such as 1., 1.1., and so on.\nTypes of content blocks:\n\nSimple text block\n\nCan be added anywhere on model documentation pages and edited to include additional documentation in text format.\n\nTest-driven block\n\nCan be added to display one of the supported metrics or threshold test results collected by the Developer Framework."
  }
]