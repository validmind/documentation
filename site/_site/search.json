[
  {
    "objectID": "guide/developer-framework-introduction.html",
    "href": "guide/developer-framework-introduction.html",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/developer-framework-introduction.html#examples",
    "href": "guide/developer-framework-introduction.html#examples",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/developer-framework-introduction.html#related-topics",
    "href": "guide/developer-framework-introduction.html#related-topics",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind overview",
    "section": "",
    "text": "ValidMind is a model risk management (MRM) solution designed for the specific needs of model developers and model validators alike. The platform automates key aspects of the MRM process, including model documentation, validation, and testing. In addition, the platform comes with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date.\nOur solution comprises two primary architectural components: the ValidMind Developer Framework and the cloud-based ValidMind MRM platform."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind overview",
    "section": "Related Topics",
    "text": "Related Topics\nReady to try out ValidMind? Try the Quickstart."
  },
  {
    "objectID": "guide/explore-developer-framework-uploads.html",
    "href": "guide/explore-developer-framework-uploads.html",
    "title": "Explore uploads",
    "section": "",
    "text": "Check the documentation, metrics, and test results uploaded to the platform by the Developer Framework."
  },
  {
    "objectID": "guide/explore-developer-framework-uploads.html#prerequisites",
    "href": "guide/explore-developer-framework-uploads.html#prerequisites",
    "title": "Explore uploads",
    "section": "Prerequisites",
    "text": "Prerequisites\nThese steps require that you have already uploaded some information from the Developer Framework to the ValidMind Platform.\nTo simplify the getting started experience, these steps use an existing project."
  },
  {
    "objectID": "guide/explore-developer-framework-uploads.html#steps",
    "href": "guide/explore-developer-framework-uploads.html#steps",
    "title": "Explore uploads",
    "section": "Steps",
    "text": "Steps\n\nLog into the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the Customer Churn Model and select it.\nThe model page shows some important information about the model, such as:\n\nThe ID of the model and its use\nThe owners, developers, and validators associated with the model\nThe approval route for the validation workflow\nThe risk tier, current version, and the last and next validation dates\nAnd more\n\nScroll down to Project History.\nNote that the model status is In Documentation. This is the status that a model starts in when you register it for initial validation.\nClick Customer Churn Model - Initial Validation.\nThe project opens and you\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/explore-developer-framework-uploads.html#troubleshooting",
    "href": "guide/explore-developer-framework-uploads.html#troubleshooting",
    "title": "Explore uploads",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/explore-developer-framework-uploads.html#whats-next",
    "href": "guide/explore-developer-framework-uploads.html#whats-next",
    "title": "Explore uploads",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/mrm-three-lines-of-defense.html",
    "href": "guide/mrm-three-lines-of-defense.html",
    "title": "The three line of defense",
    "section": "",
    "text": "Model Risk Management (MRM) is a crucial process for organizations using mathematical and statistical models to inform decision-making. MRM aims to identify, assess, and mitigate risks arising from model usage, ensuring the accuracy and reliability of model outputs. A key aspect of MRM is its three lines of defense, which provide a structured approach to managing model risk across various functions and roles."
  },
  {
    "objectID": "guide/mrm-three-lines-of-defense.html#first-line-of-defense-business-units",
    "href": "guide/mrm-three-lines-of-defense.html#first-line-of-defense-business-units",
    "title": "The three line of defense",
    "section": "First Line of Defense: Business Units",
    "text": "First Line of Defense: Business Units\nModel Development:\n\nBusiness units are responsible for creating models that accurately represent the underlying systems or processes. This involves selecting appropriate variables, data sources, and mathematical or statistical methods, as well as performing rigorous documentation and testing, including sensitivity analysis and stress testing. Ensuring that models are conceptually sound and robust is crucial to minimize model risk.\n\nModel Validation:\n\nBusiness units also play a role in the validation process, collaborating with independent validation teams to ensure that models are fit for their intended purpose. This involves reviewing the model’s assumptions, methodologies, and data quality, as well as assessing the model’s predictive accuracy and stability. Validation provides assurance that the model’s assumptions and methodologies are appropriate and that the model is fit for its intended use.\n\nModel Implementation: - Business units are responsible for integrating models into the organization’s systems and processes. Proper implementation ensures that the model is used consistently and accurately, with adequate controls and documentation in place. Model users must be trained to ensure they understand the model’s purpose, limitations, and assumptions."
  },
  {
    "objectID": "guide/mrm-three-lines-of-defense.html#second-line-of-defense-model-risk-oversight-function",
    "href": "guide/mrm-three-lines-of-defense.html#second-line-of-defense-model-risk-oversight-function",
    "title": "The three line of defense",
    "section": "Second Line of Defense: Model Risk Oversight Function",
    "text": "Second Line of Defense: Model Risk Oversight Function\nThe second line of defense is an independent model risk oversight function that provides a governance framework and guidance for model risk management. This function is responsible for:\n\nDeveloping and maintaining MRM policies and procedures that align with regulatory requirements and industry best practices.\nEnsuring that model development, validation, and implementation processes adhere to the established MRM framework.\nPeriodically reviewing and assessing the organization’s model inventory and associated risks.\nProviding guidance and support to business units in managing model risks, including the identification of emerging risks and the implementation of risk mitigation measures."
  },
  {
    "objectID": "guide/mrm-three-lines-of-defense.html#third-line-of-defense-internal-audit-function",
    "href": "guide/mrm-three-lines-of-defense.html#third-line-of-defense-internal-audit-function",
    "title": "The three line of defense",
    "section": "Third Line of Defense: Internal Audit Function",
    "text": "Third Line of Defense: Internal Audit Function\nThe internal audit function serves as the third line of defense in MRM, assessing the effectiveness of MRM practices and controls. Key responsibilities of the internal audit function include:\n\nEvaluating the adequacy and effectiveness of the MRM framework, including the first and second lines of defense.\nAssessing the organization’s compliance with MRM policies, procedures, and regulatory requirements.\nIdentifying gaps or weaknesses in MRM practices and recommending improvements.\nProviding assurance to senior management and the board of directors that the organization’s MRM practices are effective in managing model risks."
  },
  {
    "objectID": "guide/run-test-plans.html",
    "href": "guide/run-test-plans.html",
    "title": "Run test plans",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/run-test-plans.html#prerequisites",
    "href": "guide/run-test-plans.html#prerequisites",
    "title": "Run test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/run-test-plans.html#steps",
    "href": "guide/run-test-plans.html#steps",
    "title": "Run test plans",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/run-test-plans.html#troubleshooting",
    "href": "guide/run-test-plans.html#troubleshooting",
    "title": "Run test plans",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/run-test-plans.html#whats-next",
    "href": "guide/run-test-plans.html#whats-next",
    "title": "Run test plans",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developers section."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe validation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s Next",
    "text": "What’s Next\n\nCreate project findings\nWork with validation reports"
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/submit-for-approval.html#related-topics",
    "href": "guide/submit-for-approval.html#related-topics",
    "title": "Submit for approval",
    "section": "Related topics",
    "text": "Related topics\n\nConfigure Workflows"
  },
  {
    "objectID": "guide/dataset-object.html",
    "href": "guide/dataset-object.html",
    "title": "The dataset object",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/dataset-object.html#examples",
    "href": "guide/dataset-object.html#examples",
    "title": "The dataset object",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/dataset-object.html#related-topics",
    "href": "guide/dataset-object.html#related-topics",
    "title": "The dataset object",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/jupyter-notebooks.html",
    "href": "guide/jupyter-notebooks.html",
    "title": "Jupyter notebooks",
    "section": "",
    "text": "Our Jupyter notebooks are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nAfter you check out the Jupyter notebooks in this section, you can try the notebook source yourself:\nFind the source on Google Colaboratory (Colab)\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks, write and execute code, share your work to collaborate in real-time, and export our notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/jupyter-notebooks.html#related-topics",
    "href": "guide/jupyter-notebooks.html#related-topics",
    "title": "Jupyter notebooks",
    "section": "Related topics",
    "text": "Related topics\nFor an introductory example, take a look at the Jupyter notebook in the Quickstart."
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "If you are new to our products, you will need access. Check our Releases page."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nThe fastest way to explore what ValidMind can offer is with our Quickstart.\nThe Quickstart takes about 15 minutes to complete and walks you through the Developer Framework with a sample Jupyter notebook and introduces you to the ValidMind Platform.\nIf you have already tried the Quickstart, how-to instructions for different users are in our Guides:\n\nFor platform administrators — Learn how to configure the platform, from setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your exisiting workflows, and more.\nFor model developers — Find information for ValidMind test plans and tests, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nAlso check the Guides for how you integrate the Developer Framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers."
  },
  {
    "objectID": "guide/get-started.html#have-more-questions",
    "href": "guide/get-started.html#have-more-questions",
    "title": "Get started",
    "section": "Have more questions?",
    "text": "Have more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html",
    "href": "guide/use-test-plans-and-tests.html",
    "title": "When to use tests plans and tests",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#examples",
    "href": "guide/use-test-plans-and-tests.html#examples",
    "title": "When to use tests plans and tests",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#related-topics",
    "href": "guide/use-test-plans-and-tests.html#related-topics",
    "title": "When to use tests plans and tests",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/register-model.html",
    "href": "guide/register-model.html",
    "title": "Register models",
    "section": "",
    "text": "Register a model you are documenting in the model inventory."
  },
  {
    "objectID": "guide/register-model.html#prerequisites",
    "href": "guide/register-model.html#prerequisites",
    "title": "Register models",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/register-model.html#steps",
    "href": "guide/register-model.html#steps",
    "title": "Register models",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/register-model.html#troubleshooting",
    "href": "guide/register-model.html#troubleshooting",
    "title": "Register models",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/register-model.html#whats-next",
    "href": "guide/register-model.html#whats-next",
    "title": "Register models",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework, including:\n\nValidMind Python library API\nValidMind model API\nValidMind test plan API\nCore library tests\nCore library tests for SKLearn-compatible models"
  },
  {
    "objectID": "guide/manage-project-findings.html",
    "href": "guide/manage-project-findings.html",
    "title": "Manage project findings",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/manage-project-findings.html#prerequisites",
    "href": "guide/manage-project-findings.html#prerequisites",
    "title": "Manage project findings",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/manage-project-findings.html#steps",
    "href": "guide/manage-project-findings.html#steps",
    "title": "Manage project findings",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/manage-project-findings.html#troubleshooting",
    "href": "guide/manage-project-findings.html#troubleshooting",
    "title": "Manage project findings",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/manage-project-findings.html#whats-next",
    "href": "guide/manage-project-findings.html#whats-next",
    "title": "Manage project findings",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.dev.vm.validmind.ai/.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/features.html",
    "href": "guide/features.html",
    "title": "Features",
    "section": "",
    "text": "The platform is designed to allow customers to configure it directly using a user-friendly UI. Functionality includes user and role-based administration, documentation template configuration, workflow configuration, test-kit customization, and more."
  },
  {
    "objectID": "guide/try-developer-framework.html",
    "href": "guide/try-developer-framework.html",
    "title": "Try out the ValidMind Developer Framework",
    "section": "",
    "text": "For model developers, we recommend that you check out our sample Jupyter notebook and then try modifying the notebook yourself:\nThe notebook shows you how to initialize the ValidMind Developer Framework, prepare a demo dataset, run test plans, train a model, and then upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/try-developer-framework.html#steps",
    "href": "guide/try-developer-framework.html#steps",
    "title": "Try out the ValidMind Developer Framework",
    "section": "Steps",
    "text": "Steps\n\nCheck out the sample notebook:\nIntro to ValidMind: Bank Customer Churn Prediction\n\n\n\n\nGet access to the notebook source:\nFind the source on Google Colaboratory (Colab)\n\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks, write and execute code, share your work to collaborate in real-time, and export our notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/try-developer-framework.html#whats-next",
    "href": "guide/try-developer-framework.html#whats-next",
    "title": "Try out the ValidMind Developer Framework",
    "section": "What’s Next",
    "text": "What’s Next\nNext, you can try exploring our Validmind Platform."
  },
  {
    "objectID": "guide/try-developer-framework.html#related-topics",
    "href": "guide/try-developer-framework.html#related-topics",
    "title": "Try out the ValidMind Developer Framework",
    "section": "Related topics",
    "text": "Related topics\nFor more advanced examples, we offer additional Jupyter notebooks you can try out."
  },
  {
    "objectID": "guide/model-object.html",
    "href": "guide/model-object.html",
    "title": "The model object",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/model-object.html#examples",
    "href": "guide/model-object.html#examples",
    "title": "The model object",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/model-object.html#related-topics",
    "href": "guide/model-object.html#related-topics",
    "title": "The model object",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/release-pilot.html",
    "href": "guide/release-pilot.html",
    "title": "Releases",
    "section": "",
    "text": "Welcome to the Pilot Release for ValidMind. On this page you will find important information to help you get started."
  },
  {
    "objectID": "guide/release-pilot.html#get-access",
    "href": "guide/release-pilot.html#get-access",
    "title": "Releases",
    "section": "Get access",
    "text": "Get access\nWant to try out this ValidMind release but don’t have access? Email info@validmind.ai."
  },
  {
    "objectID": "guide/release-pilot.html#access-urls",
    "href": "guide/release-pilot.html#access-urls",
    "title": "Releases",
    "section": "Access URLs",
    "text": "Access URLs\n\n\n\nService\nURL\n\n\n\n\nValidMind UI login\nhttps://app.dev.vm.validmind.ai/"
  },
  {
    "objectID": "guide/release-pilot.html#users-roles",
    "href": "guide/release-pilot.html#users-roles",
    "title": "Releases",
    "section": "Users & roles",
    "text": "Users & roles\nTo ensure the security of our services, user names and passwords are supplied separately. If you do not already have this information, please contact support@validmind.ai.\nValidMind supports user roles for:\n\nmodel developers\nmodel validators\nplatform administrators\n\nFor the pilot, the initial roles will have been set up for you already."
  },
  {
    "objectID": "guide/release-pilot.html#related-topics",
    "href": "guide/release-pilot.html#related-topics",
    "title": "Releases",
    "section": "Related topics",
    "text": "Related topics\nReady to try out ValidMind? Take a look at the Quickstart."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable User Administrators (such as the Model Validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test plans and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#check-out-the-troubleshooting-content",
    "href": "guide/support.html#check-out-the-troubleshooting-content",
    "title": "Support",
    "section": "Check out the troubleshooting content",
    "text": "Check out the troubleshooting content\nAdditional troubleshooting content might be available in the Help Center."
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/release-notes.html",
    "href": "guide/release-notes.html",
    "title": "Release notes",
    "section": "",
    "text": "(Future curated release highlights)"
  },
  {
    "objectID": "guide/release-notes.html#changelog",
    "href": "guide/release-notes.html#changelog",
    "title": "Release notes",
    "section": "Changelog",
    "text": "Changelog\n(Future generated changelog)"
  },
  {
    "objectID": "guide/release-notes.html#bug-fixes",
    "href": "guide/release-notes.html#bug-fixes",
    "title": "Release notes",
    "section": "Bug fixes",
    "text": "Bug fixes\n(Future bugs we fixed)"
  },
  {
    "objectID": "guide/release-notes.html#known-issues",
    "href": "guide/release-notes.html#known-issues",
    "title": "Release notes",
    "section": "Known issues",
    "text": "Known issues\n(Future known issues our users should hear about)"
  },
  {
    "objectID": "guide/configure-workflows.html",
    "href": "guide/configure-workflows.html",
    "title": "Configure workflows",
    "section": "",
    "text": "As part of the initial setup of the ValidMind solution for your company or organization, you need to configure the validation workflows that users such as model developers and model validators will follow."
  },
  {
    "objectID": "guide/configure-workflows.html#prerequisites",
    "href": "guide/configure-workflows.html#prerequisites",
    "title": "Configure workflows",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/configure-workflows.html#steps",
    "href": "guide/configure-workflows.html#steps",
    "title": "Configure workflows",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/configure-workflows.html#troubleshooting",
    "href": "guide/configure-workflows.html#troubleshooting",
    "title": "Configure workflows",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/configure-workflows.html#whats-next",
    "href": "guide/configure-workflows.html#whats-next",
    "title": "Configure workflows",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/explore-validmind-platform.html",
    "href": "guide/explore-validmind-platform.html",
    "title": "Explore the ValidMind Platform",
    "section": "",
    "text": "The ValidMind Platform is the central place to:\nHere’s how to get started:"
  },
  {
    "objectID": "guide/explore-validmind-platform.html#whats-next",
    "href": "guide/explore-validmind-platform.html#whats-next",
    "title": "Explore the ValidMind Platform",
    "section": "What’s Next",
    "text": "What’s Next\nReady to use ValidMind for production with your own use cases? Follow our how-to guides. Or try the Developer Framework."
  },
  {
    "objectID": "guide/create-project-findings.html",
    "href": "guide/create-project-findings.html",
    "title": "Create project findings",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/create-project-findings.html#prerequisites",
    "href": "guide/create-project-findings.html#prerequisites",
    "title": "Create project findings",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/create-project-findings.html#steps",
    "href": "guide/create-project-findings.html#steps",
    "title": "Create project findings",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/create-project-findings.html#troubleshooting",
    "href": "guide/create-project-findings.html#troubleshooting",
    "title": "Create project findings",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/create-project-findings.html#whats-next",
    "href": "guide/create-project-findings.html#whats-next",
    "title": "Create project findings",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/configure-templates.html",
    "href": "guide/configure-templates.html",
    "title": "Configure templates",
    "section": "",
    "text": "As part of the initial setup of the ValidMind solution for your company or organization, you need to configure what templates are available to your users. Templates are a way to configure required validation tests and standardize the documentation table of contents for specific model use cases."
  },
  {
    "objectID": "guide/configure-templates.html#prerequisites",
    "href": "guide/configure-templates.html#prerequisites",
    "title": "Configure templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/configure-templates.html#steps",
    "href": "guide/configure-templates.html#steps",
    "title": "Configure templates",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/configure-templates.html#troubleshooting",
    "href": "guide/configure-templates.html#troubleshooting",
    "title": "Configure templates",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/configure-templates.html#whats-next",
    "href": "guide/configure-templates.html#whats-next",
    "title": "Configure templates",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/developer-framework.html",
    "href": "guide/developer-framework.html",
    "title": "Developers",
    "section": "",
    "text": "Geared towards model developers, this section includes information for:"
  },
  {
    "objectID": "guide/developer-framework.html#related-topics",
    "href": "guide/developer-framework.html#related-topics",
    "title": "Developers",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developer tasks related to documentation projects and collaborating with model validators and model owners, refer to our Guides."
  },
  {
    "objectID": "guide/tabular-data-tests.html",
    "href": "guide/tabular-data-tests.html",
    "title": "Tabular data tests",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/tabular-data-tests.html#examples",
    "href": "guide/tabular-data-tests.html#examples",
    "title": "Tabular data tests",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/tabular-data-tests.html#related-topics",
    "href": "guide/tabular-data-tests.html#related-topics",
    "title": "Tabular data tests",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/editions.html",
    "href": "guide/editions.html",
    "title": "Editions",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions.html#standard-edition",
    "href": "guide/editions.html#standard-edition",
    "title": "Editions",
    "section": "Standard Edition",
    "text": "Standard Edition\nStandard Edition serves as our entry-level offering and includes all related features."
  },
  {
    "objectID": "guide/editions.html#enterprise-edition",
    "href": "guide/editions.html#enterprise-edition",
    "title": "Editions",
    "section": "Enterprise Edition",
    "text": "Enterprise Edition\nEnterprise Edition includes all features and services of the Standard Edition, plus additional features tailored to the needs of large-scale organizations."
  },
  {
    "objectID": "guide/editions.html#virtual-private-validmind-vpv",
    "href": "guide/editions.html#virtual-private-validmind-vpv",
    "title": "Editions",
    "section": "Virtual Private ValidMind (VPV)",
    "text": "Virtual Private ValidMind (VPV)\nVirtual Private ValidMind (VPV) provides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. It encompasses all features and services of the Enterprise Edition but within a separate ValidMind environment, isolated from other ValidMind accounts (VPV accounts do not share resources with non-VPV accounts)."
  },
  {
    "objectID": "guide/editions.html#related-topics",
    "href": "guide/editions.html#related-topics",
    "title": "Editions",
    "section": "Related Topics",
    "text": "Related Topics\nFor early program access, refer to Pilot release"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\ncom.amazonaws.vpce.us-west-2.vpce-svc-0b956fa3e03afa538\nhttps://private.prod.vm.validmind.ai"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s Next",
    "text": "What’s Next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/mrm-lifecyle.html",
    "href": "guide/mrm-lifecyle.html",
    "title": "Model lifecycle and workflow",
    "section": "",
    "text": "Model Risk Management (MRM) plays an important role in ensuring the accuracy, reliability, and effectiveness of models used in decision-making processes. This article focuses on the model lifecycle and process workflow, outlining the key stages and activities that constitute a robust MRM framework."
  },
  {
    "objectID": "guide/mrm-lifecyle.html#model-lifecycle-stages",
    "href": "guide/mrm-lifecyle.html#model-lifecycle-stages",
    "title": "Model lifecycle and workflow",
    "section": "Model Lifecycle Stages",
    "text": "Model Lifecycle Stages\n\nModel Design and Development\nAt this stage, subject matter experts identify the problem, define the model’s objectives, and design a model that accurately represents the underlying system or process. This involves selecting appropriate variables, data sources, and mathematical or statistical methods. Rigorous documentation and testing, including sensitivity analysis and stress testing, are crucial to ensure model soundness.\n\n\nModel Validation\nAn independent team evaluates the model’s conceptual soundness, performance, and compliance with regulatory requirements. This step involves reviewing the model’s assumptions, methodologies, and data quality, as well as assessing the model’s predictive accuracy and stability. Validation provides assurance that the model is fit for its intended use.\n\n\nModel Implementation\nAfter validation, the model is integrated into the organization’s systems and processes. Proper implementation ensures that the model is used consistently and accurately, with adequate controls and documentation in place. Model users must be trained to ensure they understand the model’s purpose, limitations, and assumptions.\n\n\nModel Monitoring and Performance Evaluation\nOngoing monitoring of model performance, assumptions, and limitations is essential to maintain model accuracy and reliability. Regular model performance evaluations help identify potential issues and trigger model updates or recalibrations when necessary. This stage also includes monitoring regulatory changes that may impact the model’s compliance status.\n\n\nModel Maintenance and Retirement\nModel maintenance involves periodic updates and recalibrations to address changing conditions or identified issues. When a model is no longer relevant or effective, it must be retired and replaced with a more suitable model. Proper documentation and archiving of retired models are essential for regulatory and audit purposes."
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documetnation"
  },
  {
    "objectID": "guide/glossary.html",
    "href": "guide/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "ValidMind Developer Framework \n\nValidMind’s set of tools, guidelines, and standards designed to provide developers with a structured approach to documenting risk models. Includes our Python library, API endpoints, and other software components that simplify the model development process. Also see ValidMind Platform.\n\nValidMind Platform \n\nValidMind’s hosted multi-tenant architecture that includes the cloud-based web interface, APIs, databases, documentation and validation engine, and various internal services. Also see Developer Framework.\n\nmodel developer\n\nResponsible for developing, implementing, and maintaining risk models. Model developers are typically subject matter experts in their domain and are responsible for ensuring that the model is fit-for-purpose, accurate, and aligned with the business requirements. Model developers collaborate with model validators and business units to ensure that models are conceptually sound, robust, and fit for their intended purpose, ultimately contributing to effective Model Risk Management and reliable decision-making.\n\nmodel inventory\n\nA comprehensive documentation of all models used by an organization, their purpose, and their associated risks, essential for effective MRM.\n\nmodel risk management (MRM)\n\nThe process of identifying, assessing, and controlling risks associated with models used in financial institutions. Governance and risk management functions oversee the MRM process and ensure compliance with policies.\n\nmodel risk\n\nThe potential for financial loss, incorrect decisions or unintended consequences resulting from errors or inaccuracies in financial models.\n\nmodel governance\n\nThe policies, procedures, and controls put in place by financial institutions to manage and monitor the use of models.\n\nmodel validation\n\nThe process of evaluating and testing models to ensure that they are accurate, reliable, and effective with the help of our platform.\n\nmodel validator\n\nResponsible for the independent evaluation of a model’s conceptual soundness, performance, and compliance with regulatory requirements. Their role is to ensure the model’s assumptions, methodologies, and data quality are appropriate, and that the model is fit for its intended use. Model validators collaborate with business units and model developers to identify and address potential issues, ultimately helping to minimize model risk and enhance the reliability of model-driven decisions.\n\nmonitoring\n\nThe regular evaluation of model performance, assumptions, and limitations to identify and address potential issues before they become significant risks.\n\nthree lines of defense\n\nA structured approach to MRM, consisting of 1) business units responsible for model development, validation, and implementation; 2) an independent model risk oversight function that provides a governance framework and guidance; and 3) the internal audit function, which assesses the effectiveness of MRM practices and controls."
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports\nCreate project findings\nManage project findings\nView validation guidelines"
  },
  {
    "objectID": "guide/model-evaluation-tests.html",
    "href": "guide/model-evaluation-tests.html",
    "title": "Model evaluation tests",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/model-evaluation-tests.html#examples",
    "href": "guide/model-evaluation-tests.html#examples",
    "title": "Model evaluation tests",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/model-evaluation-tests.html#related-topics",
    "href": "guide/model-evaluation-tests.html#related-topics",
    "title": "Model evaluation tests",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/mrm.html",
    "href": "guide/mrm.html",
    "title": "Model risk management",
    "section": "",
    "text": "Model risk management (MRM) is a process for organizations using mathematical and statistical models to inform decision-making. MRM aims to identify, assess, and mitigate risks arising from model usage, ensuring the accuracy and reliability of model outputs."
  },
  {
    "objectID": "guide/mrm.html#model-risk",
    "href": "guide/mrm.html#model-risk",
    "title": "Model risk management",
    "section": "Model Risk",
    "text": "Model Risk\nModel risk arises from incorrect or inappropriate use of models, inaccurate assumptions, or limitations in data quality. Consequences can include financial losses, reputational damage, and regulatory penalties. Effective MRM is essential to minimize these risks."
  },
  {
    "objectID": "guide/mrm.html#three-lines-of-defense",
    "href": "guide/mrm.html#three-lines-of-defense",
    "title": "Model risk management",
    "section": "Three Lines of Defense",
    "text": "Three Lines of Defense\nMRM is structured around three lines of defense. The first line consists of business units responsible for model development, validation, and implementation. They ensure that models are accurate, robust, and fit for purpose. The second line is an independent model risk oversight function that provides a governance framework and guidance for model risk management. The third line is the internal audit function, which assesses the effectiveness of MRM practices and controls."
  },
  {
    "objectID": "guide/mrm.html#model-development-and-validation",
    "href": "guide/mrm.html#model-development-and-validation",
    "title": "Model risk management",
    "section": "Model Development and Validation",
    "text": "Model Development and Validation\nModel development involves creating a model that accurately represents the underlying system or process. This stage requires rigorous documentation and testing, including sensitivity analysis and stress testing. Model validation is the independent evaluation of a model’s conceptual soundness, performance, and adherence to regulatory requirements. It ensures that a model’s assumptions and methodologies are appropriate and that the model is fit for its intended use."
  },
  {
    "objectID": "guide/mrm.html#model-governance",
    "href": "guide/mrm.html#model-governance",
    "title": "Model risk management",
    "section": "Model Governance",
    "text": "Model Governance\nEffective MRM requires a strong governance framework, including clear roles and responsibilities, reporting lines, and escalation procedures. Model governance ensures that all models within an organization are developed, validated, and maintained in accordance with established policies and procedures."
  },
  {
    "objectID": "guide/mrm.html#model-inventory-and-monitoring",
    "href": "guide/mrm.html#model-inventory-and-monitoring",
    "title": "Model risk management",
    "section": "Model Inventory and Monitoring",
    "text": "Model Inventory and Monitoring\nA comprehensive model inventory is essential for effective MRM. It documents all models used by an organization, their purpose, and their associated risks. Regular monitoring of model performance, assumptions, and limitations helps to identify and address potential issues before they become significant risks."
  },
  {
    "objectID": "guide/architecture.html",
    "href": "guide/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/architecture.html#examples",
    "href": "guide/architecture.html#examples",
    "title": "Architecture",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/architecture.html#related-topics",
    "href": "guide/architecture.html#related-topics",
    "title": "Architecture",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/supported-models.html#examples",
    "href": "guide/supported-models.html#examples",
    "title": "Supported models",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/cloud-platforms.html",
    "href": "guide/cloud-platforms.html",
    "title": "Cloud platforms",
    "section": "",
    "text": "Provide a brief introduction to the concept that this article will cover:\n\nWhat is [Concept Name]? Explain the concept or topic that the article is about. Provide a clear and concise definition that accurately reflects the scope of the article.\nWhy is [Concept Name] Important? Explain why the concept or topic is important and why users should be interested in learning more about it.\nHow Does [Concept Name] Work? Provide a detailed explanation of how the concept or topic works. Use examples or diagrams to help illustrate key points.\nKey Terms. List important terms and concepts related to the main topic and provide clear definitions for each one."
  },
  {
    "objectID": "guide/cloud-platforms.html#examples",
    "href": "guide/cloud-platforms.html#examples",
    "title": "Cloud platforms",
    "section": "Examples",
    "text": "Examples\nProvide one or more examples that illustrate how the concept or topic works in practice. These examples should be clear and concise and should help users understand how to apply the concept or topic in real-world situations."
  },
  {
    "objectID": "guide/cloud-platforms.html#related-topics",
    "href": "guide/cloud-platforms.html#related-topics",
    "title": "Cloud platforms",
    "section": "Related Topics",
    "text": "Related Topics\nProvide links or references to related topics that users might find useful or interesting. These could be other articles, resources, or tools related to the main topic."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\nCheck Include comment threads to include comment threads in the exported file.\nCheck Section activity logs to include a history of changes in each section of the documentation.\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\nCheck Include comment threads to include comment threads in the exported file.\nCheck Section activity logs to include a history of changes in each section of the documentation.\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/manage-users-and-roles.html",
    "href": "guide/manage-users-and-roles.html",
    "title": "Manage users and roles",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#prerequisites",
    "href": "guide/manage-users-and-roles.html#prerequisites",
    "title": "Manage users and roles",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#add-users",
    "href": "guide/manage-users-and-roles.html#add-users",
    "title": "Manage users and roles",
    "section": "Add users",
    "text": "Add users\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#delete-users",
    "href": "guide/manage-users-and-roles.html#delete-users",
    "title": "Manage users and roles",
    "section": "Delete users",
    "text": "Delete users\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/manage-users-and-roles.html#update-user-roles",
    "href": "guide/manage-users-and-roles.html#update-user-roles",
    "title": "Manage users and roles",
    "section": "Update user roles",
    "text": "Update user roles\n\nSteps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]\n\n\n\n\nTroubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]\n\n\nWhat’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test plan execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test plans\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test plans and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind Platform\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\n\nLocating the project identifier, API key and secret:\nOn the Client Integration page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform,"
  },
  {
    "objectID": "guide/create-documentation-project.html#whats-next",
    "href": "guide/create-documentation-project.html#whats-next",
    "title": "Create documentation projects",
    "section": "What’s Next",
    "text": "What’s Next\nUse the ValidMind Developer Framework to document a model built in Python. For more details and examples, refer to our ValidMind Python Library Introduction."
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enbale their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/assign-model-validators.html",
    "href": "guide/assign-model-validators.html",
    "title": "Assign model validators",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/assign-model-validators.html#prerequisites",
    "href": "guide/assign-model-validators.html#prerequisites",
    "title": "Assign model validators",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/assign-model-validators.html#steps",
    "href": "guide/assign-model-validators.html#steps",
    "title": "Assign model validators",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/assign-model-validators.html#troubleshooting",
    "href": "guide/assign-model-validators.html#troubleshooting",
    "title": "Assign model validators",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/assign-model-validators.html#whats-next",
    "href": "guide/assign-model-validators.html#whats-next",
    "title": "Assign model validators",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier\n\n\n\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick CLIENT INTEGRATION and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"<project-identifier>\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s Next",
    "text": "What’s Next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/configure-sso.html",
    "href": "guide/configure-sso.html",
    "title": "Configure Single Sign-On (SSO)",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/configure-sso.html#prerequisites",
    "href": "guide/configure-sso.html#prerequisites",
    "title": "Configure Single Sign-On (SSO)",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/configure-sso.html#steps",
    "href": "guide/configure-sso.html#steps",
    "title": "Configure Single Sign-On (SSO)",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/configure-sso.html#troubleshooting",
    "href": "guide/configure-sso.html#troubleshooting",
    "title": "Configure Single Sign-On (SSO)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/configure-sso.html#whats-next",
    "href": "guide/configure-sso.html#whats-next",
    "title": "Configure Single Sign-On (SSO)",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "To try out ValidMind, you need to be a registered user on the ValidMind Platform. For early program access:"
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to access ValidMind from your company network, the following prerequisits apply:\n\nTo use the Developer Framework and to access the ValidMind user interface, add the validmind.ai domain to your allowlist (whitelist).\nIf you require VPC access from your company network to ValidMind, you can configure AWS PrivateLink.\n\nIf you use a company VPC, we do support AWS PrivateLink.\n\nDeveloper framework\nTesting out our sample Jupyter notebooks requires Python 3.8+.\n\n\nValidmind Platform UI\nYou need to be able to access our ValidMind Platform UI in a modern browser such as Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/configure-test-plans.html",
    "href": "guide/configure-test-plans.html",
    "title": "Configure test plans",
    "section": "",
    "text": "[Include a brief description of what this task is about, why it’s important, and who it’s intended for.]"
  },
  {
    "objectID": "guide/configure-test-plans.html#prerequisites",
    "href": "guide/configure-test-plans.html#prerequisites",
    "title": "Configure test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/configure-test-plans.html#steps",
    "href": "guide/configure-test-plans.html#steps",
    "title": "Configure test plans",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/configure-test-plans.html#troubleshooting",
    "href": "guide/configure-test-plans.html#troubleshooting",
    "title": "Configure test plans",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/configure-test-plans.html#whats-next",
    "href": "guide/configure-test-plans.html#whats-next",
    "title": "Configure test plans",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstart — 15 mins",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework and explore the ValidMind Platform. This Quickstart takes about 15 minutes of your time."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstart — 15 mins",
    "section": "Steps",
    "text": "Steps\n\nBefore you begin\nCheck the prerequisites for the Developer Framework and ValidMind Platform UI.\nTry out the ValidMind Developer Framework\nTry our introductory Jupyter notebook to see the Developer Framework in action and get access to the notebook source.\nExplore the ValidMind Platform\nCreate your own documentation project, upload documentation and test data from one of our sample Jupyter notebooks, and explore your uploads in our Platform UI.\nNext steps\nTry some more advanced sample notebooks or set up ValidMind for production with your own use cases."
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases?"
  },
  {
    "objectID": "guide/next-steps.html#additional-resources",
    "href": "guide/next-steps.html#additional-resources",
    "title": "Next steps",
    "section": "Additional resources",
    "text": "Additional resources\nSee our FAQ for a curated list of frequently asked questions about what ValidMind offers."
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an Administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "validmind/api.html",
    "href": "validmind/api.html",
    "title": "ValidMind",
    "section": "",
    "text": "Python Library API\nMain entrypoint to the ValidMind Python Library\n\nvalidmind.init(project, api_key=None, api_secret=None, api_host=None)\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\n\nParameters\n\nproject (str) – The project CUID\napi_key (str, optional) – The API key. Defaults to None.\napi_secret (str, optional) – The API secret. Defaults to None.\napi_host (str, optional) – The API host. Defaults to None.\n\nRaises\nValueError – If the API key and secret are not provided\nReturns\nTrue if the ping was successful\nReturn type\nbool\n\n\n\nvalidmind.init_dataset(dataset: DataFrame, type: str = ‘training’, options: dict | None = None, targets: DatasetTargets | None = None, target_column: str | None = None, class_labels: dict | None = None)\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\nRaises\nValueError – If the dataset type is not supported\nReturns\nA VM Dataset instance\nReturn type\nvm.vm.Dataset\n\n\n\nvalidmind.init_model(model: object)\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained sklearn model\nRaises\nValueError – If the model type is not supported\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.init_r_model(model_path: str, model_type: str)\nInitializes a VM Model for an R model\nR models must be saved to disk and the filetype depends on the model type… Currently we support the following model types:\n\nLogisticRegression glm model in R: saved as an RDS file with saveRDS\nLinearRegression lm model in R: saved as an RDS file with saveRDS\nXGBClassifier: saved as a .json or .bin file with xgb.save\nXGBRegressor: saved as a .json or .bin file with xgb.save\n\nLogisticRegression and LinearRegression models are converted to sklearn models by extracting the coefficients and intercept from the R model. XGB models are loaded using the xgboost since xgb models saved in .json or .bin format can be loaded directly with either Python or R\n\nParameters\n\nmodel_path (str) – The path to the R model saved as an RDS or XGB file\nmodel_type (str) – The type of the model (one of R_MODEL_TYPES)\n\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.run_test_plan(test_plan_name, send=True, **kwargs)\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘sklearn_classifier’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises\nValueError – If the test plan name is not found or if there is an error initializing the test plan\nReturns\nA dictionary of test results\nReturn type\ndict\n\n\n\nvalidmind.log_dataset(vm_dataset)\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\nvm_dataset (validmind.VMDataset) – A VM dataset object\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options (dict, optional) – Additional dataset options for analysis. Defaults to None.\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset. Defaults to None.\nfeatures (list, optional) – Optional. A list of features metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nThe VMDataset object\nReturn type\nvalidmind.VMDataset\n\n\n\nvalidmind.log_figure(data_or_path, key, metadata, run_cuid=None)\nLogs a figure\n\nParameters\n\ndata_or_path (str or matplotlib.figure.Figure) – The path of the image or the data of the plot\nkey (str) – Identifier of the figure\nmetadata (dict) – Python data structure\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metrics(metrics, run_cuid=None)\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics (list) – A list of Metric objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_model(vm_model)\nLogs model metadata and hyperparameters to ValidMind API.\n\nParameters\nvm_model (validmind.VMModel) – A VM model object\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_test_results(results, run_cuid=None, dataset_type=‘training’)\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults (list) – A list of TestResults objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nclass validmind.Dataset(raw_dataset: object, fields: list, sample: list, shape: dict, correlation_matrix: object | None = None, correlations: dict | None = None, type: str | None = None, options: dict | None = None, statistics: dict | None = None, targets: dict | None = None, target_column: str = ’’, class_labels: dict | None = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: object | None = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nget_numeric_features_columns()\nReturns list of numeric features columns\n\nReturns\nThe list of numberic features columns\nReturn type\nlist\n\n\n\nget_categorical_features_columns()\nReturns list of categorical features columns\n\nReturns\nThe list of categorical features columns\nReturn type\nlist\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.DatasetTargets(target_column: str, description: str | None = None, class_labels: dict | None = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.Figure(key: str, metadata: dict, figure: object, extras: dict | None = None)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.Metric(test_context: TestContext, params: dict | None = None, result: TestPlanMetricResult | None = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nTODO: Metric should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\ndescription()\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(metric_value: dict | list | DataFrame | None = None)\nReturn the metric summary. Should be overridden by subclasses. Defaults to None. The metric summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the metric results.\nWe return None here because the metric summary is optional.\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.Model(attributes: ModelAttributes | None = None, task: str | None = None, subtask: str | None = None, params: dict | None = None, model_id: str = ‘main’, model: object | None = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.ModelAttributes(architecture: str | None = None, framework: str | None = None, framework_version: str | None = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.TestResult(values: dict, test_name: str | None = None, column: str | None = None, passed: bool | None = None)\nBases: object\nTestResult model\n\nvalues(: dic )\n\n\ntest_name(: str | Non _ = Non_ )\n\n\ncolumn(: str | Non _ = Non_ )\n\n\npassed(: bool | Non _ = Non_ )\n\n\nserialize()\nSerializes the TestResult to a dictionary so it can be sent to the API\n\n\n\nclass validmind.TestResults(category: str, test_name: str, params: dict, passed: bool, results: List[TestResult], summary: ResultSummary | None)\nBases: object\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\nsummary(: ResultSummary | Non )\n\n\nserialize()\nSerializes the TestResults to a dictionary so it can be sent to the API\n\n\n\nclass validmind.ThresholdTest(test_context: TestContext, params: dict | None = None, test_results: TestResults | None = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nTODO: ThresholdTest should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\ndescription()\nReturn the test description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(test_results: TestResults | None = None)\nReturn the threshold test summary. Should be overridden by subclasses. Defaults to None. The test summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the test results.\nWe return None here because the test summary is optional.\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/vm_models.html",
    "href": "validmind/vm_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Models\nModels entrypoint\n\nclass validmind.vm_models.Dataset(raw_dataset: object, fields: list, sample: list, shape: dict, correlation_matrix: object | None = None, correlations: dict | None = None, type: str | None = None, options: dict | None = None, statistics: dict | None = None, targets: dict | None = None, target_column: str = ’’, class_labels: dict | None = None, _Dataset__feature_lookup: dict = , _Dataset__transformed_df: object | None = None)\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nget_numeric_features_columns()\nReturns list of numeric features columns\n\nReturns\nThe list of numberic features columns\nReturn type\nlist\n\n\n\nget_categorical_features_columns()\nReturns list of categorical features columns\n\nReturns\nThe list of categorical features columns\nReturn type\nlist\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.vm_models.DatasetTargets(target_column: str, description: str | None = None, class_labels: dict | None = None)\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.vm_models.Figure(key: str, metadata: dict, figure: object, extras: dict | None = None)\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Metric(test_context: TestContext, params: dict | None = None, result: TestPlanMetricResult | None = None)\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nTODO: Metric should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\ndescription()\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.vm_models.MetricResult(type: str, scope: str, key: dict, value: dict | list | DataFrame, value_formatter: str | None = None)\nBases: object\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\ntype(: st )\n\n\nscope(: st )\n\n\nkey(: dic )\n\n\nvalue(: dict | list | DataFram )\n\n\nvalue_formatter(: str | Non _ = Non_ )\n\n\nserialize()\nSerializes the Metric to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Model(attributes: ModelAttributes | None = None, task: str | None = None, subtask: str | None = None, params: dict | None = None, model_id: str = ‘main’, model: object | None = None)\nBases: object\nModel class wrapper\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.vm_models.ModelAttributes(architecture: str | None = None, framework: str | None = None, framework_version: str | None = None)\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.vm_models.TestContext(dataset: Dataset | None = None, model: Model | None = None, train_ds: Dataset | None = None, test_ds: Dataset | None = None, y_train_predict: object | None = None, y_test_predict: object | None = None, context_data: dict | None = None)\nBases: object\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as model predictions, shared dataset metrics, etc.\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\ncontext_data(: dic _ = Non_ )\n\n\nset_context_data(key, value)\n\n\nget_context_data(key)\n\n\n\nclass validmind.vm_models.TestContextUtils()\nBases: object\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\ntest_context(: TestContex )\n\n\nproperty dataset()\n\n\nproperty model()\n\n\nproperty train_ds()\n\n\nproperty test_ds()\n\n\nproperty y_train_predict()\n\n\nproperty y_test_predict()\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\nproperty df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame\n\n\n\nclass validmind.vm_models.TestPlan(config: {} = None, test_context: TestContext = None, _test_plan_instances: List[object] = None, dataset: Dataset = None, model: Model = None, train_ds: Dataset = None, test_ds: Dataset = None, pbar: tqdm = None)\nBases: object\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\nname(: ClassVar[str )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\ntests(: ClassVar[List[object] _ = [_ )\n\n\ntest_plans(: ClassVar[List[object] _ = [_ )\n\n\nresults(: ClassVar[List[TestPlanResult] _ = [_ )\n\n\nconfig(: { _ = Non_ )\n\n\ntest_context(: TestContex _ = Non_ )\n\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\npbar(: tqd _ = Non_ )\n\n\ntitle()\nReturns the title of the test plan. Defaults to the title version of the test plan name\n\n\ndescription()\nReturns the description of the test plan. Defaults to the docstring of the test plan\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\nget_config_params_for_test(test_name)\nReturns the config for a given test, if it exists. The config attribute is a dictionary where the keys are the test names and the values are dictionaries of config values for that test.\nThe key in the config must match the name of the test, i.e. for a test called “time_series_univariate_inspection_raw” we could pass a config like this:\n{\n“time_series_univariate_inspection_raw”: {\n\n    “columns”: [“col1”, “col2”]\n\n}\n}\n\n\nrun(send=True)\nRuns the test plan\n\n\nlog_results()\nLogs the results of the test plan to ValidMind\nThis method will be called after the test plan has been run and all results have been collected. This method will log the results to ValidMind.\n\n\nsummarize()\nSummarizes the results of the test plan\nThis method will be called after the test plan has been run and all results have been logged to ValidMind. It will summarize the results of the test plan by creating an html table with the results of each test. This html table will be displayed in an VS Code, Jupyter or other notebook environment.\n\n\nget_results(result_id: str | None = None)\nReturns one or more results of the test plan. Includes results from sub test plans.\n\n\n\nclass validmind.vm_models.TestPlanDatasetResult(result_id: str | None = None, result_metadata: List[dict] | None = None, dataset: Dataset | None = None)\nBases: TestPlanResult\nResult wrapper for datasets that run as part of a test plan\n\ndataset(: Datase _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanMetricResult(result_id: str | None = None, result_metadata: List[dict] | None = None, figures: List[Figure] | None = None, metric: MetricResult | None = None)\nBases: TestPlanResult\nResult wrapper for metrics that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\nmetric(: MetricResult | Non _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanModelResult(result_id: str | None = None, result_metadata: List[dict] | None = None, model: Model | None = None)\nBases: TestPlanResult\nResult wrapper for models that run as part of a test plan\n\nmodel(: Mode _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanTestResult(result_id: str | None = None, result_metadata: List[dict] | None = None, figures: List[Figure] | None = None, test_results: TestResults | None = None)\nBases: TestPlanResult\nResult wrapper for test results produced by the tests that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestResult(*, test_name: str | None = None, column: str | None = None, passed: bool | None = None, values: dict)\nBases: BaseResultModel\nTestResult model\n\ntest_name(: str | Non )\n\n\ncolumn(: str | Non )\n\n\npassed(: bool | Non )\n\n\nvalues(: dic )\n\n\n\nclass validmind.vm_models.TestResults(*, category: str, test_name: str, params: dict, passed: bool, results: List[TestResult])\nBases: BaseResultModel\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\n\nclass validmind.vm_models.ThresholdTest(test_context: TestContext, params: dict | None = None, test_results: TestResults | None = None)\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nTODO: ThresholdTest should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\ndescription()\nReturn the test description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/model_validation_tests_sklearn.html",
    "href": "validmind/model_validation_tests_sklearn.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions models trained with sklearn or that provide a sklearn-like API\n\n\n\nBases: Metric\nAccuracy Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCharacteristic Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculates PSI for each of the dataset features\n\n\n\n\nBases: Metric\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nF1 Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPermutation Feature Importance\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPrecision Recall Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPrecision Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nRecall Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nROC AUC Score\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nROC Curve\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: TestContextUtils\nSHAP Global Importance. Custom metric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBases: Metric\nPopulation Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the model’s prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the model’s F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the model’s ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that identify weak regions with high residuals by histogram slicing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest robustness of model by perturbing the features column values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\nAdds Gaussian noise to a list of values.\n\nParameters\n\nvalues (*list**[float]*) – A list of numerical values to which noise is added.\nx_std_dev (float) – A scaling factor for the standard deviation of the noise.\n\nReturns\nA tuple containing:\n  * A list of noisy values, where each value is the sum of the corresponding value\n\n  in the input list and a randomly generated value sampled from a Gaussian distribution\n  with mean 0 and standard deviation x_std_dev times the standard deviation of the input list.\n  - The standard deviation of the input list of values.\nReturn type\ntuple[list[float], float]"
  },
  {
    "objectID": "validmind/readme.html",
    "href": "validmind/readme.html",
    "title": "ValidMind",
    "section": "",
    "text": "pip install validmind\n\n\npip install validmind[r-support]\n\n\n\n\n\n\n\n\nEnsure you have poetry installed: https://python-poetry.org/\nAfter cloning this repo run:\n\npoetry shell\npoetry install\n\n\n\nIf you want to use the R support that is provided by the ValidMind Developer Framework, you must have R installed on your machine. You can download R from https://cran.r-project.org/. If you are on a Mac, you can install R using Homebrew:\nbrew install r\nOnce you have R installed, you can install the r-support extra to install the necessary dependencies for R by running:\npoetry install --extras r-support\n\n\n\nMake sure you bump the package version before merging a PR with the following command:\nmake version tag=patch\nThe value of tag corresponds to one of the options provided by Poetry: https://python-poetry.org/docs/cli/#version\n\n\n\n\nIf you want to integate the validmind package to your development environment, you must build the package wheel first, since we have not pushed the package to a public PyPI repository yet. Steps:\n\nRun make build to build a new Python package for the developer framework\nThis will create a new wheel file in the dist folder\nRun pip install <path-to-wheel> to install the newly built package in your environment\n\n\n\n\nAPI documentation can be generated in Markdown or HTML format. Our documentation pipeline uses Markdown documentation before generating the final HTML assets for the documentation site.\nFor local testing, HTML docs can be generated with Sphinx. Note that the output template is different since the documentation pipeline uses the source Markdown files for the final HTML output.\nMarkdown and HTML docs can be generated with the following commands:\n# Navigate to the docs folder\ncd docs/\n\n# Generate HTML and Markdown docs\nmake docs\n\n# Generate Markdown docs only\nmake docs-markdown\n\n# Generate HTML docs only\nmake docs-html\nThe resulting markdown and html under docs/_build folders will contain the generated documentation.\n\n\n\n\n\nIf you run into an error related to the ValidMind wheel, try:\npoetry add wheel\npoetry update wheel\npoetry install\nIf there are lightgbm errors partway through, run remove lightgbm, followed by poetry update wheel and poetry install."
  },
  {
    "objectID": "validmind/test_plans.html",
    "href": "validmind/test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "Test Plans entry point\n\n\nReturns a list of all available test plans\n\n\n\nReturns a list of all available tests.\n\n\n\nReturns the test plan by name\n\n\n\nReturns a description of the test plan\n\n\n\nRegisters a custom test plan\n\n\n\nTest plan for sklearn classifier models\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan for sklearn classifier metrics\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier model diagnosis tests\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for sklearn classifier models that includes both metrics and validation tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest plan for tabular datasets\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on time series datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for generic tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for time series datasets"
  },
  {
    "objectID": "validmind/index.html",
    "href": "validmind/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Developer Framework\n\nValidMind Python Client\n\nInstallation\nContributing to ValidMind Developer Framework\nIntegrating the ValidMind Developer Framework to your development environment\nGenerating Docs\nKnown Issues\n\nPython Library API\nCore Library Tests\n\nData Validation Tests\n\nCore Library Tests\n\nModel Validation Tests for SKLearn-Compatible Models\n\nTest Plans\n\nlist_plans()\nlist_tests()\nget_by_name()\ndescribe_plan()\nregister_test_plan()\nTest Plans for SKLearn-Compatible Classifiers\nTest Plans for Tabular Datasets\n\nValidMind Models"
  },
  {
    "objectID": "validmind/data_validation_tests.html",
    "href": "validmind/data_validation_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions for any Pandas-compatible datasets\n\n\n\nBases: TestContextUtils\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadat is necessary to initialize dataset object that can be related to different metrics and test results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function\n\n\n\n\nBases: Metric\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild two tables: one for summarizing numerical variables and one for categorical variables\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\n\n\n\n\n\n\n\n\n\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\n\nReturns a summarized representation of the dataset split information\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCalculates seasonal_decompose metric for each of the dataset features\n\n\n\n\n\n\n\n\n\n\n\nStores the seasonal decomposition results in the test context so they can be re-used by other tests. Note we store one sd at a time for every column in the dataset.\n\n\n\nSerializes the seasonal decomposition results for one column into a JSON serializable format that can be sent to the API.\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPlots ACF and PACF for a given time series dataset.\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\n\n\n\n\n\n\n\n\n\nPlot rolling mean and rolling standard deviation in different subplots for a given series.\n\nParameters\n\nseries – Pandas Series with time-series data\nwindow_size – Window size for the rolling calculations\nax1 – Axis object for the rolling mean plot\nax2 – Axis object for the rolling standard deviation plot\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\n\n\n\n\n\n\nPlot the spread between two time series variables.\n\nParameters\n\nseries1 – Pandas Series with time-series data for the first variable\nseries2 – Pandas Series with time-series data for the second variable\nax – Axis object for the spread plot\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique rows is greater than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe zeros test finds columns that have too many zero values.\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that find outliers for time series data using the z-score method\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that detect frequencies in the data\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Find all the information you need to use our platform for model risk management (MRM)\n\n\n\n\n    \n    \n    Column Cards\n    \n\n\n    \n        \n            \n                Model developers\n                Collect, manage, and automate your model documentation and testing with our Developer Framework.\n                Start\n            \n            \n                Model validators\n                Review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements.\n                Start\n            \n        \n    \n    \n\n\n\n\n\n\n    Three-Column Table Example\n    \n\n\n    \n        \n            \n                Join our community\n                Training\n                Support\n            \n            \n                Adopt open-source notebooks for your use case or become a contributor.\n                Learn more about effective model risk management with the ValidMind Platform.\n                Need some help? Try our self-service documentation or email us at support@validmind.com.\n            \n            \n                Jupyter Notebooks\n                The three lines of defense\n                Troubleshooting documentation\n            \n            \n                Developer Framework"
  },
  {
    "objectID": "notebooks/passing_data.html",
    "href": "notebooks/passing_data.html",
    "title": "ValidMind",
    "section": "",
    "text": "In this notebook we will demonstrate how to make use of the test context to pass data between tests. Any object can be passed to the context, but we will use a simple dictionary to demonstrate the concept.\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  # Use your project ID\n  project = \"...\"\n)\n  \n\nTrue\n\n\n\n\nWe will build two simple metrics to demonstrate the concept. The first metric will create a sample dataframe with mock data, and the second will add one more column to it.\n\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MyFirstMetric(Metric):\n    type = \"dataset\"\n    key = \"my_first_metric\"\n\n    def run(self):\n        df = pd.DataFrame({\"column_a\": [1, 2, 3, 4, 5]})\n        # Store the dataframe in the test context\n        self.test_context.set_context_data(\"some_dataset\", df)\n\n        return self.cache_results(df.to_dict(\"records\"))\n\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MySecondMetric(Metric):\n    type = \"dataset\"\n    key = \"my_second_metric\"\n\n    def run(self):\n        # Get the dataframe from the test context. We can\n        # throw an error if it doesn't exist just to be sure\n        df = self.test_context.get_context_data(\"some_dataset\")\n        if df is None:\n            raise ValueError(\"'some_dataset' not found in test context\")\n        \n        new_df = df.copy()\n        new_df[\"column_b\"] = [5, 4, 3, 2, 1]\n\n        return self.cache_results(new_df.to_dict(\"records\"))\n\nNow let’s define a test plan that will run the two metrics in sequence. We need to make sure that the first metric is run before the second one.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MyFirstMetric, MySecondMetric]\n\nmy_custom_test_plan = MyCustomTestPlan()\nmy_custom_test_plan.run()\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        my_first_metric\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'column_a': 1}, {'column_a': 2}, {'column_a': 3}, {'column_a': 4}, {'column_a': 5}]\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        my_second_metric\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'column_a': 1, 'column_b': 5}, {'column_a': 2, 'column_b': 4}, {'column_a': 3, 'column_b': 3}, {'column_a': 4, 'column_b': 2}, {'column_a': 5, 'column_b': 1}]"
  },
  {
    "objectID": "notebooks/accessing_test_plan_results.html",
    "href": "notebooks/accessing_test_plan_results.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load dotenv extension for environment variables\n%load_ext dotenv\n# Tell dotenv to read the dev.env file\n%dotenv dev.env\n# Disable test plan summarization for this notebook \n%env VM_SUMMARIZE_TEST_PLANS False\n\nenv: VM_SUMMARIZE_TEST_PLANS=False\n\n\nIt is possible to access all the test results for a test plan when it has finished executing. This allows inspecting the results of each test, whether it is a metric, a threshold test or a figure, and the results of subtest plans if they exist.\nIn this notebook we’ll run the timeseries test plan on a demo dataset and inspect the results.\n\n# ML libraries\nimport pandas as pd\n\n# ValidMind libraries \nimport validmind as vm\n\n%matplotlib inline\n\nWe load the dataset as usual:\n\ndf = pd.read_csv(\"./datasets/lending_club_loan_rates.csv\", sep='\\t')\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\n\n# Remove diff columns\ncolumns_to_remove = [col for col in df.columns if col.startswith(\"diff\")]\ndf = df.drop(columns=columns_to_remove)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n    \n  \n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clgyd137o0000pi8hcn9oukz7\"\n)\n\nTrue\n\n\n\n\nInitialize the VM dataset:\n\ntarget_variables = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column = target_variables   \n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRun the “timeseries” test plan:\n\nloan_rate_columns = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\n\ntest_plan_config = {\n    \"time_series_univariate_inspection_raw\": {\n        \"columns\": loan_rate_columns\n    },\n    \"time_series_univariate_inspection_histogram\": {\n        \"columns\": loan_rate_columns\n    }\n}\n\ntimeseries_plan = vm.run_test_plan(\n    \"timeseries\",\n    config=test_plan_config,    \n    test_ds=vm_dataset,\n    train_ds=vm_dataset,\n    dataset=vm_dataset,\n)\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n\n\n\n\n\nWe can now access all the results of the test plan, including subtest plans using test_plan.get_results().\n\ntest_plan.get_results(): With no arguments, this returns a list of all results\ntest_plan.get_results(test_id): If provided with a test id, this returns the all results that match the given test id\n\nBy default, get_results() returns a list, in case there are multiple tests with the same id.\n\ntimeseries_plan.get_results()\n\n[TestPlanMetricResult(result_id=\"time_series_univariate_inspection_histogram\", figures),\n TestPlanMetricResult(result_id=\"time_series_univariate_inspection_raw\", figures),\n TestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures),\n TestPlanMetricResult(result_id=\"seasonality_detection_with_acf_and_pacf\", metric, figures),\n TestPlanMetricResult(result_id=\"residuals_visual_inspection\", figures),\n TestPlanMetricResult(result_id=\"ljung_box\", metric, figures),\n TestPlanMetricResult(result_id=\"box_pierce\", metric, figures),\n TestPlanMetricResult(result_id=\"runs_test\", metric, figures),\n TestPlanMetricResult(result_id=\"jarque_bera\", metric, figures),\n TestPlanMetricResult(result_id=\"kolmogorov_smirnov\", metric, figures),\n TestPlanMetricResult(result_id=\"shapiro_wilk\", metric, figures),\n TestPlanMetricResult(result_id=\"lilliefors_test\", metric, figures),\n TestPlanMetricResult(result_id=\"adf\", metric, figures),\n TestPlanMetricResult(result_id=\"kpss\", metric, figures),\n TestPlanMetricResult(result_id=\"phillips_perron\", metric, figures),\n TestPlanMetricResult(result_id=\"zivot_andrews\", metric, figures),\n TestPlanMetricResult(result_id=\"dickey_fuller_gls\", metric, figures),\n TestPlanMetricResult(result_id=\"adf\", metric, figures),\n TestPlanMetricResult(result_id=\"kpss\", metric, figures),\n TestPlanMetricResult(result_id=\"phillips_perron\", metric, figures),\n TestPlanMetricResult(result_id=\"zivot_andrews\", metric, figures),\n TestPlanMetricResult(result_id=\"dickey_fuller_gls\", metric, figures)]\n\n\n\nseasonal_decomposition = timeseries_plan.get_results(\"seasonal_decompose\")[0]\nseasonal_decomposition\n\nTestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures)\n\n\n\nseasonal_decomposition.show()\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        seasonal_decompose\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': [{'Date': '2007-08-01', 'loan_rate_A': 7.7666666666666675, 'trend': nan, 'seasonal': 0.010868499397109575, 'resid': nan}, {'Date': '2007-09-01', 'loan_rate_A': 7.841428571428572, 'trend': nan, 'seasonal': 0.0013181145386968443, 'resid': nan}, {'Date': '2007-10-01', 'loan_rate_A': 7.83, 'trend': nan, 'seasonal': 0.04951104761291364, 'resid': nan}, {'Date': '2007-11-01', 'loan_rate_A': 7.779090909090908, 'trend': nan, 'seasonal': -0.034519089222105774, 'resid': nan}, {'Date': '2007-12-01', 'loan_rate_A': 7.695833333333333, 'trend': nan, 'seasonal': 0.06051880355383553, 'resid': nan}, {'Date': '2008-01-01', 'loan_rate_A': 7.961333333333333, 'trend': nan, 'seasonal': 0.029051115578029918, 'resid': nan}, {'Date': '2008-02-01', 'loan_rate_A': 8.130333333333333, 'trend': 8.005048767959094, 'seasonal': -0.010261300106122759, 'resid': 0.13554586548036146}, {'Date': '2008-03-01', 'loan_rate_A': 8.126285714285714, 'trend': 8.036669799705125, 'seasonal': -0.06065012545155052, 'resid': 0.15026604003213978}...\n                    \n                \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n\n\n\nseasonal_decomposition.metric.value[\"loan_rate_A\"][0:5]\n\n[{'Date': '2007-08-01',\n  'loan_rate_A': 7.7666666666666675,\n  'trend': nan,\n  'seasonal': 0.010868499397109575,\n  'resid': nan},\n {'Date': '2007-09-01',\n  'loan_rate_A': 7.841428571428572,\n  'trend': nan,\n  'seasonal': 0.0013181145386968443,\n  'resid': nan},\n {'Date': '2007-10-01',\n  'loan_rate_A': 7.83,\n  'trend': nan,\n  'seasonal': 0.04951104761291364,\n  'resid': nan},\n {'Date': '2007-11-01',\n  'loan_rate_A': 7.779090909090908,\n  'trend': nan,\n  'seasonal': -0.034519089222105774,\n  'resid': nan},\n {'Date': '2007-12-01',\n  'loan_rate_A': 7.695833333333333,\n  'trend': nan,\n  'seasonal': 0.06051880355383553,\n  'resid': nan}]"
  },
  {
    "objectID": "notebooks/lending_club.html",
    "href": "notebooks/lending_club.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\nTrue\n\n\n\ndf = pd.read_pickle(\"notebooks/datasets/_temp/df_loans_cleaned.pickle\")\n\ntargets = vm.DatasetTargets(\n    target_column=\"loan_status\",\n    class_labels={\n        \"Fully Paid\": \"Fully Paid\",\n        \"Charged Off\": \"Charged Off\",\n    }\n)\n\nvm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nTrue\n\n\n\nresults = vm.run_dataset_tests(df, target_column=\"loan_status\", dataset_type=\"training\", vm_dataset=vm_dataset, send=True)\n\nRunning data quality tests for \"training\" dataset...\n\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 74.72it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\n\n\n\n\n\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: pearson_correlation\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest                 Passed      # Passed    # Errors    % Passed\n-------------------  --------  ----------  ----------  ----------\nclass_imbalance      True               1           0         100\nduplicates           False              0           1           0\ncardinality          False             14           7     66.6667\nmissing              False             25          53     32.0513\npearson_correlation  False              0          10           0\nskewness             False              3           6     33.3333\nzeros                False              1           3          25\n\n\n\n\ntrain_ds, val_ds = train_test_split(df, test_size=0.20)\n\nx_train = train_ds.drop(\"loan_status\", axis=1)\nx_val = val_ds.drop(\"loan_status\", axis=1)\ny_train = train_ds.loc[:, \"loan_status\"].astype(str)\ny_val = val_ds.loc[:, \"loan_status\"].astype(str)\n\n\nxgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\nxgb_model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = xgb_model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\nvm.log_model(xgb_model)"
  },
  {
    "objectID": "notebooks/model_diagnosis_test_plan.html",
    "href": "notebooks/model_diagnosis_test_plan.html",
    "title": "ValidMind",
    "section": "",
    "text": "Dataset: bank customer churn dataset: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data\nTwo models: we want to allow the model diagnosis functions to work for statsmodels and sklearn model interfaces since they have different predict() signatures.\n\nXGBoost/SKLearn classifier\nLogistic Regression with statsmodels\n\nTest plans\n\nModel weak spots\n\nSingle variable only\n\nModel overfit\n\nSingle variable only\n\nModel robustness\n\nAll features and single feature\n\n\nReference:\n\nPiML Toolbox: https://github.com/SelfExplainML/PiML-Toolbox\nExample notebook with model diagnosis: https://colab.research.google.com/github/SelfExplainML/PiML-Toolbox/blob/main/examples/Example_BikeSharing.ipynb\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgmfxwvp0000k8rlc997oe9t\"\n)\n\nTrue\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"./notebooks/datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_df.drop(\"Exited\", axis=1)\ny_train = train_df.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.955\n\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"sklearn_classifier\")\nList available diagnosis tests: vm.test_plans.describe_plan(\"sklearn_classifier_model_diagnosis\")\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                          Description                                           \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics      Test plan for sklearn classifier metrics              \nsklearn_classifier_validation     SKLearnClassifierPerformance  Test plan for sklearn classifier models               \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis    Test plan for sklearn classifier model diagnosis tests\nsklearn_classifier                SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                       \ntabular_dataset                   TabularDataset                Test plan for generic tabular datasets                \ntabular_dataset_description       TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                       \ntabular_data_quality              TabularDataQuality            Test plan for data quality on tabular datasets        \nnormality_test_plan               NormalityTestPlan             Test plan to perform normality tests.                 \nautocorrelation_test_plan         AutocorrelationTestPlan       Test plan to perform autocorrelation tests.           \nseasonality_test_plan             SesonalityTestPlan            Test plan to perform seasonality tests.               \nunit_root                         UnitRoot                      Test plan to perform unit root tests.                 \nstationarity_test_plan            StationarityTestPlan          Test plan to perform stationarity tests.              \ntimeseries                        TimeSeries                    Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                       \ntimeseries_univariate_inspection  TimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.     \n\n\n\n\n\nvm.test_plans.describe_plan(\"sklearn_classifier\")\n\n\n\n\nAttribute       Value                                                                                                \n\n\nID              sklearn_classifier                                                                                   \nName            SKLearnClassifier                                                                                    \nDescription     Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                                                                      \nRequired Context['model', 'train_ds', 'test_ds']                                                                     \nTests                                                                                                                \nTest Plans      ['sklearn_classifier_metrics', 'sklearn_classifier_validation', 'sklearn_classifier_model_diagnosis']\n\n\n\n\n\nvm.test_plans.describe_plan(\"sklearn_classifier_model_diagnosis\")\n\n\n\n\nAttribute       Value                                                                                                                \n\n\nID              sklearn_classifier_model_diagnosis                                                                                   \nName            SKLearnClassifierDiagnosis                                                                                           \nDescription     Test plan for sklearn classifier model diagnosis tests                                                               \nRequired Context['model', 'train_ds', 'test_ds']                                                                                     \nTests           OverfitDiagnosisTest (ThresholdTest), WeakspotsDiagnosisTest (ThresholdTest), RobustnessDiagnosisTest (ThresholdTest)\nTest Plans      []                                                                                                                   \n\n\n\n\n\n\n\nWe can now run the SKLearnClassifier->SKLearnClassifierDiagnosis test plan: #### Define config\n\nconfig={\n    \"overfit_regions\": {\n        \"cut_off_percentage\": 3,\n        \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n    },\n    \"weak_spots\":{\n        \"features_columns\": [\"Age\", \"Balance\"],\n        \"accuracy_gap_threshold\": 85,\n    },\n    \"robustness\":{\n        \"features_columns\": [ \"Balance\", \"Tenure\", \"NumOfProducts\"],\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n    }\n}\nmodel_diagnosis_test_plan = vm.run_test_plan(\"sklearn_classifier_model_diagnosis\", \n                                             model=vm_model,\n                                             train_ds=vm_train_ds,\n                                             test_ds=vm_test_ds,\n                                             config=config)\n\n                                                                                                                                  \n\n\n\n        \n        \n            \n                \n                    \n                        Overfit Regions\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    overfit_regions\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'cut_off_percentage': 3, 'feature_columns': ['Age', 'Balance', 'Tenure', 'NumOfProducts']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column='Gender', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.9, 1.0]'], 'training records': [2591, 2209], 'training accuracy': [95.36858355847163, 94.6129470348574], 'test accuracy': [87.26467331118494, 84.64849354375896], 'gap': [8.103910247286692, 9.964453491098439]}), TestResult(test_name='accuracy', column='Age', passed=False, values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(84.6, 92.0]'], 'training records': [294, 1036, 1749, 908, 442, 197, 97, 65, 4], 'training accuracy': [98.63945578231292, 96.91119691119691, 94.33962264150944, 93.28193832599119, 93.21266968325791, 95.93908629441624, 96.90721649484536, 96.92307692307692, 100.0], 'test accuracy': [90.21739130434783, 93.01675977653632, 87.73747841105354, 80.13029315960912, 75.69444444444444, 82.43243243243244, 77.77777777777779, 88.23529411764706, 0.0], 'gap': [8.422064477965094, 3.894437134660592, 6.602144230455892, 13.151645166382067, 17.51822523881347, 13.506653861983807, 19.129438717067572, 8.687782805429862, 100.0]}), TestResult(test_name='accuracy', column='Tenure', passed=False, values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'training records': [679, 499, 517, 467, 473, 437, 493, 502, 475, 258], 'training accuracy': [95.87628865979381, 94.58917835671342, 95.16441005802709, 94.21841541755889, 95.9830866807611, 91.9908466819222, 95.53752535496957, 94.62151394422311, 96.21052631578947, 95.73643410852713], 'test accuracy': [83.33333333333334, 87.57396449704143, 90.25974025974025, 82.97872340425532, 84.88372093023256, 84.35754189944134, 88.57142857142857, 85.625, 90.36144578313254, 80.88235294117648], 'gap': [12.542955326460472, 7.015213859671988, 4.904669798286832, 11.239692013303568, 11.099365750528548, 7.633304782480863, 6.966096783541005, 8.99651394422311, 5.849080532656927, 14.854081167350657]}), TestResult(test_name='accuracy', column='Balance', passed=False, values={'slice': ['(-222.268, 22226.763]', '(22226.763, 44453.526]', '(44453.526, 66680.289]', '(66680.289, 88907.052]', '(88907.052, 111133.815]', '(111133.815, 133360.578]', '(133360.578, 155587.341]', '(155587.341, 177814.104]', '(177814.104, 200040.867]', '(200040.867, 222267.63]'], 'training records': [1762, 20, 94, 325, 720, 886, 642, 260, 76, 15], 'training accuracy': [94.66515323496027, 95.0, 92.5531914893617, 93.23076923076923, 95.83333333333334, 93.56659142212189, 96.41744548286604, 98.07692307692307, 100.0, 100.0], 'test accuracy': [90.25270758122743, 80.0, 86.20689655172413, 87.28813559322035, 84.84848484848484, 85.32423208191126, 80.36529680365297, 84.81012658227847, 80.76923076923077, 33.33333333333333], 'gap': [4.412445653732846, 15.0, 6.346294937637566, 5.94263363754888, 10.984848484848499, 8.242359340210626, 16.052148679213076, 13.266796494644595, 19.230769230769226, 66.66666666666667]}), TestResult(test_name='accuracy', column='NumOfProducts', passed=False, values={'slice': ['(0.997, 1.3]', '(1.9, 2.2]', '(2.8, 3.1]'], 'training records': [2438, 2197, 137], 'training accuracy': [93.39622641509435, 96.44970414201184, 100.0], 'test accuracy': [79.65686274509804, 92.92517006802721, 90.47619047619048], 'gap': [13.739363669996308, 3.5245340739846256, 9.523809523809518]}), TestResult(test_name='accuracy', column='HasCrCard', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.9, 1.0]'], 'training records': [1444, 3356], 'training accuracy': [95.77562326869807, 94.69606674612633], 'test accuracy': [84.27672955974843, 86.9100623330365], 'gap': [11.498893708949637, 7.786004413089827]}), TestResult(test_name='accuracy', column='IsActiveMember', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.9, 1.0]'], 'training records': [2276, 2524], 'training accuracy': [94.59578207381371, 95.4041204437401], 'test accuracy': [82.74044795783925, 89.179548156956], 'gap': [11.855334115974458, 6.224572286784095]}), TestResult(test_name='accuracy', column='EstimatedSalary', passed=False, values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'training records': [440, 460, 476, 504, 475, 496, 484, 465, 508, 492], 'training accuracy': [97.5, 96.52173913043478, 93.27731092436974, 95.03968253968253, 92.84210526315789, 93.14516129032258, 94.00826446280992, 94.6236559139785, 95.86614173228347, 97.5609756097561], 'test accuracy': [87.00564971751412, 83.9506172839506, 87.42514970059881, 88.0, 84.02366863905326, 89.171974522293, 85.2760736196319, 86.875, 83.22147651006712, 85.9504132231405], 'gap': [10.494350282485883, 12.571121846484175, 5.852161223770935, 7.039682539682531, 8.818436624104635, 3.973186768029578, 8.732190843178017, 7.748655913978496, 12.644665222216346, 11.610562386615598]}), TestResult(test_name='accuracy', column='Geography_France', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.9, 1.0]'], 'training records': [2398, 2402], 'training accuracy': [95.12093411175981, 94.92089925062447], 'test accuracy': [85.0253807106599, 87.192118226601], 'gap': [10.095553401099906, 7.728781024023476]}), TestResult(test_name='accuracy', column='Geography_Germany', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.9, 1.0]'], 'training records': [3590, 1210], 'training accuracy': [94.98607242339833, 95.12396694214877], 'test accuracy': [87.04013377926422, 83.41584158415841], 'gap': [7.9459386441341024, 11.708125357990355]}), TestResult(test_name='accuracy', column='Geography_Spain', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.9, 1.0]'], 'training records': [3612, 1188], 'training accuracy': [94.9889258028793, 95.1178451178451], 'test accuracy': [85.9375, 86.71875], 'gap': [9.051425802879294, 8.399095117845107]})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Weak Spots\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    weak_spots\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': ['Age', 'Balance'], 'thresholds': {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.5, 'f1': 0.7}, 'accuracy_gap_threshold': 85}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column='Age', passed=False, values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [294, 1036, 1749, 908, 442, 197, 97, 65, 8, 4, 92, 358, 579, 307, 144, 74, 27, 17, 2, 0], 'accuracy': [0.9863945578231292, 0.9691119691119691, 0.9433962264150944, 0.9328193832599119, 0.9321266968325792, 0.9593908629441624, 0.9690721649484536, 0.9692307692307692, 1.0, 1.0, 0.9021739130434783, 0.9301675977653632, 0.8773747841105354, 0.8013029315960912, 0.7569444444444444, 0.8243243243243243, 0.7777777777777778, 0.8823529411764706, 1.0, 0.0], 'precision': [1.0, 1.0, 0.984375, 0.937984496124031, 0.9264069264069265, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.5405405405405406, 0.6086956521739131, 0.825, 0.875, 0.75, 0.5, 0.0, 0.0], 'recall': [0.7894736842105263, 0.6235294117647059, 0.5650224215246636, 0.8432055749128919, 0.9427312775330396, 0.9191919191919192, 0.8928571428571429, 0.75, 1.0, 0.0, 0.1111111111111111, 0.08333333333333333, 0.2702702702702703, 0.5526315789473685, 0.7586206896551724, 0.7567567567567568, 0.375, 1.0, 0.0, 0.0], 'f1': [0.8823529411764706, 0.7681159420289855, 0.717948717948718, 0.8880733944954128, 0.9344978165938864, 0.9578947368421054, 0.9433962264150945, 0.8571428571428571, 1.0, 0.0, 0.1818181818181818, 0.13793103448275862, 0.3603603603603604, 0.5793103448275861, 0.7904191616766467, 0.8115942028985507, 0.5, 0.6666666666666666, 0.0, 0.0]}), TestResult(test_name='accuracy', column='Balance', passed=False, values={'slice': ['(-222.268, 22226.763]', '(22226.763, 44453.526]', '(44453.526, 66680.289]', '(66680.289, 88907.052]', '(88907.052, 111133.815]', '(111133.815, 133360.578]', '(133360.578, 155587.341]', '(155587.341, 177814.104]', '(177814.104, 200040.867]', '(200040.867, 222267.63]', '(-222.268, 22226.763]', '(22226.763, 44453.526]', '(44453.526, 66680.289]', '(66680.289, 88907.052]', '(88907.052, 111133.815]', '(111133.815, 133360.578]', '(133360.578, 155587.341]', '(155587.341, 177814.104]', '(177814.104, 200040.867]', '(200040.867, 222267.63]'], 'shape': [1762, 20, 94, 325, 720, 886, 642, 260, 76, 15, 554, 10, 29, 118, 264, 293, 219, 79, 26, 6], 'accuracy': [0.9466515323496028, 0.95, 0.925531914893617, 0.9323076923076923, 0.9583333333333334, 0.9356659142212189, 0.9641744548286605, 0.9807692307692307, 1.0, 1.0, 0.9025270758122743, 0.8, 0.8620689655172413, 0.8728813559322034, 0.8484848484848485, 0.8532423208191127, 0.8036529680365296, 0.8481012658227848, 0.8076923076923077, 0.3333333333333333], 'precision': [0.9349112426035503, 1.0, 0.8888888888888888, 0.9361702127659575, 0.967948717948718, 0.9360730593607306, 1.0, 1.0, 1.0, 1.0, 0.7192982456140351, 1.0, 0.75, 1.0, 0.725, 0.696969696969697, 0.6363636363636364, 0.5454545454545454, 0.75, 0.25], 'recall': [0.6556016597510373, 0.8333333333333334, 0.7619047619047619, 0.6984126984126984, 0.8579545454545454, 0.8266129032258065, 0.8357142857142857, 0.9090909090909091, 1.0, 1.0, 0.5189873417721519, 0.3333333333333333, 0.5, 0.4230769230769231, 0.5, 0.6666666666666666, 0.40384615384615385, 0.46153846153846156, 0.42857142857142855, 0.5], 'f1': [0.7707317073170732, 0.9090909090909091, 0.8205128205128205, 0.8, 0.9096385542168673, 0.8779443254817988, 0.9105058365758755, 0.9523809523809523, 1.0, 1.0, 0.6029411764705882, 0.5, 0.6, 0.5945945945945945, 0.5918367346938775, 0.6814814814814815, 0.4941176470588235, 0.4999999999999999, 0.5454545454545454, 0.3333333333333333]})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Robustness\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    robustness\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'features_columns': ['Balance', 'Tenure', 'NumOfProducts'], 'scaling_factor_std_dev_list': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column='Balance', passed=True, values={'Perturbation Size': [0.0, 0.0, 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 0.4, 0.4, 0.5, 0.5], 'Dataset Type': ['Traning', 'Test', 'Traning', 'Test', 'Traning', 'Test', 'Traning', 'Test', 'Traning', 'Test', 'Traning', 'Test'], 'Records': [4800, 1600, 4800, 1600, 4800, 1600, 4800, 1600, 4800, 1600, 4800, 1600], 'accuracy': [95.02083333333333, 86.125, 91.77083333333333, 86.25, 90.64583333333334, 85.6875, 89.9375, 85.0625, 88.45833333333334, 85.5, 86.54166666666666, 82.9375]})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "ValidMind",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv .env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv\n\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard.\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nWe can now initialize the TabularDataset test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\ntabular_plan = vm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n                                                                                                                                        \n\n\n\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        descriptive_statistics\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'numerical': [{'Name': 'RowNumber', 'Count': 8000.0, 'Mean': 5020.52, 'Std': 2885.7185155986554, 'Min': 1.0, '25%': 2518.75, '50%': 5036.5, '75%': 7512.25, '90%': 9015.1, '95%': 9516.05, 'Max': 10000.0}, {'Name': 'CustomerId', 'Count': 8000.0, 'Mean': 15690474.465625, 'Std': 71902.473335347, 'Min': 15565701.0, '25%': 15628163.75, '50%': 15690143.5, '75%': 15752378.25, '90%': 15790809.1, '95%': 15802760.55, 'Max': 15815660.0}, {'Name': 'CreditScore', 'Count': 8000.0, 'Mean': 650.159625, 'Std': 96.84623014808636, 'Min': 350.0, '25%': 583.0, '50%': 651.5, '75%': 717.0, '90%': 778.0, '95%': 813.0, 'Max': 850.0}, {'Name': 'Age', 'Count': 8000.0, 'Mean': 38.948875, 'Std': 10.458952382767269, 'Min': 18.0, '25%': 32.0, '50%': 37.0, '75%': 44.0, '90%': 53.0, '95%': 60.0, 'Max': 92.0}, {'Name': 'Tenure', 'Count': 8000.0, 'Mean': 5.033875, 'Std': 2.885267419215253, 'Min': 0.0, '25%': 3.0, '50%': 5.0, '75%': 8.0, '90%': 9.0, '95%': 9.0, 'Max': 10.0}, {'Name': 'Balance', 'Count': 8000.0, 'Mean': 76434.09651125, 'Std': 62...\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        training\n                    \n                \n                \n                    Metric Value\n                    \n                        [[{'field': 'CreditScore', 'value': 1.0}, {'field': 'Geography', 'value': 0.010103440458197478}, {'field': 'Gender', 'value': 0.008251776778083898}, {'field': 'Age', 'value': -0.007269780957496768}, {'field': 'Tenure', 'value': -0.006914675142663373}, {'field': 'NumOfProducts', 'value': 0.005677094521946256}, {'field': 'HasCrCard', 'value': -0.009291152528707963}, {'field': 'IsActiveMember', 'value': 0.030554141043824444}, {'field': 'Exited', 'value': -0.025533166369817405}], [{'field': 'CreditScore', 'value': 0.010103440458197478}, {'field': 'Geography', 'value': 1.0}, {'field': 'Gender', 'value': 0.035023152881466464}, {'field': 'Age', 'value': 0.053602289473512775}, {'field': 'Tenure', 'value': 0.015510338111733172}, {'field': 'NumOfProducts', 'value': 0.011118424429054087}, {'field': 'HasCrCard', 'value': 0.021747611293409512}, {'field': 'IsActiveMember', 'value': 0.02017951122934769}, {'field': 'Exited', 'value': 0.1784101181767361}], [{'field': 'CreditScore', 'value': 0.008251776778083898}, {'field': 'G...\n                    \n                \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n        \n        \n\n\nTest plan for data quality on tabular datasets\n        \n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(values={0: 0.798, 1: 0.202}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'n_duplicates': 0, 'p_duplicates': 0.0}, test_name=None, column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(values={'n_distinct': 2616, 'p_distinct': 0.327}, test_name=None, column='Surname', passed=False), TestResult(values={'n_distinct': 3, 'p_distinct': 0.000375}, test_name=None, column='Geography', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='Gender', passed=True), TestResult(values={'n_distinct': 4, 'p_distinct': 0.0005}, test_name=None, column='NumOfProducts', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='HasCrCard', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='IsActiveMember', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389458}]}, test_name=None, column='Balance', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='RowNumber', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='CustomerId', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Surname', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='CreditScore', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Geography', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Gender', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Age', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Tenure', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Balance', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='NumOfProducts', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='HasCrCard', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='IsActiveMember', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='EstimatedSalary', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'skewness': -0.005920679739677088}, test_name=None, column='RowNumber', passed=True), TestResult(values={'skewness': 0.010032280260684402}, test_name=None, column='CustomerId', passed=True), TestResult(values={'skewness': -0.06195161237091896}, test_name=None, column='CreditScore', passed=True), TestResult(values={'skewness': 1.0245221429799511}, test_name=None, column='Age', passed=False), TestResult(values={'skewness': 0.007692043774702702}, test_name=None, column='Tenure', passed=True), TestResult(values={'skewness': -0.13527693543111804}, test_name=None, column='Balance', passed=True), TestResult(values={'skewness': 0.009510428002077728}, test_name=None, column='EstimatedSalary', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'n_unique': 8000, 'p_unique': 1.0}, test_name=None, column='RowNumber', passed=False), TestResult(values={'n_unique': 8000, 'p_unique': 1.0}, test_name=None, column='CustomerId', passed=False), TestResult(values={'n_unique': 2616, 'p_unique': 0.327}, test_name=None, column='Surname', passed=True), TestResult(values={'n_unique': 452, 'p_unique': 0.0565}, test_name=None, column='CreditScore', passed=True), TestResult(values={'n_unique': 3, 'p_unique': 0.000375}, test_name=None, column='Geography', passed=True), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='Gender', passed=True), TestResult(values={'n_unique': 69, 'p_unique': 0.008625}, test_name=None, column='Age', passed=True), TestResult(values={'n_unique': 11, 'p_unique': 0.001375}, test_name=None, column='Tenure', passed=True), TestResult(values={'n_unique': 5088, 'p_unique': 0.636}, test_name=None, column='Balance', passed=True), TestResult(values={'n_unique': 4, 'p_unique': 0.0005}, test_name=None, column='NumOfProducts', passed=True), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='HasCrCard', passed=True), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='IsActiveMember', passed=True), TestResult(values={'n_unique': 8000, 'p_unique': 1.0}, test_name=None, column='EstimatedSalary', passed=False), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(values={'n_zeros': 323, 'p_zeros': 0.040375}, test_name=None, column='Tenure', passed=False), TestResult(values={'n_zeros': 2912, 'p_zeros': 0.364}, test_name=None, column='Balance', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                        Description                                            \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics    Test plan for sklearn classifier metrics               \nsklearn_classifier_validation     SKLearnClassifierPerformanceTest plan for sklearn classifier models                \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests \nsklearn_classifier                SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                        \ntabular_dataset                   TabularDataset              Test plan for generic tabular datasets                 \ntabular_dataset_description       TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                        \ntabular_data_quality              TabularDataQuality          Test plan for data quality on tabular datasets         \nnormality_test_plan               NormalityTestPlan           Test plan to perform normality tests.                  \nautocorrelation_test_plan         AutocorrelationTestPlan     Test plan to perform autocorrelation tests.            \nseasonality_test_plan             SesonalityTestPlan          Test plan to perform seasonality tests.                \nunit_root                         UnitRoot                    Test plan to perform unit root tests.                  \nstationarity_test_plan            StationarityTestPlan        Test plan to perform stationarity tests.               \ntimeseries                        TimeSeries                  Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                        \ntime_series_data_quality          TimeSeriesDataQuality       Test plan for data quality on time series datasets     \ntime_series_dataset               TimeSeriesDataset           Test plan for time series  datasets                    \ntime_series_univariate            TimeSeriesUnivariate        Test plan to perform time series univariate analysis.  \ntime_series_multivariate          TimeSeriesMultivariate      Test plan to perform time series multivariate analysis.\ntime_series_forecast              TimeSeriesForecast          Test plan to perform time series forecast tests.       \nregression_model_performance      RegressionModelPerformance  Test plan for statsmodels regressor models that includes\n    both metrics and validation tests                                                        \n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                         Name                        Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n\n\nCustom Test  dataset_metadata           DatasetMetadata             Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadat is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nCustom Test  shap                       SHAPGlobalImportance        SHAP Global Importance. Custom metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nMetric       acf_pacf_plot              ACFandPACFPlot              Plots ACF and PACF for a given time series dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nMetric       adf                        ADF                         Augmented Dickey-Fuller unit root test for establishing the order of integration of\n    time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       accuracy                   AccuracyScore               Accuracy Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \nMetric       box_pierce                 BoxPierce                   The Box-Pierce test is a statistical test used to determine\n    whether a given set of data has autocorrelations\n    that are different from zero.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       csi                        CharacteristicStabilityIndexCharacteristic Stability Index between two datasets                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nMetric       confusion_matrix           ConfusionMatrix             Confusion Matrix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \nMetric       dickey_fuller_gls          DFGLSArch                   Dickey-Fuller GLS unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       dataset_correlations       DatasetCorrelations         Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       dataset_description        DatasetDescription          Collects a set of descriptive statistics for a dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nMetric       dataset_split              DatasetSplit                Attempts to extract information about the dataset split from the\n    provided training, test and validation datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       descriptive_statistics     DescriptiveStatistics       Collects a set of descriptive statistics for a dataset, both for\n    numerical and categorical variables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       f1_score                   F1Score                     F1 Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nMetric       jarque_bera                JarqueBera                  The Jarque-Bera test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       kpss                       KPSS                        Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       kolmogorov_smirnov         KolmogorovSmirnov           The Kolmogorov-Smirnov metric is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       ljung_box                  LJungBox                    The Ljung-Box test is a statistical test used to determine\n    whether a given set of data has autocorrelations\n    that are different from zero.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       lagged_correlation_heatmap LaggedCorrelationHeatmap    Generates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nMetric       lilliefors_test            Lilliefors                  The Lilliefors test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       model_metadata             ModelMetadata               Custom class to collect the following metadata for a model:\n    - Model architecture\n    - Model hyperparameters\n    - Model task type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       model_prediction_ols       ModelPredictionOLS          Calculates and plots the model predictions for each of the models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \nMetric       pfi                        PermutationFeatureImportancePermutation Feature Importance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \nMetric       phillips_perron            PhillipsPerronArch          Phillips-Perron (PP) unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       psi                        PopulationStabilityIndex    Population Stability Index between two datasets                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       pr_curve                   PrecisionRecallCurve        Precision Recall Curve                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nMetric       precision                  PrecisionScore              Precision Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       roc_auc                    ROCAUCScore                 ROC AUC Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \nMetric       roc_curve                  ROCCurve                    ROC Curve                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \nMetric       recall                     RecallScore                 Recall Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \nMetric                                  RegressionModelSummary      Test that output the summary of regression models of statsmodel library.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nMetric       residuals_visual_inspectionResidualsVisualInspection   Log plots for visual inspection of residuals                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \nMetric       rolling_stats_plot         RollingStatsPlot            This class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\nMetric       runs_test                  RunsTest                    The runs test is a statistical test used to determine whether a given set\n    of data has runs of positive and negative values that are longer than expected\n    under the null hypothesis of randomness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       scatter_plot               ScatterPlot                 Generates a visual analysis of data by plotting a scatter plot matrix for all columns\n    in the dataset. The input dataset can have multiple columns (features) if necessary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       shapiro_wilk               ShapiroWilk                 The Shapiro-Wilk test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       spread_plot                SpreadPlot                  This class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.                                                                                                                                                                                                                                                                                                                             \nMetric       time_series_histogram      TimeSeriesHistogram         Generates a visual analysis of time series data by plotting the\n    histogram. The input dataset can have multiple time series if\n    necessary. In this case we produce a separate plot for each time series.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       time_series_line_plot      TimeSeriesLinePlot          Generates a visual analysis of time series data by plotting the\n    raw time series. The input dataset can have multiple time series\n    if necessary. In this case we produce a separate plot for each time series.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       zivot_andrews              ZivotAndrewsArch            Zivot-Andrews unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestclass_imbalance            ClassImbalance              The class imbalance test measures the disparity between the majority\n    class and the minority class in the target column.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestduplicates                 Duplicates                  The duplicates test measures the number of duplicate rows found in\n    the dataset. If a primary key column is specified, the dataset is\n    checked for duplicate primary keys as well.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestcardinality                HighCardinality             The high cardinality test measures the number of unique\n    values found in categorical columns.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestpearson_correlation        HighPearsonCorrelation      Test that the pairwise Pearson correlation coefficients between the\n    features in the dataset do not exceed a specified threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestaccuracy_score             MinimumAccuracy             Test that the model's prediction accuracy on a dataset meets or\n    exceeds a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestf1_score                   MinimumF1Score              Test that the model's F1 score on the validation dataset meets or\n    exceeds a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestroc_auc_score              MinimumROCAUCScore          Test that the model's ROC AUC score on the validation dataset meets or\n    exceeds a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestmissing                    MissingValues               Test that the number of missing values in the dataset across all features\n    is less than a threshold                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestoverfit_regions            OverfitDiagnosis            Test that identify overfit regions with high residuals by histogram slicing techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \nThresholdTestrobustness                 RobustnessDiagnosis         Test robustness of model by perturbing the features column values                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \nThresholdTestskewness                   Skewness                    The skewness test measures the extent to which a distribution of\n    values differs from a normal distribution. A positive skew describes\n    a longer tail of values in the right and a negative skew describes a\n    longer tail of values in the left.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTesttime_series_frequency      TimeSeriesFrequency         Test that detect frequencies in the data                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nThresholdTesttime_series_missing_values TimeSeriesMissingValues     Test that the number of missing values is less than a threshold                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTesttime_series_outliers       TimeSeriesOutliers          Test that find outliers for time series data using the z-score method                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nThresholdTestzeros                      TooManyZeroValues           The zeros test finds columns that have too many zero values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \nThresholdTesttraining_test_degradation  TrainingTestDegradation     Test that the degradation in performance between the training and test datasets\n    does not exceed a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestunique                     UniqueRows                  Test that the number of unique rows is greater than a threshold                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestweak_spots                 WeakspotsDiagnosis          Test that identify weak regions with high residuals by histogram slicing techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.858125\n\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe can now run the SKLearnClassifier test plan:\n\nmodel_plan = vm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n                                                                                                                                   \n\n\nTest plan for sklearn classifier metrics\n            \n            \n            \n                \n                    \n                        Metric Name\n                        model_metadata\n                    \n                    \n                        Metric Type\n                        model\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'architecture': 'Extreme Gradient Boosting', 'task': 'classification', 'subtask': 'binary', 'framework': 'XGBoost', 'framework_version': '1.7.5', 'params': {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'eval_metric': ['error', 'logloss', 'auc'], 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'monotone_constraints': None, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}}\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        dataset_split\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'train_ds_size': 4800, 'train_ds_proportion': 0.75, 'test_ds_size': 1600, 'test_ds_proportion': 0.25, 'total_size': 6400}\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.85375\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'tn': 1220, 'fp': 46, 'fn': 188, 'tp': 146}\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.5551330798479087\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n                \n                    Metric Value\n                    \n                        {'Gender': ([0.005500000000000016], [0.001168153814072993]), 'Age': ([0.09354166666666668], [0.004370036867375639]), 'Tenure': ([0.004833333333333356], [0.00033333333333333826]), 'Balance': ([0.022333333333333316], [0.0020258742968571877]), 'NumOfProducts': ([0.06670833333333333], [0.0018929694486000744]), 'HasCrCard': ([0.0016666666666666607], [0.00029462782549439376]), 'IsActiveMember': ([0.03441666666666667], [0.0014813657362192476]), 'EstimatedSalary': ([0.009208333333333329], [0.0014754942674688038]), 'Geography_France': ([0.0007083333333333331], [0.0005368374469469038]), 'Geography_Germany': ([0.014666666666666672], [0.0017410485346479926]), 'Geography_Spain': ([0.0001666666666666705], [0.0004639803635691593])}\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'precision': array([0.20875   , 0.20901126, 0.2130102 , ..., 1.        , 1.        ,\n       1.        ]), 'recall': array([1.        , 1.        , 1.        , ..., 0.00598802, 0.00299401,\n       0.        ]), 'thresholds': array([0.022918  , 0.02306371, 0.0250723 , ..., 0.96385795, 0.96962535,\n       0.9773121 ], dtype=float32)}\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.7604166666666666\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.437125748502994\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.7003954176954149\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'auc': 0.7003954176954149, 'fpr': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.89889415e-04,\n       7.89889415e-04, 7.89889415e-04, 7.89889415e-04, 7.89889415e-04,\n       7.89889415e-04, 1.57977883e-03, 1.57977883e-03, 1.57977883e-03,\n       1.57977883e-03, 1.57977883e-03, 1.57977883e-03, 2.36966825e-03,\n       2.36966825e-03, 2.36966825e-03, 2.36966825e-03, 3.15955766e-03,\n       3.15955766e-03, 3.94944708e-03, 3.94944708e-03, 4.73933649e-03,\n       4.73933649e-03, 5.52922591e-03, 5.52922591e-03, 6.31911532e-03,\n       6.31911532e-03, 6.31911532e-03, 6.31911532e-03, 1.02685624e-02,\n       1.02685624e-02, 1.10584518e-02, 1.10584518e-02, 1.26382306e-02,\n       1.26382306e-02, 1.34281201e-02, 1.34281201e-02, 1.42180095e-02,\n       1.42180095e-02, 1.50078989e-02, 1.50078989e-02, 1.57977883e-02,\n       1.57977883e-02, 1.65876777e-02, 1.65876777e-02, 1.73775671e-02,\n       1.73775671e-02, 1.81674566e-02, 1.81674566e-02, 1.97472354e-02,\n       1.97472354e-02, 2.05371248e-02, 2.05371248e-02, 2.13270142e...\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'Gender': 3.7e-05, 'Age': 0.000377, 'Tenure': 0.000354, 'Balance': 0.000943, 'NumOfProducts': 0.000261, 'HasCrCard': 9.3e-05, 'IsActiveMember': 0.0, 'EstimatedSalary': 0.000372, 'Geography_France': 9e-06, 'Geography_Germany': 1e-05, 'Geography_Spain': 0.0}\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                             initial  percent_initial  new  percent_new       psi\nbin                                                      \n1       2584          0.53833  855     0.534375  0.000029\n2        799          0.16646  255     0.159375  0.000308\n3        437          0.09104  149     0.093125  0.000047\n4        236          0.04917   91     0.056875  0.001123\n5        150          0.03125   58     0.036250  0.000742\n6        113          0.02354   42     0.026250  0.000295\n7        119          0.02479   37     0.023125  0.000116\n8         90          0.01875   25     0.015625  0.000570\n9        126          0.02625   37     0.023125  0.000396\n10       146          0.03042   51     0.031875  0.000068\n                    \n                \n            \n        \n        \n        \n            \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n\n\nTest plan for sklearn classifier models\n        \n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(values={'score': 0.72125, 'threshold': 0.7}, test_name=None, column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(values={'score': 0.1520912547528517, 'threshold': 0.5}, test_name=None, column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(values={'score': 0.4998486439443388, 'threshold': 0.5}, test_name=None, column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(values={'test_score': 0.72125, 'train_score': 0.728125, 'degradation': 0.009442060085837013}, test_name='accuracy', column=None, passed=True), TestResult(values={'test_score': 0.20833333333333334, 'train_score': 0.19865319865319866, 'degradation': -0.04872881355932202}, test_name='precision', column=None, passed=False), TestResult(values={'test_score': 0.11976047904191617, 'train_score': 0.1246040126715945, 'degradation': 0.03887140972292697}, test_name='recall', column=None, passed=True), TestResult(values={'test_score': 0.1520912547528517, 'train_score': 0.15314730694354314, 'degradation': 0.006895662821421627}, test_name='f1', column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\nTest plan for sklearn classifier model diagnosis tests\n        \n        \n            \n                \n                    \n                        Overfit Regions\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    overfit_regions\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'cut_off_percentage': 4}\n                \n            \n            \n                Results\n                [TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Gender', passed=True), TestResult(values={'slice': ['(62.4, 69.8]', '(69.8, 77.2]'], 'training records': [95, 62], 'training accuracy': [71.57894736842105, 82.25806451612904], 'test accuracy': [60.0, 75.0], 'gap': [11.578947368421055, 7.258064516129039]}, test_name='accuracy', column='Age', passed=False), TestResult(values={'slice': ['(-0.01, 1.0]', '(4.0, 5.0]', '(8.0, 9.0]'], 'training records': [692, 479, 468], 'training accuracy': [72.97687861271676, 72.44258872651356, 74.78632478632478], 'test accuracy': [65.625, 64.49704142011834, 69.27710843373494], 'gap': [7.35187861271676, 7.945547306395227, 5.509216352589846]}, test_name='accuracy', column='Tenure', passed=False), TestResult(values={'slice': ['(25089.809, 50179.618]', '(50179.618, 75269.427]'], 'training records': [24, 172], 'training accuracy': [75.0, 72.67441860465115], 'test accuracy': [62.5, 68.33333333333333], 'gap': [12.5, 4.341085271317823]}, test_name='accuracy', column='Balance', passed=False), TestResult(values={'slice': ['(3.7, 4.0]'], 'training records': [31], 'training accuracy': [22.58064516129032], 'test accuracy': [0.0], 'gap': [22.58064516129032]}, test_name='accuracy', column='NumOfProducts', passed=False), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='HasCrCard', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='IsActiveMember', passed=True), TestResult(values={'slice': ['(20009.67, 40007.76]', '(60005.85, 80003.94]'], 'training records': [475, 497], 'training accuracy': [73.89473684210527, 74.84909456740442], 'test accuracy': [68.96551724137932, 67.05202312138728], 'gap': [4.929219600725958, 7.797071446017142]}, test_name='accuracy', column='EstimatedSalary', passed=False), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Geography_France', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Geography_Germany', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Geography_Spain', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Weak Spots\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    weak_spots\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'thresholds': {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.5, 'f1': 0.7}}\n                \n            \n            \n                Results\n                [TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2647, 0, 0, 0, 0, 0, 0, 0, 0, 2153, 867, 0, 0, 0, 0, 0, 0, 0, 0, 733], 'accuracy': [0.7604835663014734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.688341848583372, 0.7381776239907728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7012278308321964], 'precision': [0.15868263473053892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.17647058823529413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24444444444444444], 'recall': [0.13054187192118227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12014787430683918, 0.11180124223602485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12716763005780346], 'f1': [0.14324324324324328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16229712858926343, 0.13688212927756652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1673003802281369]}, test_name='accuracy', column='Gender', passed=False), TestResult(values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [291, 1027, 1771, 904, 433, 207, 95, 62, 8, 2, 93, 343, 574, 315, 150, 80, 25, 16, 3, 1], 'accuracy': [0.8075601374570447, 0.8218111002921129, 0.784867306606437, 0.6537610619469026, 0.46882217090069284, 0.5072463768115942, 0.7157894736842105, 0.8225806451612904, 0.875, 0.5, 0.8602150537634409, 0.8221574344023324, 0.7578397212543554, 0.653968253968254, 0.52, 0.525, 0.6, 0.75, 1.0, 1.0], 'precision': [0.05263157894736842, 0.10852713178294573, 0.12962962962962962, 0.3, 0.5, 0.4166666666666667, 0.375, 0.0, 0.0, 0.0, 0.1111111111111111, 0.05, 0.1282051282051282, 0.18518518518518517, 0.7777777777777778, 0.5384615384615384, 0.16666666666666666, 0.0, 0.0, 0.0], 'recall': [0.09090909090909091, 0.17073170731707318, 0.12669683257918551, 0.13584905660377358, 0.10869565217391304, 0.10204081632653061, 0.12, 0.0, 0.0, 0.0, 0.16666666666666666, 0.08, 0.12345679012345678, 0.05434782608695652, 0.17073170731707318, 0.1794871794871795, 0.16666666666666666, 0.0, 0.0, 0.0], 'f1': [0.06666666666666667, 0.13270142180094788, 0.12814645308924483, 0.187012987012987, 0.17857142857142855, 0.16393442622950824, 0.18181818181818182, 0.0, 0.0, 0.0, 0.13333333333333333, 0.061538461538461535, 0.12578616352201258, 0.08403361344537816, 0.27999999999999997, 0.2692307692307692, 0.16666666666666666, 0.0, 0.0, 0.0]}, test_name='accuracy', column='Age', passed=False), TestResult(values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]', '(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'shape': [692, 509, 491, 476, 479, 472, 492, 495, 468, 226, 224, 175, 154, 160, 169, 147, 173, 149, 166, 83], 'accuracy': [0.7297687861271677, 0.6954813359528488, 0.714867617107943, 0.7310924369747899, 0.7244258872651357, 0.739406779661017, 0.7357723577235772, 0.7515151515151515, 0.7478632478632479, 0.6946902654867256, 0.65625, 0.7314285714285714, 0.7337662337662337, 0.71875, 0.6449704142011834, 0.7482993197278912, 0.8034682080924855, 0.7248322147651006, 0.6927710843373494, 0.8433734939759037], 'precision': [0.2112676056338028, 0.14102564102564102, 0.1694915254237288, 0.2413793103448276, 0.11428571428571428, 0.25925925925925924, 0.13846153846153847, 0.3508771929824561, 0.22641509433962265, 0.1724137931034483, 0.16666666666666666, 0.13043478260869565, 0.23076923076923078, 0.35, 0.15, 0.16666666666666666, 0.125, 0.09090909090909091, 0.3333333333333333, 0.6666666666666666], 'recall': [0.10273972602739725, 0.1111111111111111, 0.09900990099009901, 0.14285714285714285, 0.10256410256410256, 0.14432989690721648, 0.10843373493975904, 0.18867924528301888, 0.1348314606741573, 0.1, 0.11320754716981132, 0.1, 0.08823529411764706, 0.1794871794871795, 0.06521739130434782, 0.12, 0.09090909090909091, 0.08695652173913043, 0.10869565217391304, 0.375], 'f1': [0.13824884792626727, 0.12429378531073444, 0.12500000000000003, 0.1794871794871795, 0.1081081081081081, 0.18543046357615892, 0.12162162162162161, 0.24539877300613502, 0.16901408450704225, 0.1265822784810127, 0.1348314606741573, 0.11320754716981132, 0.12765957446808512, 0.23728813559322035, 0.09090909090909091, 0.13953488372093023, 0.10526315789473685, 0.08888888888888888, 0.16393442622950818, 0.4800000000000001]}, test_name='accuracy', column='Tenure', passed=False), TestResult(values={'slice': ['(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]', '(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]'], 'shape': [1750, 24, 172, 546, 1014, 859, 335, 88, 11, 1, 553, 16, 60, 186, 340, 275, 133, 29, 8, 0], 'accuracy': [0.7657142857142857, 0.75, 0.7267441860465116, 0.7435897435897436, 0.6932938856015779, 0.69965075669383, 0.7074626865671642, 0.7045454545454546, 0.2727272727272727, 0.0, 0.7649186256781193, 0.625, 0.6833333333333333, 0.7150537634408602, 0.6882352941176471, 0.7054545454545454, 0.706766917293233, 0.6896551724137931, 0.625, 0.0], 'precision': [0.136986301369863, 0.0, 0.18181818181818182, 0.140625, 0.32061068702290074, 0.25742574257425743, 0.10256410256410256, 0.17647058823529413, 0.0, 0.0, 0.10294117647058823, 0.0, 0.5, 0.18518518518518517, 0.34285714285714286, 0.25, 0.14285714285714285, 0.25, 0.0, 0.0], 'recall': [0.11952191235059761, 0.0, 0.12121212121212122, 0.09574468085106383, 0.1590909090909091, 0.12440191387559808, 0.05970149253731343, 0.2, 0.0, 0.0, 0.09210526315789473, 0.0, 0.2631578947368421, 0.1388888888888889, 0.12631578947368421, 0.12307692307692308, 0.06896551724137931, 0.14285714285714285, 0.0, 0.0], 'f1': [0.1276595744680851, 0.0, 0.14545454545454545, 0.11392405063291139, 0.21265822784810126, 0.16774193548387098, 0.07547169811320753, 0.18750000000000003, 0.0, 0.0, 0.09722222222222222, 0.0, 0.3448275862068966, 0.15873015873015875, 0.18461538461538463, 0.16494845360824742, 0.09302325581395349, 0.18181818181818182, 0.0, 0.0]}, test_name='accuracy', column='Balance', passed=False), TestResult(values={'slice': ['(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]', '(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]'], 'shape': [2447, 0, 0, 2199, 0, 0, 123, 0, 0, 31, 816, 0, 0, 728, 0, 0, 50, 0, 0, 6], 'accuracy': [0.6755210461789947, 0.0, 0.0, 0.8217371532514779, 0.0, 0.0, 0.22764227642276422, 0.0, 0.0, 0.22580645161290322, 0.6740196078431373, 0.0, 0.0, 0.8159340659340659, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0], 'precision': [0.2773972602739726, 0.0, 0.0, 0.07092198581560284, 0.0, 0.0, 0.7692307692307693, 0.0, 0.0, 1.0, 0.26126126126126126, 0.0, 0.0, 0.1038961038961039, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0], 'recall': [0.12198795180722892, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.09803921568627451, 0.0, 0.0, 0.22580645161290322, 0.13615023474178403, 0.0, 0.0, 0.1095890410958904, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0], 'f1': [0.16945606694560666, 0.0, 0.0, 0.09259259259259259, 0.0, 0.0, 0.1739130434782609, 0.0, 0.0, 0.3684210526315789, 0.17901234567901236, 0.0, 0.0, 0.10666666666666667, 0.0, 0.0, 0.13043478260869565, 0.0, 0.0, 0.0]}, test_name='accuracy', column='NumOfProducts', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [1397, 0, 0, 0, 0, 0, 0, 0, 0, 3403, 488, 0, 0, 0, 0, 0, 0, 0, 0, 1112], 'accuracy': [0.7172512526843235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7325888921539818, 0.735655737704918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7149280575539568], 'precision': [0.20418848167539266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19602977667493796, 0.288135593220339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17293233082706766], 'recall': [0.13829787234042554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11879699248120301, 0.16346153846153846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1], 'f1': [0.1649048625792812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14794007490636704, 0.20858895705521474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12672176308539942]}, test_name='accuracy', column='HasCrCard', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2292, 0, 0, 0, 0, 0, 0, 0, 0, 2508, 765, 0, 0, 0, 0, 0, 0, 0, 0, 835], 'accuracy': [0.6841186736474695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.768341307814992, 0.6679738562091503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.770059880239521], 'precision': [0.28113879003558717, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12460063897763578, 0.25287356321839083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17142857142857143], 'recall': [0.1314475873544093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11271676300578035, 0.10426540284360189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14634146341463414], 'f1': [0.17913832199546484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11836115326251898, 0.1476510067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15789473684210528]}, test_name='accuracy', column='IsActiveMember', passed=False), TestResult(values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]', '(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'shape': [466, 475, 466, 497, 483, 479, 495, 455, 507, 477, 166, 145, 170, 173, 159, 161, 162, 160, 159, 145], 'accuracy': [0.721030042918455, 0.7389473684210527, 0.7296137339055794, 0.7484909456740443, 0.7308488612836439, 0.7118997912317327, 0.7252525252525253, 0.7208791208791209, 0.7140039447731755, 0.740041928721174, 0.7108433734939759, 0.6896551724137931, 0.7764705882352941, 0.6705202312138728, 0.7484276729559748, 0.7018633540372671, 0.7777777777777778, 0.7125, 0.710691823899371, 0.7103448275862069], 'precision': [0.18032786885245902, 0.16666666666666666, 0.21428571428571427, 0.2807017543859649, 0.16666666666666666, 0.17142857142857143, 0.22033898305084745, 0.1875, 0.21818181818181817, 0.1896551724137931, 0.29411764705882354, 0.1, 0.11764705882352941, 0.21875, 0.38461538461538464, 0.23076923076923078, 0.07692307692307693, 0.15, 0.23529411764705882, 0.29411764705882354], 'recall': [0.12087912087912088, 0.08695652173913043, 0.1276595744680851, 0.16, 0.12790697674418605, 0.13043478260869565, 0.1262135922330097, 0.13793103448275862, 0.10526315789473684, 0.125, 0.12195121951219512, 0.06896551724137931, 0.08, 0.1794871794871795, 0.13513513513513514, 0.17647058823529413, 0.04, 0.09375, 0.10810810810810811, 0.14285714285714285], 'f1': [0.1447368421052632, 0.11428571428571427, 0.16, 0.2038216560509554, 0.14473684210526316, 0.14814814814814817, 0.1604938271604938, 0.15894039735099338, 0.14201183431952663, 0.1506849315068493, 0.17241379310344826, 0.08163265306122448, 0.09523809523809526, 0.19718309859154928, 0.2, 0.20000000000000004, 0.052631578947368425, 0.11538461538461538, 0.14814814814814817, 0.1923076923076923]}, test_name='accuracy', column='EstimatedSalary', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2398, 0, 0, 0, 0, 0, 0, 0, 0, 2402, 807, 0, 0, 0, 0, 0, 0, 0, 0, 793], 'accuracy': [0.69557964970809, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7606161532056619, 0.6840148698884758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7591424968474149], 'precision': [0.23890784982935154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15946843853820597, 0.2391304347826087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], 'recall': [0.12131715771230503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12972972972972974, 0.10628019323671498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14173228346456693], 'f1': [0.16091954022988506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14307004470938897, 0.14715719063545152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15859030837004404]}, test_name='accuracy', column='Geography_France', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3600, 0, 0, 0, 0, 0, 0, 0, 0, 1200, 1193, 0, 0, 0, 0, 0, 0, 0, 0, 407], 'accuracy': [0.76, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6325, 0.7627829002514669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5995085995085995], 'precision': [0.17248908296943233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2867647058823529, 0.18243243243243243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29545454545454547], 'recall': [0.1400709219858156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10182767624020887, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0896551724137931], 'f1': [0.15459882583170256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15028901734104044, 0.16023738872403562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13756613756613756]}, test_name='accuracy', column='Geography_Germany', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3602, 0, 0, 0, 0, 0, 0, 0, 0, 1198, 1200, 0, 0, 0, 0, 0, 0, 0, 0, 400], 'accuracy': [0.7179344808439756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7587646076794657, 0.705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77], 'precision': [0.19908466819221968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19745222929936307, 0.2152777777777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1875], 'recall': [0.11553784860557768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15979381443298968, 0.11397058823529412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14516129032258066], 'f1': [0.146218487394958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17663817663817666, 0.14903846153846154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16363636363636366]}, test_name='accuracy', column='Geography_Spain', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Robustness\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    robustness\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'features_columns': None, 'scaling_factor_std_dev_list': [0.01, 0.02]}\n                \n            \n            \n                Results\n                [TestResult(values={'Perturbation Size': [0.01, 0.01, 0.02, 0.02], 'Dataset Type': ['Traning', 'Test', 'Traning', 'Test'], 'Records': [4800, 1600, 4800, 1600], 'accuracy': [89.0, 85.375, 88.83333333333333, 85.4375]}, test_name='accuracy', column='Age', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/library_intro_demos.html",
    "href": "notebooks/library_intro_demos.html",
    "title": "ValidMind",
    "section": "",
    "text": "The ValidMind Python client allows model developers and validators to automatically document different aspects of the model development lifecycle.\nFor modelers, the client provides the following high level features:\n\nLog qualitative data about the model’s conceptual soundness\nLog information about datasets and models\nLog training and evaluation metrics about datasets and models\nRun data quality checks\nRun model evaluation tests\n\nFor validators, the client also provides (TBD) the ability to effectively challenge the model’s performance according to its objective, use case and specific project’s requirements.\n\n\n\nThis notebook and the ValidMind client must be executed on an environment running Python >= 3.8.\n\n\n\n\nWhile we finish the process of making the library publicly accessible pip, it can be installed with the following command that will direct pip to the S3 bucket that contains the latest version of the client.\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv('./env')\n\nTrue\n\n\n\n\n\n\nBefore we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n\nNavigate to the dashboard and click on the “Create new Project” button\nProvide a name and description for the project\nSelect a model use case\nFor modeling objective, we only support automated documentation of Binary Clasification models at the moment\n\nAfter creating the project you will be provided with client library setup instructions. We have provided similar instructions below.\n\n\nEvery validation project in the ValidMind dashboard has an associated project identifier. In order to initialize the client, we need to provide the following arguments:\n\nproject: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for /projects/cl1jyvh2c000909lg1rk0a0zb the project identifier is cl1jyvh2c000909lg1rk0a0zb\napi_host: Location of the ValidMind API. This value is already set on this notebook.\napi_key: Account API key. This can be found in the settings page in the ValidMind dashboard\napi_secret: Account Secret key. Also found in the settings page in the ValidMind dashboard\n\n\n# Lookup your own project id\n# project='cla6walda00001wl6pdzagu9v'\nproject='clar3ppjg000f1gmikrfmkld6'\n\nWe can now initialize the client library with the vm.init function:\n\nimport validmind as vm\n\nvm.init(\n    project=project\n)\n\nTrue\n\n\n\n# Necessary imports for training our demo models\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nAs of version 0.8.x of the client library, the following logging and testing functions are available:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlog_dataset\nAnalyzes a dataset and logs its description, column definitions and summary statistics\n\n\nrun_dataset_tests\nRuns dataset quality tests on the input dataset\n\n\nanalyze_dataset\nAnalyzes a dataset, computes summary statistics and runs data quality tests. This function combines log_dataset and run_dataset_tests\n\n\nlog_model\nLogs information about a model’s framework, architecture, target objective and training parameters\n\n\nlog_training_metrics\nExtracts and logs training metrics from a pre-trained model\n\n\nevaluate_model\nExtracts metadata and metrics from a train model instances and runs model evaluation tests according to the model objective, use case and specific validation requirements. This function combines log_model, log_training_metrics and an additional set of preconfigured model evaluation tests\n\n\n\nIn the example model training code in this notebook, we will demonstrate each of the documented client library functions.\n\n\nAnalyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n\nDescriptive statistics for numerical and categorical columns\nHistograms and value counts for summarizing distribution of values\nPearson correlation matrix for numerical columns\nCorelation plots for top 15 correlated features\n\nAdditionally, it will run a collection of data quality tests such as:\n\nClass imbalance test on target column\nDuplicate rows and duplicates based on primary key\nHigh cardinality test on categorical columns\nMissing values\nHighly correlated column pairs\nSkewness test\nZeros test (columns with too many zeros)\n\nArguments:\n\ndataset: Input dataset. Only Pandas DataFrames are supported at the moment\ndataset_type: Type of dataset, e.g. training, test, validation. Value needs to be set to training for now\ntargets: vm.DatasetTargets describing the label column and its values\nfeatures: Optional list of properties to specify for some features in the dataset\n\nReturns:\n\nresults: List of data quality test results\n\n\n\n\nLogs the following information about a model:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\nModel performance metrics from training, validation and test dataset\n\nAdditionally, this function runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n\nAccuracy score\nPrecision score\nRecall score\nF1 score\nROC AUC score\nROC AUC curve\nConfusion matrix\nPrecision Recall curve\nPermutation feature importance\nSHAP global importance\n\nArguments:\n\nmodel: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\ntrain_set: Training dataset tuple (x_train, y_train)\nval_set: Validation dataset tuple (x_val, y_val)\ntest_set: Test dataset tuple (x_test, y_test)\n\n\n\n\n\nWe’ll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n# Bank Customer Churn Dataset\nchurn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n\n# Health Insurance Cross-Sell Dataset\ninsurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")\n\n\nchurn_dataset2 = pd.read_csv(\"https://gist.githubusercontent.com/mehdi0501/5b9e64b51ed3bbddbe8f018fc7caf626/raw/ee9b21e5f5308299eb5f4d9dd251bc1b9c5ecc85/churn_test.csv\")\n\n\nchurn_dataset2.head()\n\n\n\n\n\n  \n    \n      \n      RowNumber\n      CustomerId\n      Surname\n      CreditScore\n      Geography\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      0\n      1\n      15634602\n      Hargrave\n      619\n      France\n      Female\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n    \n    \n      1\n      2\n      15647311\n      Hill\n      608\n      Spain\n      Female\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n    \n    \n      2\n      3\n      15619304\n      Onio\n      502\n      France\n      Female\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n    \n    \n      3\n      4\n      15701354\n      Boni\n      699\n      France\n      Female\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n    \n    \n      4\n      5\n      15737888\n      Mitchell\n      850\n      Spain\n      Female\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n    \n  \n\n\n\n\n\nchurn_dataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        8000 non-null   int64  \n 1   CustomerId       8000 non-null   int64  \n 2   Surname          8000 non-null   object \n 3   CreditScore      8000 non-null   int64  \n 4   Geography        8000 non-null   object \n 5   Gender           8000 non-null   object \n 6   Age              8000 non-null   int64  \n 7   Tenure           8000 non-null   int64  \n 8   Balance          8000 non-null   float64\n 9   NumOfProducts    8000 non-null   int64  \n 10  HasCrCard        8000 non-null   int64  \n 11  IsActiveMember   8000 non-null   int64  \n 12  EstimatedSalary  8000 non-null   float64\n 13  Exited           8000 non-null   int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 875.1+ KB\n\n\n\nchurn_dataset.describe()\n\nWe can now go to Project Overview -> Documentation -> Model Overview and verify this content has been populated on the dashboard.\n\n\nAfter loading the dataset, we can log metadata and summary statistics, and run data quality checks for it using analyze_dataset. Note that the analyze_dataset function expects a targets definition. Additional information about columns can be provided with the features argument.\n\nchurn_targets = vm.DatasetTargets(\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nchurn_features = [\n    {\n        \"id\": \"RowNumber\",\n        \"type_options\": {\n            \"primary_key\": True,\n        }\n    }\n]\n\nanalyze_results = vm.analyze_dataset(\n    dataset=churn_dataset,\n    dataset_type=\"training\",\n    targets=churn_targets,\n    features=churn_features\n)\n\nAnalyzing dataset...\nPandas dataset detected.\nInferring dataset types...\nPreparing in-memory dataset copy...\nCalculating field statistics...\nCalculating feature correlations...\nGenerating correlation plots...\nSuccessfully logged dataset metadata and statistics.\nRunning data quality tests...\nRunning data quality tests for \"training\" dataset...\n\nPreparing dataset for tests...\nPreparing in-memory dataset copy...\n\n\n100%|██████████| 6/6 [00:00<00:00, 22.63it/s]\n\n\n\nTest suite has completed.\nSending results to ValidMind...\nSuccessfully logged test results for test: class_imbalance\nSuccessfully logged test results for test: duplicates\nSuccessfully logged test results for test: cardinality\nSuccessfully logged test results for test: missing\nSuccessfully logged test results for test: skewness\nSuccessfully logged test results for test: zeros\n\nSummary of results:\n\nTest             Passed      # Passed    # Errors    % Passed\n---------------  --------  ----------  ----------  ----------\nclass_imbalance  True               1           0         100\nduplicates       True               2           0         100\ncardinality      False              6           1     85.7143\nmissing          True              14           0         100\nskewness         False              6           1     85.7143\nzeros            False              0           2           0\n\n\n\nAfter running analyze_dataset, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\nDashboard -> Project Overview -> Documentation -> Data Description\n\n\n\nWe are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance.\n\ndef preprocess_churn_dataset(df):\n    # Drop columns with no correlation to target\n    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n    # Encode binary features\n    genders = {\"Male\": 0, \"Female\": 1}\n    df.replace({\"Gender\": genders}, inplace=True)\n\n    # Encode categorical features\n    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n    df.drop(\"Geography\", axis=1, inplace=True)\n\n    return df\n\n\npreprocessed_churn = preprocess_churn_dataset(churn_dataset)\n\n\ndef train_val_test_split_dataset(df):\n    train_df, test_df = train_test_split(df, test_size=0.20)\n\n    # This guarantees a 60/20/20 split\n    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n    # For training\n    x_train = train_ds.drop(\"Exited\", axis=1)\n    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n    x_val = val_ds.drop(\"Exited\", axis=1)\n    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n    # For testing\n    x_test = test_df.drop(\"Exited\", axis=1)\n    y_test = test_df.loc[:, \"Exited\"].astype(int)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\n\nx_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)\n\n\ndef train_churn_dataset(x_train, y_train, x_val, y_val):\n    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n\n    xgb_model.set_params(\n        eval_metric=[\"error\", \"logloss\", \"auc\"],\n    )    \n\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_train, y_train), (x_val, y_val)],\n        verbose=False,\n    )\n    return xgb_model\n\n\nxgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)\n\n\ndef model_accuracy(model, x, y):\n    y_pred = model.predict_proba(x)[:, -1]\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y, predictions)\n\n    print(f\"Accuracy: {accuracy}\")    \n\n\nmodel_accuracy(xgb_model, x_val, y_val)\n\n\n\n\nFinally, after training our model, we can log its model parameters, collect performance metrics and run model evaluation tests on it using evaluate_model:\n\neval_results = vm.evaluate_model(\n    xgb_model,\n    train_set=(x_train, y_train),\n    val_set=(x_val, y_val),\n    test_set=(x_test, y_test)\n)\n\nAfter running evaluate_model, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation\nDashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability"
  },
  {
    "objectID": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "href": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "title": "ValidMind",
    "section": "",
    "text": "Introduction\n\nExecutive Summary\nBeing able to make accurate and timely estimates of future claims is a fundamental task for actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on understanding future claims, with mortality being one of the central issues facing a life insurer.\nIn this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation.\n\n\nOverview of Mortality Case Study\n\n Case Study Data \nOur dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\nFor the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old.\nMore details on this dataset can be found in Section 2 of the data report https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf\n\n\n Case Study Model \nFor the case study in this paper, we used the statsmodel’s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\nThe  response variable used in this case study is the number of deaths. Policies exposed was used as a weight in the model. We also tried to fit the mortality rate, which is number of deaths/ policies exposed using Gaussian distribution with log link, that can be found in the Appendix\nThe features used in the mortality model are:\n\nAttained Age – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\nDuration – the number of years (starting with a value of one) the policyholder has had the policy.\nSmoking Status – if the policyholder is considered a smoker or not.\nPreferred Class – an underwriting structure used by insurers to classify and price policyholders. Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\nGender – A categorical feature in the model with two levels, male and female.\nGuaranteed Term Period – the length of the policy at issue during which the premium will remain constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\nFace_Amount_Band\nObservation Year\n\n\n\n\n\nSet Up\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport sklearn \nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport os\nimport xgboost as xgb\n\nFirst, let’s download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file.\n\n# directly curl from the SOA website and unzip\n! echo Working Directory = $(pwd)\n! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n! echo \"Done\"\n\nWorking Directory = /Users/andres/code/validmind-sdk/notebooks/insurance_mortality\nData folder already exists\nFile already exists\nDone\n\n\nSecond, sample 5% from the giant file. Another 10 minutes or so the first time you run it :)\n\n#sample 5% and save it out to a sample file\nif not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n    p = 0.05\n    random.seed(42)\n    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv', \n                        skiprows = lambda i: i>0 and random.random() >p)\n    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)\n\n\n\nEDA\n\n# load sample file \nsample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n                               'Face_Amount_Band', 'Preferred_Class', \n                               'Number_Of_Deaths','Policies_Exposed', \n                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator', \n                               'Expected_Death_QX2015VBT_by_Policy',\n                               'Issue_Age', 'Issue_Year'])\n\n# target variable\nsample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n\nsample_df.head()\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Gender\n      Smoker_Status\n      Insurance_Plan\n      Issue_Age\n      Duration\n      Attained_Age\n      Face_Amount_Band\n      Issue_Year\n      Preferred_Class\n      SOA_Anticipated_Level_Term_Period\n      SOA_Guaranteed_Level_Term_Period\n      SOA_Post_level_Term_Indicator\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      0\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      4.882191\n      0.001074\n      0.0\n    \n    \n      1\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      500000-999999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      25.795943\n      0.006449\n      0.0\n    \n    \n      2\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      1.117809\n      0.000134\n      0.0\n    \n    \n      3\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      250000-499999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      70.098636\n      0.009814\n      0.0\n    \n    \n      4\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      4\n      3\n      50000-99999\n      2006\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      493.523281\n      0.034547\n      0.0\n    \n  \n\n\n\n\n\n# filter pipeline\nsample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n               & (sample_df.Smoker_Status != 'Unknown') \n               & (sample_df.Insurance_Plan == ' Term')\n               & (-sample_df.Preferred_Class.isna())\n               & (sample_df.Attained_Age >= 18)\n               & (sample_df.Issue_Year >= 1980)\n               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n               & (sample_df.mort < 1)]\n\nprint(f'Count: {sample_df.shape[0]}')\nprint()\n\n# describe data\nsample_df.describe()\n\nCount: 307233\n\n\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Issue_Age\n      Duration\n      Attained_Age\n      Issue_Year\n      Preferred_Class\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      count\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      3.072330e+05\n      307233.000000\n    \n    \n      mean\n      2014.084001\n      42.248505\n      7.951434\n      49.199939\n      2006.640537\n      2.035013\n      0.018514\n      12.504679\n      1.932158e-02\n      0.001627\n    \n    \n      std\n      1.413654\n      12.777574\n      4.793230\n      13.340539\n      4.888334\n      0.962332\n      0.147063\n      29.112019\n      5.412559e-02\n      0.023061\n    \n    \n      min\n      2012.000000\n      18.000000\n      1.000000\n      18.000000\n      1984.000000\n      1.000000\n      0.000000\n      0.002732\n      1.918000e-07\n      0.000000\n    \n    \n      25%\n      2013.000000\n      32.000000\n      4.000000\n      39.000000\n      2003.000000\n      1.000000\n      0.000000\n      0.838356\n      7.766577e-04\n      0.000000\n    \n    \n      50%\n      2014.000000\n      42.000000\n      7.000000\n      49.000000\n      2007.000000\n      2.000000\n      0.000000\n      2.612022\n      3.316641e-03\n      0.000000\n    \n    \n      75%\n      2015.000000\n      52.000000\n      12.000000\n      59.000000\n      2011.000000\n      3.000000\n      0.000000\n      10.680379\n      1.470165e-02\n      0.000000\n    \n    \n      max\n      2016.000000\n      84.000000\n      30.000000\n      91.000000\n      2016.000000\n      4.000000\n      6.000000\n      655.938021\n      2.827005e+00\n      0.981233\n    \n  \n\n\n\n\n\n# Encode categorical variables\ncat_vars = ['Observation_Year', \n     'Gender', \n     'Smoker_Status',\n     'Face_Amount_Band', \n     'Preferred_Class',\n     'SOA_Anticipated_Level_Term_Period']\n\nonehot = preprocessing.OneHotEncoder()\nresults = onehot.fit_transform(sample_df[cat_vars]).toarray()\ncat_vars_encoded = list(onehot.get_feature_names_out())\nsample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n\n\n# categorical variables\nface_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\nterm_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\nfig, ax = plt.subplots(4,2, figsize = (20,30))\nax = ax.flatten()\nfor i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n       'Face_Amount_Band', 'Preferred_Class',\n       'SOA_Guaranteed_Level_Term_Period']):\n    if column == 'Face_Amount_Band':\n        order = face_amount_order\n    elif column == 'SOA_Guaranteed_Level_Term_Period':\n        order = term_period_order\n    else:\n        order = None\n    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\nplt.show()\n\n\n\n\n\n# age and duration variables\nfig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n\nsns.histplot(x = sample_df['Duration'], ax = ax[1])\nplt.show()\n\n\n\n\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n# log mort by Attained Age\n\ndef stratify(field):\n    fig, ax = plt.subplots(figsize = (7,3))\n    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n    plt.show()\n\nstratify('Smoker_Status')\nstratify('Preferred_Class')\nstratify('Gender')\nstratify('Observation_Year')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\nTrain/test split\nFirst we split the data into 80% for training and 20% for testing.\nIn this context because we don’t really need to do hyperparameter tuning so it’s not necessary to create a validation set.\n\n# create training (80%), validation (5%) and test set (15%)\nrandom_seed = 0\ntrain_df = sample_df.sample(frac = 0.8, random_state = random_seed)\ntest_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n\n# add constant variable\ntrain_df['Const'] = 1\ntest_df['Const'] = 1\n \nprint(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')\n\nTrain size: 245786, test size: 61447\n\n\n\ntrain_df.to_csv('train_df.csv', index = False)\ntest_df.to_csv('test_df.csv', index = False)\n\n\n\nGLM modeling 101\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, \\(μ\\), of the distribution depends on the independent variables, X, through\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\nwhere:\n\n\\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on \\(X\\)\n\\(Xβ\\) is the linear predictor, a linear combination of unknown parameters \\(β\\)\n\\(g\\) is the link function.\n\n\n\nModel 1: Poisson distribution with log link on count\n Target Variable  = [Number_Of_Deaths]\n Input Variables  = [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\nAs the  target variable is a count measure, we will fit GLM with Poisson distribution and log link.\nThe target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to https://en.wikipedia.org/wiki/Poisson_regression\n\nmodel1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n                                       + Attained_Age + Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres1 = model1.fit()\nres1.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:   Number_Of_Deaths   No. Observations:     245786   \n\n\n  Model:                  GLM         Df Residuals:       3076911.54 \n\n\n  Model Family:         Poisson       Df Model:                 26   \n\n\n  Link Function:          log         Scale:                 1.0000  \n\n\n  Method:                IRLS         Log-Likelihood:     -7.1471e+05\n\n\n  Date:            Mon, 05 Dec 2022   Deviance:           9.8740e+05 \n\n\n  Time:                22:28:25       Pearson chi2:        3.17e+06  \n\n\n  No. Iterations:         24          Pseudo R-squ. (CS):   0.6540   \n\n\n  Covariance Type:     nonrobust                                     \n\n\n\n\n                                                               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                                                    -9.2794     0.158   -58.838  0.000    -9.589    -8.970\n\n\n  C(Observation_Year)[T.2013]                                  -0.0545     0.007    -8.190  0.000    -0.067    -0.041\n\n\n  C(Observation_Year)[T.2014]                                  -0.0051     0.006    -0.789  0.430    -0.018     0.008\n\n\n  C(Observation_Year)[T.2015]                                  -0.1405     0.007   -20.705  0.000    -0.154    -0.127\n\n\n  C(Observation_Year)[T.2016]                                  -0.0813     0.007   -12.377  0.000    -0.094    -0.068\n\n\n  C(Gender)[T.Male]                                             0.3527     0.005    74.784  0.000     0.343     0.362\n\n\n  C(Smoker_Status)[T.Smoker]                                    1.0350     0.015    67.166  0.000     1.005     1.065\n\n\n  C(Face_Amount_Band)[T.   10000-24999]                        -0.7187     0.118    -6.104  0.000    -0.949    -0.488\n\n\n  C(Face_Amount_Band)[T.   25000-49999]                        -0.7632     0.117    -6.500  0.000    -0.993    -0.533\n\n\n  C(Face_Amount_Band)[T.   50000-99999]                        -0.9776     0.117    -8.372  0.000    -1.206    -0.749\n\n\n  C(Face_Amount_Band)[T.  100000-249999]                       -1.6819     0.116   -14.452  0.000    -1.910    -1.454\n\n\n  C(Face_Amount_Band)[T.  250000-499999]                       -2.0061     0.116   -17.222  0.000    -2.234    -1.778\n\n\n  C(Face_Amount_Band)[T.  500000-999999]                       -2.0428     0.117   -17.521  0.000    -2.271    -1.814\n\n\n  C(Face_Amount_Band)[T. 1000000-2499999]                      -2.0690     0.117   -17.721  0.000    -2.298    -1.840\n\n\n  C(Face_Amount_Band)[T. 2500000-4999999]                      -2.0173     0.138   -14.656  0.000    -2.287    -1.747\n\n\n  C(Face_Amount_Band)[T. 5000000-9999999]                      -2.0177     0.229    -8.795  0.000    -2.467    -1.568\n\n\n  C(Face_Amount_Band)[T.10000000+]                            -23.7738  1.48e+04    -0.002  0.999 -2.89e+04  2.89e+04\n\n\n  C(Preferred_Class)[T.2.0]                                     0.4593     0.005    94.004  0.000     0.450     0.469\n\n\n  C(Preferred_Class)[T.3.0]                                     0.4168     0.007    60.272  0.000     0.403     0.430\n\n\n  C(Preferred_Class)[T.4.0]                                     0.5337     0.011    48.013  0.000     0.512     0.555\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.10 yr anticipated]    -0.1692     0.105    -1.607  0.108    -0.376     0.037\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.15 yr anticipated]    -0.2569     0.105    -2.438  0.015    -0.463    -0.050\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.20 yr anticipated]    -0.4042     0.105    -3.844  0.000    -0.610    -0.198\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.25 yr anticipated]     0.0217     0.106     0.205  0.838    -0.186     0.229\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.30 yr anticipated]    -0.2437     0.105    -2.314  0.021    -0.450    -0.037\n\n\n  Attained_Age                                                  0.0739     0.000   254.173  0.000     0.073     0.075\n\n\n  Duration                                                      0.0497     0.001    92.131  0.000     0.049     0.051\n\n\n\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nres1.save('res1.pkl')\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nloaded = sm.load('res1.pkl')\n\n\nfitted = loaded.model.fit()\n\n\nfitted.predict(train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nfitted.params[\"Intercept\"]\n\n-9.279412567322963\n\n\nFirst, we show the lift chart that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we’re only look at the high-level average for each decile.\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat1'] = res1.predict(exog = train_df)\ntrain_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\ntest_df['mort_hat1'] = res1.predict(exog = test_df)\ntest_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n\n# groupby and aggregate by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\ntemp\n\n# lift chart \nfig, ax = plt.subplots(figsize = (7,3))\ntemp.plot(ax = ax)\nplt.title('Actual vs predicted mortality rate by deciles')\nplt.show() \n\nSecond, we can plot the partial dependency chart between the log mortality rate and key covariates like Attained Age or Duration to see more granular comparisons between actual vs predicted.\nWe can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well.\n\ndef pdp(df, agg_field, title, predict_col = 'death_hat1'):\n    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n    plt.legend(['actual','predicted']) \n    plt.xlabel(agg_field)\n    plt.ylabel('log_mort')\n    plt.title(title)\n    plt.show()\n    \npdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\npdp(train_df, 'Duration', 'How well does the model fit the train set')\n\n\npdp(test_df, 'Attained_Age', 'How well does the model fit the test set')\npdp(test_df, 'Duration', 'How well does the model fit the test set')\n\nThird, we look at Prediction Error by taking the difference between the Number Of Deaths (actual) and Predicted Number of Deaths and then normalized by Policies Exposed. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\nagg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\nagg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Testing error')\nplt.show()\n\n\n\n\nValidation\n\n1. Goodness of Fit\n\n Pseudo R-squared \nIn linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\nFor GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n\n\\(R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}\\)\n\n\nres1.pseudo_rsquared()\n\n\n\n Deviance \nThe (total) deviance for a model M with estimates \\({\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}\\), based on a dataset y, may be constructed by its likelihood as:\n\n\\({\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}\\)\n\nHere \\(\\hat \\theta_0\\) denotes the fitted values of the parameters in the model M, while \\(\\hat \\theta_s\\) denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y.\nIn large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\nHere we divided the deviance by the residual degree of freedom and observed a ratio much smaller than 1\n\nres1.deviance/res1.df_resid\n\n\n\n Pearson Statistic and dispersion \nSimilar to deviance test, the Pearson Statistic is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit.\nAdditionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. Overdispersion means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X.\nHere we divided the pearson statistic by the residual degree of freedom and observed a value very close to 1\n\nres1.pearson_chi2/res1.df_resid\n\n\n\n\n2. Feature importance\n\nConfidence intervals and p-values \nConfidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available.\n\nres1.summary()\n\nFrom the summary, we can see that all of the features other than SOA_Anticipated_Level_Term_Period are significant as all p-values are < 5%.\nDirectionally, the coeficients for the main features like Gender, Smoking Status, Attained_Age or Duration are all aligned with our intuition and the EDA charts that we created previously:\n\nMortality rate for Male is higher than Female\nMortality rate for Smoker is higher than non-Smoker\nMortality rate is higher as age is higher\nMortality rate is higher as duration is longer\n\n\n\n\n3. Main Effects\nWe want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given.\n\ndef pdp2(df, x, hue, predict_col = 'death_hat1'):\n    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (6,3))\n    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n    plt.xlabel(x)\n    plt.ylabel('log_mort')\n    plt.title(f'Log mortality by {x} and {hue}')\n    plt.show()\n    \npdp2(train_df, 'Attained_Age', 'Gender')\npdp2(train_df, 'Duration', 'Gender')\npdp2(train_df, 'Attained_Age', 'Smoker_Status')\npdp2(train_df, 'Duration', 'Smoker_Status')\npdp2(train_df, 'Attained_Age', 'Preferred_Class')\npdp2(train_df, 'Duration', 'Preferred_Class')\n\nWe can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\nAdditionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it’s almost constant regardless of ages.\n\n\n4. Interaction Effects\nOne of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features.\nHere we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions.\n\n Model 2: Poisson distribution with log link on Death Count with interactions \n\nmodel2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat2'] = res2.predict(exog = train_df)\ntrain_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\ntest_df['mort_hat2'] = res2.predict(exog = test_df)\ntest_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n\nres2.summary()\n\n\n\n Compared to the vanilla model \nFirst, pearson and deviance are reasonable\n\nprint(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n\nprint(f'deviance/df = {res2.deviance/res2.df_resid}')\n\nCompared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features.\n\nprint(f'AIC for Model 1 - No interaction: {res1.aic}')\nprint(f'AIC for Model 2 - With interactions: {res2.aic}')\n\nSide note on definition of AIC:  A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value.\n\n\n\n5. Correlated Features\nFor GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\nThis is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let’s show it again below. We don’t see severe correlation between any two features that requires dropping one from the feature set.\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\nplt.show()\n\n\n\n\nConclusion\nIn this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model. - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. - We checked for any necessary inclusion of interactions and handling of correlated features.\nApparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics.\n\n\nAppendix\n\nModel 1 not using formula\nThis is the explicit setup where we don’t lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis.\n\n# Target Variable\nY = ['Number_Of_Deaths']\n\n# Predictors (aka Input Variables)\nX = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const'] \n\n# Our choice for Link function is the Gaussian distribution for the nature of death frequency\nmodel = sm.GLM(endog = train_df[Y], \n               exog = train_df[X], \n               family=sm.families.Poisson(sm.families.links.log()),\n               freq_weights = train_df['Policies_Exposed'],\n               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres = model.fit()\nres.summary()\n\n\n\nModel 3: Gaussian distribution with log link on mortality rate\nThis is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. Pseudo R-squared is far worse than Model 1\n\nmodel2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration', \n                 data = train_df,\n                 family=sm.families.Gaussian(link = sm.families.links.log()),\n                 freq_weights = train_df['Policies_Exposed'])\nres2 = model2.fit()\nres2.summary()\n\n\n\nModel 4: XGBoost\nIn this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.\nNote that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time.\n\nX = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\nY = ['mort']#['Number_Of_Deaths']\n\nX_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\nfor x in X_cat:\n    train_df[x] = train_df[x].astype(\"category\")\n    test_df[x] = test_df[x].astype('category')\n\n\n# create model instance\nbst = xgb.XGBRegressor(n_estimators=50, \n                   max_depth=4, \n                   learning_rate=0.5, \n                   objective='count:poisson', \n                   enable_categorical = True, \n                   tree_method = 'approx', \n                   booster = 'gbtree', \n                   verbosity = 1)\n\n# fit model\nbst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n\n# make predictions\npreds = bst.predict(test_df[X])\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat4'] = bst.predict(train_df[X])\ntrain_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\ntest_df['mort_hat4'] = bst.predict(test_df[X])\ntest_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']\n\nLift chart does not show too much of a difference from Model 1\n\n# lift chart by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n\n# groupby and aggregate\nfig, ax = plt.subplots(figsize = (7,3))\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\ntemp.plot(ax = ax)\nplt.title('Actual vs Predicted by deciles')\nplt.show()\n\nPlotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better.\n\n# partial dependence plots\npdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\npdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\npdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\npdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')\n\nLooking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates.\n\npdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\npdp2(train_df, 'Duration', 'Gender','death_hat4')\npdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\npdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')\n\n\n\nCompare Model 1, Model 2 and Model 4\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\ntrain_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\ntrain_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n\nagg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\n# plt.ylim(0,1)\n# plt.xlim(30,85)\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\ntest_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\ntest_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n\nagg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\nplt.ylabel('Error')\n# plt.ylim(0,1)\n# plt.xlim(30,85)\na = plt.title('Testing Error')\nplt.show()\n\n\nres1.save('mortality_model.pickle')"
  },
  {
    "objectID": "notebooks/lending_club_regression.html",
    "href": "notebooks/lending_club_regression.html",
    "title": "ValidMind",
    "section": "",
    "text": "Load the SDK code from the local package directory\nLoad the API key and secret in the .env file\n\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_rows = None\n\n\n# Initialize ValidMind SDK\nimport validmind as vm\n\n# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n# vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/loan_data_2007_2014_preprocessed.csv\")\n\n# targets = vm.DatasetTargets(\n#     target_column=\"loan_status\",\n#     class_labels={\n#         \"Fully Paid\": \"Fully Paid\",\n#         \"Charged Off\": \"Charged Off\",\n#     }\n# )\n\n# vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets)\n\nColumns (21,49) have mixed types.Specify dtype option on import or set low_memory=False.\n\n\n\nloan_data_defaults = df[df['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n\n\nloan_data_defaults.shape\n\n(43236, 209)\n\n\n\nloan_data_defaults['mths_since_last_delinq'].fillna(0, inplace=True)\nloan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) / loan_data_defaults['funded_amnt']\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nloan_data_defaults['CCF'].describe()\n\ncount    43236.000000\nmean         0.735952\nstd          0.200742\nmin          0.000438\n25%          0.632088\n50%          0.789908\n75%          0.888543\nmax          1.000000\nName: CCF, dtype: float64\n\n\n\nplt.hist(loan_data_defaults['CCF'], bins = 100)\n\n(array([   3.,   17.,   16.,   44.,   16.,   13.,   71.,   26.,    7.,\n          63.,   67.,   17.,   60.,   90.,   23.,   55.,   82.,   42.,\n          47.,  123.,   82.,   70.,  122.,   86.,   89.,  110.,  117.,\n         111.,  122.,  120.,  135.,  141.,  154.,  146.,  160.,  175.,\n         152.,  187.,  202.,  174.,  204.,  208.,  210.,  211.,  241.,\n         264.,  281.,  224.,  308.,  267.,  287.,  296.,  340.,  274.,\n         365.,  370.,  392.,  364.,  393.,  419.,  411.,  429.,  445.,\n         497.,  481.,  478.,  569.,  568.,  599.,  618.,  727.,  691.,\n         626.,  805.,  804.,  776.,  881.,  851.,  916.,  934.,  925.,\n        1078.,  933., 1218., 1041., 1082., 1336., 1040., 1374., 1073.,\n        1406., 1287.,  952., 1414.,  795., 1320.,  578.,  949.,  343.,\n         531.]),\n array([4.3800000e-04, 1.0433620e-02, 2.0429240e-02, 3.0424860e-02,\n        4.0420480e-02, 5.0416100e-02, 6.0411720e-02, 7.0407340e-02,\n        8.0402960e-02, 9.0398580e-02, 1.0039420e-01, 1.1038982e-01,\n        1.2038544e-01, 1.3038106e-01, 1.4037668e-01, 1.5037230e-01,\n        1.6036792e-01, 1.7036354e-01, 1.8035916e-01, 1.9035478e-01,\n        2.0035040e-01, 2.1034602e-01, 2.2034164e-01, 2.3033726e-01,\n        2.4033288e-01, 2.5032850e-01, 2.6032412e-01, 2.7031974e-01,\n        2.8031536e-01, 2.9031098e-01, 3.0030660e-01, 3.1030222e-01,\n        3.2029784e-01, 3.3029346e-01, 3.4028908e-01, 3.5028470e-01,\n        3.6028032e-01, 3.7027594e-01, 3.8027156e-01, 3.9026718e-01,\n        4.0026280e-01, 4.1025842e-01, 4.2025404e-01, 4.3024966e-01,\n        4.4024528e-01, 4.5024090e-01, 4.6023652e-01, 4.7023214e-01,\n        4.8022776e-01, 4.9022338e-01, 5.0021900e-01, 5.1021462e-01,\n        5.2021024e-01, 5.3020586e-01, 5.4020148e-01, 5.5019710e-01,\n        5.6019272e-01, 5.7018834e-01, 5.8018396e-01, 5.9017958e-01,\n        6.0017520e-01, 6.1017082e-01, 6.2016644e-01, 6.3016206e-01,\n        6.4015768e-01, 6.5015330e-01, 6.6014892e-01, 6.7014454e-01,\n        6.8014016e-01, 6.9013578e-01, 7.0013140e-01, 7.1012702e-01,\n        7.2012264e-01, 7.3011826e-01, 7.4011388e-01, 7.5010950e-01,\n        7.6010512e-01, 7.7010074e-01, 7.8009636e-01, 7.9009198e-01,\n        8.0008760e-01, 8.1008322e-01, 8.2007884e-01, 8.3007446e-01,\n        8.4007008e-01, 8.5006570e-01, 8.6006132e-01, 8.7005694e-01,\n        8.8005256e-01, 8.9004818e-01, 9.0004380e-01, 9.1003942e-01,\n        9.2003504e-01, 9.3003066e-01, 9.4002628e-01, 9.5002190e-01,\n        9.6001752e-01, 9.7001314e-01, 9.8000876e-01, 9.9000438e-01,\n        1.0000000e+00]),\n <BarContainer object of 100 artists>)\n\n\n\n\n\n\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n\n\nfeatures_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\n\n\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \n\n\nead_inputs_train = ead_inputs_train[features_all]\n\n\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport numpy as np\nimport scipy.stats as stat\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1, positive=False):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ / se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self\n\n\nreg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n\n\nLinearRegression()\n\n\n\nfeature_name = ead_inputs_train.columns.values\n\n\nsummary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_ead.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table\n\n\n\n\n\n  \n    \n      \n      Feature name\n      Coefficients\n      p_values\n    \n  \n  \n    \n      0\n      Intercept\n      1.109746e+00\n      NaN\n    \n    \n      1\n      grade:A\n      -3.030033e-01\n      0.000000e+00\n    \n    \n      2\n      grade:B\n      -2.364277e-01\n      0.000000e+00\n    \n    \n      3\n      grade:C\n      -1.720232e-01\n      0.000000e+00\n    \n    \n      4\n      grade:D\n      -1.198470e-01\n      0.000000e+00\n    \n    \n      5\n      grade:E\n      -6.768713e-02\n      0.000000e+00\n    \n    \n      6\n      grade:F\n      -2.045907e-02\n      4.428795e-03\n    \n    \n      7\n      home_ownership:MORTGAGE\n      -6.343341e-03\n      2.632464e-03\n    \n    \n      8\n      home_ownership:NONE\n      -5.539064e-03\n      9.318931e-01\n    \n    \n      9\n      home_ownership:OTHER\n      -2.426052e-03\n      9.335820e-01\n    \n    \n      10\n      home_ownership:OWN\n      -1.619582e-03\n      6.366112e-01\n    \n    \n      11\n      verification_status:Not Verified\n      5.339510e-05\n      9.828295e-01\n    \n    \n      12\n      verification_status:Source Verified\n      8.967822e-03\n      7.828941e-05\n    \n    \n      13\n      purpose:car\n      7.904787e-04\n      9.330252e-01\n    \n    \n      14\n      purpose:debt_consolidation\n      1.264922e-02\n      5.898438e-07\n    \n    \n      15\n      purpose:educational\n      9.643587e-02\n      1.801025e-06\n    \n    \n      16\n      purpose:home_improvement\n      1.923044e-02\n      4.873543e-05\n    \n    \n      17\n      purpose:house\n      1.607120e-02\n      1.653651e-01\n    \n    \n      18\n      purpose:major_purchase\n      2.984917e-02\n      2.197793e-05\n    \n    \n      19\n      purpose:medical\n      3.962479e-02\n      5.238263e-06\n    \n    \n      20\n      purpose:moving\n      4.577630e-02\n      2.987383e-06\n    \n    \n      21\n      purpose:other\n      3.706744e-02\n      0.000000e+00\n    \n    \n      22\n      purpose:renewable_energy\n      7.212969e-02\n      8.889877e-03\n    \n    \n      23\n      purpose:small_business\n      5.128674e-02\n      0.000000e+00\n    \n    \n      24\n      purpose:vacation\n      1.874863e-02\n      1.152702e-01\n    \n    \n      25\n      purpose:wedding\n      4.350522e-02\n      2.032121e-04\n    \n    \n      26\n      initial_list_status:w\n      1.318126e-02\n      6.115181e-09\n    \n    \n      27\n      term_int\n      4.551882e-03\n      0.000000e+00\n    \n    \n      28\n      emp_length_int\n      -1.591478e-03\n      4.404626e-10\n    \n    \n      29\n      mths_since_issue_d\n      -4.305274e-03\n      0.000000e+00\n    \n    \n      30\n      mths_since_earliest_cr_line\n      -3.634030e-05\n      2.742071e-03\n    \n    \n      31\n      funded_amnt\n      2.212126e-06\n      7.225181e-03\n    \n    \n      32\n      int_rate\n      -1.172652e-02\n      0.000000e+00\n    \n    \n      33\n      installment\n      -6.865607e-05\n      7.429261e-03\n    \n    \n      34\n      annual_inc\n      5.021817e-09\n      8.574696e-01\n    \n    \n      35\n      dti\n      2.832769e-04\n      3.632507e-02\n    \n    \n      36\n      delinq_2yrs\n      4.833234e-04\n      6.946456e-01\n    \n    \n      37\n      inq_last_6mths\n      1.131678e-02\n      0.000000e+00\n    \n    \n      38\n      mths_since_last_delinq\n      -1.965980e-04\n      3.220434e-06\n    \n    \n      39\n      mths_since_last_record\n      -5.085639e-05\n      3.291896e-01\n    \n    \n      40\n      open_acc\n      -2.142130e-03\n      4.218847e-15\n    \n    \n      41\n      pub_rec\n      6.782062e-03\n      4.252750e-02\n    \n    \n      42\n      total_acc\n      4.518110e-04\n      1.902931e-04\n    \n    \n      43\n      acc_now_delinq\n      9.974801e-03\n      5.012787e-01\n    \n    \n      44\n      total_rev_hi_lim\n      2.166527e-07\n      8.196014e-05\n    \n  \n\n\n\n\n\nead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\n\n\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\n\n\ny_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\nead_targets_test_temp = ead_targets_test\n\n\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.000000\n      0.530654\n    \n    \n      0\n      0.530654\n      1.000000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.736013\n    \n    \n      std\n      0.105194\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.161088\n    \n  \n\n\n\n\n\ny_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\n\n\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735992\n    \n    \n      std\n      0.105127\n    \n    \n      min\n      0.384774\n    \n    \n      25%\n      0.661553\n    \n    \n      50%\n      0.731750\n    \n    \n      75%\n      0.810625\n    \n    \n      max\n      1.000000\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead)\n\n0.0291749760949319\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead)\n\n0.2822776667644732\n\n\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.set_params(\n    booster='gblinear',\n    eval_metric=mean_squared_error,\n)\n\nXGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None, predictor=None, random_state=None,\n             reg_alpha=None, reg_lambda=None, ...)\n\n\n\nxgb_model.fit(ead_inputs_train, ead_targets_train, eval_set=[(ead_inputs_train, ead_targets_train), (ead_inputs_test, ead_targets_test)],)\n\n[0] validation_0-rmse:0.18787   validation_0-mean_squared_error:0.03529 validation_1-rmse:0.18834   validation_1-mean_squared_error:0.03547\n[1] validation_0-rmse:0.18491   validation_0-mean_squared_error:0.03419 validation_1-rmse:0.18507   validation_1-mean_squared_error:0.03425\n[2] validation_0-rmse:0.18372   validation_0-mean_squared_error:0.03375 validation_1-rmse:0.18379   validation_1-mean_squared_error:0.03378\n[3] validation_0-rmse:0.18299   validation_0-mean_squared_error:0.03348 validation_1-rmse:0.18312   validation_1-mean_squared_error:0.03353\n[4] validation_0-rmse:0.18238   validation_0-mean_squared_error:0.03326 validation_1-rmse:0.18246   validation_1-mean_squared_error:0.03329\n[5] validation_0-rmse:0.18177   validation_0-mean_squared_error:0.03304 validation_1-rmse:0.18202   validation_1-mean_squared_error:0.03313\n[6] validation_0-rmse:0.18120   validation_0-mean_squared_error:0.03283 validation_1-rmse:0.18152   validation_1-mean_squared_error:0.03295\n[7] validation_0-rmse:0.18075   validation_0-mean_squared_error:0.03267 validation_1-rmse:0.18116   validation_1-mean_squared_error:0.03282\n[8] validation_0-rmse:0.18032   validation_0-mean_squared_error:0.03251 validation_1-rmse:0.18073   validation_1-mean_squared_error:0.03266\n[9] validation_0-rmse:0.17989   validation_0-mean_squared_error:0.03236 validation_1-rmse:0.18038   validation_1-mean_squared_error:0.03254\n[10]    validation_0-rmse:0.17948   validation_0-mean_squared_error:0.03221 validation_1-rmse:0.17992   validation_1-mean_squared_error:0.03237\n[11]    validation_0-rmse:0.17909   validation_0-mean_squared_error:0.03207 validation_1-rmse:0.17954   validation_1-mean_squared_error:0.03224\n[12]    validation_0-rmse:0.17873   validation_0-mean_squared_error:0.03195 validation_1-rmse:0.17920   validation_1-mean_squared_error:0.03211\n[13]    validation_0-rmse:0.17841   validation_0-mean_squared_error:0.03183 validation_1-rmse:0.17896   validation_1-mean_squared_error:0.03202\n[14]    validation_0-rmse:0.17809   validation_0-mean_squared_error:0.03172 validation_1-rmse:0.17858   validation_1-mean_squared_error:0.03189\n[15]    validation_0-rmse:0.17779   validation_0-mean_squared_error:0.03161 validation_1-rmse:0.17825   validation_1-mean_squared_error:0.03177\n[16]    validation_0-rmse:0.17751   validation_0-mean_squared_error:0.03151 validation_1-rmse:0.17789   validation_1-mean_squared_error:0.03164\n[17]    validation_0-rmse:0.17717   validation_0-mean_squared_error:0.03139 validation_1-rmse:0.17761   validation_1-mean_squared_error:0.03155\n[18]    validation_0-rmse:0.17689   validation_0-mean_squared_error:0.03129 validation_1-rmse:0.17730   validation_1-mean_squared_error:0.03144\n[19]    validation_0-rmse:0.17664   validation_0-mean_squared_error:0.03120 validation_1-rmse:0.17706   validation_1-mean_squared_error:0.03135\n[20]    validation_0-rmse:0.17641   validation_0-mean_squared_error:0.03112 validation_1-rmse:0.17678   validation_1-mean_squared_error:0.03125\n[21]    validation_0-rmse:0.17619   validation_0-mean_squared_error:0.03104 validation_1-rmse:0.17656   validation_1-mean_squared_error:0.03117\n[22]    validation_0-rmse:0.17598   validation_0-mean_squared_error:0.03097 validation_1-rmse:0.17634   validation_1-mean_squared_error:0.03110\n[23]    validation_0-rmse:0.17580   validation_0-mean_squared_error:0.03090 validation_1-rmse:0.17614   validation_1-mean_squared_error:0.03102\n[24]    validation_0-rmse:0.17560   validation_0-mean_squared_error:0.03083 validation_1-rmse:0.17592   validation_1-mean_squared_error:0.03095\n[25]    validation_0-rmse:0.17542   validation_0-mean_squared_error:0.03077 validation_1-rmse:0.17574   validation_1-mean_squared_error:0.03088\n[26]    validation_0-rmse:0.17525   validation_0-mean_squared_error:0.03071 validation_1-rmse:0.17553   validation_1-mean_squared_error:0.03081\n[27]    validation_0-rmse:0.17507   validation_0-mean_squared_error:0.03065 validation_1-rmse:0.17533   validation_1-mean_squared_error:0.03074\n[28]    validation_0-rmse:0.17491   validation_0-mean_squared_error:0.03060 validation_1-rmse:0.17515   validation_1-mean_squared_error:0.03068\n[29]    validation_0-rmse:0.17481   validation_0-mean_squared_error:0.03056 validation_1-rmse:0.17505   validation_1-mean_squared_error:0.03064\n[30]    validation_0-rmse:0.17469   validation_0-mean_squared_error:0.03052 validation_1-rmse:0.17487   validation_1-mean_squared_error:0.03058\n[31]    validation_0-rmse:0.17455   validation_0-mean_squared_error:0.03047 validation_1-rmse:0.17470   validation_1-mean_squared_error:0.03052\n[32]    validation_0-rmse:0.17443   validation_0-mean_squared_error:0.03043 validation_1-rmse:0.17451   validation_1-mean_squared_error:0.03045\n[33]    validation_0-rmse:0.17431   validation_0-mean_squared_error:0.03038 validation_1-rmse:0.17443   validation_1-mean_squared_error:0.03043\n[34]    validation_0-rmse:0.17420   validation_0-mean_squared_error:0.03035 validation_1-rmse:0.17426   validation_1-mean_squared_error:0.03037\n[35]    validation_0-rmse:0.17412   validation_0-mean_squared_error:0.03032 validation_1-rmse:0.17418   validation_1-mean_squared_error:0.03034\n[36]    validation_0-rmse:0.17403   validation_0-mean_squared_error:0.03029 validation_1-rmse:0.17399   validation_1-mean_squared_error:0.03027\n[37]    validation_0-rmse:0.17393   validation_0-mean_squared_error:0.03025 validation_1-rmse:0.17388   validation_1-mean_squared_error:0.03024\n[38]    validation_0-rmse:0.17386   validation_0-mean_squared_error:0.03023 validation_1-rmse:0.17376   validation_1-mean_squared_error:0.03019\n[39]    validation_0-rmse:0.17377   validation_0-mean_squared_error:0.03020 validation_1-rmse:0.17370   validation_1-mean_squared_error:0.03017\n[40]    validation_0-rmse:0.17370   validation_0-mean_squared_error:0.03017 validation_1-rmse:0.17363   validation_1-mean_squared_error:0.03015\n[41]    validation_0-rmse:0.17363   validation_0-mean_squared_error:0.03015 validation_1-rmse:0.17358   validation_1-mean_squared_error:0.03013\n[42]    validation_0-rmse:0.17357   validation_0-mean_squared_error:0.03012 validation_1-rmse:0.17350   validation_1-mean_squared_error:0.03010\n[43]    validation_0-rmse:0.17350   validation_0-mean_squared_error:0.03010 validation_1-rmse:0.17346   validation_1-mean_squared_error:0.03009\n[44]    validation_0-rmse:0.17345   validation_0-mean_squared_error:0.03009 validation_1-rmse:0.17340   validation_1-mean_squared_error:0.03007\n[45]    validation_0-rmse:0.17339   validation_0-mean_squared_error:0.03007 validation_1-rmse:0.17334   validation_1-mean_squared_error:0.03005\n[46]    validation_0-rmse:0.17335   validation_0-mean_squared_error:0.03005 validation_1-rmse:0.17327   validation_1-mean_squared_error:0.03002\n[47]    validation_0-rmse:0.17329   validation_0-mean_squared_error:0.03003 validation_1-rmse:0.17320   validation_1-mean_squared_error:0.03000\n[48]    validation_0-rmse:0.17325   validation_0-mean_squared_error:0.03001 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[49]    validation_0-rmse:0.17321   validation_0-mean_squared_error:0.03000 validation_1-rmse:0.17311   validation_1-mean_squared_error:0.02997\n[50]    validation_0-rmse:0.17316   validation_0-mean_squared_error:0.02998 validation_1-rmse:0.17304   validation_1-mean_squared_error:0.02994\n[51]    validation_0-rmse:0.17312   validation_0-mean_squared_error:0.02997 validation_1-rmse:0.17302   validation_1-mean_squared_error:0.02994\n[52]    validation_0-rmse:0.17309   validation_0-mean_squared_error:0.02996 validation_1-rmse:0.17299   validation_1-mean_squared_error:0.02993\n[53]    validation_0-rmse:0.17305   validation_0-mean_squared_error:0.02995 validation_1-rmse:0.17293   validation_1-mean_squared_error:0.02991\n[54]    validation_0-rmse:0.17301   validation_0-mean_squared_error:0.02993 validation_1-rmse:0.17290   validation_1-mean_squared_error:0.02989\n[55]    validation_0-rmse:0.17298   validation_0-mean_squared_error:0.02992 validation_1-rmse:0.17285   validation_1-mean_squared_error:0.02988\n[56]    validation_0-rmse:0.17296   validation_0-mean_squared_error:0.02991 validation_1-rmse:0.17278   validation_1-mean_squared_error:0.02985\n[57]    validation_0-rmse:0.17292   validation_0-mean_squared_error:0.02990 validation_1-rmse:0.17276   validation_1-mean_squared_error:0.02985\n[58]    validation_0-rmse:0.17289   validation_0-mean_squared_error:0.02989 validation_1-rmse:0.17273   validation_1-mean_squared_error:0.02984\n[59]    validation_0-rmse:0.17287   validation_0-mean_squared_error:0.02988 validation_1-rmse:0.17269   validation_1-mean_squared_error:0.02982\n[60]    validation_0-rmse:0.17284   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17267   validation_1-mean_squared_error:0.02982\n[61]    validation_0-rmse:0.17282   validation_0-mean_squared_error:0.02987 validation_1-rmse:0.17264   validation_1-mean_squared_error:0.02981\n[62]    validation_0-rmse:0.17280   validation_0-mean_squared_error:0.02986 validation_1-rmse:0.17262   validation_1-mean_squared_error:0.02980\n[63]    validation_0-rmse:0.17278   validation_0-mean_squared_error:0.02985 validation_1-rmse:0.17257   validation_1-mean_squared_error:0.02978\n[64]    validation_0-rmse:0.17275   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[65]    validation_0-rmse:0.17274   validation_0-mean_squared_error:0.02984 validation_1-rmse:0.17256   validation_1-mean_squared_error:0.02978\n[66]    validation_0-rmse:0.17272   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17254   validation_1-mean_squared_error:0.02977\n[67]    validation_0-rmse:0.17270   validation_0-mean_squared_error:0.02983 validation_1-rmse:0.17249   validation_1-mean_squared_error:0.02975\n[68]    validation_0-rmse:0.17268   validation_0-mean_squared_error:0.02982 validation_1-rmse:0.17248   validation_1-mean_squared_error:0.02975\n[69]    validation_0-rmse:0.17267   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17246   validation_1-mean_squared_error:0.02974\n[70]    validation_0-rmse:0.17265   validation_0-mean_squared_error:0.02981 validation_1-rmse:0.17244   validation_1-mean_squared_error:0.02974\n[71]    validation_0-rmse:0.17264   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17240   validation_1-mean_squared_error:0.02972\n[72]    validation_0-rmse:0.17262   validation_0-mean_squared_error:0.02980 validation_1-rmse:0.17239   validation_1-mean_squared_error:0.02972\n[73]    validation_0-rmse:0.17261   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17237   validation_1-mean_squared_error:0.02971\n[74]    validation_0-rmse:0.17259   validation_0-mean_squared_error:0.02979 validation_1-rmse:0.17236   validation_1-mean_squared_error:0.02971\n[75]    validation_0-rmse:0.17258   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17234   validation_1-mean_squared_error:0.02970\n[76]    validation_0-rmse:0.17257   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17232   validation_1-mean_squared_error:0.02969\n[77]    validation_0-rmse:0.17256   validation_0-mean_squared_error:0.02978 validation_1-rmse:0.17230   validation_1-mean_squared_error:0.02969\n[78]    validation_0-rmse:0.17255   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17228   validation_1-mean_squared_error:0.02968\n[79]    validation_0-rmse:0.17254   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17226   validation_1-mean_squared_error:0.02967\n[80]    validation_0-rmse:0.17253   validation_0-mean_squared_error:0.02977 validation_1-rmse:0.17224   validation_1-mean_squared_error:0.02967\n[81]    validation_0-rmse:0.17252   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17223   validation_1-mean_squared_error:0.02966\n[82]    validation_0-rmse:0.17251   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17222   validation_1-mean_squared_error:0.02966\n[83]    validation_0-rmse:0.17250   validation_0-mean_squared_error:0.02976 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[84]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17221   validation_1-mean_squared_error:0.02966\n[85]    validation_0-rmse:0.17249   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[86]    validation_0-rmse:0.17248   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17219   validation_1-mean_squared_error:0.02965\n[87]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17218   validation_1-mean_squared_error:0.02965\n[88]    validation_0-rmse:0.17247   validation_0-mean_squared_error:0.02975 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[89]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17217   validation_1-mean_squared_error:0.02964\n[90]    validation_0-rmse:0.17246   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[91]    validation_0-rmse:0.17245   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17216   validation_1-mean_squared_error:0.02964\n[92]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17215   validation_1-mean_squared_error:0.02964\n[93]    validation_0-rmse:0.17244   validation_0-mean_squared_error:0.02974 validation_1-rmse:0.17214   validation_1-mean_squared_error:0.02963\n[94]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17213   validation_1-mean_squared_error:0.02963\n[95]    validation_0-rmse:0.17243   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[96]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17212   validation_1-mean_squared_error:0.02962\n[97]    validation_0-rmse:0.17242   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17211   validation_1-mean_squared_error:0.02962\n[98]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02973 validation_1-rmse:0.17210   validation_1-mean_squared_error:0.02962\n[99]    validation_0-rmse:0.17241   validation_0-mean_squared_error:0.02972 validation_1-rmse:0.17209   validation_1-mean_squared_error:0.02962\n\n\nXGBRegressor(base_score=0.5, booster='gblinear', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False,\n             eval_metric=<function mean_squared_error at 0x169861310>,\n             gamma=None, gpu_id=-1, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.5, max_bin=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, n_estimators=100, n_jobs=0,\n             num_parallel_tree=None, predictor=None, random_state=0,\n             reg_alpha=0, reg_lambda=0, ...)\n\n\n\ny_hat_test_ead_xgb = xgb_model.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\n\n\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead_xgb)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.\n\n\n\n\n\n  \n    \n      \n      CCF\n      0\n    \n  \n  \n    \n      CCF\n      1.00000\n      0.52144\n    \n    \n      0\n      0.52144\n      1.00000\n    \n  \n\n\n\n\n\nsns.distplot(ead_targets_test - y_hat_test_ead_xgb)\n# We plot the distribution of the residuals.\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n<AxesSubplot:xlabel='CCF', ylabel='Density'>\n\n\n\n\n\n\npd.DataFrame(y_hat_test_ead_xgb).describe()\n# Shows some descriptive statisics for the values of a column.\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      count\n      8648.000000\n    \n    \n      mean\n      0.735745\n    \n    \n      std\n      0.101577\n    \n    \n      min\n      0.408254\n    \n    \n      25%\n      0.664853\n    \n    \n      50%\n      0.728506\n    \n    \n      75%\n      0.811329\n    \n    \n      max\n      1.310113\n    \n  \n\n\n\n\n\nmean_squared_error(ead_targets_test, y_hat_test_ead_xgb)\n\n0.029612655435575855\n\n\n\nr2_score(ead_targets_test, y_hat_test_ead_xgb)\n\n0.2715104861315287"
  },
  {
    "objectID": "notebooks/send_custom_results.html",
    "href": "notebooks/send_custom_results.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  # Use your project ID\n  project = \"...\"\n)\n  \n\nTrue\n\n\n\n\nIt is possible to send custom metrics to ValidMind without implementing a Metric class. The only requirement is to construct an instance of a MetricResult that can be sent to the ValidMind API.\n\nfrom validmind.vm_models import MetricResult\n\naccuracy_metric = MetricResult(\n    type=\"evaluation\",\n    scope=\"test_dataset\",    \n    key=\"my_custom_accuracy\",\n    value=0.666\n)\n\n\nvm.log_metrics([accuracy_metric])\n\nTrue\n\n\n\n\n\nIt is possible to send custom test results to ValidMind without implementing a ThresholdTest class. The only requirement is to construct an instance of a TestResults that can be sent to the ValidMind API.\n\nfrom validmind.vm_models import TestResult, TestResults\n\ncustom_params = {\n    \"min_percent_threshold\": 0.5\n}\n\ncustom_test_result = TestResults(\n    category=\"model_performance\",\n    test_name=\"accuracy_threshold\",\n    params=custom_params,\n    passed=False,\n    results=[\n        TestResult(\n            passed=False,\n            values={\n                \"score\": 0.15,\n                \"threshold\": custom_params[\"min_percent_threshold\"],\n            },\n        )\n    ],\n)\n\n\nvm.log_test_results([custom_test_result])\n\nTrue\n\n\n\n\n\nIt is possible to implement custom metrics or threshold test classes. The are only two requirements for getting this to work:\n\nWe need to build a TestPlan that can execute the custom metric or threshold test (or add it to an existing TestPlan, TBD).\nWe need to implement a run method on the custom metric or threshold test class.\n\n\n\nThe following example shows how to implement a custom metric that calculates the mean of a list of numbers.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MeanMetric(Metric):\n    type = \"dataset\"\n    key = \"mean_of_values\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        return self.cache_results(mean)\n\n\n\n\nIt is possible to run a custom metric without running an entire test plan. This is useful for testing the metric before integrating it into a test plan.\nThe key idea is to create a TestContext object and pass it to the metric initializer. When a test plan is executed, the TestContext is created by the TestPlan class and passed down to every associated metric and threshold test. However, when we want to test a metric in isolation, we need to create the TestContext ourselves.\nIn this example we don’t need to pass any arguments to the TestContext initializer, but it is possible to pass any arguments as required by required_context.\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))\n\n\nWe can also inspect the results of the metric by accessing the result variable:\n\nmean_metric.result\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))\n\n\n\nmean_metric.result.metric.value\n\n3.0\n\n\n\nmean_metric.result.show()\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n\n\n\n\n\nThe following example shows how to implement a custom threshold test that fails if the mean of a list of numbers is greater than 5.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import ThresholdTest\n\n@dataclass\nclass MeanThresholdTest(ThresholdTest):\n    category = \"data_quality\"\n    name = \"mean_threshold\"\n    default_params = {\"mean_threshold\": 5}    \n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)        \n\n        passed = mean <= self.params[\"mean_threshold\"]\n        results = [\n            TestResult(\n                passed=passed,\n                values={\n                    \"mean\": mean,\n                    \"values\": values,\n                },\n            )\n        ]\n\n        return self.cache_results(results, passed=passed)\n\n\n\n\nSimilarly to custom metrics, it is also possible to run a custom threshold test without running an entire test plan:\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_threshold_test = MeanThresholdTest(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_threshold_test.run()\n\nTestPlanTestResult(figures=None, test_results=TestResults(category='data_quality', test_name='mean_threshold', params={'mean_threshold': 5, 'values': [1, 2, 3, 4, 5]}, passed=True, results=[TestResult(test_name=None, column=None, passed=True, values={'mean': 3.0, 'values': [1, 2, 3, 4, 5]})]))\n\n\n\ntest_results = mean_threshold_test.test_results.test_results\ntest_results.passed\n\nTrue\n\n\n\nfor result in test_results.results:\n    print(f\"passed: {result.passed}, values: {result.values}\")\n\npassed: True, values: {'mean': 3.0, 'values': [1, 2, 3, 4, 5]}\n\n\n\n\n\nThe following example shows how to implement a custom test plan that executes the custom metric and threshold test.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MeanMetric, MeanThresholdTest]\n\nmy_custom_test_plan = MyCustomTestPlan(config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\nmy_custom_test_plan.run()\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nIt is possible to register a custom test plan with ValidMind. This allows us to run the test plan using the same ValidMind Python API and combine it with other test plans as needed.\n\nvm.test_plans.register_test_plan(\"my_custom_test_plan\", MyCustomTestPlan)\n\nRegistered test plan: my_custom_test_plan\n\n\n\nvm.run_test_plan(\"my_custom_test_plan\", config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\nIt is possible to send figures with metrics and test results. The following example shows how to send a figure with a metric result.\n\n\nLet’s say we want to add a figure to our custom metric above. We can do this by adding a figures attribute to the cache_results method call. Let’s modify our custom metric code to do that.\n\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom validmind.vm_models import Figure, Metric\n\n@dataclass\nclass MeanMetricWithFigure(Metric):\n    type = \"dataset\"\n    key = \"mean_of_values_with_figure\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(key=self.key, figure=fig, metadata={})\n\n        return self.cache_results(mean, figures=[figure])\n\n\n\n\nSimilarly, we can add a figure to our custom threshold test by adding a figures attribute to the cache_results method call.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Figure, ThresholdTest\n\n@dataclass\nclass MeanThresholdTestWithFigure(ThresholdTest):\n    category = \"data_quality\"\n    name = \"mean_threshold_with_figure\"\n    default_params = {\"mean_threshold\": 5}    \n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)        \n\n        passed = mean <= self.params[\"mean_threshold\"]\n        results = [\n            TestResult(\n                passed=passed,\n                values={\n                    \"mean\": mean,\n                    \"values\": values,\n                },\n            )\n        ]\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(key=self.name, figure=fig, metadata={})        \n\n        return self.cache_results(results, passed=passed, figures=[figure])\n\nWe can now run a new test plan that includes our two new custom metrics and threshold tests.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlanWithFigures(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan_with_figures\"\n    required_context = []\n    tests = [MeanMetricWithFigure, MeanThresholdTestWithFigure]\n\nmy_custom_test_plan_with_figures = MyCustomTestPlanWithFigures(config={\n    \"mean_of_values_with_figure\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold_with_figure\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\nmy_custom_test_plan_with_figures.run()\n\n                                                                                                                                \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values_with_figure\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold With Figure\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold_with_figure\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/run_individual_tests.html",
    "href": "notebooks/run_individual_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebook shows how to run individual metrics or thresholds tests, and how to pass custom parameters to them.\n\n%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clh0yyhg700825x8h4ocp5i3u\"\n)\n\nTrue\n\n\n\n\nWe train a simple customer churn model for our test.\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nWe can now import the individual test and pass the required context and config parameters to it.\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.model_validation.sklearn.threshold_tests import WeakspotsDiagnosis\n\n\ntest_context = TestContext(model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\nws_diagnostic = WeakspotsDiagnosis(test_context)\n\n\nws_diagnostic.run()\n\nTestPlanTestResult(result_id=\"weak_spots\", test_results)\n\n\n\nws_diagnostic.test_results.show()\n\n\n        \n        \n            \n                \n                    \n                        Weak Spots\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    weak_spots\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'thresholds': {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.5, 'f1': 0.7}}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column='Gender', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2674, 0, 0, 0, 0, 0, 0, 0, 0, 2126, 856, 0, 0, 0, 0, 0, 0, 0, 0, 744], 'accuracy': [0.9199700822737472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8857008466603951, 0.8820093457943925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8373655913978495], 'precision': [0.8969465648854962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8825214899713467, 0.7448979591836735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7279411764705882], 'recall': [0.556872037914692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6039215686274509, 0.4899328859060403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5409836065573771], 'f1': [0.6871345029239767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7171129220023281, 0.5910931174089069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6206896551724138]}), TestResult(test_name='accuracy', column='Age', passed=False, values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [265, 1042, 1795, 906, 426, 207, 93, 51, 12, 3, 105, 350, 567, 290, 144, 83, 33, 25, 3, 0], 'accuracy': [0.9433962264150944, 0.9299424184261037, 0.9153203342618385, 0.8509933774834437, 0.8661971830985915, 0.9082125603864735, 0.956989247311828, 0.9607843137254902, 1.0, 1.0, 0.9238095238095239, 0.94, 0.8694885361552028, 0.803448275862069, 0.6944444444444444, 0.8433734939759037, 0.9393939393939394, 0.88, 1.0, 0.0], 'precision': [1.0, 0.9285714285714286, 0.9830508474576272, 0.8212290502793296, 0.8854625550660793, 0.9292929292929293, 0.9565217391304348, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 0.5454545454545454, 0.7076923076923077, 0.7222222222222222, 0.8666666666666667, 1.0, 0.0, 0.0, 0.0], 'recall': [0.25, 0.15294117647058825, 0.27751196172248804, 0.588, 0.8663793103448276, 0.8846153846153846, 0.88, 0.6666666666666666, 1.0, 0.0, 0.0, 0.20833333333333334, 0.15789473684210525, 0.5476190476190477, 0.7738095238095238, 0.8478260869565217, 0.7142857142857143, 0.0, 0.0, 0.0], 'f1': [0.4, 0.26262626262626265, 0.4328358208955224, 0.6853146853146853, 0.8758169934640523, 0.9064039408866995, 0.9166666666666666, 0.8, 1.0, 0.0, 0.0, 0.3225806451612903, 0.24489795918367344, 0.6174496644295303, 0.7471264367816092, 0.8571428571428571, 0.8333333333333333, 0.0, 0.0, 0.0]}), TestResult(test_name='accuracy', column='Tenure', passed=False, values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]', '(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'shape': [662, 531, 494, 470, 485, 461, 481, 508, 481, 227, 241, 167, 182, 151, 157, 153, 173, 141, 138, 97], 'accuracy': [0.9063444108761329, 0.8983050847457628, 0.9149797570850202, 0.8936170212765957, 0.9278350515463918, 0.8828633405639913, 0.9147609147609148, 0.90748031496063, 0.8814968814968815, 0.933920704845815, 0.8464730290456431, 0.8982035928143712, 0.8516483516483516, 0.8211920529801324, 0.8789808917197452, 0.8758169934640523, 0.9075144508670521, 0.8156028368794326, 0.8768115942028986, 0.8247422680412371], 'precision': [0.8952380952380953, 0.8688524590163934, 0.8873239436619719, 0.8688524590163934, 0.9104477611940298, 0.8936170212765957, 0.8846153846153846, 0.8703703703703703, 0.8615384615384616, 1.0, 0.6976744186046512, 0.78125, 0.7083333333333334, 0.75, 0.8181818181818182, 0.6818181818181818, 0.7894736842105263, 0.6521739130434783, 0.6875, 0.8461538461538461], 'recall': [0.6482758620689655, 0.5353535353535354, 0.6494845360824743, 0.5578947368421052, 0.6777777777777778, 0.46153846153846156, 0.5679012345679012, 0.5402298850574713, 0.5384615384615384, 0.6511627906976745, 0.5555555555555556, 0.7142857142857143, 0.4594594594594595, 0.40540540540540543, 0.5454545454545454, 0.5555555555555556, 0.5555555555555556, 0.45454545454545453, 0.4782608695652174, 0.4230769230769231], 'f1': [0.752, 0.6625, 0.7500000000000001, 0.6794871794871795, 0.7770700636942676, 0.6086956521739131, 0.6917293233082706, 0.6666666666666666, 0.6627218934911243, 0.7887323943661972, 0.6185567010309277, 0.7462686567164178, 0.5573770491803279, 0.5263157894736842, 0.6545454545454545, 0.6122448979591836, 0.6521739130434783, 0.5357142857142857, 0.5641025641025642, 0.5641025641025641]}), TestResult(test_name='accuracy', column='Balance', passed=False, values={'slice': ['(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]', '(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]'], 'shape': [1771, 34, 165, 542, 994, 840, 352, 86, 15, 1, 565, 10, 57, 211, 321, 281, 119, 32, 3, 1], 'accuracy': [0.9305477131564088, 0.8823529411764706, 0.8787878787878788, 0.8948339483394834, 0.8812877263581489, 0.888095238095238, 0.9090909090909091, 0.8953488372093024, 1.0, 1.0, 0.9079646017699115, 0.8, 0.8947368421052632, 0.8625592417061612, 0.8161993769470405, 0.8398576512455516, 0.7899159663865546, 0.875, 1.0, 1.0], 'precision': [0.9050632911392406, 1.0, 0.9166666666666666, 0.8793103448275862, 0.864406779661017, 0.8702290076335878, 0.9696969696969697, 0.8333333333333334, 1.0, 1.0, 0.8181818181818182, 0.0, 0.6666666666666666, 0.7368421052631579, 0.6811594202898551, 0.7843137254901961, 0.7058823529411765, 0.5, 1.0, 1.0], 'recall': [0.5697211155378487, 0.6, 0.55, 0.504950495049505, 0.6194331983805668, 0.5968586387434555, 0.5079365079365079, 0.5882352941176471, 1.0, 1.0, 0.45, 0.0, 0.5, 0.5957446808510638, 0.5595238095238095, 0.5405405405405406, 0.375, 0.5, 1.0, 1.0], 'f1': [0.6992665036674817, 0.7499999999999999, 0.6874999999999999, 0.6415094339622641, 0.7216981132075471, 0.7080745341614907, 0.6666666666666666, 0.6896551724137931, 1.0, 1.0, 0.5806451612903226, 0.0, 0.5714285714285715, 0.6588235294117647, 0.6143790849673202, 0.6399999999999999, 0.48979591836734687, 0.5, 1.0, 1.0]}), TestResult(test_name='accuracy', column='NumOfProducts', passed=False, values={'slice': ['(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]', '(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]'], 'shape': [2433, 0, 0, 2223, 0, 0, 120, 0, 0, 24, 789, 0, 0, 745, 0, 0, 54, 0, 0, 12], 'accuracy': [0.8586107685984381, 0.0, 0.0, 0.9496176338281601, 0.0, 0.0, 0.9916666666666667, 0.0, 0.0, 1.0, 0.7858048162230672, 0.0, 0.0, 0.9409395973154362, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 1.0], 'precision': [0.8652482269503546, 0.0, 0.0, 0.8529411764705882, 0.0, 0.0, 0.9895833333333334, 0.0, 0.0, 1.0, 0.6790123456790124, 0.0, 0.0, 0.6470588235294118, 0.0, 0.0, 0.9069767441860465, 0.0, 0.0, 1.0], 'recall': [0.5604900459418071, 0.0, 0.0, 0.3625, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4845814977973568, 0.0, 0.0, 0.22448979591836735, 0.0, 0.0, 0.8863636363636364, 0.0, 0.0, 1.0], 'f1': [0.6802973977695167, 0.0, 0.0, 0.5087719298245613, 0.0, 0.0, 0.9947643979057591, 0.0, 0.0, 1.0, 0.5655526992287917, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.896551724137931, 0.0, 0.0, 1.0]}), TestResult(test_name='accuracy', column='HasCrCard', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [1420, 0, 0, 0, 0, 0, 0, 0, 0, 3380, 480, 0, 0, 0, 0, 0, 0, 0, 0, 1120], 'accuracy': [0.9063380281690141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9041420118343195, 0.8520833333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8651785714285715], 'precision': [0.8917525773195877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8872901678657075, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7294117647058823], 'recall': [0.6070175438596491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5718701700154559, 0.46601941747572817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5414847161572053], 'f1': [0.7223382045929019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6954887218045114, 0.5748502994011976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6215538847117795]}), TestResult(test_name='accuracy', column='IsActiveMember', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2326, 0, 0, 0, 0, 0, 0, 0, 0, 2474, 725, 0, 0, 0, 0, 0, 0, 0, 0, 875], 'accuracy': [0.8899398108340498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9187550525464834, 0.8386206896551724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], 'precision': [0.8857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8952879581151832, 0.782051282051282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6410256410256411], 'recall': [0.6413793103448275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48579545454545453, 0.5951219512195122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3937007874015748], 'f1': [0.7439999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6298342541436464, 0.6759002770083102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4878048780487805]}), TestResult(test_name='accuracy', column='EstimatedSalary', passed=False, values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]', '(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'shape': [471, 479, 477, 494, 461, 491, 499, 470, 477, 481, 139, 152, 165, 170, 178, 160, 153, 151, 181, 151], 'accuracy': [0.9171974522292994, 0.918580375782881, 0.8888888888888888, 0.902834008097166, 0.8980477223427332, 0.8879837067209776, 0.8897795591182365, 0.9042553191489362, 0.9119496855345912, 0.9293139293139293, 0.8848920863309353, 0.881578947368421, 0.8424242424242424, 0.8352941176470589, 0.848314606741573, 0.85625, 0.9150326797385621, 0.8807947019867549, 0.850828729281768, 0.8278145695364238], 'precision': [0.9156626506024096, 0.8870967741935484, 0.8717948717948718, 0.8448275862068966, 0.875, 0.8333333333333334, 0.8703703703703703, 0.9090909090909091, 0.92, 0.9365079365079365, 0.65, 0.7368421052631579, 0.7058823529411765, 0.7692307692307693, 0.7058823529411765, 0.6086956521739131, 0.8846153846153846, 0.8421052631578947, 0.7857142857142857, 0.6363636363636364], 'recall': [0.7037037037037037, 0.632183908045977, 0.4146341463414634, 0.5568181818181818, 0.550561797752809, 0.5555555555555556, 0.49473684210526314, 0.5555555555555556, 0.6571428571428571, 0.6629213483146067, 0.5909090909090909, 0.5185185185185185, 0.36363636363636365, 0.47619047619047616, 0.5853658536585366, 0.5, 0.696969696969697, 0.5161290322580645, 0.5116279069767442, 0.4375], 'f1': [0.7958115183246074, 0.7382550335570469, 0.5619834710743802, 0.6712328767123287, 0.6758620689655173, 0.6666666666666667, 0.6308724832214765, 0.6896551724137931, 0.7666666666666667, 0.7763157894736842, 0.6190476190476191, 0.6086956521739131, 0.48000000000000004, 0.588235294117647, 0.64, 0.5490196078431373, 0.7796610169491526, 0.6399999999999999, 0.619718309859155, 0.5185185185185185]}), TestResult(test_name='accuracy', column='Geography_France', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2409, 0, 0, 0, 0, 0, 0, 0, 0, 2391, 811, 0, 0, 0, 0, 0, 0, 0, 0, 789], 'accuracy': [0.8945620589456206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9150982852363028, 0.8310727496917386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8922686945500634], 'precision': [0.8875305623471883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8910891089108911, 0.7023809523809523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8181818181818182], 'recall': [0.6357267950963222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4986149584487535, 0.5756097560975609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4251968503937008], 'f1': [0.7408163265306122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6394316163410302, 0.6327077747989276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5595854922279793]}), TestResult(test_name='accuracy', column='Geography_Germany', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3606, 0, 0, 0, 0, 0, 0, 0, 0, 1194, 1181, 0, 0, 0, 0, 0, 0, 0, 0, 419], 'accuracy': [0.9143094841930116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8760469011725294, 0.8899237933954276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7804295942720764], 'precision': [0.9041533546325878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87248322147651, 0.7798165137614679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.696], 'recall': [0.50355871886121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7027027027027027, 0.44502617801047123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6170212765957447], 'f1': [0.6468571428571428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784431137724551, 0.5666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6541353383458647]}), TestResult(test_name='accuracy', column='Geography_Spain', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3585, 0, 0, 0, 0, 0, 0, 0, 0, 1215, 1208, 0, 0, 0, 0, 0, 0, 0, 0, 392], 'accuracy': [0.902092050209205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9127572016460905, 0.8534768211920529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8852040816326531], 'precision': [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9279279279279279, 0.7382198952879581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7209302325581395], 'recall': [0.6019151846785226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5124378109452736, 0.5261194029850746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375], 'f1': [0.7148659626320065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6602564102564101, 0.6143790849673203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5794392523364486]})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n\n\n\nws_diagnostic.test_results.test_results.results\n\n[TestResult(test_name='accuracy', column='Gender', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2674, 0, 0, 0, 0, 0, 0, 0, 0, 2126, 856, 0, 0, 0, 0, 0, 0, 0, 0, 744], 'accuracy': [0.9199700822737472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8857008466603951, 0.8820093457943925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8373655913978495], 'precision': [0.8969465648854962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8825214899713467, 0.7448979591836735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7279411764705882], 'recall': [0.556872037914692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6039215686274509, 0.4899328859060403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5409836065573771], 'f1': [0.6871345029239767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7171129220023281, 0.5910931174089069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6206896551724138]}),\n TestResult(test_name='accuracy', column='Age', passed=False, values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [265, 1042, 1795, 906, 426, 207, 93, 51, 12, 3, 105, 350, 567, 290, 144, 83, 33, 25, 3, 0], 'accuracy': [0.9433962264150944, 0.9299424184261037, 0.9153203342618385, 0.8509933774834437, 0.8661971830985915, 0.9082125603864735, 0.956989247311828, 0.9607843137254902, 1.0, 1.0, 0.9238095238095239, 0.94, 0.8694885361552028, 0.803448275862069, 0.6944444444444444, 0.8433734939759037, 0.9393939393939394, 0.88, 1.0, 0.0], 'precision': [1.0, 0.9285714285714286, 0.9830508474576272, 0.8212290502793296, 0.8854625550660793, 0.9292929292929293, 0.9565217391304348, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 0.5454545454545454, 0.7076923076923077, 0.7222222222222222, 0.8666666666666667, 1.0, 0.0, 0.0, 0.0], 'recall': [0.25, 0.15294117647058825, 0.27751196172248804, 0.588, 0.8663793103448276, 0.8846153846153846, 0.88, 0.6666666666666666, 1.0, 0.0, 0.0, 0.20833333333333334, 0.15789473684210525, 0.5476190476190477, 0.7738095238095238, 0.8478260869565217, 0.7142857142857143, 0.0, 0.0, 0.0], 'f1': [0.4, 0.26262626262626265, 0.4328358208955224, 0.6853146853146853, 0.8758169934640523, 0.9064039408866995, 0.9166666666666666, 0.8, 1.0, 0.0, 0.0, 0.3225806451612903, 0.24489795918367344, 0.6174496644295303, 0.7471264367816092, 0.8571428571428571, 0.8333333333333333, 0.0, 0.0, 0.0]}),\n TestResult(test_name='accuracy', column='Tenure', passed=False, values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]', '(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'shape': [662, 531, 494, 470, 485, 461, 481, 508, 481, 227, 241, 167, 182, 151, 157, 153, 173, 141, 138, 97], 'accuracy': [0.9063444108761329, 0.8983050847457628, 0.9149797570850202, 0.8936170212765957, 0.9278350515463918, 0.8828633405639913, 0.9147609147609148, 0.90748031496063, 0.8814968814968815, 0.933920704845815, 0.8464730290456431, 0.8982035928143712, 0.8516483516483516, 0.8211920529801324, 0.8789808917197452, 0.8758169934640523, 0.9075144508670521, 0.8156028368794326, 0.8768115942028986, 0.8247422680412371], 'precision': [0.8952380952380953, 0.8688524590163934, 0.8873239436619719, 0.8688524590163934, 0.9104477611940298, 0.8936170212765957, 0.8846153846153846, 0.8703703703703703, 0.8615384615384616, 1.0, 0.6976744186046512, 0.78125, 0.7083333333333334, 0.75, 0.8181818181818182, 0.6818181818181818, 0.7894736842105263, 0.6521739130434783, 0.6875, 0.8461538461538461], 'recall': [0.6482758620689655, 0.5353535353535354, 0.6494845360824743, 0.5578947368421052, 0.6777777777777778, 0.46153846153846156, 0.5679012345679012, 0.5402298850574713, 0.5384615384615384, 0.6511627906976745, 0.5555555555555556, 0.7142857142857143, 0.4594594594594595, 0.40540540540540543, 0.5454545454545454, 0.5555555555555556, 0.5555555555555556, 0.45454545454545453, 0.4782608695652174, 0.4230769230769231], 'f1': [0.752, 0.6625, 0.7500000000000001, 0.6794871794871795, 0.7770700636942676, 0.6086956521739131, 0.6917293233082706, 0.6666666666666666, 0.6627218934911243, 0.7887323943661972, 0.6185567010309277, 0.7462686567164178, 0.5573770491803279, 0.5263157894736842, 0.6545454545454545, 0.6122448979591836, 0.6521739130434783, 0.5357142857142857, 0.5641025641025642, 0.5641025641025641]}),\n TestResult(test_name='accuracy', column='Balance', passed=False, values={'slice': ['(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]', '(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]'], 'shape': [1771, 34, 165, 542, 994, 840, 352, 86, 15, 1, 565, 10, 57, 211, 321, 281, 119, 32, 3, 1], 'accuracy': [0.9305477131564088, 0.8823529411764706, 0.8787878787878788, 0.8948339483394834, 0.8812877263581489, 0.888095238095238, 0.9090909090909091, 0.8953488372093024, 1.0, 1.0, 0.9079646017699115, 0.8, 0.8947368421052632, 0.8625592417061612, 0.8161993769470405, 0.8398576512455516, 0.7899159663865546, 0.875, 1.0, 1.0], 'precision': [0.9050632911392406, 1.0, 0.9166666666666666, 0.8793103448275862, 0.864406779661017, 0.8702290076335878, 0.9696969696969697, 0.8333333333333334, 1.0, 1.0, 0.8181818181818182, 0.0, 0.6666666666666666, 0.7368421052631579, 0.6811594202898551, 0.7843137254901961, 0.7058823529411765, 0.5, 1.0, 1.0], 'recall': [0.5697211155378487, 0.6, 0.55, 0.504950495049505, 0.6194331983805668, 0.5968586387434555, 0.5079365079365079, 0.5882352941176471, 1.0, 1.0, 0.45, 0.0, 0.5, 0.5957446808510638, 0.5595238095238095, 0.5405405405405406, 0.375, 0.5, 1.0, 1.0], 'f1': [0.6992665036674817, 0.7499999999999999, 0.6874999999999999, 0.6415094339622641, 0.7216981132075471, 0.7080745341614907, 0.6666666666666666, 0.6896551724137931, 1.0, 1.0, 0.5806451612903226, 0.0, 0.5714285714285715, 0.6588235294117647, 0.6143790849673202, 0.6399999999999999, 0.48979591836734687, 0.5, 1.0, 1.0]}),\n TestResult(test_name='accuracy', column='NumOfProducts', passed=False, values={'slice': ['(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]', '(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]'], 'shape': [2433, 0, 0, 2223, 0, 0, 120, 0, 0, 24, 789, 0, 0, 745, 0, 0, 54, 0, 0, 12], 'accuracy': [0.8586107685984381, 0.0, 0.0, 0.9496176338281601, 0.0, 0.0, 0.9916666666666667, 0.0, 0.0, 1.0, 0.7858048162230672, 0.0, 0.0, 0.9409395973154362, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 1.0], 'precision': [0.8652482269503546, 0.0, 0.0, 0.8529411764705882, 0.0, 0.0, 0.9895833333333334, 0.0, 0.0, 1.0, 0.6790123456790124, 0.0, 0.0, 0.6470588235294118, 0.0, 0.0, 0.9069767441860465, 0.0, 0.0, 1.0], 'recall': [0.5604900459418071, 0.0, 0.0, 0.3625, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4845814977973568, 0.0, 0.0, 0.22448979591836735, 0.0, 0.0, 0.8863636363636364, 0.0, 0.0, 1.0], 'f1': [0.6802973977695167, 0.0, 0.0, 0.5087719298245613, 0.0, 0.0, 0.9947643979057591, 0.0, 0.0, 1.0, 0.5655526992287917, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.896551724137931, 0.0, 0.0, 1.0]}),\n TestResult(test_name='accuracy', column='HasCrCard', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [1420, 0, 0, 0, 0, 0, 0, 0, 0, 3380, 480, 0, 0, 0, 0, 0, 0, 0, 0, 1120], 'accuracy': [0.9063380281690141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9041420118343195, 0.8520833333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8651785714285715], 'precision': [0.8917525773195877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8872901678657075, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7294117647058823], 'recall': [0.6070175438596491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5718701700154559, 0.46601941747572817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5414847161572053], 'f1': [0.7223382045929019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6954887218045114, 0.5748502994011976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6215538847117795]}),\n TestResult(test_name='accuracy', column='IsActiveMember', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2326, 0, 0, 0, 0, 0, 0, 0, 0, 2474, 725, 0, 0, 0, 0, 0, 0, 0, 0, 875], 'accuracy': [0.8899398108340498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9187550525464834, 0.8386206896551724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], 'precision': [0.8857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8952879581151832, 0.782051282051282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6410256410256411], 'recall': [0.6413793103448275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48579545454545453, 0.5951219512195122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3937007874015748], 'f1': [0.7439999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6298342541436464, 0.6759002770083102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4878048780487805]}),\n TestResult(test_name='accuracy', column='EstimatedSalary', passed=False, values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]', '(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'shape': [471, 479, 477, 494, 461, 491, 499, 470, 477, 481, 139, 152, 165, 170, 178, 160, 153, 151, 181, 151], 'accuracy': [0.9171974522292994, 0.918580375782881, 0.8888888888888888, 0.902834008097166, 0.8980477223427332, 0.8879837067209776, 0.8897795591182365, 0.9042553191489362, 0.9119496855345912, 0.9293139293139293, 0.8848920863309353, 0.881578947368421, 0.8424242424242424, 0.8352941176470589, 0.848314606741573, 0.85625, 0.9150326797385621, 0.8807947019867549, 0.850828729281768, 0.8278145695364238], 'precision': [0.9156626506024096, 0.8870967741935484, 0.8717948717948718, 0.8448275862068966, 0.875, 0.8333333333333334, 0.8703703703703703, 0.9090909090909091, 0.92, 0.9365079365079365, 0.65, 0.7368421052631579, 0.7058823529411765, 0.7692307692307693, 0.7058823529411765, 0.6086956521739131, 0.8846153846153846, 0.8421052631578947, 0.7857142857142857, 0.6363636363636364], 'recall': [0.7037037037037037, 0.632183908045977, 0.4146341463414634, 0.5568181818181818, 0.550561797752809, 0.5555555555555556, 0.49473684210526314, 0.5555555555555556, 0.6571428571428571, 0.6629213483146067, 0.5909090909090909, 0.5185185185185185, 0.36363636363636365, 0.47619047619047616, 0.5853658536585366, 0.5, 0.696969696969697, 0.5161290322580645, 0.5116279069767442, 0.4375], 'f1': [0.7958115183246074, 0.7382550335570469, 0.5619834710743802, 0.6712328767123287, 0.6758620689655173, 0.6666666666666667, 0.6308724832214765, 0.6896551724137931, 0.7666666666666667, 0.7763157894736842, 0.6190476190476191, 0.6086956521739131, 0.48000000000000004, 0.588235294117647, 0.64, 0.5490196078431373, 0.7796610169491526, 0.6399999999999999, 0.619718309859155, 0.5185185185185185]}),\n TestResult(test_name='accuracy', column='Geography_France', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2409, 0, 0, 0, 0, 0, 0, 0, 0, 2391, 811, 0, 0, 0, 0, 0, 0, 0, 0, 789], 'accuracy': [0.8945620589456206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9150982852363028, 0.8310727496917386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8922686945500634], 'precision': [0.8875305623471883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8910891089108911, 0.7023809523809523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8181818181818182], 'recall': [0.6357267950963222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4986149584487535, 0.5756097560975609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4251968503937008], 'f1': [0.7408163265306122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6394316163410302, 0.6327077747989276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5595854922279793]}),\n TestResult(test_name='accuracy', column='Geography_Germany', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3606, 0, 0, 0, 0, 0, 0, 0, 0, 1194, 1181, 0, 0, 0, 0, 0, 0, 0, 0, 419], 'accuracy': [0.9143094841930116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8760469011725294, 0.8899237933954276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7804295942720764], 'precision': [0.9041533546325878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87248322147651, 0.7798165137614679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.696], 'recall': [0.50355871886121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7027027027027027, 0.44502617801047123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6170212765957447], 'f1': [0.6468571428571428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784431137724551, 0.5666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6541353383458647]}),\n TestResult(test_name='accuracy', column='Geography_Spain', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3585, 0, 0, 0, 0, 0, 0, 0, 0, 1215, 1208, 0, 0, 0, 0, 0, 0, 0, 0, 392], 'accuracy': [0.902092050209205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9127572016460905, 0.8534768211920529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8852040816326531], 'precision': [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9279279279279279, 0.7382198952879581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7209302325581395], 'recall': [0.6019151846785226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5124378109452736, 0.5261194029850746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375], 'f1': [0.7148659626320065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6602564102564101, 0.6143790849673203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5794392523364486]})]"
  },
  {
    "objectID": "notebooks/explore_x_train_lc.html",
    "href": "notebooks/explore_x_train_lc.html",
    "title": "ValidMind",
    "section": "",
    "text": "Explore x to train LC\n\n# Quick hack to load local SDK code\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\nimport validmind as vm\n\n# PD Model\nvm.init(project=\"cl1jyvh2c000909lg1rk0a0zb\")\n\nTrue\n\n\n\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nimport scipy\n\n\ndef jeffreys_test(p: float, n: int = 0, d: int = 0) -> float:\n    \"\"\"\n    Perform a test that the test probability, p, is consistent with the observed number of \n    successes, d, from a number of trials, n.\n\n    This uses the Jeffrey's posterior probability, which is the Beta distribution with shape\n    parameters a = d + 1/2 and b = n - d + 1/2. The result is the one sided p-value representing the \n    probability that the test probability, p, is greater than the true probability.\n\n    :param p: the test probability to be compared to the number of successes given n trials\n    :param n: the number of trials\n    :param d: the number of successes [optional, default = 0]\n\n    :return p-value: one sided p-value of the test\n    \"\"\"\n    return scipy.stats.beta.cdf(p, d + 0.5, n - d + 0.5)\n\n\ndef update_result(s, d, n, dr, p, pval, out = 'Yet to decide'):\n    return ({'Segment': s,\n            'Defaults': d,\n            'Observations': n,\n            'Default Rate': dr,\n            'Calibrated PD': p,\n            'P-value': pval, \n            'Outcome': out})\n\n\ndef calculate_and_return(df = pd.DataFrame, cal_pd = {}, pool = None, obs = 'observed', threshold = 0.9):\n    \"\"\"\n    Take the input dataframe, analyse & clean, seprate poolwise.\n    Calculate the jeffreys statistic\n    \"\"\"\n    \n    result = pd.DataFrame(columns = ['Segment', 'Defaults', 'Observations', 'Default Rate', 'Calibrated PD', 'P-value', 'Outcome'])\n    \n    n = len(df[obs])\n    d = np.sum(df[obs])\n    dr = np.round(d/n,2)\n    p = cal_pd['Model']\n    pval = np.round(jeffreys_test(p, n, d),4)\n    if pval>=threshold:\n        out = 'Satisfactory'\n    else:\n        out = 'Not Satisfactory'\n    \n    result = result.append(update_result('Model', d, n, dr, p, pval, out), ignore_index = True)\n    \n    if pool != None:\n        samples = df.groupby(pool)\n        \n        for sample in samples:\n            n = len(sample[1][obs])\n            d = np.sum(sample[1][obs])\n            dr = np.round(d/n,2)\n            p = cal_pd[sample[0]]\n            pval = np.round(jeffreys_test(p, n, d),4)\n            \n            if pval>=threshold:\n                out = 'Satisfactory'\n            else:\n                out = 'Not Satisfactory'\n            \n            result = result.append(update_result(sample[0], d, n, dr, p, pval, out), ignore_index = True)\n            \n    return result\n\n\ndf = pd.read_csv(\"./notebooks/datasets/_temp/x_train_lc.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      28000.0\n      7.12\n      10\n      125000.0\n      15.97\n      0.0\n      26\n      725.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      11200.0\n      10.99\n      2\n      80600.0\n      15.93\n      0.0\n      15\n      670.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      2\n      14000.0\n      15.10\n      6\n      83000.0\n      18.17\n      0.0\n      13\n      660.0\n      1.0\n      76.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      12725.0\n      12.12\n      6\n      71300.0\n      29.70\n      0.0\n      13\n      675.0\n      2.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      7200.0\n      15.31\n      1\n      25000.0\n      32.98\n      0.0\n      8\n      700.0\n      0.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ndf[\"acc_now_delinq\"].value_counts()\n\n0.0    59802\n1.0      187\n2.0        7\n3.0        3\n5.0        1\nName: acc_now_delinq, dtype: int64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 60.0 MB\n\n\n\ntest_df = pd.read_csv(\"./notebooks/datasets/_temp/x_test_lc.csv\")\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      loan_amnt\n      int_rate\n      emp_length\n      annual_inc\n      dti\n      delinq_2yrs\n      earliest_cr_line\n      fico_range_low\n      inq_last_6mths\n      mths_since_last_delinq\n      ...\n      purpose_medical\n      purpose_moving\n      purpose_other\n      purpose_renewable_energy\n      purpose_small_business\n      purpose_vacation\n      purpose_wedding\n      initial_list_status_f\n      initial_list_status_w\n      application_type_Individual\n    \n  \n  \n    \n      0\n      15500.0\n      8.90\n      10\n      100000.0\n      0.74\n      0.0\n      14\n      715.0\n      3.0\n      25.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      1\n      10800.0\n      11.67\n      10\n      68000.0\n      15.44\n      1.0\n      20\n      670.0\n      1.0\n      8.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n    \n      2\n      15850.0\n      15.10\n      2\n      36000.0\n      26.50\n      0.0\n      31\n      720.0\n      1.0\n      0.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      3\n      16000.0\n      15.31\n      2\n      80000.0\n      24.54\n      1.0\n      12\n      705.0\n      0.0\n      21.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      4\n      14000.0\n      12.12\n      10\n      90000.0\n      14.07\n      0.0\n      14\n      695.0\n      1.0\n      44.0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n5 rows × 131 columns\n\n\n\n\ntest_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nColumns: 131 entries, loan_amnt to application_type_Individual\ndtypes: float64(60), int64(71)\nmemory usage: 20.0 MB\n\n\n\nmodel = joblib.load(\"./notebooks/datasets/_temp/lc_model.pickle\")\n\n\nsegments = [\n    {\n        \"name\": \"Grade\",\n        \"segments\": [\n            {\"name\": \"Grade A\", \"query\": \"grade_A == 1\"},\n            {\"name\": \"Grade B\", \"query\": \"grade_B == 1\"},\n            {\"name\": \"Grade C\", \"query\": \"grade_C == 1\"},\n            {\"name\": \"Grade D\", \"query\": \"grade_D == 1\"},\n            {\"name\": \"Grade E\", \"query\": \"grade_E == 1\"},\n            {\"name\": \"Grade F\", \"query\": \"grade_F == 1\"},\n            {\"name\": \"Grade G\", \"query\": \"grade_G == 1\"},\n        ]\n    },\n    {\n        \"name\": \"Delinquency\",\n        \"segments\": [\n            {\"name\": \"Delinquency: None\", \"query\": \"acc_now_delinq == 0\"},\n            {\"name\": \"Delinquency: 1 Account\", \"query\": \"acc_now_delinq == 1\"},\n            {\"name\": \"Delinquency: 2 Accounts\", \"query\": \"acc_now_delinq == 2\"},\n        ]\n    }\n]\n\n\ndef get_calibrated_pds(df, model, segments):\n    model_preds = model.predict_proba(df)[:, 1]\n    model_class_preds = (model_preds > 0.5).astype(int)\n\n    pds = {\"Model\": model_class_preds.sum() / len(model_class_preds)}\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            total_pds = class_pred.sum()\n            segment_pd = total_pds / len(class_pred)\n\n            pds[segment[\"name\"]] = segment_pd\n    return pds\n\n\ncalibrated_pds = get_calibrated_pds(df, model, segments)\ncalibrated_pds\n\n{'Model': 0.027933333333333334,\n 'Grade A': 0.0022715539494062983,\n 'Grade B': 0.007202947160059383,\n 'Grade C': 0.014655226404459197,\n 'Grade D': 0.043563336766220394,\n 'Grade E': 0.10736266241167085,\n 'Grade F': 0.1781818181818182,\n 'Grade G': 0.24396135265700483,\n 'Delinquency: None': 0.027791712651750778,\n 'Delinquency: 1 Account': 0.06951871657754011,\n 'Delinquency: 2 Accounts': 0.14285714285714285}\n\n\n\ndef process_observations(df, model, segments):\n    test_input = pd.DataFrame(columns = ['Segment', 'Observed'])\n\n    for segment in segments:\n        for segment in segment[\"segments\"]:\n            segment_df = df.query(segment[\"query\"])\n            y_pred = model.predict_proba(segment_df)[:, -1]\n            class_pred = (y_pred > 0.5).astype(int)\n            # Concat to test_input by adding all rows of class_pred and segment as a single value\n            test_input = pd.concat([test_input, pd.DataFrame({'Segment': segment[\"name\"], 'Observed': class_pred})], ignore_index=True)\n\n    return test_input\n\n\nobservations = process_observations(test_df, model, segments)\n\n\nresults = calculate_and_return(\n    observations,\n    cal_pd=calibrated_pds,\n    pool = 'Segment',\n    obs=\n    'Observed',\n    threshold = 0.85\n)\nresults\n\n\n\n\n\n  \n    \n      \n      Segment\n      Defaults\n      Observations\n      Default Rate\n      Calibrated PD\n      P-value\n      Outcome\n    \n  \n  \n    \n      0\n      Model\n      708\n      39999\n      0.02\n      0.027933\n      1.0000\n      Satisfactory\n    \n    \n      1\n      Delinquency: 1 Account\n      3\n      54\n      0.06\n      0.069519\n      0.6307\n      Not Satisfactory\n    \n    \n      2\n      Delinquency: 2 Accounts\n      0\n      6\n      0.00\n      0.142857\n      0.8352\n      Not Satisfactory\n    \n    \n      3\n      Delinquency: None\n      351\n      19939\n      0.02\n      0.027792\n      1.0000\n      Satisfactory\n    \n    \n      4\n      Grade A\n      2\n      3341\n      0.00\n      0.002272\n      0.9904\n      Satisfactory\n    \n    \n      5\n      Grade B\n      13\n      6023\n      0.00\n      0.007203\n      1.0000\n      Satisfactory\n    \n    \n      6\n      Grade C\n      24\n      5318\n      0.00\n      0.014655\n      1.0000\n      Satisfactory\n    \n    \n      7\n      Grade D\n      84\n      3133\n      0.03\n      0.043563\n      1.0000\n      Satisfactory\n    \n    \n      8\n      Grade E\n      120\n      1509\n      0.08\n      0.107363\n      0.9999\n      Satisfactory\n    \n    \n      9\n      Grade F\n      82\n      537\n      0.15\n      0.178182\n      0.9408\n      Satisfactory\n    \n    \n      10\n      Grade G\n      29\n      139\n      0.21\n      0.243961\n      0.8338\n      Not Satisfactory\n    \n  \n\n\n\n\n\n\nSend results to ValidMind\n\n# Test passed only if all values for 'Outcome' are 'Satisfactory'\npassed = results['Outcome'].all() == 'Satisfactory'\npassed\n\nFalse\n\n\n\n# Build a vm.TestResult object for each row in the results dataframe\ntest_results = []\nfor index, row in results.iterrows():\n    test_results.append(vm.TestResult(\n        passed=row['Outcome'] == 'Satisfactory',\n        values={\n            'segment': row['Segment'],\n            'defaults': row['Defaults'],\n            'observations': row['Observations'],\n            'default_rate': row['Default Rate'],\n            'calibrated_pd': row['Calibrated PD'],\n            'p_value': row['P-value']\n        }\n    ))\n\n\njeffreys_params = {\n    \"threshold\": 0.85\n}\n\njeffreys_test_result = vm.TestResults(\n    category=\"model_performance\",\n    test_name=\"jeffreys_test\",\n    params=jeffreys_params,\n    passed=passed,\n    results=test_results,\n)\n\n\nvm.log_test_results([\n    jeffreys_test_result\n])\n\nSuccessfully logged test results for test: jeffreys_test\n\n\nTrue"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-demo.html",
    "href": "notebooks/r_demo/r-ecm-demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "We want to be able to load R models using Python and describe, test and evaluate them using the ValidMind framework like we do with Python models. This notebook demonstrates how we can load R models either from an RDS file or by building the model in R directly in the notebook with the rpy2 package. Either way, we can then use the ValidMind framework to run a TestPlan designed for the model (in this case, a simple ECM model).\n\n# lets import the required libraries\nimport os\nimport tempfile\n\nimport pandas as pd\nimport rpy2.robjects as robjects\nfrom IPython.display import display_png\nfrom PIL import Image as PILImage\nfrom rpy2.robjects.packages import importr\n\n# import the R packages\ntidyverse = importr(\"tidyverse\")\nbroom = importr(\"broom\")\ngraphics = importr(\"graphics\")\ngrdevices = importr(\"grDevices\")\n\n\n# Load the RDS model we created earlier (in r-ecm-model notebook)\n# alternatively, the model could be recreated in rpy2 from scratch\nr_model = robjects.r[\"readRDS\"](\"r-ecm-model.rds\")\n\n\n# lets run summary on the model\n# in pure R, this would be: `summary(model)`\n# for this, however we want to get a string representation of the summary\n# so we can use it in python\nsummary = robjects.r[\"summary\"](r_model)\nsummary_str = str(summary)\nprint(summary_str)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\n\n\n# now lets something similar to run tidy, augment, and glance\n# in pure R, this would be: `tidy(model)`, `augment(model)`, `glance(model)`\n# however, we want to end up with a pandas dataframe containing the data in the Tibble created by these functions\ntidy = robjects.r[\"tidy\"](r_model)\ntidy_df = pd.DataFrame(robjects.conversion.rpy2py(tidy))\n\naugment = robjects.r[\"augment\"](r_model)\naugment_df = pd.DataFrame(robjects.conversion.rpy2py(augment))\n\nglance = robjects.r[\"glance\"](r_model)\nglance_df = pd.DataFrame(robjects.conversion.rpy2py(glance))\n\n# lets display the dataframes\ndisplay(tidy_df)\ndisplay(augment_df)\ndisplay(glance_df)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n    \n  \n  \n    \n      0\n      (Intercept)\n      deltaCorpProfits\n      deltaFedFundsRate\n      deltaUnempRate\n      CorpProfitsLag1\n      FedFundsRateLag1\n      UnempRateLag1\n      yLag1\n    \n    \n      1\n      -0.416021\n      0.011985\n      -0.123162\n      -1.484146\n      0.002708\n      0.065564\n      -0.053275\n      -0.033703\n    \n    \n      2\n      0.823671\n      0.002033\n      0.154749\n      0.43898\n      0.000826\n      0.049471\n      0.104092\n      0.019268\n    \n    \n      3\n      -0.505082\n      5.894868\n      -0.795883\n      -3.380892\n      3.27874\n      1.325304\n      -0.51181\n      -1.74917\n    \n    \n      4\n      0.614155\n      0.0\n      0.42721\n      0.000896\n      0.001265\n      0.186849\n      0.609448\n      0.082066\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      168\n      169\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n    \n  \n  \n    \n      0\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      ...\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n      178\n      179\n    \n    \n      1\n      -0.023333\n      0.0\n      0.126667\n      0.04\n      0.013333\n      0.063333\n      -0.05\n      -0.11\n      0.046667\n      -0.066667\n      ...\n      3.668281\n      4.628594\n      3.689168\n      2.55649\n      3.484343\n      1.330312\n      3.180822\n      2.360396\n      -3.238249\n      0.48375\n    \n    \n      2\n      4.012\n      2.183\n      4.194\n      1.068\n      3.195\n      6.352\n      11.655\n      3.034\n      1.692\n      4.836\n      ...\n      27.453\n      44.735\n      -79.877\n      86.058\n      49.698\n      15.633\n      -122.484\n      64.793\n      -58.055\n      -191.198\n    \n    \n      3\n      0.91\n      -0.723333\n      -1.21\n      0.76\n      0.44\n      0.403333\n      1.393333\n      1.28\n      2.743333\n      -0.563333\n      ...\n      -0.033333\n      0.003333\n      -0.013333\n      0.02\n      -0.003333\n      0.01\n      0.01\n      0.013333\n      0.013333\n      0.023333\n    \n    \n      4\n      0.133333\n      -0.1\n      -0.166667\n      -0.066667\n      -0.133333\n      -0.2\n      -0.433333\n      0.0\n      -0.133333\n      -0.033333\n      ...\n      -0.3\n      -0.3\n      -0.266667\n      -0.433333\n      -0.133333\n      -0.4\n      -0.133333\n      -0.166667\n      -0.3\n      -0.1\n    \n    \n      5\n      59.168\n      63.18\n      65.363\n      69.557\n      70.625\n      73.82\n      80.172\n      91.827\n      94.861\n      96.553\n      ...\n      1658.148\n      1685.601\n      1730.336\n      1650.459\n      1736.517\n      1786.215\n      1801.848\n      1679.364\n      1744.157\n      1686.102\n    \n    \n      6\n      4.563333\n      5.473333\n      4.75\n      3.54\n      4.3\n      4.74\n      5.143333\n      6.536667\n      7.816667\n      10.56\n      ...\n      0.116667\n      0.083333\n      0.086667\n      0.073333\n      0.093333\n      0.09\n      0.1\n      0.11\n      0.123333\n      0.136667\n    \n    \n      7\n      5.9\n      6.033333\n      5.933333\n      5.766667\n      5.7\n      5.566667\n      5.366667\n      4.933333\n      4.933333\n      4.8\n      ...\n      7.533333\n      7.233333\n      6.933333\n      6.666667\n      6.233333\n      6.1\n      5.7\n      5.566667\n      5.4\n      5.1\n    \n    \n      8\n      1.136667\n      1.113333\n      1.113333\n      1.24\n      1.28\n      1.293333\n      1.356667\n      1.306667\n      1.196667\n      1.243333\n      ...\n      68.969531\n      72.637812\n      77.266406\n      80.955574\n      83.512063\n      86.996406\n      88.326719\n      91.507541\n      93.867937\n      90.629688\n    \n    \n      9\n      -0.571137\n      0.018616\n      0.165415\n      -0.326461\n      -0.10769\n      0.07776\n      0.417852\n      -0.166964\n      -0.069546\n      0.416952\n      ...\n      2.133892\n      2.301005\n      0.741401\n      2.646158\n      1.939253\n      1.94912\n      -0.082572\n      1.779979\n      0.611129\n      -1.313894\n    \n    \n      10\n      0.547803\n      -0.018616\n      -0.038748\n      0.366461\n      0.121023\n      -0.014427\n      -0.467852\n      0.056964\n      0.116212\n      -0.483619\n      ...\n      1.534389\n      2.327588\n      2.947767\n      -0.089668\n      1.54509\n      -0.618807\n      3.263394\n      0.580416\n      -3.849378\n      1.797644\n    \n    \n      11\n      0.031876\n      0.026309\n      0.043736\n      0.032399\n      0.025897\n      0.024937\n      0.033407\n      0.029378\n      0.063377\n      0.031915\n      ...\n      0.035826\n      0.039586\n      0.050582\n      0.068121\n      0.053507\n      0.061846\n      0.075844\n      0.088754\n      0.087292\n      0.120518\n    \n    \n      12\n      1.715996\n      1.71653\n      1.716528\n      1.716291\n      1.716505\n      1.71653\n      1.71614\n      1.716525\n      1.716506\n      1.716114\n      ...\n      1.712317\n      1.70678\n      1.700683\n      1.716516\n      1.712178\n      1.715827\n      1.696552\n      1.715893\n      1.688317\n      1.710186\n    \n    \n      13\n      0.000436\n      0.0\n      0.000003\n      0.000198\n      0.000017\n      0.0\n      0.000334\n      0.000004\n      0.000042\n      0.00034\n      ...\n      0.003872\n      0.009922\n      0.020808\n      0.000027\n      0.006085\n      0.001148\n      0.040359\n      0.001537\n      0.066262\n      0.021487\n    \n    \n      14\n      0.325303\n      -0.011023\n      -0.023152\n      0.217675\n      0.071646\n      -0.008537\n      -0.278046\n      0.033784\n      0.070162\n      -0.287194\n      ...\n      0.913035\n      1.387735\n      1.76764\n      -0.054274\n      0.92795\n      -0.373291\n      1.983474\n      0.355264\n      -2.354259\n      1.120004\n    \n  \n\n15 rows × 178 columns\n\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      2.919125e-01\n    \n    \n      1\n      2.627560e-01\n    \n    \n      2\n      1.711475e+00\n    \n    \n      3\n      1.001190e+01\n    \n    \n      4\n      1.885342e-10\n    \n    \n      5\n      7.000000e+00\n    \n    \n      6\n      -3.441276e+02\n    \n    \n      7\n      7.062553e+02\n    \n    \n      8\n      7.348913e+02\n    \n    \n      9\n      4.979547e+02\n    \n    \n      10\n      1.700000e+02\n    \n    \n      11\n      1.780000e+02\n    \n  \n\n\n\n\n\n# finally, lets plot the model and somehow get the plots into python\n# in pure R, this would be: `plot(model)`\n# for this, however we want to get a png image of the plots\n\n# first of all, lets get a temporary file path that we can use to save the image\ntemp_file = tempfile.NamedTemporaryFile(suffix=\".png\")\n\n# now lets save the image to the temporary file using grDevices package\ngrdevices.png(temp_file.name, width=1200, height=800)\ngraphics.par(mfrow=robjects.IntVector([2, 2]))\nrobjects.r[\"plot\"](r_model) # creates 4 plots that will be combined into one image\ngrdevices.dev_off()\n\n# now we split the image into the 4 plots\nimage = PILImage.open(temp_file.name)\nwidth, height = image.size\nplot_width = width / 2\nplot_height = height / 2\nplots = [\n    image.crop((0, 0, plot_width, plot_height)),\n    image.crop((plot_width, 0, width, plot_height)),\n    image.crop((0, plot_height, plot_width, height)),\n    image.crop((plot_width, plot_height, width, height))\n]\n\n# display the plots\nfor plot in plots:\n    display_png(plot)\n\n# and finally, lets delete the temporary file\nos.remove(temp_file.name)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html",
    "href": "notebooks/r_demo/r-ecm-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "Install R with Homebrew on macOS:\nbrew install r"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "href": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "title": "ValidMind",
    "section": "Step 1: Load the Required Libraries",
    "text": "Step 1: Load the Required Libraries\nWe will start by loading the necessary libraries that we will use in this notebook. Here, we will use the ecm package to build the ECM model.\n\nlibrary(ecm)\nlibrary(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "href": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "title": "ValidMind",
    "section": "Step 2: Load the Data",
    "text": "Step 2: Load the Data\nNext, we will load the data that we will use to build the ECM model. For this example, we will use ecm to predict Wilshire 5000 index based on corporate profits, Federal Reserve funds rate, and unemployment rate\n\n# Load the data\ndata(Wilshire)\n# Use 2015-12-01 and earlier data to build models\ntrn <- Wilshire[Wilshire$date<='2015-12-01',]"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 3: Build the ECM Model",
    "text": "Step 3: Build the ECM Model\nNow, we will build the ECM model using the ecm package.\n\n# Assume all predictors are needed in the equilibrium and transient terms of ecm.\nxeq <- xtr <- trn[c('CorpProfits', 'FedFundsRate', 'UnempRate')]\nmodel <- ecm(trn$Wilshire5000, xeq, xtr, includeIntercept=TRUE)\n\nsummary(model)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\nsummary(model)$coefficients\n\n\n\nA matrix: 8 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits 0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate-1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1 0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1 0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1-0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1-0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\ntidy(model)\n\n\n\nA tibble: 8 × 5\n\n    termestimatestd.errorstatisticp.value\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    (Intercept)      -0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits  0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate   -1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1   0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1  0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1    -0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1            -0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\naugment(model)\n\n\n\nA tibble: 178 × 15\n\n    .rownamesdydeltaCorpProfitsdeltaFedFundsRatedeltaUnempRateCorpProfitsLag1FedFundsRateLag1UnempRateLag1yLag1.fitted.resid.hat.sigma.cooksd.std.resid\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    2 -0.023333333  4.012 0.91000000 0.13333333 59.168 4.5633335.9000001.1366667-0.57113659 0.5478032600.031876201.7159964.355351e-04 0.325303481\n    3  0.000000000  2.183-0.72333333-0.10000000 63.180 5.4733336.0333331.1133333 0.01861637-0.0186163730.026308501.7165304.104050e-07-0.011023359\n    4  0.126666667  4.194-1.21000000-0.16666667 65.363 4.7500005.9333331.1133333 0.16541470-0.0387480300.043736251.7165283.064464e-06-0.023152097\n    5  0.040000000  1.068 0.76000000-0.06666667 69.557 3.5400005.7666671.2400000-0.32646091 0.3664609080.032399111.7162911.983193e-04 0.217675258\n    6  0.013333333  3.195 0.44000000-0.13333333 70.625 4.3000005.7000001.2800000-0.10768956 0.1210228920.025897371.7165051.705888e-05 0.071646457\n    7  0.063333333  6.352 0.40333333-0.20000000 73.820 4.7400005.5666671.2933333 0.07776005-0.0144267170.024936681.7165302.329577e-07-0.008536516\n    8 -0.050000000 11.655 1.39333333-0.43333333 80.172 5.1433335.3666671.3566667 0.41785218-0.4678521810.033407001.7161403.339924e-04-0.278045827\n    9 -0.110000000  3.034 1.28000000 0.00000000 91.827 6.5366674.9333331.3066667-0.16696419 0.0569641920.029377801.7165254.318091e-06 0.033783635\n    10 0.046666667  1.692 2.74333333-0.13333333 94.861 7.8166674.9333331.1966667-0.06954559 0.1162122590.063376581.7165064.163627e-05 0.070161519\n    11-0.066666667  4.836-0.56333333-0.03333333 96.55310.5600004.8000001.2433333 0.41695185-0.4836185150.031915461.7161143.398987e-04-0.287194305\n    12-0.046666667  6.127-0.67333333 0.36666667101.389 9.9966674.7666671.1766667-0.16750282 0.1208361570.028032821.7165051.848956e-05 0.071614448\n    13-0.110000000  2.535 1.92666667 0.06666667107.516 9.3233335.1333331.1300000-0.13104919 0.0210491890.043999891.7165309.102812e-07 0.012578705\n    14-0.186666667  5.197 0.84000000 0.43333333110.05111.2500005.2000001.0200000-0.37615628 0.1894896130.039931241.7164666.638164e-05 0.112996186\n    15 0.003333333-13.096-2.74333333 0.96666667115.24812.0900005.6333330.8333333-0.89326786 0.8966011900.077122691.7150293.106433e-03 0.545326840\n    16 0.140000000-12.925-3.04333333 1.66666667102.152 9.3466676.6000000.8366667-2.16010566 2.3001056600.149084931.7057814.648641e-02 1.456915037\n    17 0.143333333  4.811-0.88333333 0.60000000 89.227 6.3033338.2666670.9766667-0.95851326 1.1018465940.045981031.7143362.617436e-03 0.659131663\n    18-0.043333333 16.609 0.74000000-0.40000000 94.038 5.4200008.8666671.1200000 0.38541881-0.4287521470.039334411.7162013.343566e-04-0.255593460\n    19 0.040000000  7.948-0.74666667-0.16666667110.647 6.1600008.4666671.0766667 0.23467281-0.1946728100.029288701.7164635.026893e-05-0.115448901\n    20 0.160000000  8.331-0.58666667-0.56666667118.595 5.4133338.3000001.1166667 0.79331290-0.6333128990.047063801.7158058.870848e-04-0.379066991\n    21 0.023333333  3.825 0.37000000-0.16666667126.926 4.8266677.7333331.2766667 0.03671404-0.0133807080.022620991.7165301.809310e-07-0.007908191\n    22 0.036666667  2.177 0.08666667 0.16666667130.751 5.1966677.5666671.3000000-0.40014906 0.4368157300.023399561.7161941.997742e-04 0.258267261\n    23 0.026666667  0.171-0.41000000 0.03333333132.928 5.2833337.7333331.3366667-0.16367336 0.1903400260.021229221.7164673.426111e-05 0.112413682\n    24-0.010000000 10.848-0.21333333-0.26666667133.099 4.8733337.7666671.3633333 0.35622331-0.3662233060.024874771.7162941.497267e-04-0.216693223\n    25 0.006666667  8.840 0.49666667-0.36666667143.947 4.6600007.5000001.3533333 0.42305580-0.4163891310.024524301.7162241.906916e-04-0.246331926\n    26 0.006666667  6.448 0.66333333-0.23333333152.787 5.1566677.1333331.3600000 0.25178222-0.2451155550.019335271.7164255.154897e-05-0.144623924\n    27-0.013333333  1.004 0.69333333-0.23333333159.235 5.8200006.9000001.3666667 0.25599483-0.2693281660.017280481.7164035.538973e-05-0.158743710\n    28-0.040000000  5.396 0.24333333-0.33333333160.239 6.5133336.6666671.3533333 0.57352801-0.6135280130.018415451.7158703.070187e-04-0.361826262\n    29 0.153333333 18.499 0.52666667-0.33333333165.635 6.7566676.3333331.3133333 0.74534680-0.5920134700.018631561.7159152.893460e-04-0.349176568\n    30 0.126666667  6.325 0.81666667 0.03333333184.134 7.2833336.0000001.4666667 0.11674323 0.0099234380.017495501.7165307.616416e-08 0.005849577\n    31-0.116666667 11.520 1.48333333-0.13333333190.459 8.1000006.0333331.5933333 0.40888139-0.5255480600.025206401.7160423.126651e-04-0.311018089\n    ⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n    150 -3.93984375 -40.796-0.146666667 0.666666671229.6192.086666675.33333351.19484-0.41969615-3.520147600.049995301.6939002.929332e-02-2.11021907\n    151-13.21515625-517.379-1.433333333 0.866666671188.8231.940000006.00000047.25500-6.29287534-6.922280910.450834881.5589003.056885e+00-5.45792159\n    152 -3.41295850 372.812-0.323333333 1.40000000 671.4440.506666676.86666734.03984 2.35246941-5.765427910.316976911.6304949.638047e-01-4.07608658\n    153  3.81120999  76.362-0.003333333 1.033333331044.2560.183333338.26666730.62689 0.33287856 3.478331430.119783231.6926747.982295e-02 2.16623560\n    154  4.38612351 152.571-0.023333333 0.333333331120.6180.180000009.30000034.43810 2.31067811 2.075445400.070595941.7085241.502320e-02 1.25787816\n    155  3.78281250 101.178-0.036666667 0.300000001273.1890.156666679.63333338.82422 1.99182565 1.790986850.063379231.7106189.889477e-03 1.08128468\n    156  1.98280482  75.820 0.013333333-0.100000001374.3670.120000009.93333342.60703 2.40347852-0.420673710.052818351.7162094.446089e-04-0.25255634\n    157  0.99476711 -13.432 0.060000000-0.200000001450.1870.133333339.83333344.58984 1.62109998-0.626332870.054187501.7158161.014071e-03-0.37629827\n    158 -1.36429067  62.316-0.006666667-0.166666671436.7550.193333339.63333345.58460 2.43239779-3.796688470.048396961.6902203.287650e-02-2.27408451\n    159  5.05359375  -4.611 0.000000000 0.033333331499.0710.186666679.46666744.22031 1.55575748 3.497836270.056815211.6940253.334564e-02 2.10441132\n    160  4.37222278-144.215-0.030000000-0.466666671494.4600.186666679.50000049.27391 0.44374298 3.928479800.090026541.6870467.160301e-02 2.40624425\n    161  0.91371224  72.900-0.063333333 0.033333331350.2450.156666679.03333353.64613 1.79302659-0.879314350.043377471.7151371.564013e-03-0.52529517\n    162 -3.86734127   7.241-0.010000000-0.066666671423.1450.093333339.06666754.55984 1.30859223-5.175933500.039141481.6677824.846909e-02-3.08523686\n    163 -0.01948413  76.687-0.010000000-0.366666671430.3860.083333339.00000050.69250 2.73901212-2.758496250.042936741.7027721.522165e-02-1.64752351\n    164  5.57569380 220.314 0.030000000-0.366666671507.0730.073333338.63333350.67302 4.68268067 0.893013130.098256501.7150064.112255e-03 0.54947240\n    165  0.03144905 -72.595 0.050000000-0.066666671727.3870.103333338.26666756.24871 1.15447874-1.123029690.061144801.7142143.733480e-03-0.67720706\n    166  2.23761905  31.842-0.010000000-0.166666671654.7920.153333338.20000056.28016 2.37120363-0.133584590.041488471.7164993.438860e-05-0.07972366\n    167  1.14496416 -22.685 0.016666667-0.233333331686.6340.143333338.03333358.51778 1.83236442-0.687400260.042991361.7156799.465378e-04-0.41056442\n    168  4.96225806  15.537-0.016666667-0.066666671663.9490.160000007.80000059.66274 1.96072774 3.001530320.035880771.7003481.484071e-02 1.78610468\n    169  4.34453125 -21.338-0.026666667-0.200000001679.4860.143333337.73333364.62500 1.59517181 2.749359440.035211341.7029731.220254e-02 1.63547900\n    170  3.66828125  27.453-0.033333333-0.300000001658.1480.116666677.53333368.96953 2.13389190 1.534389350.035826041.7123173.871937e-03 0.91303500\n    171  4.62859375  44.735 0.003333333-0.300000001685.6010.083333337.23333372.63781 2.30100541 2.327588340.039586121.7067809.922189e-03 1.38773488\n    172  3.68916752 -79.877-0.013333333-0.266666671730.3360.086666676.93333377.26641 0.74140097 2.947766550.050581811.7006832.080820e-02 1.76764015\n    173  2.55648972  86.058 0.020000000-0.433333331650.4590.073333336.66666780.95557 2.64615821-0.089668480.068120881.7165162.691596e-05-0.05427372\n    174  3.48434276  49.698-0.003333333-0.133333331736.5170.093333336.23333383.51206 1.93925279 1.545089970.053506951.7121786.084879e-03 0.92795006\n    175  1.33031250  15.633 0.010000000-0.400000001786.2150.090000006.10000086.99641 1.94911984-0.618807340.061846411.7158271.148277e-03-0.37329144\n    176  3.18082223-122.484 0.010000000-0.133333331801.8480.100000005.70000088.32672-0.08257207 3.263394300.075844431.6965524.035912e-02 1.98347420\n    177  2.36039552  64.793 0.013333333-0.166666671679.3640.110000005.56666791.50754 1.77997920 0.580416330.088753711.7158931.536609e-03 0.35526406\n    178 -3.23824901 -58.055 0.013333333-0.300000001744.1570.123333335.40000093.86794 0.61112886-3.849377870.087292381.6883176.626180e-02-2.35425908\n    179  0.48375000-191.198 0.023333333-0.100000001686.1020.136666675.10000090.62969-1.31389361 1.797643610.120518221.7101862.148698e-02 1.12000437\n\n\n\n\n\nglance(model)\n\n\n\nA tibble: 1 × 12\n\n    r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobs\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><int><int>\n\n\n    0.29191250.2627561.71147510.01191.885342e-107-344.1276706.2553734.8913497.9547170178\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(model)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 4: Save the ECM Model",
    "text": "Step 4: Save the ECM Model\nFinally, we will save the ECM model to a file so that we can use it later.\n\n# save the model to an RDS file\nsaveRDS(model, 'r-ecm-model.rds')"
  },
  {
    "objectID": "notebooks/r_demo/rpy2.html",
    "href": "notebooks/r_demo/rpy2.html",
    "title": "ValidMind",
    "section": "",
    "text": "from rpy2.robjects.packages import importr\n\n\nbase = importr('base')\n\n\nfrom rpy2.robjects.packages import importr\n\ntidyr = importr('tidyr')\nggplot2 = importr('ggplot2')\npurrr = importr('purrr')\nprintr = importr('printr')\npROC = importr('pROC') \nROCR = importr('ROCR') \ncaret = importr('caret')\ncar = importr('car')\nrpart = importr('rpart')\nrpart_plot = importr('rpart.plot')\n\n\nfrom rpy2.robjects import r\n\ndata = r['read.csv']('./datasets/bank_customer_churn.csv', stringsAsFactors = True)\n\n\nr['str'](data)\n\n'data.frame':   8000 obs. of  14 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : Factor w/ 2616 levels \"Abazu\",\"Abbie\",..: 1002 1060 1832 258 1634 485 156 1793 1032 970 ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : Factor w/ 3 levels \"France\",\"Germany\",..: 1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n<rpy2.rinterface_lib.sexp.NULLType object at 0x103d3c740> [RTYPES.NILSXP]\n\n\n\nr('''\n    knitr::kable(sapply(data, function(x) sum(is.na(x))), col.names = c(\"Missing Value Count\"))\n''')\n\n\n\n        StrVector with 10 elements.\n        \n        \n          \n          \n            \n            '|       ...\n            \n          \n            \n            '|:------...\n            \n          \n            \n            '|...    ...\n            \n          \n            \n            ...\n            \n          \n            \n            '|envir  ...\n            \n          \n            \n            '|overwri...\n            \n          \n            \n            '|       ...\n            \n          \n          \n        \n        \n        \n\n\n\nr(\"\"\"\n    # plot box plot\n    data[, names(data) %in% c('Age', 'Balance', 'CreditScore', 'EstimatedSalary')] %>%\n    gather() %>%\n    ggplot(aes(value)) +\n        facet_wrap(~ key, scales = \"free\") +\n        geom_boxplot() +\n        theme(axis.text.x = element_text(size = 7, angle=90), axis.text.y = element_text(size = 7))\n\"\"\")\n\nR[write to console]: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable\n\n\n\nRRuntimeError: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable"
  },
  {
    "objectID": "notebooks/r_demo/r-customer-churn-model.html",
    "href": "notebooks/r_demo/r-customer-churn-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebook demonstrates the process of creating a customer churn model using R. We will use the XGBoost and logistic regression algorithms to create two separate models and compare their performance.\nThe dataset used in this notebook is located at ../datasets/bank_customer_churn.csv.\n\n\n\n\nbrew install r\nYou additionally might need the following packages to run this notebook if you run into errors:\nbrew install harfbuzz fribidi libtiff libomp\n\n\n\ninstall.packages(\"xgboost\")\ninstall.packages(\"caret\")\ninstall.packages(\"pROC\")\n\n\n\n\nFirst, we load the required libraries.\n\n# load the required libraries\nlibrary(xgboost)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC)\n\n\n\n\nNow, we import the dataset and preprocess it by removing irrelevant columns, converting categorical variables, and one-hot encoding certain columns.\n\n# import the dataset\ndf <- read.csv(\"../datasets/bank_customer_churn.csv\", header = TRUE)\n\n# remove irrelevant columns\ndf <- df %>% select(-c(RowNumber, CustomerId, Surname, CreditScore))\n\n# Convert the 'Gender' column to 0 or 1 (assuming \"Female\" should be 0 and \"Male\" should be 1)\ndf$Gender <- ifelse(df$Gender == \"Female\", 0, 1)\n\n# one-hot encode categorical columns with caret\ndf <- dummyVars(\" ~ .\", data = df) %>% predict(df)\n\n# remove GeographySpain since it causes multicollinearity\ndf <- subset(df, select = -GeographySpain)\n\nsummary(df)\n\n GeographyFrance  GeographyGermany     Gender            Age       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :18.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:32.00  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :37.00  \n Mean   :0.5012   Mean   :0.2511   Mean   :0.5495   Mean   :38.95  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:44.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :92.00  \n     Tenure          Balance       NumOfProducts     HasCrCard     \n Min.   : 0.000   Min.   :     0   Min.   :1.000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:     0   1st Qu.:1.000   1st Qu.:0.0000  \n Median : 5.000   Median : 97264   Median :1.000   Median :1.0000  \n Mean   : 5.034   Mean   : 76434   Mean   :1.532   Mean   :0.7026  \n 3rd Qu.: 8.000   3rd Qu.:128045   3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :250898   Max.   :4.000   Max.   :1.0000  \n IsActiveMember   EstimatedSalary         Exited     \n Min.   :0.0000   Min.   :    11.58   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.: 50857.10   1st Qu.:0.000  \n Median :1.0000   Median : 99504.89   Median :0.000  \n Mean   :0.5199   Mean   : 99790.19   Mean   :0.202  \n 3rd Qu.:1.0000   3rd Qu.:149216.32   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :199992.48   Max.   :1.000  \n\n\n\n\n\nNext, we split the data into training and testing sets using a 70/30 ratio.\n\n# split data into training and testing sets\nset.seed(123)\ntrain_index <- sample(1:nrow(df), size = round(0.7*nrow(df)), replace = FALSE)\ndf_train <- df[train_index, ]\ndf_test <- df[-train_index, ]\n\n\n\n\nWe save the train and test datasets as CSV files.\n\n# save the train and test datasets as csv files\nwrite.csv(df_train, file = \"r_churn_train.csv\", row.names = FALSE)\nwrite.csv(df_test, file = \"r_churn_test.csv\", row.names = FALSE)\n\n\n\n\nWe convert the data into DMatrix format, which is required by the XGBoost library.\n\n# convert data into DMatrix format\ndtrain <- xgb.DMatrix(data = df_train[,-c(11)], label = df_train[,\"Exited\"])\ndtest <- xgb.DMatrix(data = df_test[,-c(11)], label = df_test[,\"Exited\"])\n\n\n\n\nWe set up the XGBoost parameters to be used during the training process.\n\n# set up XGBoost parameters\nparams <- list(\n  objective = \"binary:logistic\",\n  eval_metric = \"auc\",\n  max_depth = 3,\n  eta = 0.1,\n  gamma = 0.5,\n  subsample = 0.8,\n  colsample_bytree = 0.8,\n  min_child_weight = 1,\n  nthread = 4\n)\n\n\n\n\nWe train the XGBoost model using the parameters and data prepared earlier.\n\n# train the XGBoost model\nmodel <- xgb.train(\n  params = params,\n  data = dtrain,\n  nrounds = 100,\n  watchlist = list(train = dtrain, test = dtest),\n  early_stopping_rounds = 10\n)\n\n[1] train-auc:0.795105  test-auc:0.793822 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 10 rounds.\n\n[2] train-auc:0.820697  test-auc:0.808123 \n[3] train-auc:0.823965  test-auc:0.811294 \n[4] train-auc:0.837212  test-auc:0.823692 \n[5] train-auc:0.839206  test-auc:0.827146 \n[6] train-auc:0.843781  test-auc:0.832219 \n[7] train-auc:0.853531  test-auc:0.836494 \n[8] train-auc:0.857080  test-auc:0.838679 \n[9] train-auc:0.857191  test-auc:0.839732 \n[10]    train-auc:0.856166  test-auc:0.840575 \n[11]    train-auc:0.857386  test-auc:0.841168 \n[12]    train-auc:0.857084  test-auc:0.841385 \n[13]    train-auc:0.856794  test-auc:0.842336 \n[14]    train-auc:0.857827  test-auc:0.841208 \n[15]    train-auc:0.858503  test-auc:0.842312 \n[16]    train-auc:0.860074  test-auc:0.843007 \n[17]    train-auc:0.858916  test-auc:0.843317 \n[18]    train-auc:0.858676  test-auc:0.843113 \n[19]    train-auc:0.858821  test-auc:0.843309 \n[20]    train-auc:0.859816  test-auc:0.845118 \n[21]    train-auc:0.860960  test-auc:0.845355 \n[22]    train-auc:0.860677  test-auc:0.845800 \n[23]    train-auc:0.862110  test-auc:0.848165 \n[24]    train-auc:0.863008  test-auc:0.847703 \n[25]    train-auc:0.863292  test-auc:0.848459 \n[26]    train-auc:0.864104  test-auc:0.849124 \n[27]    train-auc:0.863918  test-auc:0.848931 \n[28]    train-auc:0.864840  test-auc:0.849988 \n[29]    train-auc:0.867052  test-auc:0.850861 \n[30]    train-auc:0.867307  test-auc:0.851330 \n[31]    train-auc:0.867691  test-auc:0.851321 \n[32]    train-auc:0.868384  test-auc:0.852436 \n[33]    train-auc:0.870037  test-auc:0.854785 \n[34]    train-auc:0.870912  test-auc:0.855056 \n[35]    train-auc:0.871229  test-auc:0.855566 \n[36]    train-auc:0.871868  test-auc:0.855823 \n[37]    train-auc:0.872831  test-auc:0.857390 \n[38]    train-auc:0.873618  test-auc:0.856942 \n[39]    train-auc:0.874207  test-auc:0.858250 \n[40]    train-auc:0.874417  test-auc:0.858442 \n[41]    train-auc:0.874423  test-auc:0.858399 \n[42]    train-auc:0.875082  test-auc:0.858565 \n[43]    train-auc:0.876418  test-auc:0.858693 \n[44]    train-auc:0.876752  test-auc:0.858042 \n[45]    train-auc:0.877068  test-auc:0.857847 \n[46]    train-auc:0.877774  test-auc:0.857960 \n[47]    train-auc:0.879001  test-auc:0.858793 \n[48]    train-auc:0.879490  test-auc:0.858593 \n[49]    train-auc:0.880067  test-auc:0.859837 \n[50]    train-auc:0.880651  test-auc:0.860296 \n[51]    train-auc:0.880768  test-auc:0.860439 \n[52]    train-auc:0.881091  test-auc:0.861333 \n[53]    train-auc:0.881637  test-auc:0.861322 \n[54]    train-auc:0.881942  test-auc:0.861473 \n[55]    train-auc:0.882016  test-auc:0.861432 \n[56]    train-auc:0.882721  test-auc:0.861169 \n[57]    train-auc:0.882949  test-auc:0.861621 \n[58]    train-auc:0.883306  test-auc:0.861810 \n[59]    train-auc:0.883506  test-auc:0.861506 \n[60]    train-auc:0.883632  test-auc:0.861516 \n[61]    train-auc:0.883968  test-auc:0.861404 \n[62]    train-auc:0.884255  test-auc:0.861261 \n[63]    train-auc:0.884719  test-auc:0.861073 \n[64]    train-auc:0.885060  test-auc:0.861043 \n[65]    train-auc:0.885252  test-auc:0.861357 \n[66]    train-auc:0.885285  test-auc:0.861367 \n[67]    train-auc:0.885594  test-auc:0.860972 \n[68]    train-auc:0.886119  test-auc:0.860755 \nStopping. Best iteration:\n[58]    train-auc:0.883306  test-auc:0.861810\n\n\n\n\n\n\nWe display a summary of the trained XGBoost model.\n\nsummary(model)\n\n                Length Class              Mode       \nhandle              1  xgb.Booster.handle externalptr\nraw             81624  -none-             raw        \nbest_iteration      1  -none-             numeric    \nbest_ntreelimit     1  -none-             numeric    \nbest_score          1  -none-             numeric    \nbest_msg            1  -none-             character  \nniter               1  -none-             numeric    \nevaluation_log      3  data.table         list       \ncall                6  -none-             call       \nparams             10  -none-             list       \ncallbacks           3  -none-             list       \nfeature_names      10  -none-             character  \nnfeatures           1  -none-             numeric    \n\n\n\n\n\nWe make predictions on the test data and calculate the accuracy of the model.\n\n# predict on test data\ntest_preds <- predict(model, dtest)\n\n# Convert predicted probabilities to binary predictions\ntest_preds_binary <- ifelse(test_preds > 0.5, 1, 0)\n\n# Calculate accuracy on test set\naccuracy <- sum(test_preds_binary == df_test[,\"Exited\"])/nrow(df_test)\naccuracy\n\n0.860416666666667\n\n\n\n\n\nWe calculate the confusion matrix, precision, recall, F1 score, and ROC AUC for the model.\n\n# Calculate the confusion matrix\ncm <- confusionMatrix(as.factor(test_preds_binary), as.factor(df_test[,\"Exited\"]))\n\n# Calculate precision, recall, and F1 score\nprecision <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[2, 1])\nrecall <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[1, 2])\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\ncat(\"Precision:\", precision, \"\\n\")\ncat(\"Recall:\", recall, \"\\n\")\ncat(\"F1 Score:\", f1_score, \"\\n\")\n\n# Calculate ROC AUC\nroc_obj <- roc(df_test[,\"Exited\"], test_preds)\nroc_auc <- auc(roc_obj)\ncat(\"ROC AUC:\", roc_auc, \"\\n\")\n\nPrecision: 0.7687075 \nRecall: 0.4584178 \nF1 Score: 0.5743329 \n\n\nSetting levels: control = 0, case = 1\n\nSetting direction: controls < cases\n\n\n\nROC AUC: 0.86181 \n\n\n\n\n\nWe save the trained XGBoost model as a JSON file.\n\n# save the model (notice the .json extension, we could also save it as .bin)\n# this ensures compatibility with the ValidMind sdk\nxgb.save(model, \"r_xgb_churn_model.json\")\n\nTRUE\n\n\n\n\n\nAs a comparison, we train a simple logistic regression model using the training data.\n\n# now lets train a simple logistic regression model\nlg_reg_model <- glm(Exited ~ ., data = as.data.frame(df_train), family = \"binomial\")\n\n\n\n\nWe display a summary of the trained logistic regression model.\n\nsummary(lg_reg_model)\n\n\nCall:\nglm(formula = Exited ~ ., family = \"binomial\", data = as.data.frame(df_train))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3284  -0.6470  -0.4563  -0.2781   2.8954  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      -3.732e+00  2.305e-01 -16.187  < 2e-16 ***\nGeographyFrance  -1.113e-01  9.474e-02  -1.174 0.240252    \nGeographyGermany  7.394e-01  1.054e-01   7.018 2.25e-12 ***\nGender           -4.974e-01  7.319e-02  -6.796 1.07e-11 ***\nAge               7.142e-02  3.433e-03  20.803  < 2e-16 ***\nTenure           -1.170e-02  1.263e-02  -0.926 0.354301    \nBalance           2.525e-06  6.995e-07   3.610 0.000306 ***\nNumOfProducts    -1.300e-01  6.475e-02  -2.008 0.044643 *  \nHasCrCard        -1.468e-02  8.025e-02  -0.183 0.854836    \nIsActiveMember   -9.979e-01  7.682e-02 -12.989  < 2e-16 ***\nEstimatedSalary   2.248e-07  6.337e-07   0.355 0.722854    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5612.8  on 5599  degrees of freedom\nResidual deviance: 4760.6  on 5589  degrees of freedom\nAIC: 4782.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\ncoef(lg_reg_model)\n\n(Intercept)-3.73163074444561GeographyFrance-0.111254692545161GeographyGermany0.73938562185064Gender-0.497380970370031Age0.0714194308317766Tenure-0.0116956480620419Balance2.52544411990314e-06NumOfProducts-0.130015233022276HasCrCard-0.0146825015464598IsActiveMember-0.997861450907317EstimatedSalary2.24751413745725e-07\n\n\n\n\n\nWe make predictions on the test data and calculate the accuracy of the logistic regression model.\n\n# Make predictions on test set\ntest_preds <- predict(lg_reg_model, newdata = as.data.frame(df_test), type = \"response\")\n\n# Convert predicted probabilities to binary predictions\ntest_preds_binary <- ifelse(test_preds > 0.5, 1, 0)\n\n# Calculate accuracy on test set\naccuracy <- sum(test_preds_binary == df_test[,\"Exited\"])/nrow(df_test)\naccuracy\n\n0.805416666666667\n\n\n\n\n\nWe calculate the confusion matrix, precision, recall, F1 score, and ROC AUC for the logistic regression model.\n\n# Calculate the confusion matrix\ncm <- confusionMatrix(as.factor(test_preds_binary), as.factor(df_test[,\"Exited\"]))\n\n# Calculate precision, recall, and F1 score\nprecision <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[2, 1])\nrecall <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[1, 2])\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\ncat(\"Precision:\", precision, \"\\n\")\ncat(\"Recall:\", recall, \"\\n\")\ncat(\"F1 Score:\", f1_score, \"\\n\")\n\n# Calculate ROC AUC\nroc_obj <- roc(df_test[,\"Exited\"], test_preds)\nroc_auc <- auc(roc_obj)\ncat(\"ROC AUC:\", roc_auc, \"\\n\")\n\nPrecision: 0.5890411 \nRecall: 0.1744422 \nF1 Score: 0.2691706 \n\n\nSetting levels: control = 0, case = 1\n\nSetting direction: controls < cases\n\n\n\nROC AUC: 0.7616043 \n\n\n\n\n\nWe save the trained logistic regression model as an RDS file.\n\n# save the model\nsaveRDS(lg_reg_model, \"r_log_reg_churn_model.rds\")"
  },
  {
    "objectID": "notebooks/r_demo/r-python.html",
    "href": "notebooks/r_demo/r-python.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nfrom pypmml import Model\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = Model.fromFile('./prune_dt.pmml')\n\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel.predict({\n    \"CreditScore\": 0.64,\n    \"Geography\": 0,\n    \"Gender\": 0,\n    \"Age\": 0.51936320,\n    \"Tenure\": 2,\n    \"Balance\": 0.9118043,\n    \"NumOfProducts\": 1,\n    \"HasCrCard\": 1,\n    \"IsActiveMember\": 1,\n    \"EstimatedSalary\": 0.506734893\n})\n\n\nmodel.inputNames"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_models_comparison_test_plan.html",
    "href": "notebooks/time_series/loan_rates_forecast_models_comparison_test_plan.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\n\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"../..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgmfxwvp0000k8rlc997oe9t\")\n\nTrue\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = './notebooks/datasets/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"MORTGAGE30US\"\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                        Description                                            \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics    Test plan for sklearn classifier metrics               \nsklearn_classifier_validation     SKLearnClassifierPerformanceTest plan for sklearn classifier models                \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests \nsklearn_classifier                SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                        \ntabular_dataset                   TabularDataset              Test plan for generic tabular datasets                 \ntabular_dataset_description       TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                        \ntabular_data_quality              TabularDataQuality          Test plan for data quality on tabular datasets         \nnormality_test_plan               NormalityTestPlan           Test plan to perform normality tests.                  \nautocorrelation_test_plan         AutocorrelationTestPlan     Test plan to perform autocorrelation tests.            \nseasonality_test_plan             SesonalityTestPlan          Test plan to perform seasonality tests.                \nunit_root                         UnitRoot                    Test plan to perform unit root tests.                  \nstationarity_test_plan            StationarityTestPlan        Test plan to perform stationarity tests.               \ntimeseries                        TimeSeries                  Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                        \ntime_series_data_quality          TimeSeriesDataQuality       Test plan for data quality on time series datasets     \ntime_series_dataset               TimeSeriesDataset           Test plan for time series  datasets                    \ntime_series_univariate            TimeSeriesUnivariate        Test plan to perform time series univariate analysis.  \ntime_series_multivariate          TimeSeriesMultivariate      Test plan to perform time series multivariate analysis.\nregression_model_performance      RegressionModelPerformance  Test plan for statsmodels regressor models that includes\n    both metrics and validation tests                                                        \n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\n\n\nAttribute       Value                                                                                                           \n\n\nID              time_series_data_quality                                                                                        \nName            TimeSeriesDataQuality                                                                                           \nDescription     Test plan for data quality on time series datasets                                                              \nRequired Context['dataset']                                                                                                     \nTests           TimeSeriesOutliers (ThresholdTest), TimeSeriesMissingValues (ThresholdTest), TimeSeriesFrequency (ThresholdTest)\nTest Plans      []                                                                                                              \n\n\n\n\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3.5,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\nplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\n                                                                                                                                       \n\n\nTest plan for data quality on time series datasets\n        \n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3.5}\n                \n            \n            \n                Results\n                [TestResult(test_name='outliers', column=None, passed=False, values={'Variable': ['FEDFUNDS'], 'z-score': [3.707037578539338], 'Threshold': [3.5], 'Date': ['1981-05-01']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='MORTGAGE30US', passed=False, values={'n_missing': 833, 'p_missing': 0.23458180794142494}), TestResult(test_name=None, column='UNRATE', passed=False, values={'n_missing': 2648, 'p_missing': 0.7457054350887075}), TestResult(test_name=None, column='GS10', passed=False, values={'n_missing': 2710, 'p_missing': 0.763165305547733}), TestResult(test_name=None, column='FEDFUNDS', passed=False, values={'n_missing': 2726, 'p_missing': 0.7676710785694171})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'Variable': ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'], 'Frequency': [None, 'Monthly', 'Monthly', 'Monthly']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\ndf = df.resample('MS').last()\ndf = df.dropna()\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"MORTGAGE30US\"\n)\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3.5,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\nplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                       \n\n\nTest plan for data quality on time series datasets\n        \n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3.5}\n                \n            \n            \n                Results\n                [TestResult(test_name='outliers', column=None, passed=False, values={'Variable': ['FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'UNRATE', 'UNRATE'], 'z-score': [3.537416806083848, 3.582782543564376, 3.5878231810622134, 3.572701268568703, 5.011303408667247, 4.128420561484492], 'Threshold': [3.5, 3.5, 3.5, 3.5, 3.5, 3.5], 'Date': ['1980-12-01', '1981-01-01', '1981-06-01', '1981-07-01', '2020-04-01', '2020-05-01']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='MORTGAGE30US', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='UNRATE', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='GS10', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='FEDFUNDS', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'Variable': ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'], 'Frequency': ['Monthly', 'Monthly', 'Monthly', 'Monthly']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\n\n\n\n\n\n\n\n\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()\n\n\n\n\n\n\n\nStep 1: Fit Model\n\n# Add the independent variables with no intercept\nX = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Tue, 09 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        21:34:14   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Tue, 09 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        21:34:14   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\n\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff['GS10']\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Tue, 09 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        21:34:14   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_4 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_4.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            OLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Tue, 09 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        21:34:14   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_5 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_5.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            OLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Tue, 09 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        21:34:14   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\nvm_model = vm.init_model(model_1)\nvm_train_ds = vm.init_dataset(dataset=df_train_diff, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds = vm.init_dataset(dataset=df_test_diff, type=\"generic\", target_column=\"MORTGAGE30US\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                        Description                                            \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics    Test plan for sklearn classifier metrics               \nsklearn_classifier_validation     SKLearnClassifierPerformanceTest plan for sklearn classifier models                \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests \nsklearn_classifier                SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                        \ntabular_dataset                   TabularDataset              Test plan for generic tabular datasets                 \ntabular_dataset_description       TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                        \ntabular_data_quality              TabularDataQuality          Test plan for data quality on tabular datasets         \nnormality_test_plan               NormalityTestPlan           Test plan to perform normality tests.                  \nautocorrelation_test_plan         AutocorrelationTestPlan     Test plan to perform autocorrelation tests.            \nseasonality_test_plan             SesonalityTestPlan          Test plan to perform seasonality tests.                \nunit_root                         UnitRoot                    Test plan to perform unit root tests.                  \nstationarity_test_plan            StationarityTestPlan        Test plan to perform stationarity tests.               \ntimeseries                        TimeSeries                  Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                        \ntime_series_data_quality          TimeSeriesDataQuality       Test plan for data quality on time series datasets     \ntime_series_dataset               TimeSeriesDataset           Test plan for time series  datasets                    \ntime_series_univariate            TimeSeriesUnivariate        Test plan to perform time series univariate analysis.  \ntime_series_multivariate          TimeSeriesMultivariate      Test plan to perform time series multivariate analysis.\nregression_model_performance      RegressionModelPerformance  Test plan for statsmodels regressor models that includes\n    both metrics and validation tests                                                        \n\n\n\n\n\n\n\nfrom validmind.model_validation.statsmodels.metrics import RegressionModelSummary\nfrom validmind.vm_models.test_context import TestContext\ntest_context = TestContext(model=vm_model)\nmetric = RegressionModelSummary(test_context=test_context)\nmetric.run()\nmetric.result.metric.value\n\n{'Independent Variables': ['FEDFUNDS'],\n 'R-Squared': 0.2857335514089734,\n 'Adjusted R-Squared': 0.2842963955767983,\n 'MSE': 0.0738243956495036,\n 'RMSE': 0.27170645124748805}\n\n\n\nmodel_diagnosis_test_plan = vm.run_test_plan(\"regression_model_performance\", \n                                             model=vm_model\n                                            )\n\n                                            \n\n                                                                                                                                          \n\n\nTest plan for statsmodels regressor models that includes both metrics and validation tests\n            \n            \n            \n                \n                    \n                        Metric Name\n                        \n                    \n                    \n                        Metric Type\n                        \n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'Independent Variables': ['FEDFUNDS'], 'R-Squared': 0.2857335514089734, 'Adjusted R-Squared': 0.2842963955767983, 'MSE': 0.0738243956495036, 'RMSE': 0.27170645124748805}"
  },
  {
    "objectID": "notebooks/time_series/time_series_demo.html",
    "href": "notebooks/time_series/time_series_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "Time Series Test Plan Demo\n\n# This environment variable can be set to silence the summarized output of test results, for testing purposes.\n#\n# %env VM_SUMMARIZE_TEST_PLANS = False\n\nimport pandas as pd\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clg4hlb8d0046jn8hwqnes4ak\"\n)\n  \n\nTrue\n\n\n\ndf = pd.read_csv(\"../datasets/lending_club_loan_rates.csv\", sep='\\t')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n  \n  \n    \n      0\n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      1\n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2\n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      3\n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      4\n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n  \n\n\n\n\n\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2018-08-01\n      7.218997\n      11.161286\n      15.142618\n      19.857603\n      1.91\n      0.052118\n      0.045181\n      0.056796\n      0.088167\n      0.00\n      -0.09\n    \n    \n      2018-09-01\n      7.201281\n      11.191918\n      15.139769\n      19.748459\n      1.95\n      -0.017716\n      0.030632\n      -0.002849\n      -0.109144\n      0.04\n      0.04\n    \n    \n      2018-10-01\n      7.228498\n      11.208418\n      15.129105\n      19.792163\n      2.19\n      0.027218\n      0.016500\n      -0.010665\n      0.043704\n      0.24\n      0.20\n    \n    \n      2018-11-01\n      7.536897\n      11.390483\n      15.126869\n      19.632697\n      2.20\n      0.308399\n      0.182066\n      -0.002235\n      -0.159466\n      0.01\n      -0.23\n    \n    \n      2018-12-01\n      7.715209\n      11.459631\n      15.107476\n      19.558346\n      2.27\n      0.178312\n      0.069148\n      -0.019393\n      -0.074350\n      0.07\n      0.06\n    \n  \n\n137 rows × 11 columns\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                          Description                                           \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics      Test plan for sklearn classifier metrics              \nsklearn_classifier_validation     SKLearnClassifierPerformance  Test plan for sklearn classifier models               \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis    Test plan for sklearn classifier model diagnosis tests\nsklearn_classifier                SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                       \ntabular_dataset                   TabularDataset                Test plan for generic tabular datasets                \ntabular_dataset_description       TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                       \ntabular_data_quality              TabularDataQuality            Test plan for data quality on tabular datasets        \nnormality_test_plan               NormalityTestPlan             Test plan to perform normality tests.                 \nautocorrelation_test_plan         AutocorrelationTestPlan       Test plan to perform autocorrelation tests.           \nseasonality_test_plan             SesonalityTestPlan            Test plan to perform seasonality tests.               \nunit_root                         UnitRoot                      Test plan to perform unit root tests.                 \nstationarity_test_plan            StationarityTestPlan          Test plan to perform stationarity tests.              \ntimeseries                        TimeSeries                    Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                       \ntimeseries_univariate_inspection  TimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.     \n\n\n\n\n\nloan_rate_columns = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\ndiff1_loan_rate_columns = [\"diff1_loan_rate_A\", \"diff1_loan_rate_B\", \"diff1_loan_rate_C\", \"diff1_loan_rate_D\"]\n\ntest_plan_config = {\n    \"time_series_univariate_inspection_raw\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    },\n    \"time_series_univariate_inspection_histogram\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    }\n}\n\nplan = vm.run_test_plan(\"timeseries_univariate_inspection\", config=test_plan_config, dataset=vm_dataset)\n\n                                                                                                                                                                        \n\n\nResults for TimeSeriesUnivariateInspection Test Plan:\n        This section provides a preliminary understanding of the target variable(s)\n        used in the time series dataset. It visualizations that present the raw time\n        series data and a histogram of the target variable(s).\n\n        The raw time series data provides a visual inspection of the target variable's\n        behavior over time. This helps to identify any patterns or trends in the data,\n        as well as any potential outliers or anomalies. The histogram of the target\n        variable displays the distribution of values, providing insight into the range\n        and frequency of values observed in the data.\n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/time_series_model_validation_demo.html",
    "href": "notebooks/time_series/time_series_model_validation_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# System libraries\nimport glob\nimport os\nimport pickle\n\n# ML libraries\nimport pandas as pd\n\n# ValidMind libraries \nimport validmind as vm\n\n\n\n\n\n\n\n\n\n\n\n\nmodels_folder = '../models/time_series/'\nmodel_files = os.listdir(models_folder)\n\nmodels = {}\n\nfor model_file in model_files:\n    if model_file.endswith('.pkl'):\n        model_name = os.path.splitext(model_file)[0]\n        with open(os.path.join(models_folder, model_file), 'rb') as f:\n            models[model_name] = pickle.load(f)\n\n\n\n\nGet the training dataset from one of the models.\n\nmodel_fit = models['fred_loan_rates_model_1']\n\n\n# Extract the endogenous (target) variable from the model fit\ntrain_df = pd.Series(model_fit.model.endog, index=model_fit.model.data.row_labels)\ntrain_df = train_df.to_frame()\ntarget_var_name = model_fit.model.endog_names\ntrain_df.columns = [target_var_name]\n\n# Extract the exogenous (explanatory) variables from the model fit\nexog_df = pd.DataFrame(model_fit.model.exog, index=model_fit.model.data.row_labels, columns=model_fit.model.exog_names)\n\n# Concatenate the endogenous (target) and exogenous (explanatory) variables\ntrain_df = pd.concat([train_df, exog_df], axis=1)\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1971-05-01\n      0.17\n      0.47\n    \n    \n      1971-06-01\n      0.08\n      0.28\n    \n    \n      1971-07-01\n      0.15\n      0.40\n    \n    \n      1971-08-01\n      0.00\n      0.26\n    \n    \n      1971-09-01\n      -0.02\n      -0.02\n    \n  \n\n\n\n\n\ntrain_df.tail()\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      2012-06-01\n      -0.09\n      0.00\n    \n    \n      2012-07-01\n      -0.17\n      0.00\n    \n    \n      2012-08-01\n      0.10\n      -0.03\n    \n    \n      2012-09-01\n      -0.19\n      0.01\n    \n    \n      2012-10-01\n      0.01\n      0.02\n    \n  \n\n\n\n\n\n\n\nLoad raw test dataset.\n\nfile = '../datasets/time_series/fred_loan_rates_test_1.csv'\nraw_test_df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\ndisplay(raw_test_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      UNRATE\n      GS10\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      2012-11-01\n      3.32\n      7.7\n      1.65\n      0.16\n    \n    \n      2012-12-01\n      3.35\n      7.9\n      1.72\n      0.16\n    \n    \n      2013-01-01\n      3.53\n      8.0\n      1.91\n      0.14\n    \n    \n      2013-02-01\n      3.51\n      7.7\n      1.98\n      0.15\n    \n    \n      2013-03-01\n      3.57\n      7.5\n      1.96\n      0.14\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-11-01\n      6.58\n      3.6\n      3.89\n      3.78\n    \n    \n      2022-12-01\n      6.42\n      3.5\n      3.62\n      4.10\n    \n    \n      2023-01-01\n      6.13\n      3.4\n      3.53\n      4.33\n    \n    \n      2023-02-01\n      6.50\n      3.6\n      3.75\n      4.57\n    \n    \n      2023-03-01\n      6.32\n      3.5\n      3.66\n      4.65\n    \n  \n\n125 rows × 4 columns\n\n\n\nTransform raw test dataset using same transformation used in the train dataset.\n\ntransform_func = 'diff'\nif transform_func == 'diff':\n    test_df = raw_test_df.diff().dropna()\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgo0g0rt0000fjy6ozl9pb69\"\n)\n\nTrue\n\n\n\nvm_train_ds = vm.init_dataset(dataset=train_df, type=\"generic\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nDescribe test plan.\n\nvm.test_plans.describe_plan(\"time_series_forecast\")\n\n\n\n\nAttribute       Value                                           \n\n\nID              time_series_forecast                            \nName            TimeSeriesForecast                              \nDescription     Test plan to perform time series forecast tests.\nRequired Context['models', 'test_ds']                           \nTests           ModelPredictionOLS (Metric)                     \nTest Plans      []                                              \n\n\n\n\nCreate vm models.\n\nvm_model_1 = vm.init_model(models['fred_loan_rates_model_1'])\nvm_model_3 = vm.init_model(models['fred_loan_rates_model_3'])\nlist_of_models = [vm_model_1, vm_model_3]\n\nConfigure the time series forecast.\n\nconfig= {\n    \"model_prediction_ols\": {\n        \"plot_start_date\": '2010-01-01',\n        \"plot_end_date\": '2022-01-01'\n    }\n}\n\n\nvm.run_test_plan(\"time_series_forecast\",\n                                        models=list_of_models,\n                                        test_ds=vm_test_ds,\n                                        config=config)\n\nRunning Metric: model_prediction_ols:   0%|          | 0/1 [00:00<?, ?it/s]     \n\n\n2010-01-01\n\n\n                                                                                                                                      \n\n\nThis test plan computes predictions from statsmodels OLS linear regression models against a list of models and plots the historical data alongside the forecasted data. The purpose of this test plan is to evaluate the performance of each model in predicting future values of a time series based on historical data. By comparing the historical values with the forecasted values, users can visually assess the accuracy of each model and determine which one best fits the data. In addition, this test plan can help users identify any discrepancies between the models and the actual data, allowing for potential improvements in model selection and parameter tuning.\n            \n            \n            \n                \n                    \n                        Metric Name\n                        model_prediction_ols\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'DATE': '1971-05-01', 'MORTGAGE30US_train': 0.16999999999999993, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-06-01', 'MORTGAGE30US_train': 0.08000000000000007, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-07-01', 'MORTGAGE30US_train': 0.15000000000000036, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-08-01', 'MORTGAGE30US_train': 0.0, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-09-01', 'MORTGAGE30US_train': -0.020000000000000462, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-10-01', 'MORTGAGE30US_train': -0.040000000000000036, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-11-01', 'MORTGAGE30US_train': -0.1200000000000001, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1971-12-01', 'MORTGAGE30US_train': -0.02999999999999936, 'MORTGAGE30US_test': nan, 'model_1': nan, 'model_2': nan}, {'DATE': '1972-01-01', 'MORTGAGE30...\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n\n\nTimeSeriesForecast(test_context=TestContext(dataset=None, model=None, models=[Model(attributes=ModelAttributes(architecture=None, framework=None, framework_version=None), task=None, subtask=None, params=None, model_id='main', model=<statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x106767df0>), Model(attributes=ModelAttributes(architecture=None, framework=None, framework_version=None), task=None, subtask=None, params=None, model_id='main', model=<statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x28b8a8970>)], train_ds=None, test_ds=Dataset(raw_dataset=            MORTGAGE30US  UNRATE  GS10  FEDFUNDS\nDATE                                            \n2012-12-01          0.03     0.2  0.07      0.00\n2013-01-01          0.18     0.1  0.19     -0.02\n2013-02-01         -0.02    -0.3  0.07      0.01\n2013-03-01          0.06    -0.2 -0.02     -0.01\n2013-04-01         -0.17     0.1 -0.20      0.01\n...                  ...     ...   ...       ...\n2022-11-01         -0.50    -0.1 -0.09      0.70\n2022-12-01         -0.16    -0.1 -0.27      0.32\n2023-01-01         -0.29    -0.1 -0.09      0.23\n2023-02-01          0.37     0.2  0.22      0.24\n2023-03-01         -0.18    -0.1 -0.09      0.08\n\n[124 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 0.03000000000000025, 'UNRATE': 0.20000000000000018, 'GS10': 0.07000000000000006, 'FEDFUNDS': 0.0}, {'MORTGAGE30US': 0.17999999999999972, 'UNRATE': 0.09999999999999964, 'GS10': 0.18999999999999995, 'FEDFUNDS': -0.01999999999999999}, {'MORTGAGE30US': -0.020000000000000018, 'UNRATE': -0.2999999999999998, 'GS10': 0.07000000000000006, 'FEDFUNDS': 0.009999999999999981}, {'MORTGAGE30US': 0.06000000000000005, 'UNRATE': -0.20000000000000018, 'GS10': -0.020000000000000018, 'FEDFUNDS': -0.009999999999999981}, {'MORTGAGE30US': -0.16999999999999993, 'UNRATE': 0.09999999999999964, 'GS10': -0.19999999999999996, 'FEDFUNDS': 0.009999999999999981}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': -0.5, 'UNRATE': -0.10000000000000009, 'GS10': -0.08999999999999986, 'FEDFUNDS': 0.6999999999999997}, {'MORTGAGE30US': -0.16000000000000014, 'UNRATE': -0.10000000000000009, 'GS10': -0.27, 'FEDFUNDS': 0.31999999999999984}, {'MORTGAGE30US': -0.29000000000000004, 'UNRATE': -0.10000000000000009, 'GS10': -0.0900000000000003, 'FEDFUNDS': 0.23000000000000043}, {'MORTGAGE30US': 0.3700000000000001, 'UNRATE': 0.20000000000000018, 'GS10': 0.2200000000000002, 'FEDFUNDS': 0.2400000000000002}, {'MORTGAGE30US': -0.17999999999999972, 'UNRATE': -0.10000000000000009, 'GS10': -0.08999999999999986, 'FEDFUNDS': 0.08000000000000007}]}], shape={'rows': 124, 'columns': 4}, correlation_matrix=None, correlations=None, type='generic', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), validation_ds=None, y_train_predict=None, y_test_predict=None, context_data=None), config={...})"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#data-engineering",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#data-engineering",
    "title": "ValidMind",
    "section": "4.1. Data Engineering",
    "text": "4.1. Data Engineering\n\n4.1.1. Data Collection\n\nSetup\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import coint\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\nLoad FRED Data\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = '../datasets/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\n\n\n4.1.2. Data Description\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n4.1.3. Data Quality\n\nFrequency of Series\nDistribution of frequencies in the data.\n\ndef plot_time_difference_frequency(df):\n    # Calculate the time differences between consecutive entries\n    time_diff = df.index.to_series().diff().dropna()\n\n    # Convert the time differences to a suitable unit (e.g., days)\n    time_diff_days = time_diff.dt.total_seconds() / (60 * 60 * 24)\n\n    # Create a DataFrame with the time differences\n    time_diff_df = pd.DataFrame({'Time Differences (Days)': time_diff_days})\n\n    # Plot the frequency distribution of the time differences\n    sns.histplot(data=time_diff_df, x='Time Differences (Days)', bins=50, kde=False)\n    plt.xlabel('Time Differences (Days)')\n    plt.ylabel('Frequency')\n    plt.show()\n\n\nplot_time_difference_frequency(df)\n\n\n\n\nIdentify frequencies for each variable.\n\ndef identify_frequencies(df):\n    \"\"\"\n    Identify the frequency of each series in the DataFrame.\n\n    :param df: Time-series DataFrame\n    :return: DataFrame with two columns: 'Variable' and 'Frequency'\n    \"\"\"\n    frequencies = []\n    for column in df.columns:\n        series = df[column].dropna()\n        if not series.empty:\n            freq = pd.infer_freq(series.index)\n            if freq == 'MS' or freq == 'M':\n                label = 'Monthly'\n            elif freq == 'Q':\n                label = 'Quarterly'\n            elif freq == 'A':\n                label = 'Yearly'\n            else:\n                label = freq\n        else:\n            label = None\n\n        frequencies.append({'Variable': column, 'Frequency': label})\n\n    freq_df = pd.DataFrame(frequencies)\n\n    return freq_df\n\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      UNRATE\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      FEDFUNDS\n      Monthly\n    \n  \n\n\n\n\nHandling frequencies.\n\ndf = df.resample('MS').last()\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      Monthly\n    \n    \n      1\n      UNRATE\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      FEDFUNDS\n      Monthly\n    \n  \n\n\n\n\n\n\nMissing Values\nStep 1: Identify Missing Values\nTotal number of missing values.\n\ndef plot_missing_values_bar(df):\n    \"\"\"\n    Plot a bar chart displaying the total number of missing values per variable (column) in a time-series DataFrame using seaborn.\n    \n    :param df: Time-series DataFrame\n    \"\"\"\n    # Calculate the total number of missing values per column\n    missing_values = df.isnull().sum()\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the bar chart\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=missing_values.index, y=missing_values.values)\n    \n    plt.xticks(rotation=45, ha='right')\n    plt.xlabel('Variables (Columns)')\n    plt.ylabel('Number of Missing Values')\n    plt.title('Total Number of Missing Values per Variable')\n    plt.show()\n\n\nplot_missing_values_bar(df)\n\n\n\n\nHeatmap of missing values.\n\ndef plot_missing_values_heatmap(df, start_year=None, end_year=None):\n    \"\"\"\n    Plot a heatmap of missing values with actual years in rows using seaborn.\n\n    :param df: Time-series DataFrame\n    :param start_year: Start year for zooming in, defaults to None\n    :param end_year: End year for zooming in, defaults to None\n    \"\"\"\n    # Filter the DataFrame based on the specified start_year and end_year\n    if start_year:\n        df = df[df.index.year >= start_year]\n    if end_year:\n        df = df[df.index.year <= end_year]\n\n    # Create a boolean mask for missing values\n    missing_mask = df.isnull()\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the heatmap\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(missing_mask.T, cmap='viridis', cbar=False, xticklabels=False)\n\n    # Add actual years on the x-axis\n    years = df.index.year.unique()\n    xticks = [df.index.get_loc(df.index[df.index.year == year][0]) for year in years]\n    plt.xticks(xticks, years, rotation=45, ha='right')\n\n    plt.ylabel('Columns')\n    plt.xlabel('Rows (Years)')\n    plt.title('Missing Values Heatmap with Actual Years in Rows')\n    plt.show()\n\n\nplot_missing_values_heatmap(df)\n\n\n\n\nStep 2: Handling Missing Values\nDrop missing values.\n\ndf = df.dropna()\n\n\nplot_missing_values_bar(df)\n\n\n\n\n\nplot_missing_values_heatmap(df)\n\n\n\n\n\n\nOutliers\nStep 1: Identify Outliers\n\ndef identify_outliers(df, threshold=3):\n    z_scores = pd.DataFrame(stats.zscore(df), index=df.index, columns=df.columns)\n    outliers = z_scores[(z_scores.abs() > threshold).any(axis=1)]\n    \n    outlier_table = []\n    for idx, row in outliers.iterrows():\n        for col in df.columns:\n            if abs(row[col]) > threshold:\n                outlier_table.append({\"Variable\": col, \"z-score\": row[col], \"Threshold\": threshold, \"Date\": idx})\n                \n    return pd.DataFrame(outlier_table)\n\n\noutliers_table = identify_outliers(df, threshold=3)\ndisplay(outliers_table)\n\n\n\n\n\n  \n    \n      \n      Variable\n      z-score\n      Threshold\n      Date\n    \n  \n  \n    \n      0\n      FEDFUNDS\n      3.106442\n      3\n      1980-03-01\n    \n    \n      1\n      FEDFUNDS\n      3.212296\n      3\n      1980-04-01\n    \n    \n      2\n      FEDFUNDS\n      3.537417\n      3\n      1980-12-01\n    \n    \n      3\n      FEDFUNDS\n      3.582783\n      3\n      1981-01-01\n    \n    \n      4\n      FEDFUNDS\n      3.441645\n      3\n      1981-05-01\n    \n    \n      5\n      FEDFUNDS\n      3.587823\n      3\n      1981-06-01\n    \n    \n      6\n      FEDFUNDS\n      3.572701\n      3\n      1981-07-01\n    \n    \n      7\n      FEDFUNDS\n      3.265222\n      3\n      1981-08-01\n    \n    \n      8\n      MORTGAGE30US\n      3.246766\n      3\n      1981-09-01\n    \n    \n      9\n      MORTGAGE30US\n      3.271251\n      3\n      1981-10-01\n    \n    \n      10\n      MORTGAGE30US\n      3.011098\n      3\n      1982-01-01\n    \n    \n      11\n      UNRATE\n      5.011303\n      3\n      2020-04-01\n    \n    \n      12\n      UNRATE\n      4.128421\n      3\n      2020-05-01\n    \n  \n\n\n\n\nPlot outliers.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_outliers(df, outliers_table, use_subplots=False):\n    sns.set(style=\"darkgrid\")\n    \n    if use_subplots:\n        n_variables = len(df.columns)\n        fig, axes = plt.subplots(n_variables, 1, figsize=(12, 3 * n_variables), sharex=True)\n        \n        for i, col in enumerate(df.columns):\n            sns.lineplot(data=df, x=df.index, y=col, ax=axes[i], label=col)\n            \n            variable_outliers = outliers_table[outliers_table[\"Variable\"] == col]\n            for idx, row in variable_outliers.iterrows():\n                date = row[\"Date\"]\n                outlier_value = df.loc[date, col]\n                axes[i].scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\", label=\"Outlier\" if idx == 0 else \"\")\n            \n            axes[i].legend()\n            axes[i].set_ylabel(\"Value\")\n            axes[i].set_title(f\"Time Series with Outliers for {col}\")\n\n        plt.xlabel(\"Date\")\n        plt.tight_layout()\n\n    else:\n        plt.figure(figsize=(12, 3))\n        for col in df.columns:\n            sns.lineplot(data=df, x=df.index, y=col, label=col)\n        \n        plotted_outlier_variables = set()\n        for idx, row in outliers_table.iterrows():\n            date = row[\"Date\"]\n            variable = row[\"Variable\"]\n            outlier_value = df.loc[date, variable]\n            if variable not in plotted_outlier_variables:\n                plt.scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\", label=f\"Outlier ({variable})\")\n                plotted_outlier_variables.add(variable)\n            else:\n                plt.scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\")\n\n        plt.legend()\n        plt.xlabel(\"Date\")\n        plt.ylabel(\"Value\")\n        plt.title(\"Time Series with Outliers\")\n    \n    plt.show()\n\n\nplot_outliers(df, outliers_table, use_subplots=True)\n\n\n\n\nStep 2: Handling Outliers"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#exploratory-data-analysis",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#exploratory-data-analysis",
    "title": "ValidMind",
    "section": "4.2. Exploratory Data Analysis",
    "text": "4.2. Exploratory Data Analysis\n\n4.2.1. Univariate Analysis\n\nVisual Inspection\nLine plots.\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\n\nSeasonality\nStep 1: Compute Seasonal Decomposition\n\ndef compute_seasonal_decomposition(data, model='additive'):\n    \"\"\"\n    Compute seasonal decomposition for all time-series in a DataFrame and store all the components in a new DataFrame.\n    \n    :param data: DataFrame with time-series data\n    :param period: Number of observations in each seasonal period\n    :return: DataFrame with seasonal, trend, and residual components for all time-series in the input DataFrame\n    \"\"\"\n    # Initialize an empty DataFrame to store the components for each time-series\n    decomp_df = pd.DataFrame()\n\n    # Loop over each column in the input DataFrame and perform seasonal decomposition\n    for col in data.columns:\n        res = seasonal_decompose(data[col], model=model)\n        decomp_df[f'{col}_seasonal'] = res.seasonal\n        decomp_df[f'{col}_trend'] = res.trend\n        decomp_df[f'{col}_residual'] = res.resid\n\n    # Set the index of the decomposed DataFrame to be the same as the input DataFrame\n    decomp_df.index = data.index\n\n    return decomp_df\n\n\ndecomp_df = compute_seasonal_decomposition(df)\n\nStep 2: Visualize Seasonal Decomposition\n\ndef plot_seasonal_components(decomp_df):\n    \"\"\"\n    Plot all seasonal, trend, and residual components for each variable in a DataFrame.\n    \n    :param decomp_df: DataFrame with seasonal, trend, and residual components for each variable\n    \"\"\"\n    # Initialize a figure with subplots for each variable and component\n    fig, axs = plt.subplots(nrows=len(decomp_df.columns) // 3, ncols=3, figsize=(12, 4 * (len(decomp_df.columns) // 3)))\n\n    # Loop over each variable in the input DataFrame and plot the seasonal, trend, and residual components\n    for i, col in enumerate(decomp_df.columns[::3]):\n        axs[i, 0].plot(decomp_df.index, decomp_df[f'{col}'])\n        axs[i, 0].set_title(f'Seasonal: {col[:-9]}')\n        axs[i, 1].plot(decomp_df.index, decomp_df[f'{col[:-9]}_trend'])\n        axs[i, 1].set_title(f'Trend: {col[:-9]}')\n        axs[i, 2].plot(decomp_df.index, decomp_df[f'{col[:-9]}_residual'])\n        axs[i, 2].set_title(f'Residual: {col[:-9]}')\n\n    # Set the figure title\n    fig.suptitle('Seasonal Decomposition', fontsize=16)\n\n    # Adjust the spacing between subplots\n    fig.tight_layout()\n\n    # Show the plot\n    plt.show()\n\n\nplot_seasonal_components(decomp_df)\n\n\n\n\nStep 3: Residual Analysis\n\n\nStationarity\nStep 1: Auto Stationarity\n\ndef test_stationarity(data, threshold=0.05):\n    \"\"\"\n    Perform multiple stationarity tests on each time series in a DataFrame.\n    \n    :param data: DataFrame with time-series data\n    :return: DataFrame with test results (Variable, Test, p-value, Threshold, Pass/Fail, Decision)\n    \"\"\"\n    # Initialize an empty DataFrame to store the test results\n    test_results = pd.DataFrame(columns=['Variable', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n\n    # Loop over each column in the input DataFrame and perform stationarity tests\n    for col in data.columns:\n        # Perform the ADF test\n        adf_result = adfuller(data[col], autolag='AIC')\n        adf_pvalue = adf_result[1]\n        adf_pass_fail = adf_pvalue < threshold\n        adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'ADF',\n            'p-value': adf_pvalue,\n            'Threshold': threshold,\n            'Pass/Fail': adf_pass_fail,\n            'Decision': adf_decision\n        }, ignore_index=True)\n\n        # Perform the KPSS test\n        kpss_result = kpss(data[col], regression='c', nlags='auto')\n        kpss_pvalue = kpss_result[1]\n        kpss_pass_fail = kpss_pvalue > threshold\n        kpss_decision = 'Stationary' if kpss_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'KPSS',\n            'p-value': kpss_pvalue,\n            'Threshold': threshold,\n            'Pass/Fail': kpss_pass_fail,\n            'Decision': kpss_decision\n        }, ignore_index=True)\n\n        # Perform the Phillips-Perron test\n        pp_result = PhillipsPerron(data[col], trend='ct')\n        pp_pvalue = pp_result.pvalue\n        pp_threshold = threshold\n        pp_pass_fail = pp_pvalue < pp_threshold\n        pp_decision = 'Stationary' if pp_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'PhillipsPerron',\n            'p-value': pp_pvalue,\n            'Threshold': pp_threshold,\n            'Pass/Fail': pp_pass_fail,\n            'Decision': pp_decision\n        }, ignore_index=True)\n\n        # Perform the DF-GLS test\n        dfgls_result = DFGLS(data[col], trend='ct')\n        dfgls_pvalue = dfgls_result.pvalue\n        dfgls_threshold = threshold\n        dfgls_pass_fail = dfgls_pvalue < dfgls_threshold\n        dfgls_decision = 'Stationary' if dfgls_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'DFGLS',\n            'p-value': dfgls_pvalue,\n            'Threshold': dfgls_threshold,\n            'Pass/Fail': dfgls_pass_fail,\n            'Decision': dfgls_decision\n        }, ignore_index=True)\n\n    return test_results\n\n\ndef auto_stationarity(data, max_order=5, threshold=0.05):\n    \"\"\"\n    Perform the Augmented Dickey-Fuller (ADF) stationarity test on each time series in a DataFrame,\n    testing for different integration orders until the series is stationary.\n    \n    :param data: DataFrame with time-series data\n    :param max_order: Maximum integration order to test\n    :param threshold: Significance level for the ADF test\n    :return: DataFrame with test results (Variable, Integration Order, Test, p-value, Threshold, Pass/Fail, Decision)\n    \"\"\"\n    # Initialize an empty DataFrame to store the test results\n    test_results = pd.DataFrame(columns=['Variable', 'Integration Order', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n\n    # Loop over each column in the input DataFrame and perform stationarity tests\n    for col in data.columns:\n        is_stationary = False\n        order = 0\n        \n        while not is_stationary and order <= max_order:\n            series = data[col]\n            \n            if order == 0:\n                adf_result = adfuller(series)\n            else:\n                adf_result = adfuller(np.diff(series, n=order-1))\n\n            adf_pvalue = adf_result[1]\n            adf_pass_fail = adf_pvalue < threshold\n            adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\n\n            test_results = test_results.append({\n                'Variable': col,\n                'Integration Order': order,\n                'Test': 'ADF',\n                'p-value': adf_pvalue,\n                'Threshold': threshold,\n                'Pass/Fail': 'Pass' if adf_pass_fail else 'Fail',\n                'Decision': adf_decision\n            }, ignore_index=True)\n\n            if adf_pass_fail:\n                is_stationary = True\n            \n            order += 1\n\n    return test_results\n\n\nauto_stationarity(df)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Integration Order\n      Test\n      p-value\n      Threshold\n      Pass/Fail\n      Decision\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      0\n      ADF\n      6.719476e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      1\n      MORTGAGE30US\n      1\n      ADF\n      6.719476e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      2\n      MORTGAGE30US\n      2\n      ADF\n      2.156453e-30\n      0.05\n      Pass\n      Stationary\n    \n    \n      3\n      UNRATE\n      0\n      ADF\n      1.939529e-02\n      0.05\n      Pass\n      Stationary\n    \n    \n      4\n      GS10\n      0\n      ADF\n      7.099537e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      5\n      GS10\n      1\n      ADF\n      7.099537e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      6\n      GS10\n      2\n      ADF\n      2.036674e-09\n      0.05\n      Pass\n      Stationary\n    \n    \n      7\n      FEDFUNDS\n      0\n      ADF\n      1.058010e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      8\n      FEDFUNDS\n      1\n      ADF\n      1.058010e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      9\n      FEDFUNDS\n      2\n      ADF\n      6.632874e-05\n      0.05\n      Pass\n      Stationary\n    \n  \n\n\n\n\nStep 2: Rolling Statistics\n\ndef plot_rolling_statistics(df, window_size=12):\n    \"\"\"\n    Plot rolling mean and rolling standard deviation in different subplots for each variable.\n    \n    :param df: DataFrame with time-series data\n    :param window_size: Window size for the rolling calculations\n    \"\"\"\n    for col_name in df.columns:\n        rolling_mean = df[col_name].rolling(window=window_size).mean()\n        rolling_std = df[col_name].rolling(window=window_size).std()\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))\n\n        ax1.plot(rolling_mean, label=f'{col_name} Rolling Mean')\n        ax1.legend()\n        ax1.set_ylabel('Value')\n        ax1.set_title(f'Rolling Mean for {col_name}')\n\n        ax2.plot(rolling_std, label=f'{col_name} Rolling Standard Deviation', color='orange')\n        ax2.legend()\n        ax2.set_xlabel('Time')\n        ax2.set_ylabel('Value')\n        ax2.set_title(f'Rolling Standard Deviation for {col_name}')\n\n        plt.show()\n\n\nplot_rolling_statistics(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\n\n\nAR Analysis\nStep 1: Calculate AR Orders\n\ndef calculate_ar_orders(dataset, max_order=3):\n    \"\"\"\n    This function calculates the autoregressive order of all time series in a dataset.\n    \n    Parameters:\n    dataset (pd.DataFrame): The dataset containing the time series.\n    max_order (int): The maximum order to consider for the autoregressive models.\n    \n    Returns:\n    pd.DataFrame: A table with the autoregressive order, AIC, and BIC for orders 0 up to max_order.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each column (time series) in the dataset\n    for col in dataset.columns:\n        time_series = dataset[col]\n        \n        # Test for stationarity using Augmented Dickey-Fuller test\n        adf_result = adfuller(time_series)\n        if adf_result[1] > 0.05:\n            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\n        \n        # Test different autoregressive orders and store the AIC and BIC values\n        for order in range(max_order + 1):\n            model = AutoReg(time_series, lags=order, old_names=False)\n            result = model.fit()\n            \n            # Add the current time series, order, AIC, and BIC to the results list\n            results.append({'Variable': col, 'AR order': order, 'AIC': result.aic, 'BIC': result.bic})\n\n    # Convert the results list to a DataFrame and return it\n    return pd.DataFrame(results)\n\n\ncalculate_ar_orders(df_diff)\n\n\n\n\n\n  \n    \n      \n      Variable\n      AR order\n      AIC\n      BIC\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      0\n      291.177506\n      300.046599\n    \n    \n      1\n      MORTGAGE30US\n      1\n      246.637029\n      259.935849\n    \n    \n      2\n      MORTGAGE30US\n      2\n      223.310101\n      241.035426\n    \n    \n      3\n      MORTGAGE30US\n      3\n      225.331792\n      247.480389\n    \n    \n      4\n      UNRATE\n      0\n      835.075578\n      843.944671\n    \n    \n      5\n      UNRATE\n      1\n      835.941726\n      849.240546\n    \n    \n      6\n      UNRATE\n      2\n      833.607234\n      851.332559\n    \n    \n      7\n      UNRATE\n      3\n      835.222318\n      857.370915\n    \n    \n      8\n      GS10\n      0\n      243.604950\n      252.474043\n    \n    \n      9\n      GS10\n      1\n      179.889575\n      193.188396\n    \n    \n      10\n      GS10\n      2\n      155.917382\n      173.642706\n    \n    \n      11\n      GS10\n      3\n      155.305036\n      177.453633\n    \n    \n      12\n      FEDFUNDS\n      0\n      992.528777\n      1001.397870\n    \n    \n      13\n      FEDFUNDS\n      1\n      879.316228\n      892.615048\n    \n    \n      14\n      FEDFUNDS\n      2\n      858.467020\n      876.192344\n    \n    \n      15\n      FEDFUNDS\n      3\n      858.142003\n      880.290601\n    \n  \n\n\n\n\nStep 2: Selection of AR Order\n\n\nMA Analysis\nStep 1: Calculate MA Orders\n\ndef calculate_ma_orders(dataset, max_order=3):\n    \"\"\"\n    This function calculates the moving average order of all time series in a dataset.\n    \n    Parameters:\n    dataset (pd.DataFrame): The dataset containing the time series.\n    max_order (int): The maximum order to consider for the moving average models.\n    \n    Returns:\n    pd.DataFrame: A table with the moving average order, AIC, and BIC for orders 0 up to max_order.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each column (time series) in the dataset\n    for col in dataset.columns:\n        time_series = dataset[col]\n        \n        # Test for stationarity using Augmented Dickey-Fuller test\n        adf_result = adfuller(time_series)\n        if adf_result[1] > 0.05:\n            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\n        \n        # Test different moving average orders and store the AIC and BIC values\n        for order in range(max_order + 1):\n            model = ARIMA(time_series, order=(0, 0, order))\n            result = model.fit()\n            \n            # Add the current time series, order, AIC, and BIC to the results list\n            results.append({'Variable': col, 'MA order': order, 'AIC': result.aic, 'BIC': result.bic})\n\n    # Convert the results list to a DataFrame and return it\n    return pd.DataFrame(results)\n\n\ncalculate_ma_orders(df_diff)\n\n\n\n\n\n  \n    \n      \n      Variable\n      MA order\n      AIC\n      BIC\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      0\n      291.177507\n      300.046600\n    \n    \n      1\n      MORTGAGE30US\n      1\n      228.313186\n      241.616825\n    \n    \n      2\n      MORTGAGE30US\n      2\n      227.394618\n      245.132804\n    \n    \n      3\n      MORTGAGE30US\n      3\n      225.782939\n      247.955672\n    \n    \n      4\n      UNRATE\n      0\n      835.075578\n      843.944671\n    \n    \n      5\n      UNRATE\n      1\n      836.126269\n      849.429909\n    \n    \n      6\n      UNRATE\n      2\n      833.512857\n      851.251043\n    \n    \n      7\n      UNRATE\n      3\n      835.509496\n      857.682228\n    \n    \n      8\n      GS10\n      0\n      243.604950\n      252.474043\n    \n    \n      9\n      GS10\n      1\n      155.604373\n      168.908013\n    \n    \n      10\n      GS10\n      2\n      152.550224\n      170.288410\n    \n    \n      11\n      GS10\n      3\n      154.471588\n      176.644321\n    \n    \n      12\n      FEDFUNDS\n      0\n      992.528777\n      1001.397870\n    \n    \n      13\n      FEDFUNDS\n      1\n      865.158571\n      878.462211\n    \n    \n      14\n      FEDFUNDS\n      2\n      864.192973\n      881.931159\n    \n    \n      15\n      FEDFUNDS\n      3\n      865.935637\n      888.108370\n    \n  \n\n\n\n\nStep 2: Selection of MA Order\n\n\n\n4.2.2. Multivariate Analysis\n\nCorrelations\nStep 1: Compute Correlation Matrix on Levels\n\ndef plot_corr_heatmap(df):\n    # Compute correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n\n    # Set plot title\n    plt.title('Correlation Matrix Heatmap')\n\n    # Show plot\n    plt.show()\n\nCorrrelations across Levels.\n\nplot_corr_heatmap(df)\n\n\n\n\nStep 2: Compute Correlation Matrix on First Difference\nCorrelations across First Differences.\n\nplot_corr_heatmap(df_diff)\n\n\n\n\nStep 3: Reasoning\n\n\nScatter Plots\nStep 1: Compute Scatter Plots on Levels\n\ndef plot_scatter_pairs(df):\n    # Compute pairwise scatter plots\n    sns.pairplot(df, kind='scatter')\n\n    # Show plot\n    plt.show()\n\n\nplot_scatter_pairs(df)\n\n\n\n\nStep 2: Compute Scatter Plots on First Difference\nCompute first difference.\n\nplot_scatter_pairs(df_diff)\n\n\n\n\nStep 3: Reasoning\n\n\nLag Analysis\nStep 1: Compute Correlations at Multiple Lags\n\ndef plot_heatmap_correlations(df, target_col, independent_vars, num_lags=10):\n    \"\"\"\n    Calculate the correlation between the target variable and the lags of independent variables in the dataset,\n    and plot a heatmap of these correlations.\n    :param df: DataFrame containing the target variable and independent variables of interest\n    :param target_col: Column name of the target variable in the DataFrame\n    :param independent_vars: List of column names of the independent variables in the DataFrame\n    :param num_lags: Number of lags to calculate (default is 10)\n    \"\"\"\n\n    correlations = np.zeros((len(independent_vars), num_lags + 1))\n\n    for i, ind_var_col in enumerate(independent_vars):\n        for lag in range(num_lags + 1):\n            # Create a new DataFrame with the original and lagged variable\n            temp_df = pd.DataFrame({target_col: df[target_col],\n                                    f'{ind_var_col}_lag{lag}': df[ind_var_col].shift(lag)})\n\n            # Drop NaN rows\n            temp_df = temp_df.dropna()\n\n            # Calculate the correlation between the target variable and the lagged independent variable\n            corr = temp_df[target_col].corr(temp_df[f'{ind_var_col}_lag{lag}'])\n\n            # Store the correlation in the correlations matrix\n            correlations[i, lag] = corr\n\n    # Create a DataFrame with the correlations matrix\n    correlation_df = pd.DataFrame(correlations, columns=[f'lag_{i}' for i in range(num_lags + 1)], index=independent_vars)\n\n    # Plot the heatmap\n    plt.figure(figsize=(12, 3))\n    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.title('Heatmap of Correlations between Target Variable and Lags of Independent Variables')\n    plt.xlabel('Lags')\n    plt.ylabel('Independent Variables')\n    plt.show()\n\n\ntarget_var = 'MORTGAGE30US'\nindependent_vars = ['GS10', 'UNRATE', 'FEDFUNDS']\nplot_heatmap_correlations(df_diff, target_col=target_var, independent_vars=independent_vars, num_lags=10)\n\n\n\n\nStep 2: Reasoning\n\n\nColinearity\n\n\nCointegration\nStep 1: Compute Cointegration Test\n\n# Function to calculate cointegration for each pair of variables in a DataFrame\ndef calculate_cointegration(dataframe, test=\"Engle-Granger\", threshold=0.05):\n    coint_df = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n    for i in range(len(dataframe.columns)):\n        for j in range(i+1, len(dataframe.columns)):\n            var1 = dataframe.columns[i]\n            var2 = dataframe.columns[j]\n            _, p_value, _ = coint(dataframe[var1], dataframe[var2])\n            pass_fail = \"Pass\" if p_value <= threshold else \"Fail\"\n            decision = \"Cointegrated\" if pass_fail == \"Pass\" else \"Not Cointegrated\"\n            coint_df = coint_df.append({\n                'Variable 1': var1,\n                'Variable 2': var2,\n                'Test': test,\n                'p-value': p_value,\n                'Threshold': threshold,\n                'Pass/Fail': pass_fail,\n                'Decision': decision\n            }, ignore_index=True)\n    return coint_df\n\n\n# Calculate cointegration for pairs of variables in the DataFrame\ncoint_results = calculate_cointegration(df)\ndisplay(coint_results)\n\n\n\n\n\n  \n    \n      \n      Variable 1\n      Variable 2\n      Test\n      p-value\n      Threshold\n      Pass/Fail\n      Decision\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      UNRATE\n      Engle-Granger\n      0.627876\n      0.05\n      Fail\n      Not Cointegrated\n    \n    \n      1\n      MORTGAGE30US\n      GS10\n      Engle-Granger\n      0.008688\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      2\n      MORTGAGE30US\n      FEDFUNDS\n      Engle-Granger\n      0.020417\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      3\n      UNRATE\n      GS10\n      Engle-Granger\n      0.013242\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      4\n      UNRATE\n      FEDFUNDS\n      Engle-Granger\n      0.027579\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      5\n      GS10\n      FEDFUNDS\n      Engle-Granger\n      0.005832\n      0.05\n      Pass\n      Cointegrated\n    \n  \n\n\n\n\nStep 2: Plot Spread between Variables\n\n# Function to plot the spread between all pairs of variables in a DataFrame\ndef plot_spread(dataframe):\n    num_vars = len(dataframe.columns)\n    \n    for i in range(num_vars):\n        for j in range(i+1, num_vars):\n            var1 = dataframe.columns[i]\n            var2 = dataframe.columns[j]\n\n            # Calculate the spread between the two variables\n            dataframe['spread'] = dataframe[var1] - dataframe[var2]\n\n            # Plot the difference (spread) using seaborn\n            plt.figure(figsize=(10, 4))\n            sns.lineplot(data=dataframe['spread'], label=f'Spread ({var1} - {var2})')\n            plt.title(f'Spread ({var1} - {var2})')\n            plt.legend()\n\n            # Display the plot\n            plt.tight_layout()\n            plt.show()\n\n\nplot_spread(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.3. Feature Selection"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#model-methodology",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#model-methodology",
    "title": "ValidMind",
    "section": "4.3. Model Methodology",
    "text": "4.3. Model Methodology"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#training-data",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#training-data",
    "title": "ValidMind",
    "section": "4.4. Training Data",
    "text": "4.4. Training Data\n\n4.4.1. Sampling\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#model-training",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#model-training",
    "title": "ValidMind",
    "section": "4.5. Model Training",
    "text": "4.5. Model Training\n\nModel 1: Loan Rates and FEDFUNDS\nStep 1: Fit Model\n\n# Add the independent variables with no intercept\nX = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Sat, 06 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        16:41:11   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\nModel 2: Loan Rates, constant and FEDFUNDS\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Sat, 06 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        16:41:11   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\nModel 3: Loan Rates and GS10\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff['GS10']\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Sat, 06 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        16:41:12   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nModel 4: Loan Rates, FEDFUNDS and GS10\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_4 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_4.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            OLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Sat, 06 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        16:41:12   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\nModel 5: Loan Rates, FEDFUNDS, GS10 and UNRATE\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_5 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_5.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            OLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Sat, 06 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        16:41:18   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nModel Selection\nStep 1: In-Sample Performance\n\ndef in_sample_performance_ols(models):\n    evaluation_results = []\n\n    for i, model in enumerate(models):\n        X = model.model.exog\n        X_columns = model.model.exog_names\n        y = model.model.endog\n\n        # Calculate the predicted values using the model\n        y_pred = model.predict(X)\n\n        # Calculate the residuals\n        residuals = y - y_pred\n\n        # Extract R-squared and Adjusted R-squared\n        r2 = model.rsquared\n        adj_r2 = model.rsquared_adj\n\n        # Calculate the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n        mse = model.mse_resid\n        rmse = mse ** 0.5\n\n        # Append the results to the evaluation_results list\n        evaluation_results.append({\n            'Model': f'Model_{i + 1}',\n            'Independent Variables': ', '.join(X_columns),\n            'R-Squared': r2,\n            'Adjusted R-Squared': adj_r2,\n            'MSE': mse,\n            'RMSE': rmse\n        })\n\n    # Convert the evaluation_results list to a DataFrame\n    results_df = pd.DataFrame(evaluation_results)\n    \n    return results_df\n\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\nresults_df = in_sample_performance_ols(models)\ndisplay(results_df)\n\n\n\n\n\n  \n    \n      \n      Model\n      Independent Variables\n      R-Squared\n      Adjusted R-Squared\n      MSE\n      RMSE\n    \n  \n  \n    \n      0\n      Model_1\n      FEDFUNDS\n      0.285734\n      0.284296\n      0.073824\n      0.271706\n    \n    \n      1\n      Model_2\n      const, FEDFUNDS\n      0.285602\n      0.284162\n      0.073943\n      0.271925\n    \n    \n      2\n      Model_3\n      GS10\n      0.528954\n      0.528007\n      0.048686\n      0.220649\n    \n    \n      3\n      Model_4\n      GS10, FEDFUNDS\n      0.621400\n      0.619873\n      0.039210\n      0.198015\n    \n    \n      4\n      Model_5\n      GS10, FEDFUNDS, UNRATE\n      0.621597\n      0.619304\n      0.039269\n      0.198163\n    \n  \n\n\n\n\nStep 2: In-Sample Forecast First Difference\n\ndef in_sample_forecast(models, observed_data, separate_subplots=False):\n    # Extract the observed data and dates\n    y = observed_data\n    x = observed_data.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data and in-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            X = model.model.exog\n            y_pred = model.predict(X)\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\n\n            # Get the independent variable names\n            ind_var_names = ', '.join(model.model.exog_names)\n\n            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i+1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data\n        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the in-sample predictions for each model\n        for i, model in enumerate(models):\n            X = model.model.exog\n            y_pred = model.predict(X)\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\n\n        # Customize the plot\n        plt.ylabel('Value')\n        plt.title('Observed Data and In-sample Predictions')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\nobserved_data = df_train_diff['MORTGAGE30US']\nin_sample_forecast(models, observed_data, separate_subplots=True)  # For separate subplots\n\n\n\n\nStep 3: In-Sample Forecast Levels\n\ndef in_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False):\n    # Extract the observed data (levels) and dates\n    y = original_data\n    x = original_data.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data (levels) and in-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            X = model.model.exog\n            y_diff_pred = model.predict(X)\n            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\n\n            # Get the independent variable names\n            ind_var_names = ', '.join(model.model.exog_names)\n\n            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i+1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.5, 0.04, 'DATE', ha='center')\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data (levels)\n        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the in-sample predictions for each model\n        for i, model in enumerate(models):\n            X = model.model.exog\n            y_diff_pred = model.predict(X)\n            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\n\n        # Customize the plot\n        plt.xlabel('DATE')\n        plt.ylabel('Value')\n        plt.title('Observed Data and In-sample Predictions (Levels)')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\noriginal_data = df_train['MORTGAGE30US']\ndiff_data = df_train_diff['MORTGAGE30US']\nin_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False)  # For a single plot with all series"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#model-evaluation",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#model-evaluation",
    "title": "ValidMind",
    "section": "4.6. Model Evaluation",
    "text": "4.6. Model Evaluation\n\n4.6.1. Out-of-Sample Analysis\n\nOut-of-Sample Performance\n\ndef out_of_sample_performance(model_list, model_names, test_data, target_col):\n    # Initialize a list to store results\n    results = []\n\n    for fitted_model, model_name in zip(model_list, model_names):\n        # Extract the column names of the independent variables from the model\n        independent_vars = fitted_model.model.exog_names\n\n        # Separate the target variable and features in the test dataset\n        X_test = test_data[independent_vars]\n        y_test = test_data[target_col]\n\n        # Predict the test data\n        y_pred = fitted_model.predict(X_test)\n\n        # Calculate the residuals\n        residuals = y_test - y_pred\n\n        # Calculate the mean squared error and root mean squared error\n        mse = np.mean(residuals ** 2)\n        rmse_val = np.sqrt(mse)\n\n        # Store the results\n        model_name_with_vars = f\"{model_name} ({', '.join(independent_vars)})\"\n        results.append([model_name_with_vars, mse, rmse_val])\n\n    # Create a DataFrame to display the results\n    results_df = pd.DataFrame(results, columns=['Model', 'MSE', 'RMSE'])\n\n    return results_df\n\n\nmodel_list = [model_3, model_4]\nmodel_names = ['model_3', 'model_4']\nresults_df = out_of_sample_performance(model_list, model_names=model_names, test_data=df_test_diff, target_col='MORTGAGE30US')\ndisplay(results_df)\n\n\n\n\n\n  \n    \n      \n      Model\n      MSE\n      RMSE\n    \n  \n  \n    \n      0\n      model_3 (GS10)\n      0.024310\n      0.155916\n    \n    \n      1\n      model_4 (GS10, FEDFUNDS)\n      0.027384\n      0.165482\n    \n  \n\n\n\n\n\n\nOut-of-Sample Forecast\n\ndef out_of_sample_forecast(models, model_names, test_data, target_col, separate_subplots=False):\n    # Extract the observed data and dates\n    y = test_data[target_col]\n    x = test_data.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data and out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred = model.predict(X_test)\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\n\n            ax.set_title(f'{model_names[i]} ({\", \".join(exog_names)})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i + 1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data\n        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred = model.predict(X_test)\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\n\n        # Customize the plot\n        plt.ylabel('Value')\n        plt.title('Observed Data and Out-of-sample Predictions')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\nmodels = [model_3, model_4]\nout_of_sample_forecast(models, model_names=['model_3', 'model_4'], test_data=df_test_diff, target_col='MORTGAGE30US', separate_subplots=True)\n\n\n\n\n\ndef out_of_sample_forecast_levels(models, model_names, test_data, original_data, target_col, separate_subplots=False):\n    # Extract the observed data and dates\n    y_test = test_data[target_col]\n    y_orig = original_data[original_data.index.isin(test_data.index)][target_col]\n    x = y_orig.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data and out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred_diff = model.predict(X_test)\n            y_pred = y_pred_diff + y_orig.shift(1).values\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\n\n            ax.set_title(f'{model_names[i]} ({\", \".join(exog_names)})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i + 1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data\n        sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred_diff = model.predict(X_test)\n            y_pred = y_pred_diff + y_orig.shift(1).values\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\n\n        # Customize the plot\n        plt.ylabel('Value')\n        plt.title('Observed Data and Out-of-sample Predictions')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\n\nmodels = [model_3, model_4]\nmodel_names = ['model_3', 'model_4']\noriginal_data = df_test\nout_of_sample_forecast_levels(models, model_names, test_data=df_test_diff, original_data=original_data, target_col='MORTGAGE30US', separate_subplots=False)\n\n\n\n\n\n\n\n4.6.2. Forecast Performance\n\nOne-Step Ahead Forecast\n\n\nFive-Step Ahead Forecast\n\n\n\n4.6.3. Scenario Analysis\n\nParallel Interest Rates Shocks\n\n\n\n4.6.4. Stress Testing\n\n\n4.6.5. Uncertainty Analysis"
  },
  {
    "objectID": "notebooks/time_series/explore_metrics.html",
    "href": "notebooks/time_series/explore_metrics.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Data libraries \nimport pandas as pd\n\n# ML libraries\nfrom numpy import argmax\nimport scipy.stats as stats\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\nfrom arch.unitroot import PhillipsPerron\nfrom arch.unitroot import ZivotAndrews\nfrom arch.unitroot import DFGLS\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport xgboost as xgb\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# ValidMind libraries \nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nimport validmind as vm\nfrom validmind.vm_models.test_context import TestContext\n\n\n# Quick hack to load local library code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\ndf = pd.read_csv(\"/Users/juanvalidmind/Dev/github/validmind/validmind-python/notebooks/datasets/lending_club_loan_rates.csv\", sep='\\t')\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\n\n# Remove diff columns\ncolumns_to_remove = [col for col in df.columns if col.startswith(\"diff\")]\ndf = df.drop(columns=columns_to_remove)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n    \n  \n\n\n\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgo0g0rt0000fjy6ozl9pb69\"\n)\n\nTrue\n\n\n\ntarget_variables = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column = target_variables   \n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                              Name                          Description                                      \n\n\nsklearn_classifier_metrics      SKLearnClassifierMetrics      Test plan for sklearn classifier metrics         \nsklearn_classifier_validation   SKLearnClassifierPerformance  Test plan for sklearn classifier models          \nsklearn_classifier              SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                  \ntabular_dataset                 TabularDataset                Test plan for generic tabular datasets           \ntabular_dataset_description     TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                  \ntabular_data_quality            TabularDataQuality            Test plan for data quality on tabular datasets   \nnormality_test_plan             NormalityTestPlan             Test plan to perform normality tests.            \nautocorrelation_test_plan       AutocorrelationTestPlan       Test plan to perform autocorrelation tests.      \nseasonality_test_plan           SesonalityTestPlan            Test plan to perform seasonality tests.          \nunit_root_test_plan             UnitRootTestPlan              Test plan to perform unit root tests.            \nstationarity_test_plan          StationarityTestPlan          Test plan to perform stationarity tests.         \ntimeseries                      TimeSeries                    Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                  \ntimeseries_univariate_inspectionTimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.\n\n\n\n\nCreate Train and Test Datasets\n\ntest_size = 30\ntrain_ds = df[:-test_size]\ntest_ds = df[-test_size:]\n\n\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\")\nvm_test_ds = vm.init_dataset(dataset=test_ds, type=\"generic\")\nvm_dataset = vm.init_dataset(dataset=df, type=\"generic\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nOriginal\n\nadftest = adfuller(df['loan_rate_A'])\nadftest\n\n(-1.917289312690944,\n 0.32397189281015515,\n 1,\n 135,\n {'1%': -3.479742586699182,\n  '5%': -2.88319822181578,\n  '10%': -2.578319684499314},\n -71.08908853191068)\n\n\nValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import ADF\ntest_context = TestContext(dataset=vm_dataset)\nmetric = ADF(test_context=test_context)\nmetric.run()\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='evaluation', scope='test', key='adf', value={'loan_rate_A': {'stat': -1.917289312690944, 'pvalue': 0.32397189281015515, 'usedlag': 1, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -71.08908853191068}, 'loan_rate_B': {'stat': -3.1599303710498425, 'pvalue': 0.022424413263559147, 'usedlag': 9, 'nobs': 127, 'critical_values': {'1%': -3.482920063655088, '5%': -2.884580323367261, '10%': -2.5790575441750883}, 'icbest': -42.45027033820841}, 'loan_rate_C': {'stat': -2.530666699941385, 'pvalue': 0.10818994357289696, 'usedlag': 1, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -92.19465856666866}, 'loan_rate_D': {'stat': -1.617158531178829, 'pvalue': 0.47421928207593467, 'usedlag': 6, 'nobs': 130, 'critical_values': {'1%': -3.4816817173418295, '5%': -2.8840418343195267, '10%': -2.578770059171598}, 'icbest': -4.9426661983780775}, 'FEDFUNDS': {'stat': -0.16854321128256927, 'pvalue': 0.9421687822974046, 'usedlag': 13, 'nobs': 123, 'critical_values': {'1%': -3.4846672514209773, '5%': -2.8853397507076006, '10%': -2.5794629869786503}, 'icbest': -338.8629171086535}}, value_formatter=None))\n\n\n\n\n\nOriginal\n\nkpsstest = kpss(df['loan_rate_A'])\nkpsstest\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n\n\n(1.012356679488042,\n 0.01,\n 6,\n {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739})\n\n\nValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import KPSSTest\ntest_context = TestContext(train_ds=vm_train_ds)\nmetric = KPSSTest(test_context=test_context)\nmetric.run()\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='evaluation', scope='test', key='kpss', value={'stat': 0.06862873540051123, 'pvalue': 0.1, 'usedlag': 8}, value_formatter='key_values'))\n\n\n\n\n\nOriginal\n\npp = PhillipsPerron(df['loan_rate_A'])\npp\n\n\n\nPhillips-Perron Test (Z-tau)\n\n  Test Statistic    -2.027\n\n\n  P-value            0.275\n\n\n  Lags                  13\n\nTrend: ConstantCritical Values: -3.48 (1%), -2.88 (5%), -2.58 (10%)Null Hypothesis: The process contains a unit root.Alternative Hypothesis: The process is weakly stationary.\n\n\n\npp.nobs\n\n136\n\n\nValidMind\n\n\n\nOriginal\n\nza = ZivotAndrews(df['loan_rate_A'])\nza\n\n\n\nZivot-Andrews Results\n\n  Test Statistic    -3.499\n\n\n  P-value            0.680\n\n\n  Lags                   1\n\nTrend: ConstantCritical Values: -5.28 (1%), -4.81 (5%), -4.57 (10%)Null Hypothesis: The process contains a unit root with a single structural break.Alternative Hypothesis: The process is trend and break stationary.\n\n\n\nza.nobs\n\n137\n\n\nValidMind\n\n\n\nOriginal\n\ndfgls = DFGLS(df['loan_rate_A'])\ndfgls\n\n\n\nDickey-Fuller GLS Results\n\n  Test Statistic    -1.798\n\n\n  P-value            0.071\n\n\n  Lags                   1\n\nTrend: ConstantCritical Values: -2.71 (1%), -2.09 (5%), -1.78 (10%)Null Hypothesis: The process contains a unit root.Alternative Hypothesis: The process is weakly stationary.\n\n\n\ndfgls.nobs\n\n135\n\n\nValidMind\n\n\n\nOff ValidMind\n\nsd = seasonal_decompose(df['loan_rate_A'])\nsd\n\n<statsmodels.tsa.seasonal.DecomposeResult at 0x28a928550>\n\n\n\nsd.trend\n\nDate\n2007-08-01   NaN\n2007-09-01   NaN\n2007-10-01   NaN\n2007-11-01   NaN\n2007-12-01   NaN\n              ..\n2018-08-01   NaN\n2018-09-01   NaN\n2018-10-01   NaN\n2018-11-01   NaN\n2018-12-01   NaN\nName: trend, Length: 137, dtype: float64\n\n\n\nfig, ax = plt.subplots()\n\n\n\n\nIn ValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalDecomposeMetricWithFigure\ntest_context = TestContext(train_ds=vm_train_ds)\nsd_metric = SeasonalDecomposeMetricWithFigure(test_context=test_context)\nsd_metric.run()\n\nTestPlanMetricResult(figures=[Figure(key='seasonal_decomposition_with_figure', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)], metric=MetricResult(type='evaluation', scope='', key='seasonal_decomposition_with_figure', value={'loan_rate_A': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.050284773390468294, 'resid': nan, 'observed': 7.7666666666666675}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.06087962072801919, 'resid': nan, 'observed': 7.841428571428572}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.01749661199350142, 'resid': nan, 'observed': 7.83}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.047258378330469496, 'resid': nan, 'observed': 7.779090909090908}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.0850517814632488, 'resid': nan, 'observed': 7.695833333333333}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.06564185816692857, 'resid': nan, 'observed': 7.961333333333333}, {'date': '2008-02-01', 'trend': 8.005048767959094, 'seasonal': 0.008943337297934315, 'resid': 0.1163412280763044, 'observed': 8.130333333333333}, {'date': '2008-03-01', 'trend': 8.036669799705125, 'seasonal': -0.002099404440702747, 'resid': 0.09171531902129201, 'observed': 8.126285714285714}, {'date': '2008-04-01', 'trend': 8.079229323514648, 'seasonal': 0.020168623084867585, 'resid': -0.007314613266182244, 'observed': 8.092083333333333}, {'date': '2008-05-01', 'trend': 8.144335744871071, 'seasonal': -0.01070385076597299, 'resid': 0.017796677323472867, 'observed': 8.151428571428571}, {'date': '2008-06-01', 'trend': 8.239462007497334, 'seasonal': -0.028819584167133785, 'resid': -0.005142423330199462, 'observed': 8.2055}, {'date': '2008-07-01', 'trend': 8.343548073071103, 'seasonal': 0.0027433998162858285, 'resid': -0.12585669027869376, 'observed': 8.220434782608695}, {'date': '2008-08-01', 'trend': 8.417007001892738, 'seasonal': -0.050284773390468294, 'resid': -0.0797222285022692, 'observed': 8.287}, {'date': '2008-09-01', 'trend': 8.47796981223055, 'seasonal': -0.06087962072801919, 'resid': -0.3370901915025301, 'observed': 8.08}, {'date': '2008-10-01', 'trend': 8.531636932259564, 'seasonal': 0.01749661199350142, 'resid': 0.06372359860407714, 'observed': 8.612857142857143}, {'date': '2008-11-01', 'trend': 8.589412987821856, 'seasonal': -0.047258378330469496, 'resid': 0.016633269296492413, 'observed': 8.558787878787879}, {'date': '2008-12-01', 'trend': 8.652802325527823, 'seasonal': 0.0850517814632488, 'resid': 0.4613125596755969, 'observed': 9.199166666666668}, {'date': '2009-01-01', 'trend': 8.714159305191414, 'seasonal': 0.06564185816692857, 'resid': 0.17626441041215057, 'observed': 8.956065573770493}, {'date': '2009-02-01', 'trend': 8.752934265042391, 'seasonal': 0.008943337297934315, 'resid': 0.13673778227506048, 'observed': 8.898615384615386}, {'date': '2009-03-01', 'trend': 8.775809794329618, 'seasonal': -0.002099404440702747, 'resid': 0.04740072122219648, 'observed': 8.821111111111112}, {'date': '2009-04-01', 'trend': 8.783591261550063, 'seasonal': 0.020168623084867585, 'resid': -0.11849106743062912, 'observed': 8.685268817204301}, {'date': '2009-05-01', 'trend': 8.768108091755868, 'seasonal': -0.01070385076597299, 'resid': 0.18746418006273688, 'observed': 8.944868421052632}, {'date': '2009-06-01', 'trend': 8.727065734609644, 'seasonal': -0.028819584167133785, 'resid': 0.23515810487663832, 'observed': 8.933404255319148}, {'date': '2009-07-01', 'trend': 8.660325582011376, 'seasonal': 0.0027433998162858285, 'resid': 0.30202905738802394, 'observed': 8.965098039215686}, {'date': '2009-08-01', 'trend': 8.568932625411962, 'seasonal': -0.050284773390468294, 'resid': -0.04571207220498089, 'observed': 8.472935779816513}, {'date': '2009-09-01', 'trend': 8.454561132788863, 'seasonal': -0.06087962072801919, 'resid': 0.04939541101607955, 'observed': 8.443076923076923}, {'date': '2009-10-01', 'trend': 8.34828280363049, 'seasonal': 0.01749661199350142, 'resid': 0.0707560174468764, 'observed': 8.436535433070867}, {'date': '2009-11-01', 'trend': 8.23694393241163, 'seasonal': -0.047258378330469496, 'resid': 0.1738279594323525, 'observed': 8.363513513513514}, {'date': '2009-12-01', 'trend': 8.116444096407664, 'seasonal': 0.0850517814632488, 'resid': 0.20792858256074262, 'observed': 8.409424460431655}, {'date': '2010-01-01', 'trend': 7.995096023438102, 'seasonal': 0.06564185816692857, 'resid': 0.0833062360420266, 'observed': 8.144044117647057}, {'date': '2010-02-01', 'trend': 7.889837683206626, 'seasonal': 0.008943337297934315, 'resid': -0.3815751381516201, 'observed': 7.517205882352941}, {'date': '2010-03-01', 'trend': 7.807926812466593, 'seasonal': -0.002099404440702747, 'resid': -0.34822261760672807, 'observed': 7.457604790419162}, {'date': '2010-04-01', 'trend': 7.704196284836861, 'seasonal': 0.020168623084867585, 'resid': -0.22626966982649002, 'observed': 7.498095238095239}, {'date': '2010-05-01', 'trend': 7.556635938363027, 'seasonal': -0.01070385076597299, 'resid': -0.08602299668796279, 'observed': 7.4599090909090915}, {'date': '2010-06-01', 'trend': 7.386339108881637, 'seasonal': -0.028819584167133785, 'resid': 0.1688479966530175, 'observed': 7.52636752136752}, {'date': '2010-07-01', 'trend': 7.234176921822315, 'seasonal': 0.0027433998162858285, 'resid': 0.22286070025920932, 'observed': 7.45978102189781}, {'date': '2010-08-01', 'trend': 7.139619393217407, 'seasonal': -0.050284773390468294, 'resid': 0.3627180117520081, 'observed': 7.452052631578947}, {'date': '2010-09-01', 'trend': 7.084157646540273, 'seasonal': -0.06087962072801919, 'resid': 0.47482114774146483, 'observed': 7.498099173553719}, {'date': '2010-10-01', 'trend': 7.0310647217648095, 'seasonal': 0.01749661199350142, 'resid': -0.1565808142777916, 'observed': 6.891980519480519}, {'date': '2010-11-01', 'trend': 6.991663604498002, 'seasonal': -0.047258378330469496, 'resid': -0.5777851144356879, 'observed': 6.366620111731844}, {'date': '2010-12-01', 'trend': 6.958236325031823, 'seasonal': 0.0850517814632488, 'resid': -0.7240941518351232, 'observed': 6.319193954659949}, {'date': '2011-01-01', 'trend': 6.9214056863686135, 'seasonal': 0.06564185816692857, 'resid': -0.40466541054050575, 'observed': 6.582382133995036}, {'date': '2011-02-01', 'trend': 6.889921185335679, 'seasonal': 0.008943337297934315, 'resid': -0.08937734314643411, 'observed': 6.80948717948718}, {'date': '2011-03-01', 'trend': 6.856889534975067, 'seasonal': -0.002099404440702747, 'resid': -0.020548557500655016, 'observed': 6.834241573033709}, {'date': '2011-04-01', 'trend': 6.858796195374667, 'seasonal': 0.020168623084867585, 'resid': -0.03173655758996877, 'observed': 6.847228260869565}, {'date': '2011-05-01', 'trend': 6.921615774690598, 'seasonal': -0.01070385076597299, 'resid': 0.25423732980671804, 'observed': 7.165149253731343}, {'date': '2011-06-01', 'trend': 7.018217147459357, 'seasonal': -0.028819584167133785, 'resid': 0.029475088064770363, 'observed': 7.018872651356993}, {'date': '2011-07-01', 'trend': 7.11041588830481, 'seasonal': 0.0027433998162858285, 'resid': -0.029818724129772076, 'observed': 7.083340563991324}, {'date': '2011-08-01', 'trend': 7.1830792783975985, 'seasonal': -0.050284773390468294, 'resid': -0.0599294403121207, 'observed': 7.0728650646950095}, {'date': '2011-09-01', 'trend': 7.247497405208671, 'seasonal': -0.06087962072801919, 'resid': -0.10209065269770515, 'observed': 7.084527131782947}, {'date': '2011-10-01', 'trend': 7.309379082976638, 'seasonal': 0.01749661199350142, 'resid': 0.024436715871515715, 'observed': 7.351312410841655}, {'date': '2011-11-01', 'trend': 7.353215232472669, 'seasonal': -0.047258378330469496, 'resid': 0.10900126981090005, 'observed': 7.4149581239531}, {'date': '2011-12-01', 'trend': 7.387931350546878, 'seasonal': 0.0850517814632488, 'resid': 0.11630575687876285, 'observed': 7.589288888888889}, {'date': '2012-01-01', 'trend': 7.433640607004464, 'seasonal': 0.06564185816692857, 'resid': 0.025774514885588032, 'observed': 7.525056980056981}, {'date': '2012-02-01', 'trend': 7.478542797820363, 'seasonal': 0.008943337297934315, 'resid': 0.12324756053387684, 'observed': 7.610733695652174}, {'date': '2012-03-01', 'trend': 7.5208057544537, 'seasonal': -0.002099404440702747, 'resid': 0.06032375032145173, 'observed': 7.579030100334449}, {'date': '2012-04-01', 'trend': 7.555975713482307, 'seasonal': 0.020168623084867585, 'resid': 0.011455663432825416, 'observed': 7.5876}, {'date': '2012-05-01', 'trend': 7.57810750100383, 'seasonal': -0.01070385076597299, 'resid': -0.09055854773216213, 'observed': 7.476845102505695}, {'date': '2012-06-01', 'trend': 7.596137974654016, 'seasonal': -0.028819584167133785, 'resid': -0.02695475412324547, 'observed': 7.540363636363637}, {'date': '2012-07-01', 'trend': 7.6153264121992175, 'seasonal': 0.0027433998162858285, 'resid': 0.04080192195124229, 'observed': 7.658871733966746}, {'date': '2012-08-01', 'trend': 7.628451463506009, 'seasonal': -0.050284773390468294, 'resid': -0.0031802158143688156, 'observed': 7.574986474301172}, {'date': '2012-09-01', 'trend': 7.638002595789809, 'seasonal': -0.06087962072801919, 'resid': 0.019593706315086022, 'observed': 7.596716681376876}, {'date': '2012-10-01', 'trend': 7.658826993137152, 'seasonal': 0.01749661199350142, 'resid': 0.006878272803619381, 'observed': 7.683201877934273}, {'date': '2012-11-01', 'trend': 7.677789303574363, 'seasonal': -0.047258378330469496, 'resid': -0.01629936786684364, 'observed': 7.61423155737705}, {'date': '2012-12-01', 'trend': 7.68098745426593, 'seasonal': 0.0850517814632488, 'resid': 0.0567075873402256, 'observed': 7.822746823069404}, {'date': '2013-01-01', 'trend': 7.6779520344633525, 'seasonal': 0.06564185816692857, 'resid': 0.008527654331044757, 'observed': 7.752121546961326}, {'date': '2013-02-01', 'trend': 7.677057001432264, 'seasonal': 0.008943337297934315, 'resid': 0.012670021380605325, 'observed': 7.698670360110803}, {'date': '2013-03-01', 'trend': 7.682469578470097, 'seasonal': -0.002099404440702747, 'resid': 0.039950436657629505, 'observed': 7.720320610687024}, {'date': '2013-04-01', 'trend': 7.691712575890414, 'seasonal': 0.020168623084867585, 'resid': 0.2342138270083863, 'observed': 7.946095025983668}, {'date': '2013-05-01', 'trend': 7.706156831040658, 'seasonal': -0.01070385076597299, 'resid': -0.12200745325962684, 'observed': 7.573445527015058}, {'date': '2013-06-01', 'trend': 7.718205987083048, 'seasonal': -0.028819584167133785, 'resid': -0.16886757446403147, 'observed': 7.520518828451883}, {'date': '2013-07-01', 'trend': 7.72646306080784, 'seasonal': 0.0027433998162858285, 'resid': -0.12333999400747178, 'observed': 7.605866466616654}, {'date': '2013-08-01', 'trend': 7.738107999623397, 'seasonal': -0.050284773390468294, 'resid': -0.08131227732781923, 'observed': 7.606510948905109}, {'date': '2013-09-01', 'trend': 7.7509664574560295, 'seasonal': -0.06087962072801919, 'resid': 0.005007218952953606, 'observed': 7.695094055680964}, {'date': '2013-10-01', 'trend': 7.755204227116213, 'seasonal': 0.01749661199350142, 'resid': 0.03395560260807626, 'observed': 7.806656441717791}, {'date': '2013-11-01', 'trend': 7.750869193258667, 'seasonal': -0.047258378330469496, 'resid': 0.13382830227119422, 'observed': 7.837439117199391}, {'date': '2013-12-01', 'trend': 7.741988403538943, 'seasonal': 0.0850517814632488, 'resid': 0.06167882326227134, 'observed': 7.888719008264463}, {'date': '2014-01-01', 'trend': 7.724369241349729, 'seasonal': 0.06564185816692857, 'resid': 0.09430803164457958, 'observed': 7.884319131161237}, {'date': '2014-02-01', 'trend': 7.7012496300205235, 'seasonal': 0.008943337297934315, 'resid': 0.1357583401657619, 'observed': 7.84595130748422}, {'date': '2014-03-01', 'trend': 7.667279037619656, 'seasonal': -0.002099404440702747, 'resid': 0.21646301811787813, 'observed': 7.881642651296831}, {'date': '2014-04-01', 'trend': 7.6252937267207255, 'seasonal': 0.020168623084867585, 'resid': 0.24101710741265112, 'observed': 7.886479457218244}, {'date': '2014-05-01', 'trend': 7.584095565289032, 'seasonal': -0.01070385076597299, 'resid': -0.044371431323670635, 'observed': 7.529020283199388}, {'date': '2014-06-01', 'trend': 7.536200793048272, 'seasonal': -0.028819584167133785, 'resid': -0.15557608988697594, 'observed': 7.351805118994163}, {'date': '2014-07-01', 'trend': 7.483981914831136, 'seasonal': 0.0027433998162858285, 'resid': -0.13500503111416087, 'observed': 7.351720283533261}, {'date': '2014-08-01', 'trend': 7.429040668396031, 'seasonal': -0.050284773390468294, 'resid': -0.07296943491799038, 'observed': 7.305786460087572}, {'date': '2014-09-01', 'trend': 7.36455133491674, 'seasonal': -0.06087962072801919, 'resid': -0.12314738731106348, 'observed': 7.1805243268776575}, {'date': '2014-10-01', 'trend': 7.294261771259082, 'seasonal': 0.01749661199350142, 'resid': 0.0018203256941895195, 'observed': 7.313578708946773}, {'date': '2014-11-01', 'trend': 7.234071423791347, 'seasonal': -0.047258378330469496, 'resid': 0.15494793014887884, 'observed': 7.341760975609756}, {'date': '2014-12-01', 'trend': 7.191857825197275, 'seasonal': 0.0850517814632488, 'resid': -0.04198699058463887, 'observed': 7.234922616075885}, {'date': '2015-01-01', 'trend': 7.15300977164523, 'seasonal': 0.06564185816692857, 'resid': 0.06621081632638982, 'observed': 7.284862446138549}, {'date': '2015-02-01', 'trend': 7.114847188355677, 'seasonal': 0.008943337297934315, 'resid': 0.0030275524107579964, 'observed': 7.126818078064369}, {'date': '2015-03-01', 'trend': 7.08538334834315, 'seasonal': -0.002099404440702747, 'resid': -0.030252066688752053, 'observed': 7.053031877213695}, {'date': '2015-04-01', 'trend': 7.054016766629, 'seasonal': 0.020168623084867585, 'resid': -0.04604468619627991, 'observed': 7.028140703517588}, {'date': '2015-05-01', 'trend': 7.013202303085584, 'seasonal': -0.01070385076597299, 'resid': -0.05970775464519265, 'observed': 6.942790697674418}, {'date': '2015-06-01', 'trend': 6.9738382666741785, 'seasonal': -0.028819584167133785, 'resid': -0.02011034424566108, 'observed': 6.924908338261384}, {'date': '2015-07-01', 'trend': 6.938361615724768, 'seasonal': 0.0027433998162858285, 'resid': -0.09484123652406432, 'observed': 6.846263779016989}, {'date': '2015-08-01', 'trend': 6.908688078173169, 'seasonal': -0.050284773390468294, 'resid': 0.03693766087185387, 'observed': 6.895340965654555}, {'date': '2015-09-01', 'trend': 6.87437653070801, 'seasonal': -0.06087962072801919, 'resid': 0.07034075103002743, 'observed': 6.883837661010018}, {'date': '2015-10-01', 'trend': 6.828130516887571, 'seasonal': 0.01749661199350142, 'resid': 0.011840284793749958, 'observed': 6.857467413674822}, {'date': '2015-11-01', 'trend': 6.8029077692831725, 'seasonal': -0.047258378330469496, 'resid': 0.06267575488702634, 'observed': 6.818325145839729}, {'date': '2015-12-01', 'trend': 6.809591954262458, 'seasonal': 0.0850517814632488, 'resid': -0.08102216375352447, 'observed': 6.813621571972182}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.06564185816692857, 'resid': nan, 'observed': 6.854723867456379}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.008943337297934315, 'resid': nan, 'observed': 6.844791755508173}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.002099404440702747, 'resid': nan, 'observed': 6.51158106060606}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.020168623084867585, 'resid': nan, 'observed': 6.459687188434696}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.01070385076597299, 'resid': nan, 'observed': 6.905898270251756}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.028819584167133785, 'resid': nan, 'observed': 7.12222120518688}], 'loan_rate_B': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.004683903074718656, 'resid': nan, 'observed': 9.497692307692308}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.02878733626807229, 'resid': nan, 'observed': 9.276666666666666}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.07292104494645359, 'resid': nan, 'observed': 9.433333333333334}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.03883917943439751, 'resid': nan, 'observed': 9.467777777777778}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.020002635551053742, 'resid': nan, 'observed': 9.3875}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.057087759887233934, 'resid': nan, 'observed': 9.693125}, {'date': '2008-02-01', 'trend': 9.815614077323866, 'seasonal': -0.0026761200794264053, 'resid': 0.22279237983421207, 'observed': 10.035730337078652}, {'date': '2008-03-01', 'trend': 9.903222857432645, 'seasonal': -0.00016982951529819455, 'resid': 0.15404374627620265, 'observed': 10.05709677419355}, {'date': '2008-04-01', 'trend': 10.006583968543756, 'seasonal': 3.19587595681934e-05, 'resid': 0.15461214287211555, 'observed': 10.16122807017544}, {'date': '2008-05-01', 'trend': 10.121889524099311, 'seasonal': -0.021465549087508966, 'resid': -0.022507308345133377, 'observed': 10.077916666666669}, {'date': '2008-06-01', 'trend': 10.267257038529324, 'seasonal': 0.0048551821613346344, 'resid': -0.20393040250884076, 'observed': 10.068181818181818}, {'date': '2008-07-01', 'trend': 10.43272381322504, 'seasonal': -0.07584606528025915, 'resid': -0.19872959979663263, 'observed': 10.158148148148149}, {'date': '2008-08-01', 'trend': 10.595628603740092, 'seasonal': -0.004683903074718656, 'resid': -0.14730833702901014, 'observed': 10.443636363636363}, {'date': '2008-09-01', 'trend': 10.760741481659695, 'seasonal': 0.02878733626807229, 'resid': -0.3561954845944334, 'observed': 10.433333333333334}, {'date': '2008-10-01', 'trend': 10.92033892958057, 'seasonal': 0.07292104494645359, 'resid': -0.2359266411936919, 'observed': 10.757333333333332}, {'date': '2008-11-01', 'trend': 11.06172533831073, 'seasonal': -0.03883917943439751, 'resid': -0.11177504776522074, 'observed': 10.911111111111111}, {'date': '2008-12-01', 'trend': 11.192890459921573, 'seasonal': -0.020002635551053742, 'resid': 0.26009918861649195, 'observed': 11.432987012987011}, {'date': '2009-01-01', 'trend': 11.319548541808823, 'seasonal': 0.057087759887233934, 'resid': 0.2422042780140867, 'observed': 11.618840579710144}, {'date': '2009-02-01', 'trend': 11.435830739857689, 'seasonal': -0.0026761200794264053, 'resid': 0.5865751099514681, 'observed': 12.01972972972973}, {'date': '2009-03-01', 'trend': 11.551304435676638, 'seasonal': -0.00016982951529819455, 'resid': 0.4846718454515647, 'observed': 12.035806451612904}, {'date': '2009-04-01', 'trend': 11.656503680463528, 'seasonal': 3.19587595681934e-05, 'resid': 0.35632150363404713, 'observed': 12.012857142857143}, {'date': '2009-05-01', 'trend': 11.74012123279847, 'seasonal': -0.021465549087508966, 'resid': -0.09909428020218936, 'observed': 11.619561403508772}, {'date': '2009-06-01', 'trend': 11.79583062627412, 'seasonal': 0.0048551821613346344, 'resid': -0.1261858084354538, 'observed': 11.6745}, {'date': '2009-07-01', 'trend': 11.813709500580932, 'seasonal': -0.07584606528025915, 'resid': -0.14623950367674132, 'observed': 11.591623931623932}, {'date': '2009-08-01', 'trend': 11.7577233646958, 'seasonal': -0.004683903074718656, 'resid': 0.047893871712256, 'observed': 11.800933333333337}, {'date': '2009-09-01', 'trend': 11.645087849532663, 'seasonal': 0.02878733626807229, 'resid': 0.17352987749040427, 'observed': 11.84740506329114}, {'date': '2009-10-01', 'trend': 11.531122201859514, 'seasonal': 0.07292104494645359, 'resid': 0.2640002314549018, 'observed': 11.86804347826087}, {'date': '2009-11-01', 'trend': 11.436971193428466, 'seasonal': -0.03883917943439751, 'resid': 0.40909020822815567, 'observed': 11.807222222222224}, {'date': '2009-12-01', 'trend': 11.380949115633296, 'seasonal': -0.020002635551053742, 'resid': 0.5129548652092385, 'observed': 11.87390134529148}, {'date': '2010-01-01', 'trend': 11.346807882383448, 'seasonal': 0.057087759887233934, 'resid': 0.20312358849855025, 'observed': 11.607019230769232}, {'date': '2010-02-01', 'trend': 11.308341582352016, 'seasonal': -0.0026761200794264053, 'resid': -0.6177816448452021, 'observed': 10.687883817427387}, {'date': '2010-03-01', 'trend': 11.25711953661246, 'seasonal': -0.00016982951529819455, 'resid': -0.5925497070971618, 'observed': 10.6644}, {'date': '2010-04-01', 'trend': 11.175070289518446, 'seasonal': 3.19587595681934e-05, 'resid': -0.5260141979635501, 'observed': 10.649088050314464}, {'date': '2010-05-01', 'trend': 11.034948084615989, 'seasonal': -0.021465549087508966, 'resid': -0.28977624182218864, 'observed': 10.723706293706291}, {'date': '2010-06-01', 'trend': 10.85903782999656, 'seasonal': 0.0048551821613346344, 'resid': 0.36193223056055446, 'observed': 11.225825242718448}, {'date': '2010-07-01', 'trend': 10.708866411920479, 'seasonal': -0.07584606528025915, 'resid': 0.5878887442688713, 'observed': 11.22090909090909}, {'date': '2010-08-01', 'trend': 10.639141951735194, 'seasonal': -0.004683903074718656, 'resid': 0.6139989246332928, 'observed': 11.248456973293768}, {'date': '2010-09-01', 'trend': 10.622033529095082, 'seasonal': 0.02878733626807229, 'resid': 0.5197314602182407, 'observed': 11.170552325581395}, {'date': '2010-10-01', 'trend': 10.605833809461833, 'seasonal': 0.07292104494645359, 'resid': -0.10304056869400115, 'observed': 10.575714285714286}, {'date': '2010-11-01', 'trend': 10.613230598722458, 'seasonal': -0.03883917943439751, 'resid': -0.8377729221782316, 'observed': 9.736618497109829}, {'date': '2010-12-01', 'trend': 10.624008511489265, 'seasonal': -0.020002635551053742, 'resid': -0.8813469164006392, 'observed': 9.722658959537572}, {'date': '2011-01-01', 'trend': 10.61271994083742, 'seasonal': 0.057087759887233934, 'resid': -0.5156601180274538, 'observed': 10.1541475826972}, {'date': '2011-02-01', 'trend': 10.599581112470494, 'seasonal': -0.0026761200794264053, 'resid': -0.1295365713384361, 'observed': 10.467368421052631}, {'date': '2011-03-01', 'trend': 10.596316101806579, 'seasonal': -0.00016982951529819455, 'resid': -0.12183301927923353, 'observed': 10.474313253012047}, {'date': '2011-04-01', 'trend': 10.642422990347459, 'seasonal': 3.19587595681934e-05, 'resid': -0.19207342300261188, 'observed': 10.450381526104415}, {'date': '2011-05-01', 'trend': 10.760849905635485, 'seasonal': -0.021465549087508966, 'resid': 0.3605514036233296, 'observed': 11.099935760171306}, {'date': '2011-06-01', 'trend': 10.91443738038156, 'seasonal': 0.0048551821613346344, 'resid': 0.18897312011393141, 'observed': 11.108265682656826}, {'date': '2011-07-01', 'trend': 11.0539560569123, 'seasonal': -0.07584606528025915, 'resid': 0.08943296369441832, 'observed': 11.06754295532646}, {'date': '2011-08-01', 'trend': 11.162766577022664, 'seasonal': -0.004683903074718656, 'resid': -0.07159144587776857, 'observed': 11.086491228070177}, {'date': '2011-09-01', 'trend': 11.268679036144619, 'seasonal': 0.02878733626807229, 'resid': -0.043308557541674626, 'observed': 11.254157814871016}, {'date': '2011-10-01', 'trend': 11.388977923898196, 'seasonal': 0.07292104494645359, 'resid': 0.13677515256110262, 'observed': 11.598674121405752}, {'date': '2011-11-01', 'trend': 11.488625115138078, 'seasonal': -0.03883917943439751, 'resid': 0.10611869262731599, 'observed': 11.555904628330996}, {'date': '2011-12-01', 'trend': 11.570384807610292, 'seasonal': -0.020002635551053742, 'resid': 0.039090050162983274, 'observed': 11.589472222222222}, {'date': '2012-01-01', 'trend': 11.66228664035388, 'seasonal': 0.057087759887233934, 'resid': -0.08359184349081586, 'observed': 11.635782556750298}, {'date': '2012-02-01', 'trend': 11.76319166689917, 'seasonal': -0.0026761200794264053, 'resid': -0.16332961717150074, 'observed': 11.597185929648242}, {'date': '2012-03-01', 'trend': 11.860822521307425, 'seasonal': -0.00016982951529819455, 'resid': 0.025742071551277383, 'observed': 11.886394763343404}, {'date': '2012-04-01', 'trend': 11.934068361018747, 'seasonal': 3.19587595681934e-05, 'resid': -0.008626997919452104, 'observed': 11.925473321858863}, {'date': '2012-05-01', 'trend': 11.99482340535943, 'seasonal': -0.021465549087508966, 'resid': 0.04301869790214507, 'observed': 12.016376554174066}, {'date': '2012-06-01', 'trend': 12.048930270550104, 'seasonal': 0.0048551821613346344, 'resid': 0.10027205527578197, 'observed': 12.15405750798722}, {'date': '2012-07-01', 'trend': 12.091371298420814, 'seasonal': -0.07584606528025915, 'resid': 0.21186988270164997, 'observed': 12.227395115842205}, {'date': '2012-08-01', 'trend': 12.13362143172545, 'seasonal': -0.004683903074718656, 'resid': 0.21942217599061886, 'observed': 12.34835970464135}, {'date': '2012-09-01', 'trend': 12.161887323854964, 'seasonal': 0.02878733626807229, 'resid': 0.1447551839749598, 'observed': 12.335429844097996}, {'date': '2012-10-01', 'trend': 12.17420943046534, 'seasonal': 0.07292104494645359, 'resid': 0.028171769838638658, 'observed': 12.275302245250431}, {'date': '2012-11-01', 'trend': 12.181736211600096, 'seasonal': -0.03883917943439751, 'resid': 0.19450053649706522, 'observed': 12.337397568662764}, {'date': '2012-12-01', 'trend': 12.175118735274879, 'seasonal': -0.020002635551053742, 'resid': -0.04857205325722414, 'observed': 12.1065440464666}, {'date': '2013-01-01', 'trend': 12.130128219115964, 'seasonal': 0.057087759887233934, 'resid': -0.04992057760023518, 'observed': 12.137295401402962}, {'date': '2013-02-01', 'trend': 12.05328151565516, 'seasonal': -0.0026761200794264053, 'resid': 0.059070888731092334, 'observed': 12.109676284306826}, {'date': '2013-03-01', 'trend': 11.977871938153276, 'seasonal': -0.00016982951529819455, 'resid': 0.07458371115522532, 'observed': 12.052285819793203}, {'date': '2013-04-01', 'trend': 11.921326746462128, 'seasonal': 3.19587595681934e-05, 'resid': 0.1339541188363741, 'observed': 12.05531282405807}, {'date': '2013-05-01', 'trend': 11.881069792553062, 'seasonal': -0.021465549087508966, 'resid': 0.20757555574345343, 'observed': 12.067179799209006}, {'date': '2013-06-01', 'trend': 11.854568105629241, 'seasonal': 0.0048551821613346344, 'resid': 0.08501154335650916, 'observed': 11.944434831147085}, {'date': '2013-07-01', 'trend': 11.836843225112068, 'seasonal': -0.07584606528025915, 'resid': -0.40375175496345206, 'observed': 11.357245404868356}, {'date': '2013-08-01', 'trend': 11.814459446444463, 'seasonal': -0.004683903074718656, 'resid': -0.43558701081386386, 'observed': 11.37418853255588}, {'date': '2013-09-01', 'trend': 11.789602866611254, 'seasonal': 0.02878733626807229, 'resid': -0.318619046741066, 'observed': 11.49977115613826}, {'date': '2013-10-01', 'trend': 11.76503003465179, 'seasonal': 0.07292104494645359, 'resid': -0.08407474697564411, 'observed': 11.7538763326226}, {'date': '2013-11-01', 'trend': 11.719249496700483, 'seasonal': -0.03883917943439751, 'resid': 0.2122462702069169, 'observed': 11.892656587473002}, {'date': '2013-12-01', 'trend': 11.646817938065396, 'seasonal': -0.020002635551053742, 'resid': 0.2884292389703734, 'observed': 11.915244541484716}, {'date': '2014-01-01', 'trend': 11.592579216683331, 'seasonal': 0.057087759887233934, 'resid': 0.25353079740203666, 'observed': 11.903197773972602}, {'date': '2014-02-01', 'trend': 11.558608636178356, 'seasonal': -0.0026761200794264053, 'resid': 0.25063070761575534, 'observed': 11.806563223714685}, {'date': '2014-03-01', 'trend': 11.516112499599217, 'seasonal': -0.00016982951529819455, 'resid': 0.2428982943044916, 'observed': 11.75884096438841}, {'date': '2014-04-01', 'trend': 11.460639561456194, 'seasonal': 3.19587595681934e-05, 'resid': 0.2983381922199614, 'observed': 11.759009712435724}, {'date': '2014-05-01', 'trend': 11.375487965050318, 'seasonal': -0.021465549087508966, 'resid': -0.0892724159628101, 'observed': 11.26475}, {'date': '2014-06-01', 'trend': 11.261071150944403, 'seasonal': 0.0048551821613346344, 'resid': -0.2574191099917732, 'observed': 11.008507223113964}, {'date': '2014-07-01', 'trend': 11.142598337443784, 'seasonal': -0.07584606528025915, 'resid': -0.0753085724316206, 'observed': 10.991443699731905}, {'date': '2014-08-01', 'trend': 11.019433954700187, 'seasonal': -0.004683903074718656, 'resid': -0.09005374605251894, 'observed': 10.92469630557295}, {'date': '2014-09-01', 'trend': 10.88433692264195, 'seasonal': 0.02878733626807229, 'resid': 0.016231846311807734, 'observed': 10.92935610522183}, {'date': '2014-10-01', 'trend': 10.744093252808314, 'seasonal': 0.07292104494645359, 'resid': 0.17592657035175308, 'observed': 10.99294086810652}, {'date': '2014-11-01', 'trend': 10.623740180200944, 'seasonal': -0.03883917943439751, 'resid': 0.025052737481477398, 'observed': 10.609953738248024}, {'date': '2014-12-01', 'trend': 10.532876719632691, 'seasonal': -0.020002635551053742, 'resid': -0.06093023191390368, 'observed': 10.451943852167734}, {'date': '2015-01-01', 'trend': 10.449153446358014, 'seasonal': 0.057087759887233934, 'resid': 0.016909733029543995, 'observed': 10.523150939274792}, {'date': '2015-02-01', 'trend': 10.365652693622092, 'seasonal': -0.0026761200794264053, 'resid': -0.1323117009765743, 'observed': 10.230664872566091}, {'date': '2015-03-01', 'trend': 10.28402776621621, 'seasonal': -0.00016982951529819455, 'resid': -0.1914473905615518, 'observed': 10.09241054613936}, {'date': '2015-04-01', 'trend': 10.19996188279399, 'seasonal': 3.19587595681934e-05, 'resid': -0.14040178687606952, 'observed': 10.05959205467749}, {'date': '2015-05-01', 'trend': 10.131545323404618, 'seasonal': -0.021465549087508966, 'resid': -0.03438585913579207, 'observed': 10.075693915181317}, {'date': '2015-06-01', 'trend': 10.084529148703163, 'seasonal': 0.0048551821613346344, 'resid': -0.07254407656989471, 'observed': 10.016840254294603}, {'date': '2015-07-01', 'trend': 10.038650783234946, 'seasonal': -0.07584606528025915, 'resid': 0.010947392004321585, 'observed': 9.973752109959008}, {'date': '2015-08-01', 'trend': 10.003718613520608, 'seasonal': -0.004683903074718656, 'resid': -0.06066488076219157, 'observed': 9.938369829683698}, {'date': '2015-09-01', 'trend': 9.98791271441926, 'seasonal': 0.02878733626807229, 'resid': -0.06001572731742389, 'observed': 9.956684323369908}, {'date': '2015-10-01', 'trend': 9.98083261842099, 'seasonal': 0.07292104494645359, 'resid': -0.1057222155422445, 'observed': 9.9480314478252}, {'date': '2015-11-01', 'trend': 9.973055835915497, 'seasonal': -0.03883917943439751, 'resid': 0.07864907670333576, 'observed': 10.012865733184436}, {'date': '2015-12-01', 'trend': 9.974260889533937, 'seasonal': -0.020002635551053742, 'resid': -0.033614589586505546, 'observed': 9.920643664396378}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.057087759887233934, 'resid': nan, 'observed': 9.953370355808923}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.0026761200794264053, 'resid': nan, 'observed': 9.962073382887873}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.00016982951529819455, 'resid': nan, 'observed': 9.981660457385225}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 3.19587595681934e-05, 'resid': nan, 'observed': 10.000419839473144}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.021465549087508966, 'resid': nan, 'observed': 9.948223350253809}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.0048551821613346344, 'resid': nan, 'observed': 10.17323210606468}], 'loan_rate_C': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.015376141878352585, 'resid': nan, 'observed': 10.9475}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.04035308860321538, 'resid': nan, 'observed': 10.829166666666666}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.08058524115091258, 'resid': nan, 'observed': 10.825925925925926}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05290731070035628, 'resid': nan, 'observed': 10.967037037037038}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.02602059045270812, 'resid': nan, 'observed': 10.805}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.03449672569012794, 'resid': nan, 'observed': 11.288055555555555}, {'date': '2008-02-01', 'trend': 11.327619414408064, 'seasonal': 0.022574335467601885, 'resid': 0.17544261376069673, 'observed': 11.525636363636362}, {'date': '2008-03-01', 'trend': 11.40659510885251, 'seasonal': 0.010641559838160935, 'resid': 0.1864414922288681, 'observed': 11.60367816091954}, {'date': '2008-04-01', 'trend': 11.501532490144058, 'seasonal': -0.006495259466358668, 'resid': 0.1185565193223005, 'observed': 11.61359375}, {'date': '2008-05-01', 'trend': 11.613468328225728, 'seasonal': -0.02756787427454523, 'resid': 0.04891436086363189, 'observed': 11.634814814814815}, {'date': '2008-06-01', 'trend': 11.752703752291348, 'seasonal': -0.024120179891552147, 'resid': -0.07946592534097095, 'observed': 11.649117647058825}, {'date': '2008-07-01', 'trend': 11.912461763337886, 'seasonal': -0.03616359408614569, 'resid': -0.09501611796969031, 'observed': 11.78128205128205}, {'date': '2008-08-01', 'trend': 12.059014162327783, 'seasonal': -0.015376141878352585, 'resid': -0.17488802044942972, 'observed': 11.86875}, {'date': '2008-09-01', 'trend': 12.193218649730547, 'seasonal': 0.04035308860321538, 'resid': -0.43023840500042565, 'observed': 11.803333333333336}, {'date': '2008-10-01', 'trend': 12.322309884211464, 'seasonal': 0.08058524115091258, 'resid': -0.27263871510596693, 'observed': 12.13025641025641}, {'date': '2008-11-01', 'trend': 12.448988871382019, 'seasonal': -0.05290731070035628, 'resid': -0.0469148940149938, 'observed': 12.349166666666669}, {'date': '2008-12-01', 'trend': 12.57335810715044, 'seasonal': -0.02602059045270812, 'resid': 0.21718303124747265, 'observed': 12.764520547945205}, {'date': '2009-01-01', 'trend': 12.688007401986994, 'seasonal': 0.03449672569012794, 'resid': 0.4402231450501505, 'observed': 13.162727272727272}, {'date': '2009-02-01', 'trend': 12.803605690204769, 'seasonal': 0.022574335467601885, 'resid': 0.34204219654984847, 'observed': 13.16822222222222}, {'date': '2009-03-01', 'trend': 12.93876053448086, 'seasonal': 0.010641559838160935, 'resid': 0.23259790568097963, 'observed': 13.182}, {'date': '2009-04-01', 'trend': 13.068278952698869, 'seasonal': -0.006495259466358668, 'resid': 0.07167784522903088, 'observed': 13.13346153846154}, {'date': '2009-05-01', 'trend': 13.172533700281956, 'seasonal': -0.02756787427454523, 'resid': 0.010276892439188185, 'observed': 13.1552427184466}, {'date': '2009-06-01', 'trend': 13.251917774111583, 'seasonal': -0.024120179891552147, 'resid': -0.11424619235087095, 'observed': 13.11355140186916}, {'date': '2009-07-01', 'trend': 13.296720793031207, 'seasonal': -0.03616359408614569, 'resid': -0.19212582639604223, 'observed': 13.06843137254902}, {'date': '2009-08-01', 'trend': 13.317664880122448, 'seasonal': -0.015376141878352585, 'resid': 0.053670857715500855, 'observed': 13.355959595959597}, {'date': '2009-09-01', 'trend': 13.331744536272288, 'seasonal': 0.04035308860321538, 'resid': 0.18774237512449657, 'observed': 13.55984}, {'date': '2009-10-01', 'trend': 13.350328926794504, 'seasonal': 0.08058524115091258, 'resid': 0.05127761287650147, 'observed': 13.482191780821918}, {'date': '2009-11-01', 'trend': 13.373796709193448, 'seasonal': -0.05290731070035628, 'resid': 0.17845583960214642, 'observed': 13.499345238095238}, {'date': '2009-12-01', 'trend': 13.411937154210145, 'seasonal': -0.02602059045270812, 'resid': 0.13364318467023942, 'observed': 13.519559748427676}, {'date': '2010-01-01', 'trend': 13.47182703139438, 'seasonal': 0.03449672569012794, 'resid': -0.023363230768720024, 'observed': 13.482960526315788}, {'date': '2010-02-01', 'trend': 13.524905311746926, 'seasonal': 0.022574335467601885, 'resid': -0.1968325883909979, 'observed': 13.35064705882353}, {'date': '2010-03-01', 'trend': 13.554786033403438, 'seasonal': 0.010641559838160935, 'resid': -0.22794068224683492, 'observed': 13.337486910994764}, {'date': '2010-04-01', 'trend': 13.565108944517233, 'seasonal': -0.006495259466358668, 'resid': -0.13461368505087465, 'observed': 13.424}, {'date': '2010-05-01', 'trend': 13.542554509996647, 'seasonal': -0.02756787427454523, 'resid': -0.08705560123934074, 'observed': 13.42793103448276}, {'date': '2010-06-01', 'trend': 13.496382208505665, 'seasonal': -0.024120179891552147, 'resid': 0.28397173761965344, 'observed': 13.756233766233766}, {'date': '2010-07-01', 'trend': 13.45590861730361, 'seasonal': -0.03616359408614569, 'resid': 0.44336103738859645, 'observed': 13.86310606060606}, {'date': '2010-08-01', 'trend': 13.436416752050274, 'seasonal': -0.015376141878352585, 'resid': 0.4141230261917138, 'observed': 13.835163636363635}, {'date': '2010-09-01', 'trend': 13.43336842161868, 'seasonal': 0.04035308860321538, 'resid': 0.3240517691303332, 'observed': 13.797773279352228}, {'date': '2010-10-01', 'trend': 13.424416386656207, 'seasonal': 0.08058524115091258, 'resid': -0.01299325960628371, 'observed': 13.492008368200835}, {'date': '2010-11-01', 'trend': 13.432512703846879, 'seasonal': -0.05290731070035628, 'resid': -0.4313831709243, 'observed': 12.948222222222222}, {'date': '2010-12-01', 'trend': 13.453768023204553, 'seasonal': -0.02602059045270812, 'resid': -0.4651999042347351, 'observed': 12.96254752851711}, {'date': '2011-01-01', 'trend': 13.459765699171086, 'seasonal': 0.03449672569012794, 'resid': -0.42565586748416406, 'observed': 13.06860655737705}, {'date': '2011-02-01', 'trend': 13.460094510618998, 'seasonal': 0.022574335467601885, 'resid': -0.18547258440435596, 'observed': 13.297196261682243}, {'date': '2011-03-01', 'trend': 13.474198015666648, 'seasonal': 0.010641559838160935, 'resid': -0.16706179772703109, 'observed': 13.317777777777778}, {'date': '2011-04-01', 'trend': 13.527026690304142, 'seasonal': -0.006495259466358668, 'resid': -0.2916711367201349, 'observed': 13.228860294117649}, {'date': '2011-05-01', 'trend': 13.626640663310605, 'seasonal': -0.02756787427454523, 'resid': 0.21830956390511572, 'observed': 13.817382352941175}, {'date': '2011-06-01', 'trend': 13.752231546995366, 'seasonal': -0.024120179891552147, 'resid': 0.14879874525573725, 'observed': 13.876910112359552}, {'date': '2011-07-01', 'trend': 13.87357241867144, 'seasonal': -0.03616359408614569, 'resid': 0.048965113091762456, 'observed': 13.886373937677057}, {'date': '2011-08-01', 'trend': 13.98461653130225, 'seasonal': -0.015376141878352585, 'resid': -0.1494531553813426, 'observed': 13.819787234042554}, {'date': '2011-09-01', 'trend': 14.100343020217863, 'seasonal': 0.04035308860321538, 'resid': 0.010937693995820695, 'observed': 14.1516338028169}, {'date': '2011-10-01', 'trend': 14.23031804536025, 'seasonal': 0.08058524115091258, 'resid': 0.09513274952487473, 'observed': 14.406036036036037}, {'date': '2011-11-01', 'trend': 14.339700465210372, 'seasonal': -0.05290731070035628, 'resid': 0.13813675203204037, 'observed': 14.424929906542056}, {'date': '2011-12-01', 'trend': 14.423699396916051, 'seasonal': -0.02602059045270812, 'resid': 0.1023422461682363, 'observed': 14.50002105263158}, {'date': '2012-01-01', 'trend': 14.525801405280475, 'seasonal': 0.03449672569012794, 'resid': -0.11698417748223075, 'observed': 14.443313953488373}, {'date': '2012-02-01', 'trend': 14.653494230019042, 'seasonal': 0.022574335467601885, 'resid': -0.08852099677628465, 'observed': 14.58754756871036}, {'date': '2012-03-01', 'trend': 14.776156584948811, 'seasonal': 0.010641559838160935, 'resid': 0.018064059937437444, 'observed': 14.80486220472441}, {'date': '2012-04-01', 'trend': 14.874257215713214, 'seasonal': -0.006495259466358668, 'resid': -0.006585485658620364, 'observed': 14.861176470588235}, {'date': '2012-05-01', 'trend': 14.960668914357903, 'seasonal': -0.02756787427454523, 'resid': -0.1228567872097933, 'observed': 14.810244252873565}, {'date': '2012-06-01', 'trend': 15.055737285304788, 'seasonal': -0.024120179891552147, 'resid': -0.13159453204980495, 'observed': 14.900022573363431}, {'date': '2012-07-01', 'trend': 15.163703517893621, 'seasonal': -0.03616359408614569, 'resid': 0.18616975361187818, 'observed': 15.313709677419354}, {'date': '2012-08-01', 'trend': 15.269229382631286, 'seasonal': -0.015376141878352585, 'resid': 0.20322604727295804, 'observed': 15.457079288025891}, {'date': '2012-09-01', 'trend': 15.360602120013258, 'seasonal': 0.04035308860321538, 'resid': 0.057283058531540164, 'observed': 15.458238267148014}, {'date': '2012-10-01', 'trend': 15.441987157155793, 'seasonal': 0.08058524115091258, 'resid': -0.06872568825608992, 'observed': 15.453846710050616}, {'date': '2012-11-01', 'trend': 15.522336244532635, 'seasonal': -0.05290731070035628, 'resid': -0.018428933832278373, 'observed': 15.451}, {'date': '2012-12-01', 'trend': 15.600145184438405, 'seasonal': -0.02602059045270812, 'resid': 0.1814672679131936, 'observed': 15.75559186189889}, {'date': '2013-01-01', 'trend': 15.634210920934038, 'seasonal': 0.03449672569012794, 'resid': 0.11022507972889475, 'observed': 15.77893272635306}, {'date': '2013-02-01', 'trend': 15.620066570421177, 'seasonal': 0.022574335467601885, 'resid': 0.14190864366076897, 'observed': 15.784549549549547}, {'date': '2013-03-01', 'trend': 15.605915442823965, 'seasonal': 0.010641559838160935, 'resid': 0.18424891839050375, 'observed': 15.80080592105263}, {'date': '2013-04-01', 'trend': 15.611799860495061, 'seasonal': -0.006495259466358668, 'resid': 0.2131690446521172, 'observed': 15.81847364568082}, {'date': '2013-05-01', 'trend': 15.625247369996853, 'seasonal': -0.02756787427454523, 'resid': 0.18364567910286636, 'observed': 15.781325174825174}, {'date': '2013-06-01', 'trend': 15.604135339634787, 'seasonal': -0.024120179891552147, 'resid': 0.21634104940709334, 'observed': 15.796356209150328}, {'date': '2013-07-01', 'trend': 15.547031858857277, 'seasonal': -0.03616359408614569, 'resid': -0.2759145472435113, 'observed': 15.23495371752762}, {'date': '2013-08-01', 'trend': 15.47551431977326, 'seasonal': -0.015376141878352585, 'resid': -0.263767342285952, 'observed': 15.196370835608956}, {'date': '2013-09-01', 'trend': 15.388764348476775, 'seasonal': 0.04035308860321538, 'resid': -0.04979777984810272, 'observed': 15.379319657231887}, {'date': '2013-10-01', 'trend': 15.294595030065839, 'seasonal': 0.08058524115091258, 'resid': 0.298811072856343, 'observed': 15.673991344073094}, {'date': '2013-11-01', 'trend': 15.181142980694998, 'seasonal': -0.05290731070035628, 'resid': 0.42535992402581335, 'observed': 15.553595594020456}, {'date': '2013-12-01', 'trend': 15.037494206306443, 'seasonal': -0.02602059045270812, 'resid': 0.13483392333511915, 'observed': 15.146307539188854}, {'date': '2014-01-01', 'trend': 14.907041932227768, 'seasonal': 0.03449672569012794, 'resid': 0.07619485248493643, 'observed': 15.017733510402833}, {'date': '2014-02-01', 'trend': 14.801318577692147, 'seasonal': 0.022574335467601885, 'resid': 0.00543491432361955, 'observed': 14.829327827483368}, {'date': '2014-03-01', 'trend': 14.68834291186608, 'seasonal': 0.010641559838160935, 'resid': -0.024956139701048135, 'observed': 14.674028332003193}, {'date': '2014-04-01', 'trend': 14.55425093073973, 'seasonal': -0.006495259466358668, 'resid': 0.13743192159438591, 'observed': 14.685187592867758}, {'date': '2014-05-01', 'trend': 14.399014204506061, 'seasonal': -0.02756787427454523, 'resid': -0.17968428749337514, 'observed': 14.19176204273814}, {'date': '2014-06-01', 'trend': 14.251438055411992, 'seasonal': -0.024120179891552147, 'resid': -0.28896911960845106, 'observed': 13.938348755911989}, {'date': '2014-07-01', 'trend': 14.12541654679463, 'seasonal': -0.03616359408614569, 'resid': -0.12714635983071618, 'observed': 13.962106592877769}, {'date': '2014-08-01', 'trend': 14.00546444533696, 'seasonal': -0.015376141878352585, 'resid': -0.05823085205471999, 'observed': 13.931857451403888}, {'date': '2014-09-01', 'trend': 13.888750175213392, 'seasonal': 0.04035308860321538, 'resid': 0.0033137977947689454, 'observed': 13.932417061611376}, {'date': '2014-10-01', 'trend': 13.776028499486884, 'seasonal': 0.08058524115091258, 'resid': 0.04607265202341461, 'observed': 13.902686392661211}, {'date': '2014-11-01', 'trend': 13.683189812670596, 'seasonal': -0.05290731070035628, 'resid': -0.031063386145972684, 'observed': 13.599219115824267}, {'date': '2014-12-01', 'trend': 13.620293788031292, 'seasonal': -0.02602059045270812, 'resid': -0.03541675845120716, 'observed': 13.558856439127377}, {'date': '2015-01-01', 'trend': 13.567226974222327, 'seasonal': 0.03449672569012794, 'resid': -0.021055296264795055, 'observed': 13.58066840364766}, {'date': '2015-02-01', 'trend': 13.513730927922582, 'seasonal': 0.022574335467601885, 'resid': -0.14876276413578457, 'observed': 13.3875424992544}, {'date': '2015-03-01', 'trend': 13.460183939403779, 'seasonal': 0.010641559838160935, 'resid': -0.15615432197536414, 'observed': 13.314671177266575}, {'date': '2015-04-01', 'trend': 13.408445378415205, 'seasonal': -0.006495259466358668, 'resid': -0.062725588780694, 'observed': 13.339224530168153}, {'date': '2015-05-01', 'trend': 13.3634748819022, 'seasonal': -0.02756787427454523, 'resid': -0.026310385780782358, 'observed': 13.309596621846872}, {'date': '2015-06-01', 'trend': 13.324726093696405, 'seasonal': -0.024120179891552147, 'resid': 0.010403671655124475, 'observed': 13.311009585459978}, {'date': '2015-07-01', 'trend': 13.295059444065489, 'seasonal': -0.03616359408614569, 'resid': 0.05694638193523351, 'observed': 13.315842231914576}, {'date': '2015-08-01', 'trend': 13.28903396947279, 'seasonal': -0.015376141878352585, 'resid': 0.020558873578782184, 'observed': 13.29421670117322}, {'date': '2015-09-01', 'trend': 13.302630073928466, 'seasonal': 0.04035308860321538, 'resid': -0.0580530751409206, 'observed': 13.28493008739076}, {'date': '2015-10-01', 'trend': 13.319559651730444, 'seasonal': 0.08058524115091258, 'resid': -0.09169698972528272, 'observed': 13.308447903156074}, {'date': '2015-11-01', 'trend': 13.335995695872539, 'seasonal': -0.05290731070035628, 'resid': -0.16892269615494468, 'observed': 13.114165689017238}, {'date': '2015-12-01', 'trend': 13.36357309550888, 'seasonal': -0.02602059045270812, 'resid': -0.2236135560608082, 'observed': 13.113938948995363}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.03449672569012794, 'resid': nan, 'observed': 13.313586302637669}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.022574335467601885, 'resid': nan, 'observed': 13.51001321003963}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.010641559838160935, 'resid': nan, 'observed': 13.518506973417573}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.006495259466358668, 'resid': nan, 'observed': 13.54169860126461}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.02756787427454523, 'resid': nan, 'observed': 13.501587610160705}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.024120179891552147, 'resid': nan, 'observed': 13.780876188418324}], 'loan_rate_D': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.04677017162484018, 'resid': nan, 'observed': 12.267}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.050974187840284596, 'resid': nan, 'observed': 12.436666666666667}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.08472596878988434, 'resid': nan, 'observed': 12.737368421052633}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05303848060048236, 'resid': nan, 'observed': 12.609444444444444}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.11016855981499699, 'resid': nan, 'observed': 12.47888888888889}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.09055776109670126, 'resid': nan, 'observed': 13.0082}, {'date': '2008-02-01', 'trend': 12.946022013302766, 'seasonal': 0.014301916284665678, 'resid': 0.27560199633849425, 'observed': 13.235925925925926}, {'date': '2008-03-01', 'trend': 13.039515068858323, 'seasonal': 0.05402694293008556, 'resid': 0.05243359796768923, 'observed': 13.145975609756098}, {'date': '2008-04-01', 'trend': 13.122614914059561, 'seasonal': 0.02189789187409405, 'resid': 0.113748063631563, 'observed': 13.258260869565218}, {'date': '2008-05-01', 'trend': 13.21303629450572, 'seasonal': 0.00613243158641516, 'resid': -0.245168726092135, 'observed': 12.974}, {'date': '2008-06-01', 'trend': 13.345142221285036, 'seasonal': -0.030307932806885735, 'resid': -0.024417621811482743, 'observed': 13.290416666666667}, {'date': '2008-07-01', 'trend': 13.493951933332248, 'seasonal': 0.005243223388796764, 'resid': -0.18599515672104458, 'observed': 13.3132}, {'date': '2008-08-01', 'trend': 13.620062329512132, 'seasonal': 0.04677017162484018, 'resid': -0.20599916780364066, 'observed': 13.460833333333332}, {'date': '2008-09-01', 'trend': 13.739128813596224, 'seasonal': 0.050974187840284596, 'resid': -0.30343633476984244, 'observed': 13.486666666666666}, {'date': '2008-10-01', 'trend': 13.860800265690038, 'seasonal': 0.08472596878988434, 'resid': -0.26376152859756974, 'observed': 13.681764705882353}, {'date': '2008-11-01', 'trend': 13.987063226033499, 'seasonal': -0.05303848060048236, 'resid': -0.09886345511043668, 'observed': 13.83516129032258}, {'date': '2008-12-01', 'trend': 14.103115309366833, 'seasonal': -0.11016855981499699, 'resid': 0.43076753616244823, 'observed': 14.423714285714285}, {'date': '2009-01-01', 'trend': 14.210347506336527, 'seasonal': -0.09055776109670126, 'resid': 0.5150179470678666, 'observed': 14.634807692307692}, {'date': '2009-02-01', 'trend': 14.336290452098496, 'seasonal': 0.014301916284665678, 'resid': 0.28537537355232334, 'observed': 14.635967741935485}, {'date': '2009-03-01', 'trend': 14.475571637398703, 'seasonal': 0.05402694293008556, 'resid': 0.0739308314359172, 'observed': 14.603529411764706}, {'date': '2009-04-01', 'trend': 14.613243023661541, 'seasonal': 0.02189789187409405, 'resid': 0.08568100227258282, 'observed': 14.720821917808218}, {'date': '2009-05-01', 'trend': 14.738657673096528, 'seasonal': 0.00613243158641516, 'resid': -0.2030401046829425, 'observed': 14.54175}, {'date': '2009-06-01', 'trend': 14.82944765925895, 'seasonal': -0.030307932806885735, 'resid': -0.29122305978539564, 'observed': 14.507916666666668}, {'date': '2009-07-01', 'trend': 14.885117054715643, 'seasonal': 0.005243223388796764, 'resid': -0.22108755083171103, 'observed': 14.669272727272729}, {'date': '2009-08-01', 'trend': 14.9323876132182, 'seasonal': 0.04677017162484018, 'resid': 0.14823351950478605, 'observed': 15.127391304347826}, {'date': '2009-09-01', 'trend': 14.981868158243856, 'seasonal': 0.050974187840284596, 'resid': 0.13001479677300157, 'observed': 15.162857142857142}, {'date': '2009-10-01', 'trend': 15.024424507165973, 'seasonal': 0.08472596878988434, 'resid': 0.20053702404414345, 'observed': 15.3096875}, {'date': '2009-11-01', 'trend': 15.073536093923963, 'seasonal': -0.05303848060048236, 'resid': 0.19669246932114728, 'observed': 15.217190082644628}, {'date': '2009-12-01', 'trend': 15.147354517126578, 'seasonal': -0.11016855981499699, 'resid': 0.1834592039787434, 'observed': 15.220645161290324}, {'date': '2010-01-01', 'trend': 15.227469522958767, 'seasonal': -0.09055776109670126, 'resid': 0.03703054583024161, 'observed': 15.173942307692307}, {'date': '2010-02-01', 'trend': 15.281231545226769, 'seasonal': 0.014301916284665678, 'resid': -0.06420693089919113, 'observed': 15.231326530612243}, {'date': '2010-03-01', 'trend': 15.315594738321419, 'seasonal': 0.05402694293008556, 'resid': -0.17391797754780133, 'observed': 15.195703703703703}, {'date': '2010-04-01', 'trend': 15.333071365101393, 'seasonal': 0.02189789187409405, 'resid': -0.2049692569754867, 'observed': 15.15}, {'date': '2010-05-01', 'trend': 15.317257753325439, 'seasonal': 0.00613243158641516, 'resid': -0.032140184911854414, 'observed': 15.29125}, {'date': '2010-06-01', 'trend': 15.284795966826582, 'seasonal': -0.030307932806885735, 'resid': 0.2755707895097156, 'observed': 15.530058823529412}, {'date': '2010-07-01', 'trend': 15.264769830143157, 'seasonal': 0.005243223388796764, 'resid': 0.2998776568505589, 'observed': 15.569890710382513}, {'date': '2010-08-01', 'trend': 15.268900778305587, 'seasonal': 0.04677017162484018, 'resid': 0.20139090573967602, 'observed': 15.517061855670104}, {'date': '2010-09-01', 'trend': 15.281649843601524, 'seasonal': 0.050974187840284596, 'resid': 0.2652791943646423, 'observed': 15.597903225806451}, {'date': '2010-10-01', 'trend': 15.293769553052561, 'seasonal': 0.08472596878988434, 'resid': -0.08441506207232927, 'observed': 15.294080459770116}, {'date': '2010-11-01', 'trend': 15.349641612544133, 'seasonal': -0.05303848060048236, 'resid': -0.44333269169207823, 'observed': 14.853270440251572}, {'date': '2010-12-01', 'trend': 15.440148556758828, 'seasonal': -0.11016855981499699, 'resid': -0.5244980692329876, 'observed': 14.805481927710844}, {'date': '2011-01-01', 'trend': 15.519144828167612, 'seasonal': -0.09055776109670126, 'resid': -0.3201088062013446, 'observed': 15.108478260869566}, {'date': '2011-02-01', 'trend': 15.597932287226568, 'seasonal': 0.014301916284665678, 'resid': -0.21630087017790176, 'observed': 15.395933333333332}, {'date': '2011-03-01', 'trend': 15.683697841260013, 'seasonal': 0.05402694293008556, 'resid': -0.4006503161049927, 'observed': 15.337074468085106}, {'date': '2011-04-01', 'trend': 15.810629568251665, 'seasonal': 0.02189789187409405, 'resid': -0.5330251976823192, 'observed': 15.29950226244344}, {'date': '2011-05-01', 'trend': 15.991605993913954, 'seasonal': 0.00613243158641516, 'resid': 0.4849387398539594, 'observed': 16.48267716535433}, {'date': '2011-06-01', 'trend': 16.191505172815155, 'seasonal': -0.030307932806885735, 'resid': 0.3496010793194603, 'observed': 16.51079831932773}, {'date': '2011-07-01', 'trend': 16.374712577701146, 'seasonal': 0.005243223388796764, 'resid': 0.10510592730511684, 'observed': 16.48506172839506}, {'date': '2011-08-01', 'trend': 16.541513156944013, 'seasonal': 0.04677017162484018, 'resid': -0.09549347349638734, 'observed': 16.492789855072466}, {'date': '2011-09-01', 'trend': 16.74736510541923, 'seasonal': 0.050974187840284596, 'resid': -0.11779077005276303, 'observed': 16.680548523206753}, {'date': '2011-10-01', 'trend': 16.988299020223373, 'seasonal': 0.08472596878988434, 'resid': 0.18477162115623194, 'observed': 17.25779661016949}, {'date': '2011-11-01', 'trend': 17.1806862665515, 'seasonal': -0.05303848060048236, 'resid': 0.10534071979610485, 'observed': 17.232988505747123}, {'date': '2011-12-01', 'trend': 17.32296555978484, 'seasonal': -0.11016855981499699, 'resid': 0.01054715587431257, 'observed': 17.223344155844156}, {'date': '2012-01-01', 'trend': 17.47914706744071, 'seasonal': -0.09055776109670126, 'resid': -0.30099555634400893, 'observed': 17.08759375}, {'date': '2012-02-01', 'trend': 17.65589571162486, 'seasonal': 0.014301916284665678, 'resid': -0.2501658818777842, 'observed': 17.420031746031743}, {'date': '2012-03-01', 'trend': 17.82761029986977, 'seasonal': 0.05402694293008556, 'resid': 0.37178557599208595, 'observed': 18.253422818791943}, {'date': '2012-04-01', 'trend': 17.968315594668944, 'seasonal': 0.02189789187409405, 'resid': 0.1753543804929724, 'observed': 18.16556786703601}, {'date': '2012-05-01', 'trend': 18.08628632218981, 'seasonal': 0.00613243158641516, 'resid': 0.14148671886059142, 'observed': 18.233905472636817}, {'date': '2012-06-01', 'trend': 18.202423731325155, 'seasonal': -0.030307932806885735, 'resid': 0.0021572511271205352, 'observed': 18.17427304964539}, {'date': '2012-07-01', 'trend': 18.320598459912215, 'seasonal': 0.005243223388796764, 'resid': 0.24410149851717358, 'observed': 18.569943181818186}, {'date': '2012-08-01', 'trend': 18.42990775735452, 'seasonal': 0.04677017162484018, 'resid': 0.17319793308960643, 'observed': 18.649875862068967}, {'date': '2012-09-01', 'trend': 18.494241138814044, 'seasonal': 0.050974187840284596, 'resid': 0.0993973074338729, 'observed': 18.6446126340882}, {'date': '2012-10-01', 'trend': 18.528943565510193, 'seasonal': 0.08472596878988434, 'resid': 0.056990040168004935, 'observed': 18.670659574468083}, {'date': '2012-11-01', 'trend': 18.56571075355779, 'seasonal': -0.05303848060048236, 'resid': 0.13875072899200927, 'observed': 18.65142300194932}, {'date': '2012-12-01', 'trend': 18.60424801768697, 'seasonal': -0.11016855981499699, 'resid': 0.09812802101825394, 'observed': 18.592207478890227}, {'date': '2013-01-01', 'trend': 18.629536628909282, 'seasonal': -0.09055776109670126, 'resid': 0.01594504523090072, 'observed': 18.55492391304348}, {'date': '2013-02-01', 'trend': 18.63721728737489, 'seasonal': 0.014301916284665678, 'resid': -0.0753944820559945, 'observed': 18.576124721603563}, {'date': '2013-03-01', 'trend': 18.645549887500888, 'seasonal': 0.05402694293008556, 'resid': -0.05824583218228971, 'observed': 18.641330998248684}, {'date': '2013-04-01', 'trend': 18.660288795601627, 'seasonal': 0.02189789187409405, 'resid': -0.07166875918886748, 'observed': 18.610517928286853}, {'date': '2013-05-01', 'trend': 18.679619934627297, 'seasonal': 0.00613243158641516, 'resid': -0.01438444168541117, 'observed': 18.6713679245283}, {'date': '2013-06-01', 'trend': 18.68159073892394, 'seasonal': -0.030307932806885735, 'resid': 0.0104221307371343, 'observed': 18.66170493685419}, {'date': '2013-07-01', 'trend': 18.656660551740597, 'seasonal': 0.005243223388796764, 'resid': 0.027534188815457356, 'observed': 18.68943796394485}, {'date': '2013-08-01', 'trend': 18.616553704147748, 'seasonal': 0.04677017162484018, 'resid': 0.051393007344301106, 'observed': 18.71471688311689}, {'date': '2013-09-01', 'trend': 18.563483001311955, 'seasonal': 0.050974187840284596, 'resid': 0.1652968269120169, 'observed': 18.779754016064256}, {'date': '2013-10-01', 'trend': 18.507362836112627, 'seasonal': 0.08472596878988434, 'resid': 0.29716318200725855, 'observed': 18.88925198690977}, {'date': '2013-11-01', 'trend': 18.41784152773823, 'seasonal': -0.05303848060048236, 'resid': 0.5319748789859714, 'observed': 18.89677792612372}, {'date': '2013-12-01', 'trend': 18.275788119034477, 'seasonal': -0.11016855981499699, 'resid': 0.2285322986157389, 'observed': 18.39415185783522}, {'date': '2014-01-01', 'trend': 18.117404473515993, 'seasonal': -0.09055776109670126, 'resid': 0.12780832927896363, 'observed': 18.154655041698256}, {'date': '2014-02-01', 'trend': 17.95915940844471, 'seasonal': 0.014301916284665678, 'resid': 0.04036792599108799, 'observed': 18.013829250720462}, {'date': '2014-03-01', 'trend': 17.798431684504315, 'seasonal': 0.05402694293008556, 'resid': 0.07747097363834579, 'observed': 17.929929601072747}, {'date': '2014-04-01', 'trend': 17.627098506088636, 'seasonal': 0.02189789187409405, 'resid': 0.32603896271619315, 'observed': 17.975035360678923}, {'date': '2014-05-01', 'trend': 17.438528742234112, 'seasonal': 0.00613243158641516, 'resid': -0.28632208266985126, 'observed': 17.158339091150676}, {'date': '2014-06-01', 'trend': 17.258728659263024, 'seasonal': -0.030307932806885735, 'resid': -0.46296876511446683, 'observed': 16.76545196134167}, {'date': '2014-07-01', 'trend': 17.108719410371155, 'seasonal': 0.005243223388796764, 'resid': -0.3294791867461425, 'observed': 16.78448344701381}, {'date': '2014-08-01', 'trend': 16.98323278819809, 'seasonal': 0.04677017162484018, 'resid': -0.2082131214857493, 'observed': 16.821789838337182}, {'date': '2014-09-01', 'trend': 16.877995723697357, 'seasonal': 0.050974187840284596, 'resid': -0.1137542252631317, 'observed': 16.81521568627451}, {'date': '2014-10-01', 'trend': 16.778774884200576, 'seasonal': 0.08472596878988434, 'resid': -0.12170681826720817, 'observed': 16.741794034723252}, {'date': '2014-11-01', 'trend': 16.71401294490989, 'seasonal': -0.05303848060048236, 'resid': -0.14241291850788204, 'observed': 16.518561545801525}, {'date': '2014-12-01', 'trend': 16.69983988983558, 'seasonal': -0.11016855981499699, 'resid': -0.13250508316919682, 'observed': 16.457166246851386}, {'date': '2015-01-01', 'trend': 16.699303658669496, 'seasonal': -0.09055776109670126, 'resid': -0.11732721829564548, 'observed': 16.49141867927715}, {'date': '2015-02-01', 'trend': 16.69508156806944, 'seasonal': 0.014301916284665678, 'resid': -0.04399680336592135, 'observed': 16.665386680988185}, {'date': '2015-03-01', 'trend': 16.69018220555109, 'seasonal': 0.05402694293008556, 'resid': 0.008473474306158219, 'observed': 16.752682622787333}, {'date': '2015-04-01', 'trend': 16.688963166928986, 'seasonal': 0.02189789187409405, 'resid': 0.06012113223847462, 'observed': 16.770982191041554}, {'date': '2015-05-01', 'trend': 16.69606287739243, 'seasonal': 0.00613243158641516, 'resid': 0.10591040883275618, 'observed': 16.8081057178116}, {'date': '2015-06-01', 'trend': 16.713701422181142, 'seasonal': -0.030307932806885735, 'resid': 0.09213852352302712, 'observed': 16.775532012897283}, {'date': '2015-07-01', 'trend': 16.74506767376765, 'seasonal': 0.005243223388796764, 'resid': 0.011222950315704111, 'observed': 16.76153384747215}, {'date': '2015-08-01', 'trend': 16.809868367240238, 'seasonal': 0.04677017162484018, 'resid': -0.1132292753874796, 'observed': 16.7434092634776}, {'date': '2015-09-01', 'trend': 16.898763840746042, 'seasonal': 0.050974187840284596, 'resid': -0.17372646789268384, 'observed': 16.776011560693643}, {'date': '2015-10-01', 'trend': 16.985313395517174, 'seasonal': 0.08472596878988434, 'resid': -0.3182981309334191, 'observed': 16.75174123337364}, {'date': '2015-11-01', 'trend': 17.06891528315394, 'seasonal': -0.05303848060048236, 'resid': -0.33686940427972323, 'observed': 16.679007398273736}, {'date': '2015-12-01', 'trend': 17.17336476486541, 'seasonal': -0.11016855981499699, 'resid': -0.3431507357422, 'observed': 16.720045469308214}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.09055776109670126, 'resid': nan, 'observed': 16.981329494896624}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.014301916284665678, 'resid': nan, 'observed': 17.7306925087108}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.05402694293008556, 'resid': nan, 'observed': 17.82086815920398}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.02189789187409405, 'resid': nan, 'observed': 17.77998596913209}, {'date': '2016-05-01', 'trend': nan, 'seasonal': 0.00613243158641516, 'resid': nan, 'observed': 17.805547243003602}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.030307932806885735, 'resid': nan, 'observed': 18.284878048780485}], 'FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.0908829365079365, 'resid': nan, 'observed': 5.02}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.0898933531746032, 'resid': nan, 'observed': 4.94}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.0028100198412698354, 'resid': nan, 'observed': 4.76}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05135664682539679, 'resid': nan, 'observed': 4.49}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.045783730158730156, 'resid': nan, 'observed': 4.24}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.043670634920634936, 'resid': nan, 'observed': 3.94}, {'date': '2008-02-01', 'trend': 3.311666666666667, 'seasonal': -0.04234623015873019, 'resid': -0.2893204365079368, 'observed': 2.98}, {'date': '2008-03-01', 'trend': 3.0554166666666664, 'seasonal': -0.03817956349206348, 'resid': -0.4072371031746031, 'observed': 2.61}, {'date': '2008-04-01', 'trend': 2.7670833333333333, 'seasonal': -0.029012896825396865, 'resid': -0.45807043650793666, 'observed': 2.28}, {'date': '2008-05-01', 'trend': 2.438333333333333, 'seasonal': -0.019585813492063467, 'resid': -0.4387475198412696, 'observed': 1.98}, {'date': '2008-06-01', 'trend': 2.0975, 'seasonal': 0.027757936507936475, 'resid': -0.12525793650793662, 'observed': 2.0}, {'date': '2008-07-01', 'trend': 1.7695833333333333, 'seasonal': 0.05859126984126982, 'resid': 0.18182539682539667, 'observed': 2.01}, {'date': '2008-08-01', 'trend': 1.4966666666666666, 'seasonal': 0.0908829365079365, 'resid': 0.4124503968253969, 'observed': 2.0}, {'date': '2008-09-01', 'trend': 1.2804166666666665, 'seasonal': 0.0898933531746032, 'resid': 0.4396899801587303, 'observed': 1.81}, {'date': '2008-10-01', 'trend': 1.0904166666666666, 'seasonal': 0.0028100198412698354, 'resid': -0.12322668650793644, 'observed': 0.97}, {'date': '2008-11-01', 'trend': 0.9266666666666664, 'seasonal': -0.05135664682539679, 'resid': -0.4853100198412696, 'observed': 0.39}, {'date': '2008-12-01', 'trend': 0.7770833333333333, 'seasonal': -0.045783730158730156, 'resid': -0.5712996031746032, 'observed': 0.16}, {'date': '2009-01-01', 'trend': 0.6254166666666667, 'seasonal': -0.043670634920634936, 'resid': -0.43174603174603177, 'observed': 0.15}, {'date': '2009-02-01', 'trend': 0.4716666666666666, 'seasonal': -0.04234623015873019, 'resid': -0.2093204365079364, 'observed': 0.22}, {'date': '2009-03-01', 'trend': 0.3258333333333333, 'seasonal': -0.03817956349206348, 'resid': -0.10765376984126984, 'observed': 0.18}, {'date': '2009-04-01', 'trend': 0.22125, 'seasonal': -0.029012896825396865, 'resid': -0.04223710317460314, 'observed': 0.15}, {'date': '2009-05-01', 'trend': 0.1745833333333333, 'seasonal': -0.019585813492063467, 'resid': 0.025002480158730148, 'observed': 0.18}, {'date': '2009-06-01', 'trend': 0.16166666666666665, 'seasonal': 0.027757936507936475, 'resid': 0.020575396825396865, 'observed': 0.21}, {'date': '2009-07-01', 'trend': 0.1583333333333333, 'seasonal': 0.05859126984126982, 'resid': -0.056924603174603114, 'observed': 0.16}, {'date': '2009-08-01', 'trend': 0.15291666666666665, 'seasonal': 0.0908829365079365, 'resid': -0.08379960317460314, 'observed': 0.16}, {'date': '2009-09-01', 'trend': 0.14833333333333332, 'seasonal': 0.0898933531746032, 'resid': -0.08822668650793652, 'observed': 0.15}, {'date': '2009-10-01', 'trend': 0.14958333333333332, 'seasonal': 0.0028100198412698354, 'resid': -0.03239335317460316, 'observed': 0.12}, {'date': '2009-11-01', 'trend': 0.1525, 'seasonal': -0.05135664682539679, 'resid': 0.01885664682539679, 'observed': 0.12}, {'date': '2009-12-01', 'trend': 0.15208333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.01370039682539683, 'observed': 0.12}, {'date': '2010-01-01', 'trend': 0.15166666666666667, 'seasonal': -0.043670634920634936, 'resid': 0.002003968253968265, 'observed': 0.11}, {'date': '2010-02-01', 'trend': 0.15374999999999997, 'seasonal': -0.04234623015873019, 'resid': 0.01859623015873022, 'observed': 0.13}, {'date': '2010-03-01', 'trend': 0.15666666666666665, 'seasonal': -0.03817956349206348, 'resid': 0.041512896825396835, 'observed': 0.16}, {'date': '2010-04-01', 'trend': 0.16124999999999998, 'seasonal': -0.029012896825396865, 'resid': 0.0677628968253969, 'observed': 0.2}, {'date': '2010-05-01', 'trend': 0.1670833333333333, 'seasonal': -0.019585813492063467, 'resid': 0.05250248015873017, 'observed': 0.2}, {'date': '2010-06-01', 'trend': 0.17250000000000001, 'seasonal': 0.027757936507936475, 'resid': -0.020257936507936496, 'observed': 0.18}, {'date': '2010-07-01', 'trend': 0.1775, 'seasonal': 0.05859126984126982, 'resid': -0.05609126984126982, 'observed': 0.18}, {'date': '2010-08-01', 'trend': 0.18124999999999997, 'seasonal': 0.0908829365079365, 'resid': -0.08213293650793646, 'observed': 0.19}, {'date': '2010-09-01', 'trend': 0.18166666666666667, 'seasonal': 0.0898933531746032, 'resid': -0.08156001984126987, 'observed': 0.19}, {'date': '2010-10-01', 'trend': 0.17666666666666667, 'seasonal': 0.0028100198412698354, 'resid': 0.0105233134920635, 'observed': 0.19}, {'date': '2010-11-01', 'trend': 0.16791666666666666, 'seasonal': -0.05135664682539679, 'resid': 0.07343998015873013, 'observed': 0.19}, {'date': '2010-12-01', 'trend': 0.15958333333333333, 'seasonal': -0.045783730158730156, 'resid': 0.06620039682539683, 'observed': 0.18}, {'date': '2011-01-01', 'trend': 0.15125000000000002, 'seasonal': -0.043670634920634936, 'resid': 0.062420634920634925, 'observed': 0.17}, {'date': '2011-02-01', 'trend': 0.14291666666666666, 'seasonal': -0.04234623015873019, 'resid': 0.05942956349206353, 'observed': 0.16}, {'date': '2011-03-01', 'trend': 0.1345833333333333, 'seasonal': -0.03817956349206348, 'resid': 0.04359623015873019, 'observed': 0.14}, {'date': '2011-04-01', 'trend': 0.12499999999999999, 'seasonal': -0.029012896825396865, 'resid': 0.004012896825396885, 'observed': 0.1}, {'date': '2011-05-01', 'trend': 0.11541666666666667, 'seasonal': -0.019585813492063467, 'resid': -0.005830853174603204, 'observed': 0.09}, {'date': '2011-06-01', 'trend': 0.10625, 'seasonal': 0.027757936507936475, 'resid': -0.044007936507936475, 'observed': 0.09}, {'date': '2011-07-01', 'trend': 0.09791666666666668, 'seasonal': 0.05859126984126982, 'resid': -0.08650793650793649, 'observed': 0.07}, {'date': '2011-08-01', 'trend': 0.09166666666666666, 'seasonal': 0.0908829365079365, 'resid': -0.08254960317460315, 'observed': 0.1}, {'date': '2011-09-01', 'trend': 0.08875, 'seasonal': 0.0898933531746032, 'resid': -0.0986433531746032, 'observed': 0.08}, {'date': '2011-10-01', 'trend': 0.09, 'seasonal': 0.0028100198412698354, 'resid': -0.022810019841269825, 'observed': 0.07}, {'date': '2011-11-01', 'trend': 0.09458333333333334, 'seasonal': -0.05135664682539679, 'resid': 0.036773313492063454, 'observed': 0.08}, {'date': '2011-12-01', 'trend': 0.10041666666666667, 'seasonal': -0.045783730158730156, 'resid': 0.015367063492063494, 'observed': 0.07}, {'date': '2012-01-01', 'trend': 0.10708333333333334, 'seasonal': -0.043670634920634936, 'resid': 0.016587301587301602, 'observed': 0.08}, {'date': '2012-02-01', 'trend': 0.11208333333333333, 'seasonal': -0.04234623015873019, 'resid': 0.030262896825396866, 'observed': 0.1}, {'date': '2012-03-01', 'trend': 0.11583333333333333, 'seasonal': -0.03817956349206348, 'resid': 0.052346230158730155, 'observed': 0.13}, {'date': '2012-04-01', 'trend': 0.12208333333333334, 'seasonal': -0.029012896825396865, 'resid': 0.04692956349206354, 'observed': 0.14}, {'date': '2012-05-01', 'trend': 0.12916666666666665, 'seasonal': -0.019585813492063467, 'resid': 0.05041914682539682, 'observed': 0.16}, {'date': '2012-06-01', 'trend': 0.13624999999999998, 'seasonal': 0.027757936507936475, 'resid': -0.0040079365079364535, 'observed': 0.16}, {'date': '2012-07-01', 'trend': 0.1425, 'seasonal': 0.05859126984126982, 'resid': -0.041091269841269804, 'observed': 0.16}, {'date': '2012-08-01', 'trend': 0.14708333333333334, 'seasonal': 0.0908829365079365, 'resid': -0.10796626984126984, 'observed': 0.13}, {'date': '2012-09-01', 'trend': 0.14958333333333332, 'seasonal': 0.0898933531746032, 'resid': -0.09947668650793651, 'observed': 0.14}, {'date': '2012-10-01', 'trend': 0.15041666666666667, 'seasonal': 0.0028100198412698354, 'resid': 0.006773313492063497, 'observed': 0.16}, {'date': '2012-11-01', 'trend': 0.14875, 'seasonal': -0.05135664682539679, 'resid': 0.0626066468253968, 'observed': 0.16}, {'date': '2012-12-01', 'trend': 0.14375000000000002, 'seasonal': -0.045783730158730156, 'resid': 0.06203373015873014, 'observed': 0.16}, {'date': '2013-01-01', 'trend': 0.13791666666666666, 'seasonal': -0.043670634920634936, 'resid': 0.04575396825396829, 'observed': 0.14}, {'date': '2013-02-01', 'trend': 0.13291666666666666, 'seasonal': -0.04234623015873019, 'resid': 0.05942956349206353, 'observed': 0.15}, {'date': '2013-03-01', 'trend': 0.12833333333333333, 'seasonal': -0.03817956349206348, 'resid': 0.049846230158730166, 'observed': 0.14}, {'date': '2013-04-01', 'trend': 0.12291666666666667, 'seasonal': -0.029012896825396865, 'resid': 0.056096230158730186, 'observed': 0.15}, {'date': '2013-05-01', 'trend': 0.11666666666666667, 'seasonal': -0.019585813492063467, 'resid': 0.012919146825396799, 'observed': 0.11}, {'date': '2013-06-01', 'trend': 0.11041666666666666, 'seasonal': 0.027757936507936475, 'resid': -0.04817460317460314, 'observed': 0.09}, {'date': '2013-07-01', 'trend': 0.10458333333333333, 'seasonal': 0.05859126984126982, 'resid': -0.07317460317460316, 'observed': 0.09}, {'date': '2013-08-01', 'trend': 0.09833333333333334, 'seasonal': 0.0908829365079365, 'resid': -0.10921626984126984, 'observed': 0.08}, {'date': '2013-09-01', 'trend': 0.0925, 'seasonal': 0.0898933531746032, 'resid': -0.1023933531746032, 'observed': 0.08}, {'date': '2013-10-01', 'trend': 0.08750000000000001, 'seasonal': 0.0028100198412698354, 'resid': -0.0003100198412698471, 'observed': 0.09}, {'date': '2013-11-01', 'trend': 0.08416666666666667, 'seasonal': -0.05135664682539679, 'resid': 0.047189980158730126, 'observed': 0.08}, {'date': '2013-12-01', 'trend': 0.08374999999999999, 'seasonal': -0.045783730158730156, 'resid': 0.05203373015873016, 'observed': 0.09}, {'date': '2014-01-01', 'trend': 0.08416666666666667, 'seasonal': -0.043670634920634936, 'resid': 0.029503968253968275, 'observed': 0.07}, {'date': '2014-02-01', 'trend': 0.08458333333333333, 'seasonal': -0.04234623015873019, 'resid': 0.027762896825396864, 'observed': 0.07}, {'date': '2014-03-01', 'trend': 0.08541666666666667, 'seasonal': -0.03817956349206348, 'resid': 0.03276289682539681, 'observed': 0.08}, {'date': '2014-04-01', 'trend': 0.08583333333333333, 'seasonal': -0.029012896825396865, 'resid': 0.03317956349206353, 'observed': 0.09}, {'date': '2014-05-01', 'trend': 0.08625, 'seasonal': -0.019585813492063467, 'resid': 0.02333581349206347, 'observed': 0.09}, {'date': '2014-06-01', 'trend': 0.08791666666666667, 'seasonal': 0.027757936507936475, 'resid': -0.01567460317460314, 'observed': 0.1}, {'date': '2014-07-01', 'trend': 0.09083333333333332, 'seasonal': 0.05859126984126982, 'resid': -0.059424603174603144, 'observed': 0.09}, {'date': '2014-08-01', 'trend': 0.09416666666666666, 'seasonal': 0.0908829365079365, 'resid': -0.09504960317460316, 'observed': 0.09}, {'date': '2014-09-01', 'trend': 0.09708333333333333, 'seasonal': 0.0898933531746032, 'resid': -0.09697668650793653, 'observed': 0.09}, {'date': '2014-10-01', 'trend': 0.09958333333333334, 'seasonal': 0.0028100198412698354, 'resid': -0.012393353174603182, 'observed': 0.09}, {'date': '2014-11-01', 'trend': 0.10208333333333333, 'seasonal': -0.05135664682539679, 'resid': 0.03927331349206346, 'observed': 0.09}, {'date': '2014-12-01', 'trend': 0.10458333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.06120039682539683, 'observed': 0.12}, {'date': '2015-01-01', 'trend': 0.10749999999999998, 'seasonal': -0.043670634920634936, 'resid': 0.04617063492063495, 'observed': 0.11}, {'date': '2015-02-01', 'trend': 0.11124999999999999, 'seasonal': -0.04234623015873019, 'resid': 0.0410962301587302, 'observed': 0.11}, {'date': '2015-03-01', 'trend': 0.11541666666666667, 'seasonal': -0.03817956349206348, 'resid': 0.03276289682539681, 'observed': 0.11}, {'date': '2015-04-01', 'trend': 0.11875, 'seasonal': -0.029012896825396865, 'resid': 0.030262896825396866, 'observed': 0.12}, {'date': '2015-05-01', 'trend': 0.12125, 'seasonal': -0.019585813492063467, 'resid': 0.018335813492063466, 'observed': 0.12}, {'date': '2015-06-01', 'trend': 0.1275, 'seasonal': 0.027757936507936475, 'resid': -0.025257936507936472, 'observed': 0.13}, {'date': '2015-07-01', 'trend': 0.1420833333333333, 'seasonal': 0.05859126984126982, 'resid': -0.07067460317460313, 'observed': 0.13}, {'date': '2015-08-01', 'trend': 0.16291666666666668, 'seasonal': 0.0908829365079365, 'resid': -0.11379960317460316, 'observed': 0.14}, {'date': '2015-09-01', 'trend': 0.1845833333333333, 'seasonal': 0.0898933531746032, 'resid': -0.13447668650793648, 'observed': 0.14}, {'date': '2015-10-01', 'trend': 0.20541666666666664, 'seasonal': 0.0028100198412698354, 'resid': -0.08822668650793647, 'observed': 0.12}, {'date': '2015-11-01', 'trend': 0.22624999999999998, 'seasonal': -0.05135664682539679, 'resid': -0.05489335317460319, 'observed': 0.12}, {'date': '2015-12-01', 'trend': 0.24708333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.038700396825396825, 'observed': 0.24}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.043670634920634936, 'resid': nan, 'observed': 0.34}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.04234623015873019, 'resid': nan, 'observed': 0.38}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.03817956349206348, 'resid': nan, 'observed': 0.36}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.029012896825396865, 'resid': nan, 'observed': 0.37}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.019585813492063467, 'resid': nan, 'observed': 0.37}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.027757936507936475, 'resid': nan, 'observed': 0.38}], 'diff1_loan_rate_A': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.052743362293617736, 'resid': nan, 'observed': 0.0600000000000013}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.010310036424414365, 'resid': nan, 'observed': 0.0747619047619041}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.07866104363465767, 'resid': nan, 'observed': -0.0114285714285715}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.06447017941083495, 'resid': nan, 'observed': -0.0509090909090916}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.13259497070685478, 'resid': nan, 'observed': -0.0832575757575755}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.031714528696978224, 'resid': nan, 'observed': 0.2655000000000003}, {'date': '2008-02-01', 'trend': 0.04308756038647344, 'seasonal': -0.04724202459970091, 'resid': 0.17315446421322706, 'observed': 0.1689999999999996}, {'date': '2008-03-01', 'trend': 0.031621031746031744, 'seasonal': -0.010757930825500861, 'resid': -0.02491071996814928, 'observed': -0.0040476190476184}, {'date': '2008-04-01', 'trend': 0.04255952380952382, 'seasonal': 0.02255283843870718, 'resid': -0.0993147432006122, 'observed': -0.0342023809523812}, {'date': '2008-05-01', 'trend': 0.06510642135642139, 'seasonal': -0.030587662937704367, 'resid': 0.024826479676521074, 'observed': 0.0593452380952381}, {'date': '2008-06-01', 'trend': 0.09512626262626267, 'seasonal': -0.017830922488024405, 'resid': -0.023223911566808864, 'observed': 0.0540714285714294}, {'date': '2008-07-01', 'trend': 0.10408606557377058, 'seasonal': 0.03184779489655618, 'resid': -0.12099907786163225, 'observed': 0.0149347826086945}, {'date': '2008-08-01', 'trend': 0.07345892882163388, 'seasonal': -0.052743362293617736, 'resid': 0.04584965086328945, 'observed': 0.0665652173913056}, {'date': '2008-09-01', 'trend': 0.06096281033781046, 'seasonal': -0.010310036424414365, 'resid': -0.2576527739133968, 'observed': -0.2070000000000007}, {'date': '2008-10-01', 'trend': 0.053667120029015226, 'seasonal': 0.07866104363465767, 'resid': 0.40052897919346997, 'observed': 0.5328571428571429}, {'date': '2008-11-01', 'trend': 0.057776055562292906, 'seasonal': -0.06447017941083495, 'resid': -0.047375140220722256, 'observed': -0.0540692640692643}, {'date': '2008-12-01', 'trend': 0.06338933770596704, 'seasonal': 0.13259497070685478, 'resid': 0.44439447946596633, 'observed': 0.6403787878787881}, {'date': '2009-01-01', 'trend': 0.06135697966358909, 'seasonal': -0.031714528696978224, 'resid': -0.2727435438627845, 'observed': -0.2431010928961736}, {'date': '2009-02-01', 'trend': 0.038774959850979306, 'seasonal': -0.04724202459970091, 'resid': -0.04898312440638529, 'observed': -0.0574501891551069}, {'date': '2009-03-01', 'trend': 0.022875529287226484, 'seasonal': -0.010757930825500861, 'resid': -0.08962187196600012, 'observed': -0.0775042735042745}, {'date': '2009-04-01', 'trend': 0.007781467220443654, 'seasonal': 0.02255283843870718, 'resid': -0.16617659956596104, 'observed': -0.1358422939068102}, {'date': '2009-05-01', 'trend': -0.015483169794193343, 'seasonal': -0.030587662937704367, 'resid': 0.3056704365802287, 'observed': 0.259599603848331}, {'date': '2009-06-01', 'trend': -0.04104235714622401, 'seasonal': -0.017830922488024405, 'resid': 0.04740911390076451, 'observed': -0.0114641657334839}, {'date': '2009-07-01', 'trend': -0.06674015259826861, 'seasonal': 0.03184779489655618, 'resid': 0.06658614159825002, 'observed': 0.0316937838965376}, {'date': '2009-08-01', 'trend': -0.09139295659941168, 'seasonal': -0.052743362293617736, 'resid': -0.3480259405061433, 'observed': -0.4921622593991728}, {'date': '2009-09-01', 'trend': -0.11437149262309977, 'seasonal': -0.010310036424414365, 'resid': 0.09482267230792384, 'observed': -0.0298588567395903}, {'date': '2009-10-01', 'trend': -0.1062783291583755, 'seasonal': 0.07866104363465767, 'resid': 0.021075795517662227, 'observed': -0.0065414900060556}, {'date': '2009-11-01', 'trend': -0.11133887121885848, 'seasonal': -0.06447017941083495, 'resid': 0.10278713107233983, 'observed': -0.0730219195573536}, {'date': '2009-12-01', 'trend': -0.12049983600396537, 'seasonal': 0.13259497070685478, 'resid': 0.0338158122152519, 'observed': 0.0459109469181413}, {'date': '2010-01-01', 'trend': -0.12134807296956264, 'seasonal': -0.031714528696978224, 'resid': -0.11231774111805665, 'observed': -0.2653803427845975}, {'date': '2010-02-01', 'trend': -0.10525834023147676, 'seasonal': -0.04724202459970091, 'resid': -0.47433787046293924, 'observed': -0.6268382352941169}, {'date': '2010-03-01', 'trend': -0.0819108707400321, 'seasonal': -0.010757930825500861, 'resid': 0.033067709631754365, 'observed': -0.0596010919337786}, {'date': '2010-04-01', 'trend': -0.10373052762973133, 'seasonal': 0.02255283843870718, 'resid': 0.12166813686710085, 'observed': 0.0404904476760767}, {'date': '2010-05-01', 'trend': -0.1475603464738341, 'seasonal': -0.030587662937704367, 'resid': 0.13996186222539117, 'observed': -0.0381861471861473}, {'date': '2010-06-01', 'trend': -0.17029682948139066, 'seasonal': -0.017830922488024405, 'resid': 0.25458618242784387, 'observed': 0.0664584304584288}, {'date': '2010-07-01', 'trend': -0.15216218705932197, 'seasonal': 0.03184779489655618, 'resid': 0.053727892693055786, 'observed': -0.06658649946971}, {'date': '2010-08-01', 'trend': -0.09455752860490758, 'seasonal': -0.052743362293617736, 'resid': 0.13957250057966172, 'observed': -0.0077283903188636}, {'date': '2010-09-01', 'trend': -0.05546174667713392, 'seasonal': -0.010310036424414365, 'resid': 0.11181832507632047, 'observed': 0.0460465419747722}, {'date': '2010-10-01', 'trend': -0.0530929247754636, 'seasonal': 0.07866104363465767, 'resid': -0.6316867729323936, 'observed': -0.6061186540731995}, {'date': '2010-11-01', 'trend': -0.039401117266809255, 'seasonal': -0.06447017941083495, 'resid': -0.42148911107103154, 'observed': -0.5253604077486758}, {'date': '2010-12-01', 'trend': -0.03342727946617814, 'seasonal': 0.13259497070685478, 'resid': -0.14659384831257133, 'observed': -0.0474261570718947}, {'date': '2011-01-01', 'trend': -0.03683063866320891, 'seasonal': -0.031714528696978224, 'resid': 0.3317333466952747, 'observed': 0.2631881793350876}, {'date': '2011-02-01', 'trend': -0.031484501032934334, 'seasonal': -0.04724202459970091, 'resid': 0.30583157112477843, 'observed': 0.2271050454921432}, {'date': '2011-03-01', 'trend': -0.03303165036061288, 'seasonal': -0.010757930825500861, 'resid': 0.06854397473264304, 'observed': 0.0247543935465293}, {'date': '2011-04-01', 'trend': 0.0019066603995984734, 'seasonal': 0.02255283843870718, 'resid': -0.011472811002449254, 'observed': 0.0129866878358564}, {'date': '2011-05-01', 'trend': 0.06281957931593297, 'seasonal': -0.030587662937704367, 'resid': 0.285689076483549, 'observed': 0.3179209928617776}, {'date': '2011-06-01', 'trend': 0.09660137276875817, 'seasonal': -0.017830922488024405, 'resid': -0.2250470526550836, 'observed': -0.1462766023743498}, {'date': '2011-07-01', 'trend': 0.09219874084545351, 'seasonal': 0.03184779489655618, 'resid': -0.0595786231076794, 'observed': 0.0644679126343303}, {'date': '2011-08-01', 'trend': 0.07266339009278908, 'seasonal': -0.052743362293617736, 'resid': -0.03039552709548534, 'observed': -0.010475499296314}, {'date': '2011-09-01', 'trend': 0.06441812681107223, 'seasonal': -0.010310036424414365, 'resid': -0.042446023298720666, 'observed': 0.0116620670879372}, {'date': '2011-10-01', 'trend': 0.06188167776796559, 'seasonal': 0.07866104363465767, 'resid': 0.1262425576560849, 'observed': 0.2667852790587082}, {'date': '2011-11-01', 'trend': 0.04383614949603275, 'seasonal': -0.06447017941083495, 'resid': 0.0842797430262469, 'observed': 0.0636457131114447}, {'date': '2011-12-01', 'trend': 0.03471611807420816, 'seasonal': 0.13259497070685478, 'resid': 0.007019676154726762, 'observed': 0.1743307649357897}, {'date': '2012-01-01', 'trend': 0.04570925645758612, 'seasonal': -0.031714528696978224, 'resid': -0.07822663659251661, 'observed': -0.0642319088319087}, {'date': '2012-02-01', 'trend': 0.044902190815899364, 'seasonal': -0.04724202459970091, 'resid': 0.08801654937899464, 'observed': 0.0856767155951931}, {'date': '2012-03-01', 'trend': 0.042262956633337116, 'seasonal': -0.010757930825500861, 'resid': -0.06320862112556136, 'observed': -0.0317035953177251}, {'date': '2012-04-01', 'trend': 0.03516995902860612, 'seasonal': 0.02255283843870718, 'resid': -0.0491528978017619, 'observed': 0.0085698996655514}, {'date': '2012-05-01', 'trend': 0.02213178752152366, 'seasonal': -0.030587662937704367, 'resid': -0.1022990220781248, 'observed': -0.1107548974943055}, {'date': '2012-06-01', 'trend': 0.018030473650186018, 'seasonal': -0.017830922488024405, 'resid': 0.06331898269578129, 'observed': 0.0635185338579429}, {'date': '2012-07-01', 'trend': 0.019188437545202486, 'seasonal': 0.03184779489655618, 'resid': 0.06747186516135023, 'observed': 0.1185080976031089}, {'date': '2012-08-01', 'trend': 0.0131250513067906, 'seasonal': -0.052743362293617736, 'resid': -0.04426694867874766, 'observed': -0.0838852596655748}, {'date': '2012-09-01', 'trend': 0.009551132283800178, 'seasonal': -0.010310036424414365, 'resid': 0.022489111216318088, 'observed': 0.0217302070757039}, {'date': '2012-10-01', 'trend': 0.020824397347343444, 'seasonal': 0.07866104363465767, 'resid': -0.013000244424603513, 'observed': 0.0864851965573976}, {'date': '2012-11-01', 'trend': 0.01896231043720962, 'seasonal': -0.06447017941083495, 'resid': -0.023462451583598262, 'observed': -0.0689703205572236}, {'date': '2012-12-01', 'trend': 0.0031981506915670166, 'seasonal': 0.13259497070685478, 'resid': 0.07272214429393273, 'observed': 0.2085152656923545}, {'date': '2013-01-01', 'trend': -0.003035419802576943, 'seasonal': -0.031714528696978224, 'resid': -0.035875327608523126, 'observed': -0.0706252761080783}, {'date': '2013-02-01', 'trend': -0.0008950330310897714, 'seasonal': -0.04724202459970091, 'resid': -0.005314129219731817, 'observed': -0.0534511868505225}, {'date': '2013-03-01', 'trend': 0.005412577037834394, 'seasonal': -0.010757930825500861, 'resid': 0.026995604363886767, 'observed': 0.0216502505762203}, {'date': '2013-04-01', 'trend': 0.009242997420316925, 'seasonal': 0.02255283843870718, 'resid': 0.1939785794376204, 'observed': 0.2257744152966445}, {'date': '2013-05-01', 'trend': 0.014444255150244148, 'seasonal': -0.030587662937704367, 'resid': -0.3565060911811502, 'observed': -0.3726494989686104}, {'date': '2013-06-01', 'trend': 0.012049156042391663, 'seasonal': -0.017830922488024405, 'resid': -0.04714493211754186, 'observed': -0.0529266985631746}, {'date': '2013-07-01', 'trend': 0.008257073724790395, 'seasonal': 0.03184779489655618, 'resid': 0.04524276954342472, 'observed': 0.0853476381647713}, {'date': '2013-08-01', 'trend': 0.011644938815555297, 'seasonal': -0.052743362293617736, 'resid': 0.041742905766517235, 'observed': 0.0006444822884548}, {'date': '2013-09-01', 'trend': 0.012858457832634297, 'seasonal': -0.010310036424414365, 'resid': 0.08603468536763466, 'observed': 0.0885831067758546}, {'date': '2013-10-01', 'trend': 0.004237769660182612, 'seasonal': 0.07866104363465767, 'resid': 0.02866357274198722, 'observed': 0.1115623860368275}, {'date': '2013-11-01', 'trend': -0.004335033857545581, 'seasonal': -0.06447017941083495, 'resid': 0.09958788874998034, 'observed': 0.0307826754815998}, {'date': '2013-12-01', 'trend': -0.008880789719724598, 'seasonal': 0.13259497070685478, 'resid': -0.07243428992205868, 'observed': 0.0512798910650715}, {'date': '2014-01-01', 'trend': -0.017619162189213094, 'seasonal': -0.031714528696978224, 'resid': 0.044933813782965515, 'observed': -0.0043998771032258}, {'date': '2014-02-01', 'trend': -0.02311961132920546, 'seasonal': -0.04724202459970091, 'resid': 0.03199381225188907, 'observed': -0.0383678236770173}, {'date': '2014-03-01', 'trend': -0.03397059240086849, 'seasonal': -0.010757930825500861, 'resid': 0.08041986703898045, 'observed': 0.0356913438126111}, {'date': '2014-04-01', 'trend': -0.041985310898930206, 'seasonal': 0.02255283843870718, 'resid': 0.024269278381636228, 'observed': 0.0048368059214132}, {'date': '2014-05-01', 'trend': -0.041198161431693886, 'seasonal': -0.030587662937704367, 'resid': -0.2856733496494574, 'observed': -0.3574591740188557}, {'date': '2014-06-01', 'trend': -0.04789477224075883, 'seasonal': -0.017830922488024405, 'resid': -0.11148946947644248, 'observed': -0.1772151642052257}, {'date': '2014-07-01', 'trend': -0.052218878217136055, 'seasonal': 0.03184779489655618, 'resid': 0.02028624785967833, 'observed': -8.483546090154448e-05}, {'date': '2014-08-01', 'trend': -0.05494124643510578, 'seasonal': -0.052743362293617736, 'resid': 0.06175078528303441, 'observed': -0.0459338234456891}, {'date': '2014-09-01', 'trend': -0.06448933347929107, 'seasonal': -0.010310036424414365, 'resid': -0.05046276330620895, 'observed': -0.1252621332099144}, {'date': '2014-10-01', 'trend': -0.07028956365765798, 'seasonal': 0.07866104363465767, 'resid': 0.12468290209211572, 'observed': 0.1330543820691154}, {'date': '2014-11-01', 'trend': -0.06019034746773441, 'seasonal': -0.06447017941083495, 'resid': 0.15284279354155278, 'observed': 0.0281822666629834}, {'date': '2014-12-01', 'trend': -0.04221359859407286, 'seasonal': 0.13259497070685478, 'resid': -0.1972197316466527, 'observed': -0.1068383595338708}, {'date': '2015-01-01', 'trend': -0.038848053552043765, 'seasonal': -0.031714528696978224, 'resid': 0.12050241231168518, 'observed': 0.0499398300626632}, {'date': '2015-02-01', 'trend': -0.0381625832895537, 'seasonal': -0.04724202459970091, 'resid': -0.0726397601849248, 'observed': -0.1580443680741794}, {'date': '2015-03-01', 'trend': -0.029463840012527352, 'seasonal': -0.010757930825500861, 'resid': -0.033564430012645786, 'observed': -0.073786200850674}, {'date': '2015-04-01', 'trend': -0.03136658171414957, 'seasonal': 0.02255283843870718, 'resid': -0.01607743042066501, 'observed': -0.0248911736961074}, {'date': '2015-05-01', 'trend': -0.04081446354341571, 'seasonal': -0.030587662937704367, 'resid': -0.013947879362049422, 'observed': -0.0853500058431695}, {'date': '2015-06-01', 'trend': -0.03936403641140541, 'seasonal': -0.017830922488024405, 'resid': 0.03931259948639521, 'observed': -0.0178823594130346}, {'date': '2015-07-01', 'trend': -0.03547665094941137, 'seasonal': 0.03184779489655618, 'resid': -0.0750157031915393, 'observed': -0.0786445592443945}, {'date': '2015-08-01', 'trend': -0.02967353755159856, 'seasonal': -0.052743362293617736, 'resid': 0.13149408648278169, 'observed': 0.0490771866375654}, {'date': '2015-09-01', 'trend': -0.034311547465159596, 'seasonal': -0.010310036424414365, 'resid': 0.03311827924503746, 'observed': -0.0115033046445365}, {'date': '2015-10-01', 'trend': -0.04624601382043859, 'seasonal': 0.07866104363465767, 'resid': -0.05878527714941489, 'observed': -0.0263702473351958}, {'date': '2015-11-01', 'trend': -0.02522274760439807, 'seasonal': -0.06447017941083495, 'resid': 0.050550659180140324, 'observed': -0.0391422678350927}, {'date': '2015-12-01', 'trend': 0.006684184979284752, 'seasonal': 0.13259497070685478, 'resid': -0.14398272955368682, 'observed': -0.0047035738675473}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.031714528696978224, 'resid': nan, 'observed': 0.0411022954841966}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.04724202459970091, 'resid': nan, 'observed': -0.0099321119482054}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.010757930825500861, 'resid': nan, 'observed': -0.3332106949021129}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.02255283843870718, 'resid': nan, 'observed': -0.0518938721713642}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.030587662937704367, 'resid': nan, 'observed': 0.4462110818170597}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.017830922488024405, 'resid': nan, 'observed': 0.2163229349351239}], 'diff1_loan_rate_B': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.07046143882383622, 'resid': nan, 'observed': 0.1343589743589746}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.032770515961085024, 'resid': nan, 'observed': -0.2210256410256423}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.0434329852966769, 'resid': nan, 'observed': 0.156666666666668}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.11246094776255657, 'resid': nan, 'observed': 0.0344444444444445}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.018135820501638387, 'resid': nan, 'observed': -0.0802777777777787}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.07022848869063908, 'resid': nan, 'observed': 0.3056250000000009}, {'date': '2008-02-01', 'trend': 0.07253161961495286, 'seasonal': -0.045894739401959414, 'resid': 0.31596845686565844, 'observed': 0.3426053370786519}, {'date': '2008-03-01', 'trend': 0.08760878010878013, 'seasonal': 0.0018055671824235378, 'resid': -0.06804791017630608, 'observed': 0.0213664371148976}, {'date': '2008-04-01', 'trend': 0.10336111111111115, 'seasonal': -0.0004989351068384237, 'resid': 0.0012691199776175741, 'observed': 0.1041312959818903}, {'date': '2008-05-01', 'trend': 0.11530555555555554, 'seasonal': -0.022198231228783477, 'resid': -0.17641872783554527, 'observed': -0.0833114035087732}, {'date': '2008-06-01', 'trend': 0.14536751443001444, 'seasonal': 0.02562000786713773, 'resid': -0.18072237078200068, 'observed': -0.0097348484848485}, {'date': '2008-07-01', 'trend': 0.16546677469571494, 'seasonal': -0.081401970823299, 'resid': 0.005901526093912465, 'observed': 0.0899663299663284}, {'date': '2008-08-01', 'trend': 0.16290479051505097, 'seasonal': 0.07046143882383622, 'resid': 0.05212198614932932, 'observed': 0.2854882154882165}, {'date': '2008-09-01', 'trend': 0.16511287791960133, 'seasonal': 0.032770515961085024, 'resid': -0.20818642418371616, 'observed': -0.0103030303030298}, {'date': '2008-10-01', 'trend': 0.15959744792087738, 'seasonal': 0.0434329852966769, 'resid': 0.12096956678244551, 'observed': 0.3239999999999998}, {'date': '2008-11-01', 'trend': 0.14138640873015867, 'seasonal': -0.11246094776255657, 'resid': 0.1248523168101759, 'observed': 0.153777777777778}, {'date': '2008-12-01', 'trend': 0.13116512161084526, 'seasonal': 0.018135820501638387, 'resid': 0.372574959763418, 'observed': 0.5218759018759016}, {'date': '2009-01-01', 'trend': 0.12665808188724859, 'seasonal': 0.07022848869063908, 'resid': -0.011033003854755474, 'observed': 0.1858535667231322}, {'date': '2009-02-01', 'trend': 0.11628219804886486, 'seasonal': -0.045894739401959414, 'resid': 0.33050169137268, 'observed': 0.4008891500195854}, {'date': '2009-03-01', 'trend': 0.11547369581894905, 'seasonal': 0.0018055671824235378, 'resid': -0.101202541118199, 'observed': 0.0160767218831736}, {'date': '2009-04-01', 'trend': 0.10519924478688923, 'seasonal': -0.0004989351068384237, 'resid': -0.1276496184358118, 'observed': -0.022949308755761}, {'date': '2009-05-01', 'trend': 0.08361755233494368, 'seasonal': -0.022198231228783477, 'resid': -0.45471506045453125, 'observed': -0.3932957393483711}, {'date': '2009-06-01', 'trend': 0.05570939347564919, 'seasonal': 0.02562000786713773, 'resid': -0.02639080485155902, 'observed': 0.0549385964912279}, {'date': '2009-07-01', 'trend': 0.017878874306814792, 'seasonal': -0.081401970823299, 'resid': -0.01935297185958379, 'observed': -0.082876068376068}, {'date': '2009-08-01', 'trend': -0.05598613588513567, 'seasonal': 0.07046143882383622, 'resid': 0.19483409877070224, 'observed': 0.2093094017094028}, {'date': '2009-09-01', 'trend': -0.11263551516313529, 'seasonal': 0.032770515961085024, 'resid': 0.12633672915985508, 'observed': 0.0464717299578048}, {'date': '2009-10-01', 'trend': -0.11396564767314926, 'seasonal': 0.0434329852966769, 'resid': 0.09117107734620196, 'observed': 0.0206384149697296}, {'date': '2009-11-01', 'trend': -0.09415100843104823, 'seasonal': -0.11246094776255657, 'resid': 0.1457907001549597, 'observed': -0.0608212560386451}, {'date': '2009-12-01', 'trend': -0.05602207779516801, 'seasonal': 0.018135820501638387, 'resid': 0.10456538036278631, 'observed': 0.0666791230692567}, {'date': '2010-01-01', 'trend': -0.03414123324984977, 'seasonal': 0.07022848869063908, 'resid': -0.3029693699630377, 'observed': -0.2668821145222484}, {'date': '2010-02-01', 'trend': -0.038466300031433656, 'seasonal': -0.045894739401959414, 'resid': -0.8347743739084519, 'observed': -0.919135413341845}, {'date': '2010-03-01', 'trend': -0.051222045739554606, 'seasonal': 0.0018055671824235378, 'resid': 0.025932661129744267, 'observed': -0.0234838174273868}, {'date': '2010-04-01', 'trend': -0.08204924709401365, 'seasonal': -0.0004989351068384237, 'resid': 0.06723623251531607, 'observed': -0.015311949685536}, {'date': '2010-05-01', 'trend': -0.14012220490245753, 'seasonal': -0.022198231228783477, 'resid': 0.2369386795230695, 'observed': 0.0746182433918285}, {'date': '2010-06-01', 'trend': -0.1759102546194294, 'seasonal': 0.02562000786713773, 'resid': 0.6524091957644453, 'observed': 0.5021189490121536}, {'date': '2010-07-01', 'trend': -0.15017141807608086, 'seasonal': -0.081401970823299, 'resid': 0.22665723709002406, 'observed': -0.0049161518093558}, {'date': '2010-08-01', 'trend': -0.0697244601852828, 'seasonal': 0.07046143882383622, 'resid': 0.02681090374612409, 'observed': 0.0275478823846775}, {'date': '2010-09-01', 'trend': -0.017108422640112805, 'seasonal': 0.032770515961085024, 'resid': -0.093566741033345, 'observed': -0.0779046477123728}, {'date': '2010-10-01', 'trend': -0.01619971963324996, 'seasonal': 0.0434329852966769, 'resid': -0.6220713055305368, 'observed': -0.5948380398671098}, {'date': '2010-11-01', 'trend': 0.007396789260623559, 'seasonal': -0.11246094776255657, 'resid': -0.7340316301025256, 'observed': -0.8390957886044585}, {'date': '2010-12-01', 'trend': 0.010777912766808017, 'seasonal': 0.018135820501638387, 'resid': -0.042873270840701305, 'observed': -0.0139595375722549}, {'date': '2011-01-01', 'trend': -0.011288570651843756, 'seasonal': 0.07022848869063908, 'resid': 0.37254870512083327, 'observed': 0.4314886231596286}, {'date': '2011-02-01', 'trend': -0.013138828366925894, 'seasonal': -0.045894739401959414, 'resid': 0.372254406124316, 'observed': 0.3132208383554307}, {'date': '2011-03-01', 'trend': -0.0032650106639154704, 'seasonal': 0.0018055671824235378, 'resid': 0.008404275440909534, 'observed': 0.0069448319594176}, {'date': '2011-04-01', 'trend': 0.046106888540878604, 'seasonal': -0.0004989351068384237, 'resid': -0.06953968034167218, 'observed': -0.023931726907632}, {'date': '2011-05-01', 'trend': 0.11842691528802644, 'seasonal': -0.022198231228783477, 'resid': 0.5533255500076458, 'observed': 0.6495542340668887}, {'date': '2011-06-01', 'trend': 0.15358747474607576, 'seasonal': 0.02562000786713773, 'resid': -0.17087756012769287, 'observed': 0.0083299224855206}, {'date': '2011-07-01', 'trend': 0.13951867653073943, 'seasonal': -0.081401970823299, 'resid': -0.09883943303780582, 'observed': -0.0407227273303654}, {'date': '2011-08-01', 'trend': 0.10881052011036284, 'seasonal': 0.07046143882383622, 'resid': -0.16032368619048354, 'observed': 0.0189482727437155}, {'date': '2011-09-01', 'trend': 0.10591245912195688, 'seasonal': 0.032770515961085024, 'resid': 0.028983611717797612, 'observed': 0.1676665868008395}, {'date': '2011-10-01', 'trend': 0.12029888775357502, 'seasonal': 0.0434329852966769, 'resid': 0.18078443348448392, 'observed': 0.3445163065347358}, {'date': '2011-11-01', 'trend': 0.09964719123988357, 'seasonal': -0.11246094776255657, 'resid': -0.029955736552082904, 'observed': -0.0427694930747559}, {'date': '2011-12-01', 'trend': 0.08175969247221472, 'seasonal': 0.018135820501638387, 'resid': -0.0663279190826275, 'observed': 0.0335675938912256}, {'date': '2012-01-01', 'trend': 0.09190183274358896, 'seasonal': 0.07022848869063908, 'resid': -0.11581998690615164, 'observed': 0.0463103345280764}, {'date': '2012-02-01', 'trend': 0.1009050265452881, 'seasonal': -0.045894739401959414, 'resid': -0.0936069142453843, 'observed': -0.0385966271020556}, {'date': '2012-03-01', 'trend': 0.09763085440825638, 'seasonal': 0.0018055671824235378, 'resid': 0.18977241210448131, 'observed': 0.2892088336951612}, {'date': '2012-04-01', 'trend': 0.07324583971131912, 'seasonal': -0.0004989351068384237, 'resid': -0.0336683460890211, 'observed': 0.0390785585154596}, {'date': '2012-05-01', 'trend': 0.06075504434068528, 'seasonal': -0.022198231228783477, 'resid': 0.0523464192033007, 'observed': 0.0909032323152025}, {'date': '2012-06-01', 'trend': 0.054106865190672766, 'seasonal': 0.02562000786713773, 'resid': 0.057954080755343716, 'observed': 0.1376809538131542}, {'date': '2012-07-01', 'trend': 0.042441027870710114, 'seasonal': -0.081401970823299, 'resid': 0.11229855080757159, 'observed': 0.0733376078549827}, {'date': '2012-08-01', 'trend': 0.042250133304635284, 'seasonal': 0.07046143882383622, 'resid': 0.00825301667067549, 'observed': 0.120964588799147}, {'date': '2012-09-01', 'trend': 0.028265892129516017, 'seasonal': 0.032770515961085024, 'resid': -0.07396626863395464, 'observed': -0.0129298605433536}, {'date': '2012-10-01', 'trend': 0.012322106610375349, 'seasonal': 0.0434329852966769, 'resid': -0.11588269075461714, 'observed': -0.0601275988475649}, {'date': '2012-11-01', 'trend': 0.0075267811347561175, 'seasonal': -0.11246094776255657, 'resid': 0.16702949004013315, 'observed': 0.0620953234123327}, {'date': '2012-12-01', 'trend': -0.006617476325216561, 'seasonal': 0.018135820501638387, 'resid': -0.24237186637258523, 'observed': -0.2308535221961634}, {'date': '2013-01-01', 'trend': -0.044990516158915986, 'seasonal': 0.07022848869063908, 'resid': 0.0055133824046385005, 'observed': 0.0307513549363616}, {'date': '2013-02-01', 'trend': -0.07684670346080483, 'seasonal': -0.045894739401959414, 'resid': 0.09512232576662744, 'observed': -0.0276191170961368}, {'date': '2013-03-01', 'trend': -0.07540957750188355, 'seasonal': 0.0018055671824235378, 'resid': 0.016213545805839818, 'observed': -0.0573904645136202}, {'date': '2013-04-01', 'trend': -0.05654519169114858, 'seasonal': -0.0004989351068384237, 'resid': 0.0600711310628522, 'observed': 0.0030270042648652}, {'date': '2013-05-01', 'trend': -0.04025695390906634, 'seasonal': -0.022198231228783477, 'resid': 0.07432216028878522, 'observed': 0.0118669751509354}, {'date': '2013-06-01', 'trend': -0.026501686923818615, 'seasonal': 0.02562000786713773, 'resid': -0.12186328900524213, 'observed': -0.122744968061923}, {'date': '2013-07-01', 'trend': -0.017724880517176876, 'seasonal': -0.081401970823299, 'resid': -0.48806257493825056, 'observed': -0.5871894262787265}, {'date': '2013-08-01', 'trend': -0.022383778667604298, 'seasonal': 0.07046143882383622, 'resid': -0.031134532468707926, 'observed': 0.016943127687524}, {'date': '2013-09-01', 'trend': -0.024856579833205748, 'seasonal': 0.032770515961085024, 'resid': 0.11766868745450074, 'observed': 0.12558262358238}, {'date': '2013-10-01', 'trend': -0.024572831959464192, 'seasonal': 0.0434329852966769, 'resid': 0.2352450231471282, 'observed': 0.2541051764843409}, {'date': '2013-11-01', 'trend': -0.04578053795130625, 'seasonal': -0.11246094776255657, 'resid': 0.2970217405642637, 'observed': 0.1387802548504009}, {'date': '2013-12-01', 'trend': -0.07243155863508847, 'seasonal': 0.018135820501638387, 'resid': 0.07688369214516377, 'observed': 0.0225879540117137}, {'date': '2014-01-01', 'trend': -0.05423872138206551, 'seasonal': 0.07022848869063908, 'resid': -0.028036534820687575, 'observed': -0.012046767512114}, {'date': '2014-02-01', 'trend': -0.03397058050497438, 'seasonal': -0.045894739401959414, 'resid': -0.016769230350985306, 'observed': -0.0966345502579191}, {'date': '2014-03-01', 'trend': -0.0424961365791401, 'seasonal': 0.0018055671824235378, 'resid': -0.007031689929556136, 'observed': -0.0477222593262727}, {'date': '2014-04-01', 'trend': -0.05547293814302131, 'seasonal': -0.0004989351068384237, 'resid': 0.056140621297174835, 'observed': 0.0001687480473151}, {'date': '2014-05-01', 'trend': -0.08515159640587747, 'seasonal': -0.022198231228783477, 'resid': -0.38690988480106325, 'observed': -0.4942597124357242}, {'date': '2014-06-01', 'trend': -0.11441681410591502, 'seasonal': 0.02562000786713773, 'resid': -0.16744597064725938, 'observed': -0.2562427768860367}, {'date': '2014-07-01', 'trend': -0.11847281350061632, 'seasonal': -0.081401970823299, 'resid': 0.18281126094185382, 'observed': -0.0170635233820615}, {'date': '2014-08-01', 'trend': -0.12316438274360003, 'seasonal': 0.07046143882383622, 'resid': -0.014044450239189987, 'observed': -0.0667473941589538}, {'date': '2014-09-01', 'trend': -0.135097032058235, 'seasonal': 0.032770515961085024, 'resid': 0.1069863157460306, 'observed': 0.0046597996488806}, {'date': '2014-10-01', 'trend': -0.14024366983363687, 'seasonal': 0.0434329852966769, 'resid': 0.16039544742165096, 'observed': 0.063584762884691}, {'date': '2014-11-01', 'trend': -0.12035307260737171, 'seasonal': -0.11246094776255657, 'resid': -0.15017310948856893, 'observed': -0.3829871298584972}, {'date': '2014-12-01', 'trend': -0.09086346056825195, 'seasonal': 0.018135820501638387, 'resid': -0.08528224601367623, 'observed': -0.1580098860802898}, {'date': '2015-01-01', 'trend': -0.08372327327467737, 'seasonal': 0.07022848869063908, 'resid': 0.084701871691097, 'observed': 0.0712070871070587}, {'date': '2015-02-01', 'trend': -0.0835007527359228, 'seasonal': -0.045894739401959414, 'resid': -0.16309057457081877, 'observed': -0.292486066708701}, {'date': '2015-03-01', 'trend': -0.0816249274058822, 'seasonal': 0.0018055671824235378, 'resid': -0.05843496620327213, 'observed': -0.1382543264267308}, {'date': '2015-04-01', 'trend': -0.08406588342221845, 'seasonal': -0.0004989351068384237, 'resid': 0.051746327067185974, 'observed': -0.0328184914618709}, {'date': '2015-05-01', 'trend': -0.0684165593893712, 'seasonal': -0.022198231228783477, 'resid': 0.10671665112198009, 'observed': 0.0161018605038254}, {'date': '2015-06-01', 'trend': -0.04701617470145597, 'seasonal': 0.02562000786713773, 'resid': -0.03745749405239336, 'observed': -0.0588536608867116}, {'date': '2015-07-01', 'trend': -0.04587836546821761, 'seasonal': -0.081401970823299, 'resid': 0.08419219195591982, 'observed': -0.0430881443355968}, {'date': '2015-08-01', 'trend': -0.034932169714336984, 'seasonal': 0.07046143882383622, 'resid': -0.07091154938480815, 'observed': -0.0353822802753089}, {'date': '2015-09-01', 'trend': -0.01580589910134819, 'seasonal': 0.032770515961085024, 'resid': 0.0013498768264733688, 'observed': 0.0183144936862102}, {'date': '2015-10-01', 'trend': -0.007080095998270134, 'seasonal': 0.0434329852966769, 'resid': -0.04500576484311507, 'observed': -0.0086528755447083}, {'date': '2015-11-01', 'trend': -0.007776782505493978, 'seasonal': -0.11246094776255657, 'resid': 0.18507201562728653, 'observed': 0.064834285359236}, {'date': '2015-12-01', 'trend': 0.0012050536184403146, 'seasonal': 0.018135820501638387, 'resid': -0.1115629429081362, 'observed': -0.0922220687880575}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.07022848869063908, 'resid': nan, 'observed': 0.032726691412547}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.045894739401959414, 'resid': nan, 'observed': 0.0087030270789458}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.0018055671824235378, 'resid': nan, 'observed': 0.0195870744973536}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.0004989351068384237, 'resid': nan, 'observed': 0.0187593820879179}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.022198231228783477, 'resid': nan, 'observed': -0.0521964892193356}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.02562000786713773, 'resid': nan, 'observed': 0.2250087558108724}], 'diff1_loan_rate_C': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.023538262427489662, 'resid': nan, 'observed': 0.2074999999999995}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.05848004070126249, 'resid': nan, 'observed': -0.118333333333334}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.042982962767393634, 'resid': nan, 'observed': -0.0032407407407397}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.13074174163157376, 'resid': nan, 'observed': 0.1411111111111118}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.02963753046734331, 'resid': nan, 'observed': -0.1620370370370381}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.03051548559335375, 'resid': nan, 'observed': 0.4830555555555555}, {'date': '2008-02-01', 'trend': 0.08177216880341873, 'seasonal': -0.009428661869998278, 'resid': 0.16523730114738655, 'observed': 0.237580808080807}, {'date': '2008-03-01', 'trend': 0.07897569444444455, 'seasonal': -0.009181965409746102, 'resid': 0.008248068248478558, 'observed': 0.078041797283177}, {'date': '2008-04-01', 'trend': 0.09493738129154798, 'seasonal': -0.014386009084824822, 'resid': -0.07063578312626287, 'observed': 0.0099155890804603}, {'date': '2008-05-01', 'trend': 0.11193583808167135, 'seasonal': -0.018321804588489938, 'resid': -0.07239296867836602, 'observed': 0.0212210648148154}, {'date': '2008-06-01', 'trend': 0.13923542406561806, 'seasonal': 0.006198504602688547, 'resid': -0.1311310964242989, 'observed': 0.0143028322440077}, {'date': '2008-07-01', 'trend': 0.15975801104653842, 'seasonal': -0.009292603974898495, 'resid': -0.01830100284841311, 'observed': 0.1321644042232268}, {'date': '2008-08-01', 'trend': 0.14655239898989897, 'seasonal': 0.023538262427489662, 'resid': -0.08262271269943802, 'observed': 0.0874679487179506}, {'date': '2008-09-01', 'trend': 0.13420448740276325, 'seasonal': 0.05848004070126249, 'resid': -0.25810119477069143, 'observed': -0.0654166666666657}, {'date': '2008-10-01', 'trend': 0.12909123448091658, 'seasonal': 0.042982962767393634, 'resid': 0.15484887967476466, 'observed': 0.3269230769230749}, {'date': '2008-11-01', 'trend': 0.1266789871705552, 'seasonal': -0.13074174163157376, 'resid': 0.22297301087127616, 'observed': 0.2189102564102576}, {'date': '2008-12-01', 'trend': 0.12436923576842174, 'seasonal': 0.02963753046734331, 'resid': 0.26134711504277275, 'observed': 0.4153538812785378}, {'date': '2009-01-01', 'trend': 0.11464929483655441, 'seasonal': 0.03051548559335375, 'resid': 0.25304194435215915, 'observed': 0.3982067247820673}, {'date': '2009-02-01', 'trend': 0.11559828821777357, 'seasonal': -0.009428661869998278, 'resid': -0.1006746768528264, 'observed': 0.0054949494949489}, {'date': '2009-03-01', 'trend': 0.1351548442760942, 'seasonal': -0.009181965409746102, 'resid': -0.1121951010885707, 'observed': 0.0137777777777774}, {'date': '2009-04-01', 'trend': 0.1295184182180072, 'seasonal': -0.014386009084824822, 'resid': -0.16367087067164196, 'observed': -0.0485384615384596}, {'date': '2009-05-01', 'trend': 0.10425474758308659, 'seasonal': -0.018321804588489938, 'resid': -0.06415176300953446, 'observed': 0.0217811799850622}, {'date': '2009-06-01', 'trend': 0.07938407382962666, 'seasonal': 0.006198504602688547, 'resid': -0.12727389500975692, 'observed': -0.0416913165774417}, {'date': '2009-07-01', 'trend': 0.04480301891962437, 'seasonal': -0.009292603974898495, 'resid': -0.08063044426486578, 'observed': -0.0451200293201399}, {'date': '2009-08-01', 'trend': 0.020944087091242646, 'seasonal': 0.023538262427489662, 'resid': 0.24304587389184482, 'observed': 0.2875282234105771}, {'date': '2009-09-01', 'trend': 0.014079656149836417, 'seasonal': 0.05848004070126249, 'resid': 0.13132070718930378, 'observed': 0.2038804040404027}, {'date': '2009-10-01', 'trend': 0.018584390522217834, 'seasonal': 0.042982962767393634, 'resid': -0.13921557246769328, 'observed': -0.0776482191780818}, {'date': '2009-11-01', 'trend': 0.023467782398942504, 'seasonal': -0.13074174163157376, 'resid': 0.12442741650595125, 'observed': 0.01715345727332}, {'date': '2009-12-01', 'trend': 0.03814044501669851, 'seasonal': 0.02963753046734331, 'resid': -0.04756346515160492, 'observed': 0.0202145103324369}, {'date': '2010-01-01', 'trend': 0.05988987718423532, 'seasonal': 0.03051548559335375, 'resid': -0.12700458488947597, 'observed': -0.0365992221118869}, {'date': '2010-02-01', 'trend': 0.05307828035254506, 'seasonal': -0.009428661869998278, 'resid': -0.1759630859748049, 'observed': -0.1323134674922581}, {'date': '2010-03-01', 'trend': 0.0298807216565112, 'seasonal': -0.009181965409746102, 'resid': -0.0338589040755303, 'observed': -0.0131601478287652}, {'date': '2010-04-01', 'trend': 0.01032291111379784, 'seasonal': -0.014386009084824822, 'resid': 0.09057618697626399, 'observed': 0.086513089005237}, {'date': '2010-05-01', 'trend': -0.022554434520587318, 'seasonal': -0.018321804588489938, 'resid': 0.044807273591834854, 'observed': 0.0039310344827576}, {'date': '2010-06-01', 'trend': -0.04617230149098248, 'seasonal': 0.006198504602688547, 'resid': 0.36827652863930116, 'observed': 0.3283027317510072}, {'date': '2010-07-01', 'trend': -0.040473591202054245, 'seasonal': -0.009292603974898495, 'resid': 0.15663848954924736, 'observed': 0.1068722943722946}, {'date': '2010-08-01', 'trend': -0.019491865253334312, 'seasonal': 0.023538262427489662, 'resid': -0.03198882141657905, 'observed': -0.0279424242424237}, {'date': '2010-09-01', 'trend': -0.0030483304315947, 'seasonal': 0.05848004070126249, 'resid': -0.09282206728107689, 'observed': -0.0373903570114091}, {'date': '2010-10-01', 'trend': -0.008952034962472556, 'seasonal': 0.042982962767393634, 'resid': -0.3397958389563118, 'observed': -0.3057649111513907}, {'date': '2010-11-01', 'trend': 0.008096317190669238, 'seasonal': -0.13074174163157376, 'resid': -0.42114072153771037, 'observed': -0.5437861459786149}, {'date': '2010-12-01', 'trend': 0.021255319357675065, 'seasonal': 0.02963753046734331, 'resid': -0.03656754353013088, 'observed': 0.0143253062948875}, {'date': '2011-01-01', 'trend': 0.005997675966532471, 'seasonal': 0.03051548559335375, 'resid': 0.06954586730005408, 'observed': 0.1060590288599403}, {'date': '2011-02-01', 'trend': 0.000328811447912961, 'seasonal': -0.009428661869998278, 'resid': 0.2376895547272785, 'observed': 0.2285897043051932}, {'date': '2011-03-01', 'trend': 0.014103505047649581, 'seasonal': -0.009181965409746102, 'resid': 0.01565997645763072, 'observed': 0.0205815160955342}, {'date': '2011-04-01', 'trend': 0.05282867463749468, 'seasonal': -0.014386009084824822, 'resid': -0.12736014921280076, 'observed': -0.0889174836601309}, {'date': '2011-05-01', 'trend': 0.09961397300645969, 'seasonal': -0.018321804588489938, 'resid': 0.5072298904055588, 'observed': 0.5885220588235285}, {'date': '2011-06-01', 'trend': 0.1255908836847626, 'seasonal': 0.006198504602688547, 'resid': -0.07226162886907493, 'observed': 0.0595277594183762}, {'date': '2011-07-01', 'trend': 0.12134087167607463, 'seasonal': -0.009292603974898495, 'resid': -0.10258444238367274, 'observed': 0.0094638253175034}, {'date': '2011-08-01', 'trend': 0.1110441126308099, 'seasonal': 0.023538262427489662, 'resid': -0.20116907869280046, 'observed': -0.0665867036345009}, {'date': '2011-09-01', 'trend': 0.11572648891561448, 'seasonal': 0.05848004070126249, 'resid': 0.1576400391574702, 'observed': 0.3318465687743472}, {'date': '2011-10-01', 'trend': 0.12997502514238415, 'seasonal': 0.042982962767393634, 'resid': 0.08144424530935751, 'observed': 0.2544022332191353}, {'date': '2011-11-01', 'trend': 0.10938241985012398, 'seasonal': -0.13074174163157376, 'resid': 0.04025319228746928, 'observed': 0.0188938705060195}, {'date': '2011-12-01', 'trend': 0.08399893170567779, 'seasonal': 0.02963753046734331, 'resid': -0.03854531608349801, 'observed': 0.0750911460895231}, {'date': '2012-01-01', 'trend': 0.10210200836442407, 'seasonal': 0.03051548559335375, 'resid': -0.1893245931009846, 'observed': -0.0567070991432068}, {'date': '2012-02-01', 'trend': 0.12769282473856813, 'seasonal': -0.009428661869998278, 'resid': 0.025969452353417057, 'observed': 0.1442336152219869}, {'date': '2012-03-01', 'trend': 0.12266235492976871, 'seasonal': -0.009181965409746102, 'resid': 0.10383424649402768, 'observed': 0.2173146360140503}, {'date': '2012-04-01', 'trend': 0.0981006307644038, 'seasonal': -0.014386009084824822, 'resid': -0.02740035581575338, 'observed': 0.0563142658638256}, {'date': '2012-05-01', 'trend': 0.08641169864468848, 'seasonal': -0.018321804588489938, 'resid': -0.11902211177087095, 'observed': -0.0509322177146724}, {'date': '2012-06-01', 'trend': 0.09506837094688565, 'seasonal': 0.006198504602688547, 'resid': -0.011488555059705906, 'observed': 0.0897783204898683}, {'date': '2012-07-01', 'trend': 0.1079662325888333, 'seasonal': -0.009292603974898495, 'resid': 0.3150134754419876, 'observed': 0.4136871040559224}, {'date': '2012-08-01', 'trend': 0.10552586473766148, 'seasonal': 0.023538262427489662, 'resid': 0.01430548344138646, 'observed': 0.1433696106065376}, {'date': '2012-09-01', 'trend': 0.09137273738197532, 'seasonal': 0.05848004070126249, 'resid': -0.14869379896111562, 'observed': 0.0011589791221222}, {'date': '2012-10-01', 'trend': 0.0813850371425335, 'seasonal': 0.042982962767393634, 'resid': -0.12875955700732464, 'observed': -0.0043915570973975}, {'date': '2012-11-01', 'trend': 0.08034908737684145, 'seasonal': -0.13074174163157376, 'resid': 0.04754594420411681, 'observed': -0.0028467100506155}, {'date': '2012-12-01', 'trend': 0.07780893990577106, 'seasonal': 0.02963753046734331, 'resid': 0.19714539152577595, 'observed': 0.3045918618988903}, {'date': '2013-01-01', 'trend': 0.03406573649563173, 'seasonal': 0.03051548559335375, 'resid': -0.041240357634815775, 'observed': 0.0233408644541697}, {'date': '2013-02-01', 'trend': -0.014144350512861207, 'seasonal': -0.009428661869998278, 'resid': 0.029189835579346284, 'observed': 0.0056168231964868}, {'date': '2013-03-01', 'trend': -0.014151127597210836, 'seasonal': -0.009181965409746102, 'resid': 0.03958946451003964, 'observed': 0.0162563715030827}, {'date': '2013-04-01', 'trend': 0.005884417671098063, 'seasonal': -0.014386009084824822, 'resid': 0.02616931604191646, 'observed': 0.0176677246281897}, {'date': '2013-05-01', 'trend': 0.01344750950178895, 'seasonal': -0.018321804588489938, 'resid': -0.032274175768945215, 'observed': -0.0371484708556462}, {'date': '2013-06-01', 'trend': -0.021112030362065867, 'seasonal': 0.006198504602688547, 'resid': 0.029944560084530225, 'observed': 0.0150310343251529}, {'date': '2013-07-01', 'trend': -0.057103480777511054, 'seasonal': -0.009292603974898495, 'resid': -0.49500640687029673, 'observed': -0.5614024916227063}, {'date': '2013-08-01', 'trend': -0.07151753908401697, 'seasonal': 0.023538262427489662, 'resid': 0.009396394737863103, 'observed': -0.0385828819186642}, {'date': '2013-09-01', 'trend': -0.0867499712964841, 'seasonal': 0.05848004070126249, 'resid': 0.2112187522181545, 'observed': 0.1829488216229329}, {'date': '2013-10-01', 'trend': -0.09416931841093755, 'seasonal': 0.042982962767393634, 'resid': 0.3458580424847493, 'observed': 0.2946716868412053}, {'date': '2013-11-01', 'trend': -0.11345204937083736, 'seasonal': -0.13074174163157376, 'resid': 0.12379804094977402, 'observed': -0.1203957500526371}, {'date': '2013-12-01', 'trend': -0.1436487743885572, 'seasonal': 0.02963753046734331, 'resid': -0.2932768109103897, 'observed': -0.4072880548316036}, {'date': '2014-01-01', 'trend': -0.13045227407867463, 'seasonal': 0.03051548559335375, 'resid': -0.02863724030070021, 'observed': -0.1285740287860211}, {'date': '2014-02-01', 'trend': -0.10572335453562173, 'seasonal': -0.009428661869998278, 'resid': -0.0732536665138444, 'observed': -0.1884056829194644}, {'date': '2014-03-01', 'trend': -0.11297566582606595, 'seasonal': -0.009181965409746102, 'resid': -0.03314186424436505, 'observed': -0.1552994954801771}, {'date': '2014-04-01', 'trend': -0.1340919811263499, 'seasonal': -0.014386009084824822, 'resid': 0.15963725107574123, 'observed': 0.0111592608645665}, {'date': '2014-05-01', 'trend': -0.15523672623366971, 'seasonal': -0.018321804588489938, 'resid': -0.3198670193074588, 'observed': -0.4934255501296185}, {'date': '2014-06-01', 'trend': -0.14757614909406946, 'seasonal': 0.006198504602688547, 'resid': -0.11203564233476958, 'observed': -0.2534132868261505}, {'date': '2014-07-01', 'trend': -0.12602150861736044, 'seasonal': -0.009292603974898495, 'resid': 0.15907194955803716, 'observed': 0.0237578369657782}, {'date': '2014-08-01', 'trend': -0.11995210145767261, 'seasonal': 0.023538262427489662, 'resid': 0.06616469755630375, 'observed': -0.0302491414738792}, {'date': '2014-09-01', 'trend': -0.11671427012356601, 'seasonal': 0.05848004070126249, 'resid': 0.05879383962979062, 'observed': 0.0005596102074871}, {'date': '2014-10-01', 'trend': -0.11272167572650926, 'seasonal': 0.042982962767393634, 'resid': 0.04000804400895212, 'observed': -0.0297306689501635}, {'date': '2014-11-01', 'trend': -0.0928386868162864, 'seasonal': -0.13074174163157376, 'resid': -0.07988684838908375, 'observed': -0.3034672768369439}, {'date': '2014-12-01', 'trend': -0.06289602463930324, 'seasonal': 0.02963753046734331, 'resid': -0.007104182524930561, 'observed': -0.0403626766968905}, {'date': '2015-01-01', 'trend': -0.05306681380896677, 'seasonal': 0.03051548559335375, 'resid': 0.04436329273589483, 'observed': 0.0218119645202818}, {'date': '2015-02-01', 'trend': -0.05349604629974409, 'seasonal': -0.009428661869998278, 'resid': -0.13020119622351675, 'observed': -0.1931259043932591}, {'date': '2015-03-01', 'trend': -0.053546988518803344, 'seasonal': -0.009181965409746102, 'resid': -0.01014236805927456, 'observed': -0.072871321987824}, {'date': '2015-04-01', 'trend': -0.05173856098857298, 'seasonal': -0.014386009084824822, 'resid': 0.0906779229749733, 'observed': 0.0245533529015755}, {'date': '2015-05-01', 'trend': -0.04497049651300695, 'seasonal': -0.018321804588489938, 'resid': 0.03366439278021779, 'observed': -0.0296279083212791}, {'date': '2015-06-01', 'trend': -0.03874878820579345, 'seasonal': 0.006198504602688547, 'resid': 0.03396324721621081, 'observed': 0.0014129636131059}, {'date': '2015-07-01', 'trend': -0.029666649630916878, 'seasonal': -0.009292603974898495, 'resid': 0.04379190006041227, 'observed': 0.0048326464545969}, {'date': '2015-08-01', 'trend': -0.006025474592698351, 'seasonal': 0.023538262427489662, 'resid': -0.03913831857614461, 'observed': -0.0216255307413533}, {'date': '2015-09-01', 'trend': 0.013596104455676111, 'seasonal': 0.05848004070126249, 'resid': -0.0813627589393993, 'observed': -0.0092866137824607}, {'date': '2015-10-01', 'trend': 0.01692957780197725, 'seasonal': 0.042982962767393634, 'resid': -0.03639472480405788, 'observed': 0.023517815765313}, {'date': '2015-11-01', 'trend': 0.01643604414209548, 'seasonal': -0.13074174163157376, 'resid': -0.07997651664935754, 'observed': -0.1942822141388358}, {'date': '2015-12-01', 'trend': 0.02757739963634078, 'seasonal': 0.02963753046734331, 'resid': -0.05744167012555859, 'observed': -0.0002267400218745}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.03051548559335375, 'resid': nan, 'observed': 0.1996473536423035}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.009428661869998278, 'resid': nan, 'observed': 0.196426907401964}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.009181965409746102, 'resid': nan, 'observed': 0.00849376337794}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.014386009084824822, 'resid': nan, 'observed': 0.0231916278470389}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.018321804588489938, 'resid': nan, 'observed': -0.040110991103905}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.006198504602688547, 'resid': nan, 'observed': 0.2792885782576189}], 'diff1_loan_rate_D': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.04796336379262829, 'resid': nan, 'observed': -0.4674444444444443}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.01064043177203127, 'resid': nan, 'observed': 0.1696666666666679}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.040188196506185894, 'resid': nan, 'observed': 0.3007017543859636}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.13132803383378105, 'resid': nan, 'observed': -0.1279239766081872}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.050693663657927046, 'resid': nan, 'observed': -0.1305555555555546}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.022104325250881424, 'resid': nan, 'observed': 0.5293111111111095}, {'date': '2008-02-01', 'trend': 0.07385787037037043, 'seasonal': 0.08221064578468242, 'resid': 0.07165740977087458, 'observed': 0.2277259259259274}, {'date': '2008-03-01', 'trend': 0.09349305555555552, 'seasonal': 0.046161442202006006, 'resid': -0.22960481392738952, 'observed': -0.089950316169828}, {'date': '2008-04-01', 'trend': 0.08309984520123835, 'seasonal': -0.025692635499406227, 'resid': 0.054878050107287774, 'observed': 0.1122852598091199}, {'date': '2008-05-01', 'trend': 0.09042138044616072, 'seasonal': -0.009329044731091838, 'resid': -0.36535320528028664, 'observed': -0.2842608695652178}, {'date': '2008-06-01', 'trend': 0.13210592677931376, 'seasonal': -0.0300039488367143, 'resid': 0.21431468872406756, 'observed': 0.316416666666667}, {'date': '2008-07-01', 'trend': 0.14880971204721205, 'seasonal': 0.04198757175226802, 'resid': -0.16801395046614717, 'observed': 0.0227833333333329}, {'date': '2008-08-01', 'trend': 0.1261103961798854, 'seasonal': 0.04796336379262829, 'resid': -0.02644042663918049, 'observed': 0.1476333333333332}, {'date': '2008-09-01', 'trend': 0.11906648408409015, 'seasonal': 0.01064043177203127, 'resid': -0.10387358252278851, 'observed': 0.0258333333333329}, {'date': '2008-10-01', 'trend': 0.12167145209381695, 'seasonal': 0.040188196506185894, 'resid': 0.03323839061568354, 'observed': 0.1950980392156864}, {'date': '2008-11-01', 'trend': 0.1262629603434582, 'seasonal': -0.13132803383378105, 'resid': 0.15846165793054953, 'observed': 0.1533965844402267}, {'date': '2008-12-01', 'trend': 0.1160520833333332, 'seasonal': -0.050693663657927046, 'resid': 0.5231945757162989, 'observed': 0.588552995391705}, {'date': '2009-01-01', 'trend': 0.1072321969696969, 'seasonal': -0.022104325250881424, 'resid': 0.12596553487459233, 'observed': 0.2110934065934078}, {'date': '2009-02-01', 'trend': 0.12594294576196746, 'seasonal': 0.08221064578468242, 'resid': -0.20699354191885957, 'observed': 0.0011600496277903}, {'date': '2009-03-01', 'trend': 0.13928118530020703, 'seasonal': 0.046161442202006006, 'resid': -0.21788095767298993, 'observed': -0.0324383301707769}, {'date': '2009-04-01', 'trend': 0.13767138626283845, 'seasonal': -0.025692635499406227, 'resid': 0.005313755280079668, 'observed': 0.1172925060435119}, {'date': '2009-05-01', 'trend': 0.1254146494349873, 'seasonal': -0.009329044731091838, 'resid': -0.2951575225121146, 'observed': -0.1790719178082191}, {'date': '2009-06-01', 'trend': 0.09078998616242023, 'seasonal': -0.0300039488367143, 'resid': -0.09461937065903793, 'observed': -0.033833333333332}, {'date': '2009-07-01', 'trend': 0.055669395456693874, 'seasonal': 0.04198757175226802, 'resid': 0.06369909339709831, 'observed': 0.1613560606060602}, {'date': '2009-08-01', 'trend': 0.04727055850255738, 'seasonal': 0.04796336379262829, 'resid': 0.36288465477991383, 'observed': 0.4581185770750995}, {'date': '2009-09-01', 'trend': 0.04948054502565673, 'seasonal': 0.01064043177203127, 'resid': -0.024655138288372395, 'observed': 0.0354658385093156}, {'date': '2009-10-01', 'trend': 0.04255634892211584, 'seasonal': 0.040188196506185894, 'resid': 0.06408581171455706, 'observed': 0.1468303571428588}, {'date': '2009-11-01', 'trend': 0.049111586757990926, 'seasonal': -0.13132803383378105, 'resid': -0.01028097027958308, 'observed': -0.0924974173553732}, {'date': '2009-12-01', 'trend': 0.07381842320261445, 'seasonal': -0.050693663657927046, 'resid': -0.019669680898992403, 'observed': 0.003455078645695}, {'date': '2010-01-01', 'trend': 0.08011500583218881, 'seasonal': -0.022104325250881424, 'resid': -0.10471353417932248, 'observed': -0.0467028535980151}, {'date': '2010-02-01', 'trend': 0.05376202226800264, 'seasonal': 0.08221064578468242, 'resid': -0.07858844513274756, 'observed': 0.0573842229199375}, {'date': '2010-03-01', 'trend': 0.03436319309464945, 'seasonal': 0.046161442202006006, 'resid': -0.11614746220519535, 'observed': -0.0356228269085399}, {'date': '2010-04-01', 'trend': 0.017476626779975968, 'seasonal': -0.025692635499406227, 'resid': -0.037487694984276135, 'observed': -0.0457037037037064}, {'date': '2010-05-01', 'trend': -0.01581361177595589, 'seasonal': -0.009329044731091838, 'resid': 0.16639265650704893, 'observed': 0.1412500000000012}, {'date': '2010-06-01', 'trend': -0.03246178649885558, 'seasonal': -0.0300039488367143, 'resid': 0.30127455886498217, 'observed': 0.2388088235294123}, {'date': '2010-07-01', 'trend': -0.020026136683425844, 'seasonal': 0.04198757175226802, 'resid': 0.017870451784258425, 'observed': 0.0398318868531006}, {'date': '2010-08-01', 'trend': 0.004130948162431072, 'seasonal': 0.04796336379262829, 'resid': -0.10492316666746845, 'observed': -0.0528288547124091}, {'date': '2010-09-01', 'trend': 0.012749065295937015, 'seasonal': 0.01064043177203127, 'resid': 0.05745187306837942, 'observed': 0.0808413701363477}, {'date': '2010-10-01', 'trend': 0.012119709451035108, 'seasonal': 0.040188196506185894, 'resid': -0.3561306719935578, 'observed': -0.3038227660363368}, {'date': '2010-11-01', 'trend': 0.05587205949157374, 'seasonal': -0.13132803383378105, 'resid': -0.3653540451763349, 'observed': -0.4408100195185422}, {'date': '2010-12-01', 'trend': 0.09050694421469359, 'seasonal': -0.050693663657927046, 'resid': -0.08760179309749513, 'observed': -0.0477885125407286}, {'date': '2011-01-01', 'trend': 0.07899627140878604, 'seasonal': -0.022104325250881424, 'resid': 0.2461043870008176, 'observed': 0.3029963331587222}, {'date': '2011-02-01', 'trend': 0.07878745905895457, 'seasonal': 0.08221064578468242, 'resid': 0.12645696762012915, 'observed': 0.2874550724637661}, {'date': '2011-03-01', 'trend': 0.08576555403344432, 'seasonal': 0.046161442202006006, 'resid': -0.19078586148367616, 'observed': -0.0588588652482258}, {'date': '2011-04-01', 'trend': 0.12693172699165314, 'seasonal': -0.025692635499406227, 'resid': -0.13881129713391321, 'observed': -0.0375722056416663}, {'date': '2011-05-01', 'trend': 0.18097642566228855, 'seasonal': -0.009329044731091838, 'resid': 1.0115275219796918, 'observed': 1.1831749029108884}, {'date': '2011-06-01', 'trend': 0.19989917890120262, 'seasonal': -0.0300039488367143, 'resid': -0.14177407609108672, 'observed': 0.0281211539734016}, {'date': '2011-07-01', 'trend': 0.1832074048859894, 'seasonal': 0.04198757175226802, 'resid': -0.25093156757092755, 'observed': -0.0257365909326701}, {'date': '2011-08-01', 'trend': 0.1668005792428685, 'seasonal': 0.04796336379262829, 'resid': -0.20703581635809032, 'observed': 0.0077281266774065}, {'date': '2011-09-01', 'trend': 0.20585194847521862, 'seasonal': 0.01064043177203127, 'resid': -0.02873371211296368, 'observed': 0.1877586681342862}, {'date': '2011-10-01', 'trend': 0.24093391480414197, 'seasonal': 0.040188196506185894, 'resid': 0.2961259756524087, 'observed': 0.5772480869627366}, {'date': '2011-11-01', 'trend': 0.19238724632812745, 'seasonal': -0.13132803383378105, 'resid': -0.0858673169167124, 'observed': -0.024808104422366}, {'date': '2011-12-01', 'trend': 0.14227929323333952, 'seasonal': -0.050693663657927046, 'resid': -0.10122997947837928, 'observed': -0.0096443499029668}, {'date': '2012-01-01', 'trend': 0.15618150765586591, 'seasonal': -0.022104325250881424, 'resid': -0.2698275882491408, 'observed': -0.1357504058441563}, {'date': '2012-02-01', 'trend': 0.17674864418415093, 'seasonal': 0.08221064578468242, 'resid': 0.07347870606290956, 'observed': 0.3324379960317429}, {'date': '2012-03-01', 'trend': 0.17171458824491456, 'seasonal': 0.046161442202006006, 'resid': 0.6155150423132797, 'observed': 0.8333910727602003}, {'date': '2012-04-01', 'trend': 0.1407052947991684, 'seasonal': -0.025692635499406227, 'resid': -0.20286761105569498, 'observed': -0.0878549517559328}, {'date': '2012-05-01', 'trend': 0.11797072752086618, 'seasonal': -0.009329044731091838, 'resid': -0.040304077188967946, 'observed': 0.0683376056008064}, {'date': '2012-06-01', 'trend': 0.11613740913534441, 'seasonal': -0.0300039488367143, 'resid': -0.1457658832900574, 'observed': -0.0596324229914273}, {'date': '2012-07-01', 'trend': 0.11817472858706453, 'seasonal': 0.04198757175226802, 'resid': 0.23550783183346014, 'observed': 0.3956701321727927}, {'date': '2012-08-01', 'trend': 0.10930929744230405, 'seasonal': 0.04796336379262829, 'resid': -0.07733998098414745, 'observed': 0.0799326802507849}, {'date': '2012-09-01', 'trend': 0.06433338145952333, 'seasonal': 0.01064043177203127, 'resid': -0.0802370412123206, 'observed': -0.005263227980766}, {'date': '2012-10-01', 'trend': 0.034702426696149306, 'seasonal': 0.040188196506185894, 'resid': -0.0488436828224537, 'observed': 0.0260469403798815}, {'date': '2012-11-01', 'trend': 0.03676718804759696, 'seasonal': -0.13132803383378105, 'resid': 0.07532427326741989, 'observed': -0.0192365725187642}, {'date': '2012-12-01', 'trend': 0.03853726412917851, 'seasonal': -0.050693663657927046, 'resid': -0.047059123530342556, 'observed': -0.0592155230590911}, {'date': '2013-01-01', 'trend': 0.025288611222311335, 'seasonal': -0.022104325250881424, 'resid': -0.040467851818179315, 'observed': -0.0372835658467494}, {'date': '2013-02-01', 'trend': 0.007680658465607946, 'seasonal': 0.08221064578468242, 'resid': -0.06869049569020547, 'observed': 0.0212008085600849}, {'date': '2013-03-01', 'trend': 0.008332600125998891, 'seasonal': 0.046161442202006006, 'resid': 0.010712234317116001, 'observed': 0.0652062766451209}, {'date': '2013-04-01', 'trend': 0.014738908100739245, 'seasonal': -0.025692635499406227, 'resid': -0.01985934256316332, 'observed': -0.0308130699618303}, {'date': '2013-05-01', 'trend': 0.019331139025670276, 'seasonal': -0.009329044731091838, 'resid': 0.050847901946869264, 'observed': 0.0608499962414477}, {'date': '2013-06-01', 'trend': 0.001970804296641291, 'seasonal': -0.0300039488367143, 'resid': 0.01837015686596121, 'observed': -0.0096629876741118}, {'date': '2013-07-01', 'trend': -0.02493018718334297, 'seasonal': 0.04198757175226802, 'resid': 0.01067564252174015, 'observed': 0.0277330270906652}, {'date': '2013-08-01', 'trend': -0.04010684759284679, 'seasonal': 0.04796336379262829, 'resid': 0.017422402972249493, 'observed': 0.025278919172031}, {'date': '2013-09-01', 'trend': -0.053070702835793224, 'seasonal': 0.01064043177203127, 'resid': 0.10746740401113256, 'observed': 0.0650371329473706}, {'date': '2013-10-01', 'trend': -0.0561201651993278, 'seasonal': 0.040188196506185894, 'resid': 0.12542993953865528, 'observed': 0.1094979708455134}, {'date': '2013-11-01', 'trend': -0.08952130837439813, 'seasonal': -0.13132803383378105, 'resid': 0.2283752814221278, 'observed': 0.0075259392139486}, {'date': '2013-12-01', 'trend': -0.14205340870375593, 'seasonal': -0.050693663657927046, 'resid': -0.30987899592681656, 'observed': -0.5026260682884995}, {'date': '2014-01-01', 'trend': -0.1583836455184818, 'seasonal': -0.022104325250881424, 'resid': -0.05900884536759997, 'observed': -0.2394968161369632}, {'date': '2014-02-01', 'trend': -0.15824506507128125, 'seasonal': 0.08221064578468242, 'resid': -0.06479137169119416, 'observed': -0.140825790977793}, {'date': '2014-03-01', 'trend': -0.16072772394039378, 'seasonal': 0.046161442202006006, 'resid': 0.03066663209067208, 'observed': -0.0838996496477157}, {'date': '2014-04-01', 'trend': -0.1713331784156777, 'seasonal': -0.025692635499406227, 'resid': 0.2421315735212604, 'observed': 0.0451057596061765}, {'date': '2014-05-01', 'trend': -0.18856976385452964, 'seasonal': -0.009329044731091838, 'resid': -0.6187974609426257, 'observed': -0.8166962695282471}, {'date': '2014-06-01', 'trend': -0.1798000829710844, 'seasonal': -0.0300039488367143, 'resid': -0.1830830980012057, 'observed': -0.3928871298090044}, {'date': '2014-07-01', 'trend': -0.15000924889187242, 'seasonal': 0.04198757175226802, 'resid': 0.1270531628117415, 'observed': 0.0190314856721371}, {'date': '2014-08-01', 'trend': -0.12548662217305767, 'seasonal': 0.04796336379262829, 'resid': 0.11482964970380218, 'observed': 0.0373063913233728}, {'date': '2014-09-01', 'trend': -0.10523706450073718, 'seasonal': 0.01064043177203127, 'resid': 0.08802248066603362, 'observed': -0.0065741520626723}, {'date': '2014-10-01', 'trend': -0.09922083949678266, 'seasonal': 0.040188196506185894, 'resid': -0.014389008560660332, 'observed': -0.0734216515512571}, {'date': '2014-11-01', 'trend': -0.06476193929068524, 'seasonal': -0.13132803383378105, 'resid': -0.027142515797261096, 'observed': -0.2232324889217274}, {'date': '2014-12-01', 'trend': -0.014173055074311044, 'seasonal': -0.050693663657927046, 'resid': 0.003471419782099597, 'observed': -0.0613952989501385}, {'date': '2015-01-01', 'trend': -0.0005362311660852649, 'seasonal': -0.022104325250881424, 'resid': 0.05689298884272978, 'observed': 0.0342524324257631}, {'date': '2015-02-01', 'trend': -0.0042220906000517075, 'seasonal': 0.08221064578468242, 'resid': 0.09597944652640449, 'observed': 0.1739680017110352}, {'date': '2015-03-01', 'trend': -0.004899362518352068, 'seasonal': 0.046161442202006006, 'resid': 0.046033862115494165, 'observed': 0.0872959417991481}, {'date': '2015-04-01', 'trend': -0.001219038622103299, 'seasonal': -0.025692635499406227, 'resid': 0.04521124237573082, 'observed': 0.0182995682542213}, {'date': '2015-05-01', 'trend': 0.007099710463441603, 'seasonal': -0.009329044731091838, 'resid': 0.039352861037695835, 'observed': 0.0371235267700456}, {'date': '2015-06-01', 'trend': 0.017638544788709968, 'seasonal': -0.0300039488367143, 'resid': -0.020208300866312073, 'observed': -0.0325737049143164}, {'date': '2015-07-01', 'trend': 0.03136625158651259, 'seasonal': 0.04198757175226802, 'resid': -0.08735198876391281, 'observed': -0.0139981654251322}, {'date': '2015-08-01', 'trend': 0.06480069347258703, 'seasonal': 0.04796336379262829, 'resid': -0.13088864125976782, 'observed': -0.0181245839945525}, {'date': '2015-09-01', 'trend': 0.08889547350580251, 'seasonal': 0.01064043177203127, 'resid': -0.06693360806178938, 'observed': 0.0326022972160444}, {'date': '2015-10-01', 'trend': 0.08654955477113249, 'seasonal': 0.040188196506185894, 'resid': -0.1510080785973219, 'observed': -0.0242703273200035}, {'date': '2015-11-01', 'trend': 0.08360188763677234, 'seasonal': -0.13132803383378105, 'resid': -0.025007688902894593, 'observed': -0.0727338350999033}, {'date': '2015-12-01', 'trend': 0.1044494817114668, 'seasonal': -0.050693663657927046, 'resid': -0.01271774701906156, 'observed': 0.0410380710344782}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.022104325250881424, 'resid': nan, 'observed': 0.2612840255884094}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.08221064578468242, 'resid': nan, 'observed': 0.7493630138141754}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.046161442202006006, 'resid': nan, 'observed': 0.0901756504931796}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.025692635499406227, 'resid': nan, 'observed': -0.0408821900718905}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.009329044731091838, 'resid': nan, 'observed': 0.0255612738715136}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.0300039488367143, 'resid': nan, 'observed': 0.4793308057768826}], 'diff1_FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.03606088789682545, 'resid': nan, 'observed': -0.2400000000000002}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.00277963789682541, 'resid': nan, 'observed': -0.0799999999999991}, {'date': '2007-10-01', 'trend': nan, 'seasonal': -0.08331411210317463, 'resid': nan, 'observed': -0.1800000000000006}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05039744543650794, 'resid': nan, 'observed': -0.2699999999999996}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.009342137896825394, 'resid': nan, 'observed': -0.25}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.01609064980158735, 'resid': nan, 'observed': -0.3000000000000002}, {'date': '2008-02-01', 'trend': -0.2612499999999999, 'seasonal': -0.05034536210317461, 'resid': -0.6484046378968253, 'observed': -0.96}, {'date': '2008-03-01', 'trend': -0.2562499999999999, 'seasonal': 0.007935887896825382, 'resid': -0.12168588789682556, 'observed': -0.3700000000000001}, {'date': '2008-04-01', 'trend': -0.2883333333333333, 'seasonal': 0.012935887896825347, 'resid': -0.054602554563492084, 'observed': -0.33}, {'date': '2008-05-01', 'trend': -0.32875, 'seasonal': 0.013196304563492082, 'resid': 0.015553695436508082, 'observed': -0.2999999999999998}, {'date': '2008-06-01', 'trend': -0.3408333333333333, 'seasonal': 0.05111297123015873, 'resid': 0.3097203621031746, 'observed': 0.02}, {'date': '2008-07-01', 'trend': -0.32791666666666663, 'seasonal': 0.03460255456349204, 'resid': 0.30331411210317427, 'observed': 0.0099999999999997}, {'date': '2008-08-01', 'trend': -0.27291666666666664, 'seasonal': 0.03606088789682545, 'resid': 0.22685577876984153, 'observed': -0.0099999999999997}, {'date': '2008-09-01', 'trend': -0.21625, 'seasonal': 0.00277963789682541, 'resid': 0.023470362103174696, 'observed': -0.1899999999999999}, {'date': '2008-10-01', 'trend': -0.18999999999999997, 'seasonal': -0.08331411210317463, 'resid': -0.5666858878968255, 'observed': -0.8400000000000001}, {'date': '2008-11-01', 'trend': -0.16374999999999998, 'seasonal': -0.05039744543650794, 'resid': -0.3658525545634921, 'observed': -0.58}, {'date': '2008-12-01', 'trend': -0.14958333333333332, 'seasonal': 0.009342137896825394, 'resid': -0.08975880456349208, 'observed': -0.23}, {'date': '2009-01-01', 'trend': -0.15166666666666664, 'seasonal': 0.01609064980158735, 'resid': 0.1255760168650793, 'observed': -0.01}, {'date': '2009-02-01', 'trend': -0.15374999999999994, 'seasonal': -0.05034536210317461, 'resid': 0.27409536210317453, 'observed': 0.07}, {'date': '2009-03-01', 'trend': -0.14583333333333331, 'seasonal': 0.007935887896825382, 'resid': 0.09789744543650793, 'observed': -0.04}, {'date': '2009-04-01', 'trend': -0.1045833333333333, 'seasonal': 0.012935887896825347, 'resid': 0.061647445436507964, 'observed': -0.03}, {'date': '2009-05-01', 'trend': -0.04666666666666665, 'seasonal': 0.013196304563492082, 'resid': 0.06347036210317457, 'observed': 0.03}, {'date': '2009-06-01', 'trend': -0.012916666666666656, 'seasonal': 0.05111297123015873, 'resid': -0.008196304563492074, 'observed': 0.03}, {'date': '2009-07-01', 'trend': -0.00333333333333332, 'seasonal': 0.03460255456349204, 'resid': -0.08126922123015862, 'observed': -0.0499999999999999}, {'date': '2009-08-01', 'trend': -0.0054166666666666495, 'seasonal': 0.03606088789682545, 'resid': -0.030644221230158798, 'observed': 0.0}, {'date': '2009-09-01', 'trend': -0.004583333333333315, 'seasonal': 0.00277963789682541, 'resid': -0.008196304563492095, 'observed': -0.01}, {'date': '2009-10-01', 'trend': 0.0012500000000000163, 'seasonal': -0.08331411210317463, 'resid': 0.05206411210317462, 'observed': -0.03}, {'date': '2009-11-01', 'trend': 0.002916666666666683, 'seasonal': -0.05039744543650794, 'resid': 0.047480778769841255, 'observed': 0.0}, {'date': '2009-12-01', 'trend': -0.00041666666666665005, 'seasonal': 0.009342137896825394, 'resid': -0.008925471230158744, 'observed': 0.0}, {'date': '2010-01-01', 'trend': -0.00041666666666665374, 'seasonal': 0.01609064980158735, 'resid': -0.025673983134920596, 'observed': -0.0099999999999999}, {'date': '2010-02-01', 'trend': 0.0020833333333333415, 'seasonal': -0.05034536210317461, 'resid': 0.06826202876984128, 'observed': 0.02}, {'date': '2010-03-01', 'trend': 0.002916666666666675, 'seasonal': 0.007935887896825382, 'resid': 0.01914744543650794, 'observed': 0.03}, {'date': '2010-04-01', 'trend': 0.004583333333333341, 'seasonal': 0.012935887896825347, 'resid': 0.022480778769841312, 'observed': 0.04}, {'date': '2010-05-01', 'trend': 0.0058333333333333405, 'seasonal': 0.013196304563492082, 'resid': -0.01902963789682542, 'observed': 0.0}, {'date': '2010-06-01', 'trend': 0.005416666666666674, 'seasonal': 0.05111297123015873, 'resid': -0.0765296378968254, 'observed': -0.02}, {'date': '2010-07-01', 'trend': 0.005000000000000007, 'seasonal': 0.03460255456349204, 'resid': -0.03960255456349204, 'observed': 0.0}, {'date': '2010-08-01', 'trend': 0.0037500000000000077, 'seasonal': 0.03606088789682545, 'resid': -0.029810887896825455, 'observed': 0.01}, {'date': '2010-09-01', 'trend': 0.00041666666666667894, 'seasonal': 0.00277963789682541, 'resid': -0.003196304563492089, 'observed': 0.0}, {'date': '2010-10-01', 'trend': -0.004999999999999984, 'seasonal': -0.08331411210317463, 'resid': 0.08831411210317461, 'observed': 0.0}, {'date': '2010-11-01', 'trend': -0.008749999999999982, 'seasonal': -0.05039744543650794, 'resid': 0.05914744543650792, 'observed': 0.0}, {'date': '2010-12-01', 'trend': -0.008333333333333316, 'seasonal': 0.009342137896825394, 'resid': -0.011008804563492078, 'observed': -0.01}, {'date': '2011-01-01', 'trend': -0.008333333333333312, 'seasonal': 0.01609064980158735, 'resid': -0.017757316468253938, 'observed': -0.0099999999999999}, {'date': '2011-02-01', 'trend': -0.008333333333333309, 'seasonal': -0.05034536210317461, 'resid': 0.04867869543650792, 'observed': -0.01}, {'date': '2011-03-01', 'trend': -0.008333333333333307, 'seasonal': 0.007935887896825382, 'resid': -0.019602554563491977, 'observed': -0.0199999999999999}, {'date': '2011-04-01', 'trend': -0.009583333333333305, 'seasonal': 0.012935887896825347, 'resid': -0.043352554563492046, 'observed': -0.04}, {'date': '2011-05-01', 'trend': -0.009583333333333305, 'seasonal': 0.013196304563492082, 'resid': -0.013612971230158777, 'observed': -0.01}, {'date': '2011-06-01', 'trend': -0.009166666666666639, 'seasonal': 0.05111297123015873, 'resid': -0.04194630456349209, 'observed': 0.0}, {'date': '2011-07-01', 'trend': -0.008333333333333309, 'seasonal': 0.03460255456349204, 'resid': -0.04626922123015863, 'observed': -0.0199999999999999}, {'date': '2011-08-01', 'trend': -0.006249999999999984, 'seasonal': 0.03606088789682545, 'resid': 0.00018911210317453658, 'observed': 0.03}, {'date': '2011-09-01', 'trend': -0.002916666666666654, 'seasonal': 0.00277963789682541, 'resid': -0.019862971230158757, 'observed': -0.02}, {'date': '2011-10-01', 'trend': 0.001250000000000007, 'seasonal': -0.08331411210317463, 'resid': 0.07206411210317473, 'observed': -0.0099999999999999}, {'date': '2011-11-01', 'trend': 0.004583333333333336, 'seasonal': -0.05039744543650794, 'resid': 0.0558141121031745, 'observed': 0.0099999999999999}, {'date': '2011-12-01', 'trend': 0.005833333333333333, 'seasonal': 0.009342137896825394, 'resid': -0.025175471230158626, 'observed': -0.0099999999999999}, {'date': '2012-01-01', 'trend': 0.006666666666666661, 'seasonal': 0.01609064980158735, 'resid': -0.012757316468254112, 'observed': 0.0099999999999999}, {'date': '2012-02-01', 'trend': 0.004999999999999991, 'seasonal': -0.05034536210317461, 'resid': 0.06534536210317463, 'observed': 0.02}, {'date': '2012-03-01', 'trend': 0.003749999999999992, 'seasonal': 0.007935887896825382, 'resid': 0.018314112103174622, 'observed': 0.03}, {'date': '2012-04-01', 'trend': 0.006249999999999983, 'seasonal': 0.012935887896825347, 'resid': -0.00918588789682533, 'observed': 0.01}, {'date': '2012-05-01', 'trend': 0.007083333333333312, 'seasonal': 0.013196304563492082, 'resid': -0.00027963789682549506, 'observed': 0.0199999999999999}, {'date': '2012-06-01', 'trend': 0.007083333333333312, 'seasonal': 0.05111297123015873, 'resid': -0.05819630456349204, 'observed': 0.0}, {'date': '2012-07-01', 'trend': 0.006249999999999983, 'seasonal': 0.03460255456349204, 'resid': -0.04085255456349202, 'observed': 0.0}, {'date': '2012-08-01', 'trend': 0.004583333333333321, 'seasonal': 0.03606088789682545, 'resid': -0.07064422123015876, 'observed': -0.03}, {'date': '2012-09-01', 'trend': 0.0024999999999999875, 'seasonal': 0.00277963789682541, 'resid': 0.004720362103174603, 'observed': 0.01}, {'date': '2012-10-01', 'trend': 0.0008333333333333214, 'seasonal': -0.08331411210317463, 'resid': 0.10248077876984121, 'observed': 0.0199999999999999}, {'date': '2012-11-01', 'trend': -0.0016666666666666746, 'seasonal': -0.05039744543650794, 'resid': 0.05206411210317462, 'observed': 0.0}, {'date': '2012-12-01', 'trend': -0.005, 'seasonal': 0.009342137896825394, 'resid': -0.004342137896825394, 'observed': 0.0}, {'date': '2013-01-01', 'trend': -0.005833333333333333, 'seasonal': 0.01609064980158735, 'resid': -0.030257316468253918, 'observed': -0.0199999999999999}, {'date': '2013-02-01', 'trend': -0.004999999999999996, 'seasonal': -0.05034536210317461, 'resid': 0.06534536210317451, 'observed': 0.0099999999999999}, {'date': '2013-03-01', 'trend': -0.004583333333333325, 'seasonal': 0.007935887896825382, 'resid': -0.013352554563491957, 'observed': -0.0099999999999999}, {'date': '2013-04-01', 'trend': -0.005416666666666658, 'seasonal': 0.012935887896825347, 'resid': 0.002480778769841211, 'observed': 0.0099999999999999}, {'date': '2013-05-01', 'trend': -0.0062499999999999865, 'seasonal': 0.013196304563492082, 'resid': -0.046946304563492, 'observed': -0.0399999999999999}, {'date': '2013-06-01', 'trend': -0.0062499999999999865, 'seasonal': 0.05111297123015873, 'resid': -0.06486297123015874, 'observed': -0.02}, {'date': '2013-07-01', 'trend': -0.005833333333333324, 'seasonal': 0.03460255456349204, 'resid': -0.028769221230158716, 'observed': 0.0}, {'date': '2013-08-01', 'trend': -0.0062499999999999865, 'seasonal': 0.03606088789682545, 'resid': -0.03981088789682536, 'observed': -0.0099999999999999}, {'date': '2013-09-01', 'trend': -0.005833333333333324, 'seasonal': 0.00277963789682541, 'resid': 0.003053695436507914, 'observed': 0.0}, {'date': '2013-10-01', 'trend': -0.004999999999999999, 'seasonal': -0.08331411210317463, 'resid': 0.09831411210317453, 'observed': 0.0099999999999999}, {'date': '2013-11-01', 'trend': -0.0033333333333333366, 'seasonal': -0.05039744543650794, 'resid': 0.043730778769841376, 'observed': -0.0099999999999999}, {'date': '2013-12-01', 'trend': -0.0004166666666666752, 'seasonal': 0.009342137896825394, 'resid': 0.0010745287698411803, 'observed': 0.0099999999999999}, {'date': '2014-01-01', 'trend': 0.0004166666666666582, 'seasonal': 0.01609064980158735, 'resid': -0.036507316468253906, 'observed': -0.0199999999999999}, {'date': '2014-02-01', 'trend': 0.00041666666666665374, 'seasonal': -0.05034536210317461, 'resid': 0.04992869543650796, 'observed': 0.0}, {'date': '2014-03-01', 'trend': 0.0008333333333333161, 'seasonal': 0.007935887896825382, 'resid': 0.0012307787698412013, 'observed': 0.0099999999999999}, {'date': '2014-04-01', 'trend': 0.00041666666666665374, 'seasonal': 0.012935887896825347, 'resid': -0.003352554563492101, 'observed': 0.0099999999999999}, {'date': '2014-05-01', 'trend': 0.00041666666666665374, 'seasonal': 0.013196304563492082, 'resid': -0.013612971230158736, 'observed': 0.0}, {'date': '2014-06-01', 'trend': 0.001666666666666654, 'seasonal': 0.05111297123015873, 'resid': -0.04277963789682539, 'observed': 0.01}, {'date': '2014-07-01', 'trend': 0.002916666666666658, 'seasonal': 0.03460255456349204, 'resid': -0.0475192212301587, 'observed': -0.01}, {'date': '2014-08-01', 'trend': 0.0033333333333333244, 'seasonal': 0.03606088789682545, 'resid': -0.039394221230158774, 'observed': 0.0}, {'date': '2014-09-01', 'trend': 0.002916666666666662, 'seasonal': 0.00277963789682541, 'resid': -0.005696304563492072, 'observed': 0.0}, {'date': '2014-10-01', 'trend': 0.0024999999999999996, 'seasonal': -0.08331411210317463, 'resid': 0.08081411210317463, 'observed': 0.0}, {'date': '2014-11-01', 'trend': 0.0024999999999999996, 'seasonal': -0.05039744543650794, 'resid': 0.04789744543650794, 'observed': 0.0}, {'date': '2014-12-01', 'trend': 0.0024999999999999996, 'seasonal': 0.009342137896825394, 'resid': 0.018157862103174605, 'observed': 0.03}, {'date': '2015-01-01', 'trend': 0.0029166666666666664, 'seasonal': 0.01609064980158735, 'resid': -0.029007316468253917, 'observed': -0.0099999999999999}, {'date': '2015-02-01', 'trend': 0.00375, 'seasonal': -0.05034536210317461, 'resid': 0.04659536210317461, 'observed': 0.0}, {'date': '2015-03-01', 'trend': 0.004166666666666667, 'seasonal': 0.007935887896825382, 'resid': -0.01210255456349205, 'observed': 0.0}, {'date': '2015-04-01', 'trend': 0.003333333333333333, 'seasonal': 0.012935887896825347, 'resid': -0.00626922123015878, 'observed': 0.0099999999999999}, {'date': '2015-05-01', 'trend': 0.0025, 'seasonal': 0.013196304563492082, 'resid': -0.01569630456349208, 'observed': 0.0}, {'date': '2015-06-01', 'trend': 0.0062499999999999995, 'seasonal': 0.05111297123015873, 'resid': -0.04736297123015873, 'observed': 0.01}, {'date': '2015-07-01', 'trend': 0.014583333333333328, 'seasonal': 0.03460255456349204, 'resid': -0.04918588789682537, 'observed': 0.0}, {'date': '2015-08-01', 'trend': 0.020833333333333322, 'seasonal': 0.03606088789682545, 'resid': -0.04689422123015877, 'observed': 0.01}, {'date': '2015-09-01', 'trend': 0.021666666666666647, 'seasonal': 0.00277963789682541, 'resid': -0.024446304563492057, 'observed': 0.0}, {'date': '2015-10-01', 'trend': 0.02083333333333332, 'seasonal': -0.08331411210317463, 'resid': 0.04248077876984131, 'observed': -0.02}, {'date': '2015-11-01', 'trend': 0.020833333333333322, 'seasonal': -0.05039744543650794, 'resid': 0.029564112103174618, 'observed': 0.0}, {'date': '2015-12-01', 'trend': 0.020833333333333322, 'seasonal': 0.009342137896825394, 'resid': 0.08982452876984129, 'observed': 0.12}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.01609064980158735, 'resid': nan, 'observed': 0.1}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.05034536210317461, 'resid': nan, 'observed': 0.0399999999999999}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.007935887896825382, 'resid': nan, 'observed': -0.02}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.012935887896825347, 'resid': nan, 'observed': 0.01}, {'date': '2016-05-01', 'trend': nan, 'seasonal': 0.013196304563492082, 'resid': nan, 'observed': 0.0}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.05111297123015873, 'resid': nan, 'observed': 0.01}], 'diff2_FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -8.8045634920585e-05, 'resid': nan, 'observed': -0.25}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.034827628968254, 'resid': nan, 'observed': 0.160000000000001}, {'date': '2007-10-01', 'trend': nan, 'seasonal': -0.087640128968254, 'resid': nan, 'observed': -0.1000000000000014}, {'date': '2007-11-01', 'trend': nan, 'seasonal': 0.03137028769841272, 'resid': nan, 'observed': -0.0899999999999989}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.05819320436507936, 'resid': nan, 'observed': 0.0199999999999995}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.018572668650793697, 'resid': nan, 'observed': -0.0500000000000002}, {'date': '2008-02-01', 'trend': 0.009583333333333338, 'seasonal': -0.06279637896825398, 'resid': -0.6067869543650791, 'observed': -0.6599999999999997}, {'date': '2008-03-01', 'trend': 0.004999999999999993, 'seasonal': 0.056734871031745994, 'resid': 0.5282651289682538, 'observed': 0.5899999999999999}, {'date': '2008-04-01', 'trend': -0.03208333333333333, 'seasonal': 0.0034536210317460195, 'resid': 0.06862971230158732, 'observed': 0.04}, {'date': '2008-05-01', 'trend': -0.040416666666666656, 'seasonal': -0.001285962301587255, 'resid': 0.0717026289682541, 'observed': 0.0300000000000002}, {'date': '2008-06-01', 'trend': -0.012083333333333354, 'seasonal': 0.03637028769841267, 'resid': 0.29571304563492046, 'observed': 0.3199999999999998}, {'date': '2008-07-01', 'trend': 0.012916666666666684, 'seasonal': -0.018056795634920637, 'resid': -0.004859871031746246, 'observed': -0.0100000000000002}, {'date': '2008-08-01', 'trend': 0.055, 'seasonal': -8.8045634920585e-05, 'resid': -0.07491195436507891, 'observed': -0.0199999999999995}, {'date': '2008-09-01', 'trend': 0.056666666666666664, 'seasonal': -0.034827628968254, 'resid': -0.20183903769841277, 'observed': -0.1800000000000001}, {'date': '2008-10-01', 'trend': 0.026250000000000006, 'seasonal': -0.087640128968254, 'resid': -0.5886098710317461, 'observed': -0.6500000000000001}, {'date': '2008-11-01', 'trend': 0.02625, 'seasonal': 0.03137028769841272, 'resid': 0.2023797123015874, 'observed': 0.2600000000000001}, {'date': '2008-12-01', 'trend': 0.014166666666666671, 'seasonal': 0.05819320436507936, 'resid': 0.27764012896825396, 'observed': 0.35}, {'date': '2009-01-01', 'trend': -0.0020833333333333073, 'seasonal': 0.018572668650793697, 'resid': 0.2035106646825396, 'observed': 0.22}, {'date': '2009-02-01', 'trend': -0.0020833333333333194, 'seasonal': -0.06279637896825398, 'resid': 0.1448797123015873, 'observed': 0.08}, {'date': '2009-03-01', 'trend': 0.007916666666666655, 'seasonal': 0.056734871031745994, 'resid': -0.17465153769841266, 'observed': -0.11}, {'date': '2009-04-01', 'trend': 0.04125, 'seasonal': 0.0034536210317460195, 'resid': -0.03470362103174602, 'observed': 0.01}, {'date': '2009-05-01', 'trend': 0.05791666666666667, 'seasonal': -0.001285962301587255, 'resid': 0.003369295634920581, 'observed': 0.06}, {'date': '2009-06-01', 'trend': 0.03375, 'seasonal': 0.03637028769841267, 'resid': -0.07012028769841266, 'observed': 0.0}, {'date': '2009-07-01', 'trend': 0.009583333333333345, 'seasonal': -0.018056795634920637, 'resid': -0.0715265376984126, 'observed': -0.0799999999999999}, {'date': '2009-08-01', 'trend': -0.0020833333333333186, 'seasonal': -8.8045634920585e-05, 'resid': 0.0521713789682538, 'observed': 0.0499999999999999}, {'date': '2009-09-01', 'trend': 0.0008333333333333439, 'seasonal': -0.034827628968254, 'resid': 0.023994295634920653, 'observed': -0.01}, {'date': '2009-10-01', 'trend': 0.00583333333333334, 'seasonal': -0.087640128968254, 'resid': 0.061806795634920766, 'observed': -0.0199999999999999}, {'date': '2009-11-01', 'trend': 0.0016666666666666735, 'seasonal': 0.03137028769841272, 'resid': -0.0030369543650793952, 'observed': 0.03}, {'date': '2009-12-01', 'trend': -0.0033333333333333253, 'seasonal': 0.05819320436507936, 'resid': -0.054859871031746034, 'observed': 0.0}, {'date': '2010-01-01', 'trend': 3.2786273695961652e-18, 'seasonal': 0.018572668650793697, 'resid': -0.028572668650793602, 'observed': -0.0099999999999999}, {'date': '2010-02-01', 'trend': 0.002500000000000003, 'seasonal': -0.06279637896825398, 'resid': 0.09029637896825397, 'observed': 0.03}, {'date': '2010-03-01', 'trend': 0.000833333333333341, 'seasonal': 0.056734871031745994, 'resid': -0.047568204365079435, 'observed': 0.0099999999999999}, {'date': '2010-04-01', 'trend': 0.0016666666666666705, 'seasonal': 0.0034536210317460195, 'resid': 0.00487971230158731, 'observed': 0.01}, {'date': '2010-05-01', 'trend': 0.0012499999999999996, 'seasonal': -0.001285962301587255, 'resid': -0.039964037698412745, 'observed': -0.04}, {'date': '2010-06-01', 'trend': -0.00041666666666666664, 'seasonal': 0.03637028769841267, 'resid': -0.055953621031746004, 'observed': -0.02}, {'date': '2010-07-01', 'trend': -0.00041666666666666973, 'seasonal': -0.018056795634920637, 'resid': 0.03847346230158731, 'observed': 0.02}, {'date': '2010-08-01', 'trend': -0.0012500000000000074, 'seasonal': -8.8045634920585e-05, 'resid': 0.011338045634920593, 'observed': 0.01}, {'date': '2010-09-01', 'trend': -0.003333333333333333, 'seasonal': -0.034827628968254, 'resid': 0.02816096230158733, 'observed': -0.01}, {'date': '2010-10-01', 'trend': -0.005416666666666658, 'seasonal': -0.087640128968254, 'resid': 0.09305679563492066, 'observed': 0.0}, {'date': '2010-11-01', 'trend': -0.0037499999999999916, 'seasonal': 0.03137028769841272, 'resid': -0.027620287698412727, 'observed': 0.0}, {'date': '2010-12-01', 'trend': 0.00041666666666667445, 'seasonal': 0.05819320436507936, 'resid': -0.06860987103174604, 'observed': -0.01}, {'date': '2011-01-01', 'trend': 1.2189323624530364e-17, 'seasonal': 0.018572668650793697, 'resid': -0.018572668650793683, 'observed': 2.775557561562892e-17}, {'date': '2011-02-01', 'trend': 1.2171976389770596e-17, 'seasonal': -0.06279637896825398, 'resid': 0.06279637896825394, 'observed': -2.775557561562892e-17}, {'date': '2011-03-01', 'trend': 8.211024452956887e-18, 'seasonal': 0.056734871031745994, 'resid': -0.0667348710317459, 'observed': -0.0099999999999999}, {'date': '2011-04-01', 'trend': -0.0012499999999999924, 'seasonal': 0.0034536210317460195, 'resid': -0.02220362103174603, 'observed': -0.02}, {'date': '2011-05-01', 'trend': 3.4231876592608994e-18, 'seasonal': -0.001285962301587255, 'resid': 0.03128596230158725, 'observed': 0.03}, {'date': '2011-06-01', 'trend': 0.00041666666666667027, 'seasonal': 0.03637028769841267, 'resid': -0.02678695436507934, 'observed': 0.01}, {'date': '2011-07-01', 'trend': 0.0008333333333333352, 'seasonal': -0.018056795634920637, 'resid': -0.0027765376984125976, 'observed': -0.0199999999999999}, {'date': '2011-08-01', 'trend': 0.002083333333333332, 'seasonal': -8.8045634920585e-05, 'resid': 0.04800471230158715, 'observed': 0.0499999999999999}, {'date': '2011-09-01', 'trend': 0.0033333333333333244, 'seasonal': -0.034827628968254, 'resid': -0.018505704365079333, 'observed': -0.05}, {'date': '2011-10-01', 'trend': 0.004166666666666654, 'seasonal': -0.087640128968254, 'resid': 0.09347346230158735, 'observed': 0.01}, {'date': '2011-11-01', 'trend': 0.003333333333333321, 'seasonal': 0.03137028769841272, 'resid': -0.01470362103174614, 'observed': 0.0199999999999999}, {'date': '2011-12-01', 'trend': 0.0012499999999999872, 'seasonal': 0.05819320436507936, 'resid': -0.07944320436507925, 'observed': -0.0199999999999999}, {'date': '2012-01-01', 'trend': 0.0008333333333333207, 'seasonal': 0.018572668650793697, 'resid': 0.0005939980158728812, 'observed': 0.0199999999999999}, {'date': '2012-02-01', 'trend': -0.0016666666666666794, 'seasonal': -0.06279637896825398, 'resid': 0.07446304563492065, 'observed': 0.01}, {'date': '2012-03-01', 'trend': -0.001250000000000009, 'seasonal': 0.056734871031745994, 'resid': -0.04548487103174609, 'observed': 0.0099999999999999}, {'date': '2012-04-01', 'trend': 0.0024999999999999883, 'seasonal': 0.0034536210317460195, 'resid': -0.025953621031745908, 'observed': -0.0199999999999999}, {'date': '2012-05-01', 'trend': 0.0008333333333333248, 'seasonal': -0.001285962301587255, 'resid': 0.01045262896825383, 'observed': 0.0099999999999999}, {'date': '2012-06-01', 'trend': -4.336808689942018e-18, 'seasonal': 0.03637028769841267, 'resid': -0.05637028769841257, 'observed': -0.0199999999999999}, {'date': '2012-07-01', 'trend': -0.0008333333333333335, 'seasonal': -0.018056795634920637, 'resid': 0.01889012896825397, 'observed': 0.0}, {'date': '2012-08-01', 'trend': -0.0016666666666666629, 'seasonal': -8.8045634920585e-05, 'resid': -0.028245287698412752, 'observed': -0.03}, {'date': '2012-09-01', 'trend': -0.0020833333333333255, 'seasonal': -0.034827628968254, 'resid': 0.07691096230158732, 'observed': 0.04}, {'date': '2012-10-01', 'trend': -0.0016666666666666588, 'seasonal': -0.087640128968254, 'resid': 0.09930679563492056, 'observed': 0.0099999999999999}, {'date': '2012-11-01', 'trend': -0.002499999999999992, 'seasonal': 0.03137028769841272, 'resid': -0.04887028769841263, 'observed': -0.0199999999999999}, {'date': '2012-12-01', 'trend': -0.0033333333333333244, 'seasonal': 0.05819320436507936, 'resid': -0.054859871031746034, 'observed': 0.0}, {'date': '2013-01-01', 'trend': -0.0008333333333333337, 'seasonal': 0.018572668650793697, 'resid': -0.037739335317460265, 'observed': -0.0199999999999999}, {'date': '2013-02-01', 'trend': 0.0008333333333333378, 'seasonal': -0.06279637896825398, 'resid': 0.09196304563492054, 'observed': 0.0299999999999999}, {'date': '2013-03-01', 'trend': 0.00041666666666667114, 'seasonal': 0.056734871031745994, 'resid': -0.07715153769841257, 'observed': -0.0199999999999999}, {'date': '2013-04-01', 'trend': -0.0008333333333333318, 'seasonal': 0.0034536210317460195, 'resid': 0.017379712301587212, 'observed': 0.0199999999999999}, {'date': '2013-05-01', 'trend': -0.0008333333333333326, 'seasonal': -0.001285962301587255, 'resid': -0.04788070436507931, 'observed': -0.0499999999999999}, {'date': '2013-06-01', 'trend': -4.383067982634732e-18, 'seasonal': 0.03637028769841267, 'resid': -0.016370287698412766, 'observed': 0.0199999999999999}, {'date': '2013-07-01', 'trend': 0.00041666666666665916, 'seasonal': -0.018056795634920637, 'resid': 0.03764012896825398, 'observed': 0.02}, {'date': '2013-08-01', 'trend': -0.0004166666666666755, 'seasonal': -8.8045634920585e-05, 'resid': -0.009495287698412638, 'observed': -0.0099999999999999}, {'date': '2013-09-01', 'trend': 0.0004166666666666503, 'seasonal': -0.034827628968254, 'resid': 0.044410962301587247, 'observed': 0.0099999999999999}, {'date': '2013-10-01', 'trend': 0.0008333333333333127, 'seasonal': -0.087640128968254, 'resid': 0.09680679563492059, 'observed': 0.0099999999999999}, {'date': '2013-11-01', 'trend': 0.00166666666666665, 'seasonal': 0.03137028769841272, 'resid': -0.05303695436507927, 'observed': -0.0199999999999999}, {'date': '2013-12-01', 'trend': 0.0029166666666666542, 'seasonal': 0.05819320436507936, 'resid': -0.04110987103174611, 'observed': 0.0199999999999999}, {'date': '2014-01-01', 'trend': 0.0008333333333333255, 'seasonal': 0.018572668650793697, 'resid': -0.049406001984126924, 'observed': -0.0299999999999999}, {'date': '2014-02-01', 'trend': -1.2238474123016374e-17, 'seasonal': -0.06279637896825398, 'resid': 0.0827963789682539, 'observed': 0.0199999999999999}, {'date': '2014-03-01', 'trend': 0.0004166666666666546, 'seasonal': 0.056734871031745994, 'resid': -0.04715153769841275, 'observed': 0.0099999999999999}, {'date': '2014-04-01', 'trend': -0.0004166666666666711, 'seasonal': 0.0034536210317460195, 'resid': -0.0030369543650793484, 'observed': 0.0}, {'date': '2014-05-01', 'trend': -3.469446951953614e-18, 'seasonal': -0.001285962301587255, 'resid': -0.008714037698412641, 'observed': -0.0099999999999999}, {'date': '2014-06-01', 'trend': 0.0012499999999999963, 'seasonal': 0.03637028769841267, 'resid': -0.027620287698412665, 'observed': 0.01}, {'date': '2014-07-01', 'trend': 0.0012500000000000007, 'seasonal': -0.018056795634920637, 'resid': -0.003193204365079364, 'observed': -0.02}, {'date': '2014-08-01', 'trend': 0.00041666666666666767, 'seasonal': -8.8045634920585e-05, 'resid': 0.009671378968253918, 'observed': 0.01}, {'date': '2014-09-01', 'trend': -0.0004166666666666624, 'seasonal': -0.034827628968254, 'resid': 0.03524429563492066, 'observed': 0.0}, {'date': '2014-10-01', 'trend': -0.00041666666666666236, 'seasonal': -0.087640128968254, 'resid': 0.08805679563492066, 'observed': 0.0}, {'date': '2014-11-01', 'trend': -4.9150498486009767e-20, 'seasonal': 0.03137028769841272, 'resid': -0.03137028769841272, 'observed': 0.0}, {'date': '2014-12-01', 'trend': 7.719519468096792e-19, 'seasonal': 0.05819320436507936, 'resid': -0.028193204365079362, 'observed': 0.03}, {'date': '2015-01-01', 'trend': 0.00041666666666666686, 'seasonal': 0.018572668650793697, 'resid': -0.05898933531746026, 'observed': -0.0399999999999999}, {'date': '2015-02-01', 'trend': 0.0008333333333333334, 'seasonal': -0.06279637896825398, 'resid': 0.07196304563492054, 'observed': 0.0099999999999999}, {'date': '2015-03-01', 'trend': 0.00041666666666666686, 'seasonal': 0.056734871031745994, 'resid': -0.05715153769841266, 'observed': 0.0}, {'date': '2015-04-01', 'trend': -0.0008333333333333333, 'seasonal': 0.0034536210317460195, 'resid': 0.007379712301587214, 'observed': 0.0099999999999999}, {'date': '2015-05-01', 'trend': -0.0008333333333333328, 'seasonal': -0.001285962301587255, 'resid': -0.007880704365079311, 'observed': -0.0099999999999999}, {'date': '2015-06-01', 'trend': 0.003749999999999999, 'seasonal': 0.03637028769841267, 'resid': -0.030120287698412667, 'observed': 0.01}, {'date': '2015-07-01', 'trend': 0.008333333333333333, 'seasonal': -0.018056795634920637, 'resid': -0.00027653769841269604, 'observed': -0.01}, {'date': '2015-08-01', 'trend': 0.006250000000000003, 'seasonal': -8.8045634920585e-05, 'resid': 0.0038380456349205823, 'observed': 0.01}, {'date': '2015-09-01', 'trend': 0.0008333333333333408, 'seasonal': -0.034827628968254, 'resid': 0.023994295634920657, 'observed': -0.01}, {'date': '2015-10-01', 'trend': -0.0008333333333333208, 'seasonal': -0.087640128968254, 'resid': 0.06847346230158732, 'observed': -0.02}, {'date': '2015-11-01', 'trend': 1.2238474123016374e-17, 'seasonal': 0.03137028769841272, 'resid': -0.011370287698412734, 'observed': 0.02}, {'date': '2015-12-01', 'trend': 8.578207588705312e-18, 'seasonal': 0.05819320436507936, 'resid': 0.06180679563492062, 'observed': 0.12}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.018572668650793697, 'resid': nan, 'observed': -0.0199999999999999}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.06279637896825398, 'resid': nan, 'observed': -0.06}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.056734871031745994, 'resid': nan, 'observed': -0.06}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.0034536210317460195, 'resid': nan, 'observed': 0.03}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.001285962301587255, 'resid': nan, 'observed': -0.01}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.03637028769841267, 'resid': nan, 'observed': 0.01}]}, value_formatter=None))\n\n\n\n\n\nOff ValidMind\n\n# Seasonal decomposition\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nsd = seasonal_decompose(df['loan_rate_A'], model=\"additive\") \nresiduals = sd.resid\n\n\n # Create subplots\nfrom statsmodels.graphics.tsaplots import plot_acf\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\n# Plot 1: Residuals histogram\nsns.histplot(residuals, kde=True, ax=axes[0, 0])\naxes[0, 0].set_xlabel(\"Residuals\")\naxes[0, 0].set_title(\"Residuals Histogram\")\n\n# Plot 2: Residuals Q-Q plot\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title(\"Residuals Q-Q Plot\")\n\n# Plot 3: Residuals autocorrelation plot\nplot_acf(residuals, ax=axes[1, 0], lags=100)\naxes[1, 0].set_title(\"Residuals Autocorrelation\")\n\n# Plot 4: Residuals box plot\nsns.boxplot(y=residuals, ax=axes[1, 1])\naxes[1, 1].set_ylabel(\"Residuals\")\naxes[1, 1].set_title(\"Residuals Box Plot\")\n\n# Adjust the layout\nplt.tight_layout()\n\nWarning: converting a masked element to nan.\n\n\n\n\n\nIn ValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import ResidualsVisualInspection\ntest_context = TestContext(train_ds=vm_train_ds)\nrvi_test = ResidualsVisualInspection(test_context=test_context)\nrvi_test.run()\n\nTestPlanMetricResult(figures=[Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)], metric=None)\n\n\n\nrvi_test.result.figures\n\n[Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)]\n\n\n\nrvi_test.result.figures[0].figure\n\n\n\n\n\nrvi_test.result.figures[1].figure"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#import-libraries",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#import-libraries",
    "title": "ValidMind",
    "section": "Import Libraries",
    "text": "Import Libraries\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\n\n# ValidMind libraries \nimport validmind as vm\n\n# Plotting libraries"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#data-collection",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#data-collection",
    "title": "ValidMind",
    "section": "Data Collection",
    "text": "Data Collection\n\nLoad FRED Data\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile = '../datasets/time_series/fred_loan_rates.csv'\nfred_df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\ndisplay(fred_df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\n\n\nPreselection of Variables\n\ntarget_column = ['MORTGAGE30US']\nfeature_columns = ['UNRATE', 'GS10', 'FEDFUNDS']\nfred_df = fred_df[target_column + feature_columns]\ndisplay(fred_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      UNRATE\n      GS10\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n    \n    \n      2023-04-06\n      6.28\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      6.27\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      6.39\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      6.43\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 4 columns"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#validmind-setup",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#validmind-setup",
    "title": "ValidMind",
    "section": "ValidMind Setup",
    "text": "ValidMind Setup\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgo0g0rt0000fjy6ozl9pb69\"\n)\n  \n\nConnected to ValidMind\n\n\n\ndf = fred_df\nvm_dataset = vm.init_dataset(dataset=df)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#data-description",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#data-description",
    "title": "ValidMind",
    "section": "Data Description",
    "text": "Data Description"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#data-quality",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#data-quality",
    "title": "ValidMind",
    "section": "Data Quality",
    "text": "Data Quality\n\nFrequency of the Series\nHandling Frequencies\n\ndf = df.resample('MS').last()\nvm_dataset = vm.init_dataset(dataset=df)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#univariate-analysis",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#univariate-analysis",
    "title": "ValidMind",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\n\n\nAttribute       Value                                                                                                                                                                                                                           \n\n\nID              time_series_univariate                                                                                                                                                                                                          \nName            TimeSeriesUnivariate                                                                                                                                                                                                            \nDescription     Test plan to perform time series univariate analysis.                                                                                                                                                                           \nRequired Context['dataset']                                                                                                                                                                                                                     \nTests           TimeSeriesLinePlot (Metric), TimeSeriesHistogram (Metric), ACFandPACFPlot (Metric), SeasonalDecompose (Metric), AutoSeasonality (Metric), AutoStationarity (Metric), RollingStatsPlot (Metric), AutoAR (Metric), AutoMA (Metric)\nTest Plans      []                                                                                                                                                                                                                              \n\n\n\n\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\n\nvm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nRunning Metric: acf_pacf_plot:  22%|██▏       | 2/9 [00:00<00:02,  3.15it/s]        The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nRunning Metric: seasonal_decompose:  33%|███▎      | 3/9 [00:01<00:02,  2.35it/s]The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nRunning Metric: auto_ma:  89%|████████▉ | 8/9 [00:04<00:00,  1.92it/s]           Non-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nFailed to log result: TestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures) for test plan result 'TestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures)':  33%|███▎      | 3/9 [00:12<00:24,  4.16s/it]\n\n\nCould not log metrics to ValidMind API\n{\"code\":\"IntenalError\",\"message\":\"Internal server error\"}\n\n\n\nException: {\"code\":\"IntenalError\",\"message\":\"Internal server error\"}"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#multivariate-analysis",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#multivariate-analysis",
    "title": "ValidMind",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\n\nvm.test_plans.describe_plan(\"time_series_multivariate\")\n\n\n\n\nAttribute       Value                                                                                                   \n\n\nID              time_series_multivariate                                                                                \nName            TimeSeriesMultivariate                                                                                  \nDescription     Test plan to perform time series multivariate analysis.                                                 \nRequired Context['dataset']                                                                                             \nTests           ScatterPlot (Metric), LaggedCorrelationHeatmap (Metric), EngleGrangerCoint (Metric), SpreadPlot (Metric)\nTest Plans      []                                                                                                      \n\n\n\n\n\ntest_plan_config = {\n    \"scatter_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"lagged_correlation_heatmap\": {\n        \"target_col\": target_column,\n        \"independent_vars\": feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\n\nvm.run_test_plan(\"time_series_multivariate\", config=test_plan_config, dataset=vm_dataset)\n\n                                                                                                                                     \n\n\nResults for Time Series Multivariate Test Plan:\n        This test plan provides a preliminary understanding of the features\n        and relationship in multivariate dataset. It presents various\n        multivariate visualizations that can help identify patterns, trends,\n        and relationships between pairs of variables. The visualizations are\n        designed to explore the relationships between multiple features\n        simultaneously. They allow you to quickly identify any patterns or\n        trends in the data, as well as any potential outliers or anomalies.\n        The individual feature distribution can also be explored to provide\n        insight into the range and frequency of values observed in the data.\n        This multivariate analysis test plan aims to provide an overview of\n        the data structure and guide further exploration and modeling.\n        \n            Logged the following plot\n            to the ValidMind platform:\n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            Logged the following plot\n            to the ValidMind platform:\n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        engle_granger_coint\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'Variable 1': 'MORTGAGE30US', 'Variable 2': 'UNRATE', 'Test': 'Engle-Granger', 'p-value': 0.6278763587562293, 'Threshold': 0.05, 'Pass/Fail': 'Fail', 'Decision': 'Not cointegrated'}, {'Variable 1': 'MORTGAGE30US', 'Variable 2': 'GS10', 'Test': 'Engle-Granger', 'p-value': 0.00868759943010125, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'MORTGAGE30US', 'Variable 2': 'FEDFUNDS', 'Test': 'Engle-Granger', 'p-value': 0.02041650659325254, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'UNRATE', 'Variable 2': 'GS10', 'Test': 'Engle-Granger', 'p-value': 0.013241818497465465, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'UNRATE', 'Variable 2': 'FEDFUNDS', 'Test': 'Engle-Granger', 'p-value': 0.027578724502124778, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'GS10', 'Variable 2': 'FEDFUNDS', 'Test': 'Engle-Granger', 'p-value': 0.005832335278420137, 'Threshold': 0.05,...\n                    \n                \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/time_series_data_validation_demo.html",
    "href": "notebooks/time_series/time_series_data_validation_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Demo notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework. As model developers working in the financial sector, ensuring the quality and reliability of time series data is essential for accurate model predictions and robust decision-making processes.\nIn this demo, we will walk through different data validation suites of tests tailored for time series data, showcasing how these tools can assist you in identifying potential issues and inconsistencies in the data. By utilizing the ValidMind MRM platform and developer framework, you can streamline your data validation process, allowing you to focus on building and refining your models with confidence.\nLet’s get started!\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis. Finally, establish a connection to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# System libraries\nimport glob\nimport os\nimport pickle\n\n# ML libraries\nimport pandas as pd\n\n# ValidMind libraries \nimport validmind as vm\n\n\n\n\n\nfrom validmind.datasets.regression import fred\niris_df = fred.load_data()\n\n\ndataset = 'fred'\n\nif dataset == 'lending_club':\n    target_column = ['loan_rate_A']\n    feature_columns = ['loan_rate_B', 'loan_rate_C', 'loan_rate_D']\n    from validmind.datasets.regression import lending_club\n    raw_df = lending_club.load_data()\nif dataset == 'fred':\n    target_column = ['MORTGAGE30US']\n    feature_columns = ['FEDFUNDS', 'GS10', 'UNRATE']\n    from validmind.datasets.regression import fred\n    raw_df = fred.load_data()\n    selected_cols = target_column + feature_columns\n    raw_df = raw_df[selected_cols]\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\n\n\ndisplay(raw_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n      GS10\n      UNRATE\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n    \n    \n      2023-04-06\n      6.28\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      6.27\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      6.39\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      6.43\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 4 columns\n\n\n\n\nraw_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   FEDFUNDS      825 non-null    float64\n 2   GS10          841 non-null    float64\n 3   UNRATE        903 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\n\nThe vm.test_plans.list_plans() function is a part of the ValidMind (vm) library that provides a comprehensive list of available test plans. These test plans are pre-built sets of tests designed to perform automated data and model validation, such as data quality, exploratory data analysis, and model performance.\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                        Description                                            \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics    Test plan for sklearn classifier metrics               \nsklearn_classifier_validation     SKLearnClassifierPerformanceTest plan for sklearn classifier models                \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests \nsklearn_classifier                SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                        \ntabular_dataset                   TabularDataset              Test plan for generic tabular datasets                 \ntabular_dataset_description       TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                        \ntabular_data_quality              TabularDataQuality          Test plan for data quality on tabular datasets         \nnormality_test_plan               NormalityTestPlan           Test plan to perform normality tests.                  \nautocorrelation_test_plan         AutocorrelationTestPlan     Test plan to perform autocorrelation tests.            \nseasonality_test_plan             SesonalityTestPlan          Test plan to perform seasonality tests.                \nunit_root                         UnitRoot                    Test plan to perform unit root tests.                  \nstationarity_test_plan            StationarityTestPlan        Test plan to perform stationarity tests.               \ntimeseries                        TimeSeries                  Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                        \ntime_series_data_quality          TimeSeriesDataQuality       Test plan for data quality on time series datasets     \ntime_series_dataset               TimeSeriesDataset           Test plan for time series  datasets                    \ntime_series_univariate            TimeSeriesUnivariate        Test plan to perform time series univariate analysis.  \ntime_series_multivariate          TimeSeriesMultivariate      Test plan to perform time series multivariate analysis.\ntime_series_forecast              TimeSeriesForecast          Test plan to perform time series forecast tests.       \nregression_model_performance      RegressionModelPerformance  Test plan for statsmodels regressor models that includes\n    both metrics and validation tests                                                        \n\n\n\n\n\n\n\n\n\nUse the ValidMind (vm) library to perform data quality tests on a time series dataset. The process begins by describing a test plan specifically designed for time series data quality. This test plan contains a set of tests that evaluate the quality of the provided time series data.\nNext, the raw DataFrame is used to initialize a dataset using the vm library. This newly created dataset object, vm_dataset, is then utilized for further processing. The test plan parameters are configured to define the z-score threshold for outlier detection and the minimum threshold for identifying missing values.\nFinally, the test plan, time_series_data_quality, is executed using the vm.run_test_plan() function with the initialized dataset and the configuration settings provided. This function applies the specified tests to the dataset and generates a report on the quality of the time series data based on the configured parameters.\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\n\n\nAttribute       Value                                                                                                           \n\n\nID              time_series_data_quality                                                                                        \nName            TimeSeriesDataQuality                                                                                           \nDescription     Test plan for data quality on time series datasets                                                              \nRequired Context['dataset']                                                                                                     \nTests           TimeSeriesOutliers (ThresholdTest), TimeSeriesMissingValues (ThresholdTest), TimeSeriesFrequency (ThresholdTest)\nTest Plans      []                                                                                                              \n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=raw_df\n)\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\nvm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRunning ThresholdTest: time_series_outliers:   0%|          | 0/3 [00:00<?, ?it/s]  \n\n\n   Variable   z-score  Threshold       Date\n0  FEDFUNDS  3.707038          3 1981-05-01\n\n\n                                                                                                                                       \n\n\nTest plan for data quality on time series datasets\n        \n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3}\n                \n            \n            \n                Results\n                [TestResult(values={'Variable': ['FEDFUNDS'], 'z-score': [3.707037578539338], 'Threshold': [3], 'Date': ['1981-05-01']}, test_name='outliers', column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(values={'n_missing': 833, 'p_missing': 0.23458180794142494}, test_name=None, column='MORTGAGE30US', passed=False), TestResult(values={'n_missing': 2726, 'p_missing': 0.7676710785694171}, test_name=None, column='FEDFUNDS', passed=False), TestResult(values={'n_missing': 2710, 'p_missing': 0.763165305547733}, test_name=None, column='GS10', passed=False), TestResult(values={'n_missing': 2648, 'p_missing': 0.7457054350887075}, test_name=None, column='UNRATE', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(values={'Variable': ['MORTGAGE30US', 'FEDFUNDS', 'GS10', 'UNRATE'], 'Frequency': [None, 'Monthly', 'Monthly', 'Monthly']}, test_name=None, column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n\n\nTimeSeriesDataQuality(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1947-01-01           NaN       NaN   NaN     NaN\n1947-02-01           NaN       NaN   NaN     NaN\n1947-03-01           NaN       NaN   NaN     NaN\n1947-04-01           NaN       NaN   NaN     NaN\n1947-05-01           NaN       NaN   NaN     NaN\n...                  ...       ...   ...     ...\n2023-04-01           NaN       NaN  3.46     NaN\n2023-04-06          6.28       NaN   NaN     NaN\n2023-04-13          6.27       NaN   NaN     NaN\n2023-04-20          6.39       NaN   NaN     NaN\n2023-04-27          6.43       NaN   NaN     NaN\n\n[3551 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': 3.46, 'UNRATE': nan}, {'MORTGAGE30US': 6.28, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': 6.27, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': 6.39, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': 6.43, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}]}], shape={'rows': 3551, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), model=None, models=_CountingAttr(counter=41, _default=NOTHING, repr=True, eq=True, order=True, hash=None, init=True, on_setattr=None, alias=None, metadata={}), train_ds=None, test_ds=None, validation_ds=None, y_train_predict=None, y_test_predict=None, context_data=None), config={...})\n\n\nHandling Frequencies.\n\ndef identify_frequencies(df):\n    \"\"\"\n    Identify the frequency of each series in the DataFrame.\n\n    :param df: Time-series DataFrame\n    :return: DataFrame with two columns: 'Variable' and 'Frequency'\n    \"\"\"\n    frequencies = []\n    for column in df.columns:\n        series = df[column].dropna()\n        if not series.empty:\n            freq = pd.infer_freq(series.index)\n            if freq == 'MS' or freq == 'M':\n                label = 'Monthly'\n            elif freq == 'Q':\n                label = 'Quarterly'\n            elif freq == 'A':\n                label = 'Yearly'\n            else:\n                label = freq\n        else:\n            label = None\n\n        frequencies.append({'Variable': column, 'Frequency': label})\n\n    freq_df = pd.DataFrame(frequencies)\n\n    return freq_df\n\n\nfrequencies = identify_frequencies(raw_df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      FEDFUNDS\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      UNRATE\n      Monthly\n    \n  \n\n\n\n\nResample.\n\npreprocessed_df = raw_df.resample('MS').last()\nfrequencies = identify_frequencies(preprocessed_df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      Monthly\n    \n    \n      1\n      FEDFUNDS\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      UNRATE\n      Monthly\n    \n  \n\n\n\n\nRun Data Quality Test Plan.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df\n)\nvm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRunning ThresholdTest: time_series_outliers:   0%|          | 0/3 [00:00<?, ?it/s]  \n\n\n        Variable   z-score  Threshold       Date\n0       FEDFUNDS  3.106442          3 1980-03-01\n1       FEDFUNDS  3.212296          3 1980-04-01\n2       FEDFUNDS  3.537417          3 1980-12-01\n3       FEDFUNDS  3.582783          3 1981-01-01\n4       FEDFUNDS  3.441645          3 1981-05-01\n5       FEDFUNDS  3.587823          3 1981-06-01\n6       FEDFUNDS  3.572701          3 1981-07-01\n7       FEDFUNDS  3.265222          3 1981-08-01\n8   MORTGAGE30US  3.246766          3 1981-09-01\n9   MORTGAGE30US  3.271251          3 1981-10-01\n10  MORTGAGE30US  3.011098          3 1982-01-01\n11        UNRATE  5.011303          3 2020-04-01\n12        UNRATE  4.128421          3 2020-05-01\n\n\n                                                                                                                                       \n\n\nTest plan for data quality on time series datasets\n        \n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3}\n                \n            \n            \n                Results\n                [TestResult(values={'Variable': ['FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'MORTGAGE30US', 'MORTGAGE30US', 'MORTGAGE30US', 'UNRATE', 'UNRATE'], 'z-score': [3.1064423000188297, 3.2122956874733952, 3.537416806083848, 3.582782543564376, 3.441644693624955, 3.5878231810622134, 3.572701268568703, 3.2652223812006786, 3.2467663610829294, 3.27125137909717, 3.0110980626958725, 5.011303408667247, 4.128420561484492], 'Threshold': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'Date': ['1980-03-01', '1980-04-01', '1980-12-01', '1981-01-01', '1981-05-01', '1981-06-01', '1981-07-01', '1981-08-01', '1981-09-01', '1981-10-01', '1982-01-01', '2020-04-01', '2020-05-01']}, test_name='outliers', column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(values={'n_missing': 291, 'p_missing': 0.31768558951965065}, test_name=None, column='MORTGAGE30US', passed=False), TestResult(values={'n_missing': 91, 'p_missing': 0.09934497816593886}, test_name=None, column='FEDFUNDS', passed=False), TestResult(values={'n_missing': 75, 'p_missing': 0.08187772925764192}, test_name=None, column='GS10', passed=False), TestResult(values={'n_missing': 13, 'p_missing': 0.014192139737991267}, test_name=None, column='UNRATE', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(values={'Variable': ['MORTGAGE30US', 'FEDFUNDS', 'GS10', 'UNRATE'], 'Frequency': ['Monthly', 'Monthly', 'Monthly', 'Monthly']}, test_name=None, column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n\n\nTimeSeriesDataQuality(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1947-01-01           NaN       NaN   NaN     NaN\n1947-02-01           NaN       NaN   NaN     NaN\n1947-03-01           NaN       NaN   NaN     NaN\n1947-04-01           NaN       NaN   NaN     NaN\n1947-05-01           NaN       NaN   NaN     NaN\n...                  ...       ...   ...     ...\n2022-12-01          6.42      4.10  3.62     3.5\n2023-01-01          6.13      4.33  3.53     3.4\n2023-02-01          6.50      4.57  3.75     3.6\n2023-03-01          6.32      4.65  3.66     3.5\n2023-04-01          6.43       NaN  3.46     NaN\n\n[916 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': 6.42, 'FEDFUNDS': 4.1, 'GS10': 3.62, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.13, 'FEDFUNDS': 4.33, 'GS10': 3.53, 'UNRATE': 3.4}, {'MORTGAGE30US': 6.5, 'FEDFUNDS': 4.57, 'GS10': 3.75, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.32, 'FEDFUNDS': 4.65, 'GS10': 3.66, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.43, 'FEDFUNDS': nan, 'GS10': 3.46, 'UNRATE': nan}]}], shape={'rows': 916, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), model=None, models=_CountingAttr(counter=41, _default=NOTHING, repr=True, eq=True, order=True, hash=None, init=True, on_setattr=None, alias=None, metadata={}), train_ds=None, test_ds=None, validation_ds=None, y_train_predict=None, y_test_predict=None, context_data=None), config={...})\n\n\nRemove missing values.\n\npreprocessed_df = preprocessed_df.dropna()\n\nRun Data Quality Test Plan.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=target_column\n)\nvm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRunning ThresholdTest: time_series_outliers:   0%|          | 0/3 [00:00<?, ?it/s]  \n\n\n        Variable   z-score  Threshold       Date\n0       FEDFUNDS  3.106442          3 1980-03-01\n1       FEDFUNDS  3.212296          3 1980-04-01\n2       FEDFUNDS  3.537417          3 1980-12-01\n3       FEDFUNDS  3.582783          3 1981-01-01\n4       FEDFUNDS  3.441645          3 1981-05-01\n5       FEDFUNDS  3.587823          3 1981-06-01\n6       FEDFUNDS  3.572701          3 1981-07-01\n7       FEDFUNDS  3.265222          3 1981-08-01\n8   MORTGAGE30US  3.246766          3 1981-09-01\n9   MORTGAGE30US  3.271251          3 1981-10-01\n10  MORTGAGE30US  3.011098          3 1982-01-01\n11        UNRATE  5.011303          3 2020-04-01\n12        UNRATE  4.128421          3 2020-05-01\n\n\n                                                                                                                                       \n\n\nTest plan for data quality on time series datasets\n        \n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3}\n                \n            \n            \n                Results\n                [TestResult(values={'Variable': ['FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'MORTGAGE30US', 'MORTGAGE30US', 'MORTGAGE30US', 'UNRATE', 'UNRATE'], 'z-score': [3.1064423000188297, 3.2122956874733952, 3.537416806083848, 3.582782543564376, 3.441644693624955, 3.5878231810622134, 3.572701268568703, 3.2652223812006786, 3.2467663610829294, 3.27125137909717, 3.0110980626958725, 5.011303408667247, 4.128420561484492], 'Threshold': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'Date': ['1980-03-01', '1980-04-01', '1980-12-01', '1981-01-01', '1981-05-01', '1981-06-01', '1981-07-01', '1981-08-01', '1981-09-01', '1981-10-01', '1982-01-01', '2020-04-01', '2020-05-01']}, test_name='outliers', column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='MORTGAGE30US', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='FEDFUNDS', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='GS10', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='UNRATE', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(values={'Variable': ['MORTGAGE30US', 'FEDFUNDS', 'GS10', 'UNRATE'], 'Frequency': ['Monthly', 'Monthly', 'Monthly', 'Monthly']}, test_name=None, column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n\n\nTimeSeriesDataQuality(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1971-04-01          7.29      4.16  5.83     5.9\n1971-05-01          7.46      4.63  6.39     5.9\n1971-06-01          7.54      4.91  6.52     5.9\n1971-07-01          7.69      5.31  6.73     6.0\n1971-08-01          7.69      5.57  6.58     6.1\n...                  ...       ...   ...     ...\n2022-11-01          6.58      3.78  3.89     3.6\n2022-12-01          6.42      4.10  3.62     3.5\n2023-01-01          6.13      4.33  3.53     3.4\n2023-02-01          6.50      4.57  3.75     3.6\n2023-03-01          6.32      4.65  3.66     3.5\n\n[624 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 7.29, 'FEDFUNDS': 4.16, 'GS10': 5.83, 'UNRATE': 5.9}, {'MORTGAGE30US': 7.46, 'FEDFUNDS': 4.63, 'GS10': 6.39, 'UNRATE': 5.9}, {'MORTGAGE30US': 7.54, 'FEDFUNDS': 4.91, 'GS10': 6.52, 'UNRATE': 5.9}, {'MORTGAGE30US': 7.69, 'FEDFUNDS': 5.31, 'GS10': 6.73, 'UNRATE': 6.0}, {'MORTGAGE30US': 7.69, 'FEDFUNDS': 5.57, 'GS10': 6.58, 'UNRATE': 6.1}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': 6.58, 'FEDFUNDS': 3.78, 'GS10': 3.89, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.42, 'FEDFUNDS': 4.1, 'GS10': 3.62, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.13, 'FEDFUNDS': 4.33, 'GS10': 3.53, 'UNRATE': 3.4}, {'MORTGAGE30US': 6.5, 'FEDFUNDS': 4.57, 'GS10': 3.75, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.32, 'FEDFUNDS': 4.65, 'GS10': 3.66, 'UNRATE': 3.5}]}], shape={'rows': 624, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=['MORTGAGE30US'], class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), model=None, models=_CountingAttr(counter=41, _default=NOTHING, repr=True, eq=True, order=True, hash=None, init=True, on_setattr=None, alias=None, metadata={}), train_ds=None, test_ds=None, validation_ds=None, y_train_predict=None, y_test_predict=None, context_data=None), config={...})\n\n\n\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\n\n\nAttribute       Value                                                                                                                                                                                                                           \n\n\nID              time_series_univariate                                                                                                                                                                                                          \nName            TimeSeriesUnivariate                                                                                                                                                                                                            \nDescription     Test plan to perform time series univariate analysis.                                                                                                                                                                           \nRequired Context['dataset']                                                                                                                                                                                                                     \nTests           TimeSeriesLinePlot (Metric), TimeSeriesHistogram (Metric), ACFandPACFPlot (Metric), SeasonalDecompose (Metric), AutoSeasonality (Metric), AutoStationarity (Metric), RollingStatsPlot (Metric), AutoAR (Metric), AutoMA (Metric)\nTest Plans      []                                                                                                                                                                                                                              \n\n\n\n\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df\n)\nvm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRunning Metric: acf_pacf_plot:  22%|██▏       | 2/9 [00:00<00:01,  4.27it/s]        The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nRunning Metric: seasonal_decompose:  33%|███▎      | 3/9 [00:01<00:02,  2.24it/s]The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nRunning Metric: auto_ma:  89%|████████▉ | 8/9 [00:04<00:00,  1.71it/s]           Non-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n                                                                                                                                    \n\n\nThis test plan provides a preliminary understanding of the target variable(s) used in the time series dataset. It visualizations that present the raw time series data and a histogram of the target variable(s).\n\nThe raw time series data provides a visual inspection of the target variable's behavior over time. This helps to identify any patterns or trends in the data, as well as any potential outliers or anomalies. The histogram of the target variable displays the distribution of values, providing insight into the range and frequency of values observed in the data.\n            \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n            \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n            \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/time_series_demo_data_quality_test_plan.html",
    "href": "notebooks/time_series/time_series_demo_data_quality_test_plan.html",
    "title": "ValidMind",
    "section": "",
    "text": "Time Series Test Plan Demo\n\n# Quick hack to load local SDK code\n\nimport os\n\nos.chdir(os.path.join(os.getcwd(), \"../..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgmfxwvp0000k8rlc997oe9t\")\n\nTrue\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = './notebooks/datasets/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\ndef plot_time_difference_frequency(df):\n    # Calculate the time differences between consecutive entries\n    time_diff = df.index.to_series().diff().dropna()\n\n    # Convert the time differences to a suitable unit (e.g., days)\n    time_diff_days = time_diff.dt.total_seconds() / (60 * 60 * 24)\n\n    # Create a DataFrame with the time differences\n    time_diff_df = pd.DataFrame({'Time Differences (Days)': time_diff_days})\n\n    # Plot the frequency distribution of the time differences\n    sns.histplot(data=time_diff_df, x='Time Differences (Days)', bins=50, kde=False)\n    plt.xlabel('Time Differences (Days)')\n    plt.ylabel('Frequency')\n    plt.show()\n\n\nplot_time_difference_frequency(df)\n\n\n\n\n\ndef identify_frequencies(df):\n    \"\"\"\n    Identify the frequency of each series in the DataFrame.\n\n    :param df: Time-series DataFrame\n    :return: DataFrame with two columns: 'Variable' and 'Frequency'\n    \"\"\"\n    frequencies = []\n    for column in df.columns:\n        series = df[column].dropna()\n        if not series.empty:\n            freq = pd.infer_freq(series.index)\n            if freq == 'MS' or freq == 'M':\n                label = 'Monthly'\n            elif freq == 'Q':\n                label = 'Quarterly'\n            elif freq == 'A':\n                label = 'Yearly'\n            else:\n                label = freq\n        else:\n            label = None\n\n        frequencies.append({'Variable': column, 'Frequency': label})\n\n    freq_df = pd.DataFrame(frequencies)\n\n    return freq_df\n\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      UNRATE\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      FEDFUNDS\n      Monthly\n    \n  \n\n\n\n\n\n# df = df.resample('MS').last()\n# frequencies = identify_frequencies(df)\n# display(frequencies)\n\n\ndef plot_missing_values_bar(df):\n    \"\"\"\n    Plot a bar chart displaying the total number of missing values per variable (column) in a time-series DataFrame using seaborn.\n    \n    :param df: Time-series DataFrame\n    \"\"\"\n    # Calculate the total number of missing values per column\n    missing_values = df.isnull().sum()\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the bar chart\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=missing_values.index, y=missing_values.values)\n    \n    plt.xticks(rotation=45, ha='right')\n    plt.xlabel('Variables (Columns)')\n    plt.ylabel('Number of Missing Values')\n    plt.title('Total Number of Missing Values per Variable')\n    plt.show()\n\n\nplot_missing_values_bar(df)\n\n\n\n\n\ndef plot_missing_values_heatmap(df, start_year=None, end_year=None):\n    \"\"\"\n    Plot a heatmap of missing values with actual years in rows using seaborn.\n\n    :param df: Time-series DataFrame\n    :param start_year: Start year for zooming in, defaults to None\n    :param end_year: End year for zooming in, defaults to None\n    \"\"\"\n    # Filter the DataFrame based on the specified start_year and end_year\n    if start_year:\n        df = df[df.index.year >= start_year]\n    if end_year:\n        df = df[df.index.year <= end_year]\n\n    # Create a boolean mask for missing values\n    missing_mask = df.isnull()\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the heatmap\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(missing_mask.T, cmap='viridis', cbar=False, xticklabels=False)\n\n    # Add actual years on the x-axis\n    years = df.index.year.unique()\n    xticks = [df.index.get_loc(df.index[df.index.year == year][0]) for year in years]\n    plt.xticks(xticks, years, rotation=45, ha='right')\n\n    plt.ylabel('Columns')\n    plt.xlabel('Rows (Years)')\n    plt.title('Missing Values Heatmap with Actual Years in Rows')\n    plt.show()\n\n\nplot_missing_values_heatmap(df)\n\n\n\n\n\n# df = df.dropna()\nplot_missing_values_heatmap(df)\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"MORTGAGE30US\"\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                        Description                                            \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics    Test plan for sklearn classifier metrics               \nsklearn_classifier_validation     SKLearnClassifierPerformanceTest plan for sklearn classifier models                \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests \nsklearn_classifier                SKLearnClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                        \ntabular_dataset                   TabularDataset              Test plan for generic tabular datasets                 \ntabular_dataset_description       TabularDatasetDescription   Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                        \ntabular_data_quality              TabularDataQuality          Test plan for data quality on tabular datasets         \nnormality_test_plan               NormalityTestPlan           Test plan to perform normality tests.                  \nautocorrelation_test_plan         AutocorrelationTestPlan     Test plan to perform autocorrelation tests.            \nseasonality_test_plan             SesonalityTestPlan          Test plan to perform seasonality tests.                \nunit_root                         UnitRoot                    Test plan to perform unit root tests.                  \nstationarity_test_plan            StationarityTestPlan        Test plan to perform stationarity tests.               \ntimeseries                        TimeSeries                  Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                        \ntime_series_data_quality          TimeSeriesDataQuality       Test plan for data quality on time series datasets     \ntime_series_dataset               TimeSeriesDataset           Test plan for time series  datasets                    \ntime_series_univariate            TimeSeriesUnivariate        Test plan to perform time series univariate analysis.  \ntime_series_multivariate          TimeSeriesMultivariate      Test plan to perform time series multivariate analysis.\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\n\n\nAttribute       Value                                                                                                           \n\n\nID              time_series_data_quality                                                                                        \nName            TimeSeriesDataQuality                                                                                           \nDescription     Test plan for data quality on time series datasets                                                              \nRequired Context['dataset']                                                                                                     \nTests           TimeSeriesOutliers (ThresholdTest), TimeSeriesMissingValues (ThresholdTest), TimeSeriesFrequency (ThresholdTest)\nTest Plans      []                                                                                                              \n\n\n\n\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3.5,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\nplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\n                                                                                                                                       \n\n\nResults for Time Series Data Quality Test Plan:Test plan for data quality on time series datasets\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3.5}\n                \n            \n            \n                Results\n                [TestResult(test_name='outliers', column=None, passed=False, values={'Variable': ['FEDFUNDS'], 'z-score': [3.707037578539338], 'Threshold': [3.5], 'Date': ['1981-05-01']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='MORTGAGE30US', passed=False, values={'n_missing': 833, 'p_missing': 0.23458180794142494}), TestResult(test_name=None, column='UNRATE', passed=False, values={'n_missing': 2648, 'p_missing': 0.7457054350887075}), TestResult(test_name=None, column='GS10', passed=False, values={'n_missing': 2710, 'p_missing': 0.763165305547733}), TestResult(test_name=None, column='FEDFUNDS', passed=False, values={'n_missing': 2726, 'p_missing': 0.7676710785694171})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'Variable': ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'], 'Frequency': [None, 'Monthly', 'Monthly', 'Monthly']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\nRemove missing values\n\ndf = df.resample('MS').last()\ndf = df.dropna()\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"MORTGAGE30US\"\n)\nplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                       \n\n\nResults for Time Series Data Quality Test Plan:Test plan for data quality on time series datasets\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Time Series Outliers\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_outliers\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'zscore_threshold': 3.5}\n                \n            \n            \n                Results\n                [TestResult(test_name='outliers', column=None, passed=False, values={'Variable': ['FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'UNRATE', 'UNRATE'], 'z-score': [3.537416806083848, 3.582782543564376, 3.5878231810622134, 3.572701268568703, 5.011303408667247, 4.128420561484492], 'Threshold': [3.5, 3.5, 3.5, 3.5, 3.5, 3.5], 'Date': ['1980-12-01', '1981-01-01', '1981-06-01', '1981-07-01', '2020-04-01', '2020-05-01']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Time Series Missing Values\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_missing_values\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='MORTGAGE30US', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='UNRATE', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='GS10', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='FEDFUNDS', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Time Series Frequency\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    time_series_frequency\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'Variable': ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'], 'Frequency': ['Monthly', 'Monthly', 'Monthly', 'Monthly']})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#data-quality-and-relevance",
    "href": "notebooks/time_series/cre_rate_model_demo.html#data-quality-and-relevance",
    "title": "ValidMind",
    "section": "4.1.2. Data Quality and Relevance",
    "text": "4.1.2. Data Quality and Relevance"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#data-process-adjustments-and-treatment",
    "href": "notebooks/time_series/cre_rate_model_demo.html#data-process-adjustments-and-treatment",
    "title": "ValidMind",
    "section": "4.1.3. Data Process, Adjustments and Treatment",
    "text": "4.1.3. Data Process, Adjustments and Treatment\n\nA. Missing Values Analysis\nStep 1: Calculate the percentage of missing values in each column\nStep 2: Display the missing values percentage in a table format\nStep 3: Visualize the missing values\n### B. Outliers Analysis\nStep 1: Visualize the dataset using box plots\nVisualize the data using box plots to get an initial sense of the presence of outliers.\nStep 2: Calculate Z-scores\nStep 3: Set a threshold and identify outliers\nSet a threshold (e.g., 3) to identify data points with Z-scores higher than the threshold.\nStep 4: Analyze the outliers\nAnalyze the outliers by looking at their frequency, index, and corresponding column.\n### C. Stationarity Analysis\nStep 1: Run Unit Root Tests\n\nfrom validmind.test_plans.statsmodels_timeseries import UnitRoot \ntest_context = TestContext(dataset=vm_dataset)\nur_test_plan = UnitRoot(test_context=test_context)\nur_test_plan.run()\n\nRunning Metric: kpss:  20%|██        | 1/5 [00:00<00:00, 89.93it/s]  The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n                                                                                                         \n\n\nResults for unit_root Test Plan:\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        adf\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -1.917289312690944, 'pvalue': 0.32397189281015515, 'usedlag': 1, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -71.08908853191068}, 'loan_rate_B': {'stat': -3.1599303710498425, 'pvalue': 0.022424413263559147, 'usedlag': 9, 'nobs': 127, 'critical_values': {'1%': -3.482920063655088, '5%': -2.884580323367261, '10%': -2.5790575441750883}, 'icbest': -42.45027033820841}, 'loan_rate_C': {'stat': -2.530666699941385, 'pvalue': 0.10818994357289696, 'usedlag': 1, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -92.19465856666866}, 'loan_rate_D': {'stat': -1.617158531178829, 'pvalue': 0.47421928207593467, 'usedlag': 6, 'nobs': 130, 'critical_values': {'1%': -3.4816817173418295, '5%': -2.8840418343195267, '10%': -2.578770059171598}, 'icbest': -4.9426661983780775}, 'FEDFUNDS': {'stat': -0.16854321128256927, 'pvalue': 0.9421687822974046, 'usedlag': 13,...\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        kpss\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 1.012356679488042, 'pvalue': 0.01, 'usedlag': 6, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'loan_rate_B': {'stat': 0.26307336980308743, 'pvalue': 0.1, 'usedlag': 6, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'loan_rate_C': {'stat': 0.8099610581510324, 'pvalue': 0.01, 'usedlag': 6, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'loan_rate_D': {'stat': 1.5641456258111677, 'pvalue': 0.01, 'usedlag': 6, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'FEDFUNDS': {'stat': 0.37574599218114024, 'pvalue': 0.08760948612881886, 'usedlag': 6, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        phillips_perron\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -2.027461766308415, 'pvalue': 0.2746633686983735, 'usedlag': 13, 'nobs': 136}, 'loan_rate_B': {'stat': -2.4460840425020693, 'pvalue': 0.1291495662057986, 'usedlag': 13, 'nobs': 136}, 'loan_rate_C': {'stat': -2.264992296812233, 'pvalue': 0.18350727828330493, 'usedlag': 13, 'nobs': 136}, 'loan_rate_D': {'stat': -1.8536131757387562, 'pvalue': 0.35418744654020773, 'usedlag': 13, 'nobs': 136}, 'FEDFUNDS': {'stat': -3.970771700595016, 'pvalue': 0.00157153429138061, 'usedlag': 13, 'nobs': 136}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        zivot_andrews\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -3.4986701764941164, 'pvalue': 0.6804771233224802, 'usedlag': 1, 'nobs': 137}, 'loan_rate_B': {'stat': -5.24634607418534, 'pvalue': 0.011857344006006859, 'usedlag': 9, 'nobs': 137}, 'loan_rate_C': {'stat': -4.681009266047663, 'pvalue': 0.07413460165891156, 'usedlag': 10, 'nobs': 137}, 'loan_rate_D': {'stat': -4.5661357909991125, 'pvalue': 0.10001302102995041, 'usedlag': 5, 'nobs': 137}, 'FEDFUNDS': {'stat': -4.263682850153341, 'pvalue': 0.20816298878228218, 'usedlag': 13, 'nobs': 137}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dickey_fuller_gls\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -1.79800835619108, 'pvalue': 0.07127713927441981, 'usedlag': 1, 'nobs': 135}, 'loan_rate_B': {'stat': -1.3606975485926263, 'pvalue': 0.16677847263199563, 'usedlag': 9, 'nobs': 127}, 'loan_rate_C': {'stat': -0.4700035237018868, 'pvalue': 0.5125489900593104, 'usedlag': 10, 'nobs': 126}, 'loan_rate_D': {'stat': 0.2675751876186429, 'pvalue': 0.777908700165412, 'usedlag': 5, 'nobs': 131}, 'FEDFUNDS': {'stat': -1.4167584741514279, 'pvalue': 0.15109358273536777, 'usedlag': 13, 'nobs': 123}}\n                    \n                \n            \n        \n        \n        \n        \n        \n\n\nUnit Root Tests with Stationarity Decision.\n\n# Question: Ideally we would like to show results of unit root test plan like this (results hardcoded)\nunit_root_test_results = {\n    'Series': ['loan_rate_A', 'loan_rate_A', 'loan_rate_A', 'loan_rate_A', 'loan_rate_A', 'loan_rate_B', 'loan_rate_B', 'loan_rate_B', 'loan_rate_B', 'loan_rate_B', 'loan_rate_C', 'loan_rate_C', 'loan_rate_C', 'loan_rate_C', 'loan_rate_C', 'loan_rate_D', 'loan_rate_D', 'loan_rate_D', 'loan_rate_D', 'loan_rate_D', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS', 'FEDFUNDS'],\n    'Test': ['ADF', 'KPSS', 'Phillips-Perron', 'Zivot-Andrews', 'DFGLS', 'ADF', 'KPSS', 'Phillips-Perron', 'Zivot-Andrews', 'DFGLS', 'ADF', 'KPSS', 'Phillips-Perron', 'Zivot-Andrews', 'DFGLS', 'ADF', 'KPSS', 'Phillips-Perron', 'Zivot-Andrews', 'DFGLS', 'ADF', 'KPSS', 'Phillips-Perron', 'Zivot-Andrews', 'DFGLS'],\n    'p-value': [0.323972, 0.010000, 0.274663, 0.680477, 0.071277, 0.022424, 0.100000, 0.129150, 0.011857, 0.166778, 0.108190, 0.010000, 0.183507, 0.074135, 0.512549, 0.474219, 0.010000, 0.354187, 0.100013, 0.777909, 0.942169, 0.087609, 0.001572, 0.208163, 0.151094],\n    'Threshold': [0.05] * 25,\n    'Pass/Fail': ['Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Fail', 'Fail', 'Pass', 'Fail', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Fail', 'Fail', 'Pass', 'Pass'],\n    'Decision': ['non-stationary', 'stationary', 'non-stationary', 'non-stationary', 'non-stationary', 'stationary', 'non-stationary', 'non-stationary', 'stationary', 'non-stationary', 'non-stationary', 'stationary', 'non-stationary', 'non-stationary', 'non-stationary', 'non-stationary', 'stationary', 'non-stationary', 'non-stationary', 'non-stationary', 'non-stationary', 'non-stationary', 'stationary', 'non-stationary', 'non-stationary']\n}\n\ndisplay(pd.DataFrame(unit_root_test_results))\n\n\n\n\n\n  \n    \n      \n      Series\n      Test\n      p-value\n      Threshold\n      Pass/Fail\n      Decision\n    \n  \n  \n    \n      0\n      loan_rate_A\n      ADF\n      0.323972\n      0.05\n      Pass\n      non-stationary\n    \n    \n      1\n      loan_rate_A\n      KPSS\n      0.010000\n      0.05\n      Pass\n      stationary\n    \n    \n      2\n      loan_rate_A\n      Phillips-Perron\n      0.274663\n      0.05\n      Pass\n      non-stationary\n    \n    \n      3\n      loan_rate_A\n      Zivot-Andrews\n      0.680477\n      0.05\n      Pass\n      non-stationary\n    \n    \n      4\n      loan_rate_A\n      DFGLS\n      0.071277\n      0.05\n      Pass\n      non-stationary\n    \n    \n      5\n      loan_rate_B\n      ADF\n      0.022424\n      0.05\n      Fail\n      stationary\n    \n    \n      6\n      loan_rate_B\n      KPSS\n      0.100000\n      0.05\n      Fail\n      non-stationary\n    \n    \n      7\n      loan_rate_B\n      Phillips-Perron\n      0.129150\n      0.05\n      Pass\n      non-stationary\n    \n    \n      8\n      loan_rate_B\n      Zivot-Andrews\n      0.011857\n      0.05\n      Fail\n      stationary\n    \n    \n      9\n      loan_rate_B\n      DFGLS\n      0.166778\n      0.05\n      Pass\n      non-stationary\n    \n    \n      10\n      loan_rate_C\n      ADF\n      0.108190\n      0.05\n      Pass\n      non-stationary\n    \n    \n      11\n      loan_rate_C\n      KPSS\n      0.010000\n      0.05\n      Pass\n      stationary\n    \n    \n      12\n      loan_rate_C\n      Phillips-Perron\n      0.183507\n      0.05\n      Pass\n      non-stationary\n    \n    \n      13\n      loan_rate_C\n      Zivot-Andrews\n      0.074135\n      0.05\n      Pass\n      non-stationary\n    \n    \n      14\n      loan_rate_C\n      DFGLS\n      0.512549\n      0.05\n      Pass\n      non-stationary\n    \n    \n      15\n      loan_rate_D\n      ADF\n      0.474219\n      0.05\n      Pass\n      non-stationary\n    \n    \n      16\n      loan_rate_D\n      KPSS\n      0.010000\n      0.05\n      Pass\n      stationary\n    \n    \n      17\n      loan_rate_D\n      Phillips-Perron\n      0.354187\n      0.05\n      Pass\n      non-stationary\n    \n    \n      18\n      loan_rate_D\n      Zivot-Andrews\n      0.100013\n      0.05\n      Pass\n      non-stationary\n    \n    \n      19\n      loan_rate_D\n      DFGLS\n      0.777909\n      0.05\n      Pass\n      non-stationary\n    \n    \n      20\n      FEDFUNDS\n      ADF\n      0.942169\n      0.05\n      Pass\n      non-stationary\n    \n    \n      21\n      FEDFUNDS\n      KPSS\n      0.087609\n      0.05\n      Fail\n      non-stationary\n    \n    \n      22\n      FEDFUNDS\n      Phillips-Perron\n      0.001572\n      0.05\n      Fail\n      stationary\n    \n    \n      23\n      FEDFUNDS\n      Zivot-Andrews\n      0.208163\n      0.05\n      Pass\n      non-stationary\n    \n    \n      24\n      FEDFUNDS\n      DFGLS\n      0.151094\n      0.05\n      Pass\n      non-stationary\n    \n  \n\n\n\n\nInterpretation of Unit Root Tests.\nStep 2: Making Series Stationary\nCompute first difference.\n\ndiff_df = df.diff().dropna()\n\nInspect time series.\nStep 3: Run Unit Root Tests\n\n# Pass first difference to VM dataset\n# Question: I am now overwriting the df, can I log both raw and first diff dataset and use them as required later on? \nvm_dataset = vm.init_dataset(dataset=diff_df)\ntest_context = TestContext(dataset=vm_dataset)\nur_test_plan = UnitRoot(test_context=test_context)\nur_test_plan.run()\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRunning Metric: kpss:  20%|██        | 1/5 [00:00<00:00, 82.73it/s]  The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n                                                                                                         \n\n\nResults for unit_root Test Plan:\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        adf\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -10.288173999889596, 'pvalue': 3.628391608895891e-18, 'usedlag': 0, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -77.77997210900043}, 'loan_rate_B': {'stat': -8.581774306100574, 'pvalue': 7.693931170347862e-14, 'usedlag': 0, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -39.517450997610695}, 'loan_rate_C': {'stat': -8.497490203712452, 'pvalue': 1.264293522721377e-13, 'usedlag': 0, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -89.85288082240504}, 'loan_rate_D': {'stat': -4.089047184282974, 'pvalue': 0.0010096960791906875, 'usedlag': 5, 'nobs': 130, 'critical_values': {'1%': -3.4816817173418295, '5%': -2.8840418343195267, '10%': -2.578770059171598}, 'icbest': -2.8530273746351327}, 'FEDFUNDS': {'stat': -7.7635076306973865, 'pvalue': 9.31881384691925e-12, 'usedla...\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        kpss\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': 0.08690727348877204, 'pvalue': 0.1, 'usedlag': 2, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'loan_rate_B': {'stat': 0.1406230557198794, 'pvalue': 0.1, 'usedlag': 3, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'loan_rate_C': {'stat': 0.302111183227079, 'pvalue': 0.1, 'usedlag': 3, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'loan_rate_D': {'stat': 0.2150982513758707, 'pvalue': 0.1, 'usedlag': 1, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}, 'FEDFUNDS': {'stat': 1.1100238305291938, 'pvalue': 0.01, 'usedlag': 5, 'critical_values': {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        phillips_perron\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -10.31157577193113, 'pvalue': 3.175252114879247e-18, 'usedlag': 13, 'nobs': 135}, 'loan_rate_B': {'stat': -8.656685376413767, 'pvalue': 4.9471754291376904e-14, 'usedlag': 13, 'nobs': 135}, 'loan_rate_C': {'stat': -8.968716395365107, 'pvalue': 7.859548454989337e-15, 'usedlag': 13, 'nobs': 135}, 'loan_rate_D': {'stat': -9.359989759533976, 'pvalue': 7.870234405243031e-16, 'usedlag': 13, 'nobs': 135}, 'FEDFUNDS': {'stat': -5.577757930833498, 'pvalue': 1.4193167380557158e-06, 'usedlag': 13, 'nobs': 135}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        zivot_andrews\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -10.647554732355527, 'pvalue': 1e-05, 'usedlag': 0, 'nobs': 136}, 'loan_rate_B': {'stat': -8.878000651641708, 'pvalue': 1e-05, 'usedlag': 0, 'nobs': 136}, 'loan_rate_C': {'stat': -9.176489623666406, 'pvalue': 1e-05, 'usedlag': 0, 'nobs': 136}, 'loan_rate_D': {'stat': -4.7362602346101195, 'pvalue': 0.06385037699908429, 'usedlag': 5, 'nobs': 136}, 'FEDFUNDS': {'stat': -8.74079625000198, 'pvalue': 1e-05, 'usedlag': 13, 'nobs': 136}}\n                    \n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dickey_fuller_gls\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': {'stat': -9.407796046868018, 'pvalue': 8.038704892435365e-16, 'usedlag': 0, 'nobs': 135}, 'loan_rate_B': {'stat': -6.1286593160527945, 'pvalue': 6.978825245356776e-09, 'usedlag': 0, 'nobs': 135}, 'loan_rate_C': {'stat': -6.7890425662958975, 'pvalue': 2.863415572617712e-10, 'usedlag': 0, 'nobs': 135}, 'loan_rate_D': {'stat': -3.2170698858406395, 'pvalue': 0.0013827003089759758, 'usedlag': 5, 'nobs': 130}, 'FEDFUNDS': {'stat': -3.775689552643456, 'pvalue': 0.0001896667660634073, 'usedlag': 13, 'nobs': 122}}\n                    \n                \n            \n        \n        \n        \n        \n        \n\n\nUnit Root Tests with Stationarity Decision.\nInterpretation of Unit Root Tests.\nStep 4: Decision\nSeries is stationary after first difference.\n\n\nD. Seasonality Analysis\nStep 1: Seasonal decomposition\nPerform seasonal decomposition on each time series.\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalDecompose\ntest_context = TestContext(train_ds=vm_train_ds)\nsd_metric = SeasonalDecompose(test_context=test_context)\n\nNameError: name 'vm_train_ds' is not defined\n\n\nStep 2: Visualize seasonal decomposition\nCreate plots for observed, trend, seasonal and residual components.\n\nsd_metric.run()\nsd_metric.result.show()\n\nSeasonality Detection using ACF and PACF.\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalityDetectionWithACFandPACF\ntest_context = TestContext(train_ds=vm_train_ds)\nacf_metric = SeasonalityDetectionWithACFandPACF(test_context=test_context)\nacf_metric.run()\nacf_metric.result.show()\n\nStep 3: Residuals Analysis\nResiduals series, histogram, Q-Q and ACF plots.\n\n# Comment: How do I pass the residuals of seasonal decomponsition done before using SeasonalDecomposeMetricWithFigure?\nfrom validmind.model_validation.statsmodels.metrics import ResidualsVisualInspection\ntest_context = TestContext(train_ds=vm_train_ds)\nrvi_metric = ResidualsVisualInspection(test_context=test_context)\nrvi_metric.run()\n\n\nrvi_metric.result.show()\n\nTest if Residuals are Normaly Distributed.\n\n# Comment: How do I pass the residuals of seasonal decomponsition done before using SeasonalDecomposeMetricWithFigure?\nvm.run_test_plan(\"normality_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nTest if Residuals are Autocorrelated.\n\n# Comment: How do I pass the residuals of seasonal decomponsition done before using SeasonalDecomposeMetricWithFigure?\nvm.run_test_plan(\"autocorrelation_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nStep 4: Test for seasonality using the Augmented Dickey-Fuller (ADF) test\nStep 5: Analyze the seasonality test results\nStep 6: Interpret the results\nStep 7: Handle seasonality"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#variable-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#variable-analysis",
    "title": "ValidMind",
    "section": "4.2.4 Variable Analysis",
    "text": "4.2.4 Variable Analysis\n## A. Feature Analysis"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#a.1.-univariate-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#a.1.-univariate-analysis",
    "title": "ValidMind",
    "section": "A.1. Univariate Analysis",
    "text": "A.1. Univariate Analysis\n\nVisual Inspection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#a.2-multivariave-analysis",
    "href": "notebooks/time_series/cre_rate_model_demo.html#a.2-multivariave-analysis",
    "title": "ValidMind",
    "section": "A.2 Multivariave Analysis",
    "text": "A.2 Multivariave Analysis\n\nVisual Inspection"
  },
  {
    "objectID": "notebooks/time_series/cre_rate_model_demo.html#b.-variable-selection",
    "href": "notebooks/time_series/cre_rate_model_demo.html#b.-variable-selection",
    "title": "ValidMind",
    "section": "B. Variable Selection",
    "text": "B. Variable Selection\n\nARIMA Analysis\nStep 1: Identify the Integration order (Stationarity Analysis)\nUnit Root Tests.\n\nvm.run_test_plan(\"unit_root_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nStep 2: Identify the AR order\nStep 3: Identify the MA order\n\nvm.run_test_plan(\"normality_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRun SeasonalDecomposeMetricWithFigure Test\n\n# test_context = TestContext(train_ds=vm_train_ds)\n# sd_metric = SeasonalDecomposeMetricWithFigure(test_context=test_context)\n# sd_metric.run()\n\nRun ResidualsVisualInspection Test\n\ntest_context = TestContext(train_ds=vm_train_ds, test_ds=vm_test_ds)\nrvi_test = ResidualsVisualInspection(test_context=test_context)\nrvi_test.run()\n\n\nvm.run_test_plan(\"seasonality_test_plan\", train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n\nloan_rate_columns = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\ndiff1_loan_rate_columns = [\"diff1_loan_rate_A\", \"diff1_loan_rate_B\", \"diff1_loan_rate_C\", \"diff1_loan_rate_D\"]\n\ntest_plan_config = {\n    \"time_series_univariate_inspection_raw\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    },\n    \"time_series_univariate_inspection_histogram\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    }\n}\n\nvm.run_test_plan(\n    \"timeseries_test_plan\",\n    config=test_plan_config,    \n    test_ds=vm_test_ds,\n    train_ds=vm_train_ds,\n    dataset=vm_train_ds,\n)\n\nRunning Metric: time_series_univariate_inspection_raw: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]      \n\n\nResults for time_series_univariate_inspection Test Plan:\n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/time_series_model_validation_poc.html",
    "href": "notebooks/time_series/time_series_model_validation_poc.html",
    "title": "ValidMind",
    "section": "",
    "text": "# System libraries\nimport glob\n\n# ML libraries\nimport pickle \nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import coint\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_1.pkl', 'rb') as f:\n    model_1 = pickle.load(f)\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Tue, 09 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        13:36:27   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_2.pkl', 'rb') as f:\n    model_2 = pickle.load(f)\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Tue, 09 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        13:36:27   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_3.pkl', 'rb') as f:\n    model_3 = pickle.load(f)\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Tue, 09 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        13:36:27   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_4.pkl', 'rb') as f:\n    model_4 = pickle.load(f)\nprint(model_4.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            OLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Tue, 09 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        13:36:27   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_5.pkl', 'rb') as f:\n    model_5 = pickle.load(f)\nprint(model_5.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            OLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Tue, 09 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        13:36:27   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n# Extract the endogenous (target) variable from the model fit\ntrain_df = pd.Series(model_1.model.endog, index=model_1.model.data.row_labels)\ntrain_df = train_df.to_frame()\ntarget_var_name = model_1.model.endog_names\ntrain_df.columns = [target_var_name]\n\n# Extract the exogenous (explanatory) variables from the model fit\nexog_df = pd.DataFrame(model_1.model.exog, index=model_1.model.data.row_labels, columns=model_1.model.exog_names)\n\n# Concatenate the endogenous (target) and exogenous (explanatory) variables\ntrain_df = pd.concat([train_df, exog_df], axis=1)\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1971-05-01\n      0.17\n      0.47\n    \n    \n      1971-06-01\n      0.08\n      0.28\n    \n    \n      1971-07-01\n      0.15\n      0.40\n    \n    \n      1971-08-01\n      0.00\n      0.26\n    \n    \n      1971-09-01\n      -0.02\n      -0.02\n    \n  \n\n\n\n\n\ntrain_df.tail()\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      2012-06-01\n      -0.09\n      0.00\n    \n    \n      2012-07-01\n      -0.17\n      0.00\n    \n    \n      2012-08-01\n      0.10\n      -0.03\n    \n    \n      2012-09-01\n      -0.19\n      0.01\n    \n    \n      2012-10-01\n      0.01\n      0.02\n    \n  \n\n\n\n\n\n\n\nLoad raw test dataset.\n\nfile = '../datasets/time_series/fred_loan_rates_test_1.csv'\nraw_test_df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\ndisplay(raw_test_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      UNRATE\n      GS10\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      2012-11-01\n      3.32\n      7.7\n      1.65\n      0.16\n    \n    \n      2012-12-01\n      3.35\n      7.9\n      1.72\n      0.16\n    \n    \n      2013-01-01\n      3.53\n      8.0\n      1.91\n      0.14\n    \n    \n      2013-02-01\n      3.51\n      7.7\n      1.98\n      0.15\n    \n    \n      2013-03-01\n      3.57\n      7.5\n      1.96\n      0.14\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-11-01\n      6.58\n      3.6\n      3.89\n      3.78\n    \n    \n      2022-12-01\n      6.42\n      3.5\n      3.62\n      4.10\n    \n    \n      2023-01-01\n      6.13\n      3.4\n      3.53\n      4.33\n    \n    \n      2023-02-01\n      6.50\n      3.6\n      3.75\n      4.57\n    \n    \n      2023-03-01\n      6.32\n      3.5\n      3.66\n      4.65\n    \n  \n\n125 rows × 4 columns\n\n\n\nTransform raw test dataset using same transformation used in the train dataset.\n\ntransform_func = 'diff'\nif transform_func == 'diff':\n    test_df = raw_test_df.diff().dropna()\n\n\n\n\n\ndef get_model_prediction(model_fits_dict, df_test):\n    # Extract the training data from the first model fit\n    first_model_fit = list(model_fits_dict.values())[0]\n    train_data = pd.Series(first_model_fit.model.endog, index=first_model_fit.model.data.row_labels)\n    train_data = train_data.to_frame()\n    target_var_name = first_model_fit.model.endog_names\n    train_data.columns = [f'{target_var_name}_train']\n\n    # Initialize an empty DataFrame to store the predictions\n    prediction_df = pd.DataFrame(index=df_test.index)\n    prediction_df[f'{target_var_name}_test'] = np.nan\n\n    # Concatenate the train_data and prediction_df\n    combined_df = pd.concat([train_data, prediction_df], axis=0)\n\n    # Loop through each model fit\n    for model_name, model_fit in model_fits_dict.items():\n        # Prepare the test dataset\n        exog_names = model_fit.model.exog_names\n        X_test = df_test.copy()\n\n        # Add the constant if it's missing\n        if 'const' in exog_names and 'const' not in X_test.columns:\n            X_test['const'] = 1.0\n\n        # Select the necessary columns\n        X_test = X_test[exog_names]\n\n        # Generate the predictions\n        predictions = model_fit.predict(X_test)\n\n        # Add the predictions to the DataFrame\n        combined_df[model_name] = np.nan\n        combined_df[model_name].iloc[len(train_data):] = predictions\n\n    # Add the test data to the '<target_variable>_test' column\n    combined_df[f'{target_var_name}_test'].iloc[len(train_data):] = df_test[target_var_name]\n\n    return combined_df\n\n\n\n# Replace with your list of model fits\nmodel_fits = {\n    'model_1': model_1,\n    'model_3': model_3\n}  \nprediction_df = get_model_prediction(model_fits, test_df)\ndisplay(prediction_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US_train\n      MORTGAGE30US_test\n      model_1\n      model_3\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      1971-05-01\n      0.17\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-06-01\n      0.08\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-07-01\n      0.15\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-08-01\n      0.00\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-09-01\n      -0.02\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-11-01\n      NaN\n      -0.50\n      0.203037\n      -0.066856\n    \n    \n      2022-12-01\n      NaN\n      -0.16\n      0.092817\n      -0.200567\n    \n    \n      2023-01-01\n      NaN\n      -0.29\n      0.066712\n      -0.066856\n    \n    \n      2023-02-01\n      NaN\n      0.37\n      0.069613\n      0.163425\n    \n    \n      2023-03-01\n      NaN\n      -0.18\n      0.023204\n      -0.066856\n    \n  \n\n622 rows × 4 columns\n\n\n\n\ndef plot_predictions(prediction_df, subplot=True):\n    n_models = prediction_df.shape[1] - 2\n    \n    if subplot:\n        fig, axes = plt.subplots(n_models, 1, figsize=(12, 6 * n_models), sharex=True)\n        \n        for i in range(n_models):\n            axes[i].plot(prediction_df.index, prediction_df.iloc[:, 0], label=prediction_df.columns[0], color='grey')\n            axes[i].plot(prediction_df.index, prediction_df.iloc[:, 1], label=prediction_df.columns[1], color='lightgrey')\n            axes[i].plot(prediction_df.index, prediction_df.iloc[:, i + 2], label=prediction_df.columns[i + 2], linestyle='-')\n            axes[i].set_ylabel('Target Variable')\n            axes[i].set_title(f'Test Data vs. {prediction_df.columns[i + 2]}')\n            axes[i].legend()\n            axes[i].grid(True)\n        plt.xlabel('Date')\n        plt.tight_layout()\n        plt.show()\n        \n    else:\n        plt.figure(figsize=(12, 6))\n        plt.plot(prediction_df.index, prediction_df.iloc[:, 0], label=prediction_df.columns[0], color='grey')\n        plt.plot(prediction_df.index, prediction_df.iloc[:, 1], label=prediction_df.columns[1], color='lightgrey')\n        \n        for i in range(2, prediction_df.shape[1]):\n            plt.plot(prediction_df.index, prediction_df.iloc[:, i], label=prediction_df.columns[i], linestyle='-')\n        \n        plt.xlabel('Date')\n        plt.ylabel('Target Variable')\n        plt.title('Test Data vs. Model Forecasts')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n\nplot_predictions(prediction_df, subplot=True)"
  },
  {
    "objectID": "notebooks/intro-r.html",
    "href": "notebooks/intro-r.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "notebooks/intro-r.html#initializing-the-validmind-library",
    "href": "notebooks/intro-r.html#initializing-the-validmind-library",
    "title": "ValidMind",
    "section": "Initializing the ValidMind Library",
    "text": "Initializing the ValidMind Library\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\nvm.init(\n      api_host = \"http://localhost:3000/api/v1/tracking\",\n      api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n      api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n      project = \"clfmztk8t0000qvoo2wh30ex7\"\n)\n\nTrue\n\n\n\nUsing a demo dataset\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\nRunning a dataset evaluation test plan\nWe will now run a basic tabular dataset test plan that will compute and log the description, metrics and tests for the dataset. For now, ValidMind supports metrics and tests for a single dataset, so we first will combine the test and train sets into one to get complete datapoints.\n\n## load the train and test dataframes\n# assume that the train and test datasets have been saved to disk as csv files\n# see the r-customer-churn-model.ipynb notebook for details on how this can be done\ndf_train = pd.read_csv(\"r_demo/r_churn_train.csv\")\ndf_test = pd.read_csv(\"r_demo/r_churn_test.csv\")\n\nvm_train_ds = vm.init_dataset(dataset=df_train, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=df_test, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n# run dataset tests on df_train and df_test combined\nvm_dataset_combined = vm.init_dataset(\n    dataset=pd.concat([df_train, df_test]),\n    type=\"generic\",\n    target_column=\"Exited\",\n)\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset_combined)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                                                                                                 \n\n\n\nResults for tabular_dataset_description Test Plan:Logged the following dataset to the ValidMind platform:\n  \n    \n      \n      GeographyFrance\n      GeographyGermany\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.00000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      0.50125\n      0.251125\n      0.549500\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      0.50003\n      0.433687\n      0.497575\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      0.00000\n      0.000000\n      0.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      0.00000\n      0.000000\n      0.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      1.00000\n      0.000000\n      1.000000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      1.00000\n      1.000000\n      1.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      1.00000\n      1.000000\n      1.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        generic\n                    \n                \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for tabular_data_quality Test Plan:\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=False, values={'correlations': [{'column': 'GeographyGermany', 'correlation': -0.5805318438630581}]}), TestResult(test_name=None, column='GeographyGermany', passed=False, values={'correlations': [{'column': 'Balance', 'correlation': 0.4062612295669473}]}), TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389454}]})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.024522142979951}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.0076920437747027195}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111807}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077725})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\n\nRunning a model evaluation test plan for our LogisticRegression R model\nWe will now run a basic model evaluation test plan that is compatible with the R model we will be loading.\n\n## load the model\n# assume that the model has been saved to disk as a serialized R object (.rds)\n# see the r-customer-churn-model.ipynb notebook for details on how this can be done\n# model_type must be passed for R models:\n# Currently, LogisticRegression, LinearRegression (glm and lm in R) XGBClassifier and XGBRegressor are supported\nvm_model = vm.init_r_model(\"r_demo/r_log_reg_churn_model.rds\", model_type=\"LogisticRegression\")\n\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n                                                                                                                                                                                                                 \n\n\nResults for sklearn_classifier_metrics Test Plan:\n        Logged the following model to the ValidMind platform:\n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.8604166666666667\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.5743329097839899\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7687074829931972\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.45841784989858014\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7113798740840568\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'GeographyFrance': 2e-06, 'GeographyGermany': 5e-05, 'Gender': 7e-06, 'Age': 0.000446, 'Tenure': 0.000397, 'Balance': 0.001007, 'NumOfProducts': 0.000273, 'HasCrCard': 0.000234, 'IsActiveMember': 4.2e-05, 'EstimatedSalary': 0.000501}\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial   new  percent_new       psi\nbin                                                       \n1       2840          0.50714  1218     0.507500  0.000000\n2       1155          0.20625   474     0.197500  0.000379\n3        464          0.08286   218     0.090833  0.000733\n4        301          0.05375   126     0.052500  0.000029\n5        172          0.03071    72     0.030000  0.000017\n6        174          0.03107    73     0.030417  0.000014\n7        116          0.02071    57     0.023750  0.000415\n8        121          0.02161    54     0.022500  0.000036\n9        152          0.02714    67     0.027917  0.000022\n10       105          0.01875    41     0.017083  0.000155\n                \n            \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for sklearn_classifier_validation Test Plan:\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.72125, 'threshold': 0.7})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.14993646759847523, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.49822262593987565, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=True, values={'test_score': 0.72125, 'train_score': 0.7255357142857143, 'degradation': 0.005906965296578953}), TestResult(test_name='precision', column=None, passed=False, values={'test_score': 0.20068027210884354, 'train_score': 0.19011976047904192, 'degradation': -0.05554662810005893}), TestResult(test_name='recall', column=None, passed=False, values={'test_score': 0.11967545638945233, 'train_score': 0.11308993766696349, 'degradation': -0.05823257893980294}), TestResult(test_name='f1', column=None, passed=False, values={'test_score': 0.14993646759847523, 'train_score': 0.14182021217197097, 'degradation': -0.05722918688531157})]"
  },
  {
    "objectID": "notebooks/test_plan_summmary.html",
    "href": "notebooks/test_plan_summmary.html",
    "title": "ValidMind",
    "section": "",
    "text": "import validmind as vm\n\nvm.init(\n    api_host = \"http://localhost:3000/api/v1/tracking\",\n    api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n    api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n    project = \"cleytvf7i0000w5oo2a9ygkmi\"\n)\n\nTrue\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n                                                                                                                                                                                                                 \n\n\n\nResults for tabular_dataset_description Test Plan:Logged the following dataset to the ValidMind platform:\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        training\n                    \n                \n            \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for tabular_data_quality Test Plan:Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Surname', passed=False, values={'n_distinct': 2616, 'p_distinct': 0.327}), TestResult(test_name=None, column='Geography', passed=True, values={'n_distinct': 3, 'p_distinct': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389458}]})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CustomerId', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Geography', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=True, values={'skewness': -0.005920679739677088}), TestResult(test_name=None, column='CustomerId', passed=True, values={'skewness': 0.010032280260684402}), TestResult(test_name=None, column='CreditScore', passed=True, values={'skewness': -0.06195161237091896}), TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.0245221429799511}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.007692043774702702}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111804}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077728})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='RowNumber', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='CustomerId', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_unique': 2616, 'p_unique': 0.327}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_unique': 452, 'p_unique': 0.0565}), TestResult(test_name=None, column='Geography', passed=True, values={'n_unique': 3, 'p_unique': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.853125\n\n\n\nvm_model = vm.init_model(model)\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nRunning SHAPGlobalImportance: shap:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 12/13 [00:00<00:00, 33.23it/s]ntree_limit is deprecated, use `iteration_range` or model slicing instead.\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n                                                                                                                                                                                                                 \n\n\nResults for sklearn_classifier_metrics Test Plan:Logged the following model to the ValidMind platform:\n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.879375\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.6053169734151329\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.783068783068783\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.49333333333333335\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7308974358974359\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'Gender': 1.4e-05, 'Age': 0.000816, 'Tenure': 0.000625, 'Balance': 0.000977, 'NumOfProducts': 2.6e-05, 'HasCrCard': 2e-06, 'IsActiveMember': 6.3e-05, 'EstimatedSalary': 0.000863, 'Geography_France': 0.00021, 'Geography_Germany': 2.9e-05, 'Geography_Spain': 0.000131}\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial  new  percent_new       psi\nbin                                                      \n1       2607         0.543125  858        0.536  0.000088\n2        748         0.155833  253        0.158  0.000033\n3        398         0.082917  148        0.092  0.001048\n4        267         0.055625   97        0.061  0.000430\n5        177         0.036875   58        0.036  0.000011\n6        118         0.024583   48        0.030  0.001079\n7        101         0.021042   24        0.015  0.002045\n8         89         0.018542   31        0.019  0.000037\n9        109         0.022708   24        0.015  0.003197\n10       186         0.038750   59        0.037  0.000093\n                \n            \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for sklearn_classifier_validation Test Plan:Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.733125, 'threshold': 0.7})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.12678936605316973, 'threshold': 0.5})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.49089743589743584, 'threshold': 0.5})]\n            \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=False, values={'test_score': 0.733125, 'train_score': 0.7170833333333333, 'degradation': -0.022370714700755467}), TestResult(test_name='precision', column=None, passed=True, values={'test_score': 0.164021164021164, 'train_score': 0.18407960199004975, 'degradation': 0.10896610896610899}), TestResult(test_name='recall', column=None, passed=True, values={'test_score': 0.10333333333333333, 'train_score': 0.11361310133060389, 'degradation': 0.09048048048048046}), TestResult(test_name='f1', column=None, passed=True, values={'test_score': 0.12678936605316973, 'train_score': 0.14050632911392405, 'degradation': 0.09762523259455776})]"
  },
  {
    "objectID": "notebooks/log_image.html",
    "href": "notebooks/log_image.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialize ValidMind SDK\nimport validmind as vm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(os.getcwd())\n\n/Users/panchicore/www/validmind/validmind-sdk\n\n\n\n\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0\n\n\n\n\n\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  }
]