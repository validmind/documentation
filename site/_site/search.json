[
  {
    "objectID": "guide/developer-framework-introduction.html",
    "href": "guide/developer-framework-introduction.html",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "This page provides an introduction for:\n\nWhat the ValidMind Developer Framework is, key concepts, and what functionality it provides\nHow ValidMind documentation projects are structured"
  },
  {
    "objectID": "guide/developer-framework-introduction.html#brief-introduction",
    "href": "guide/developer-framework-introduction.html#brief-introduction",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "This page provides an introduction for:\n\nWhat the ValidMind Developer Framework is, key concepts, and what functionality it provides\nHow ValidMind documentation projects are structured"
  },
  {
    "objectID": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "href": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "What ValidMind’s Developer Framework is",
    "text": "What ValidMind’s Developer Framework is\n\nValidMind’s Python Developer Framework is a library of developer tools and methods designed to automate the documentation and validation of your models.\nThe Developer Framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python library will provide all the standard functionality without requiring your developers to rewrite any functions.\nThe Developer Framework provides a rich suite of documentation tools and test plans, from documenting descriptions of your dataset to testing your models for weak spots and overfit areas. The Developer Framework helps you automate the generation of model documentation by feeding the ValidMind platform with documentation artifacts and test results to the ValidMind platform."
  },
  {
    "objectID": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "href": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "ValidMind Documentation Project Structure",
    "text": "ValidMind Documentation Project Structure\n\n\nProjects\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s MRM lifecycle will constitute a new project, and may be configured with its own templates and workflows.\n\nModel documentation\n\nA comprehensive record and description of a quantitative model. It should encompass all relevant information about the model in accordance with regulatory requirements (set by regulatory bodies) and model risk policies (set by an institution’s MRM team), assumptions, methodologies, data and inputs, model performance evaluation, limitations, and intended use. The purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest plans\n\nA collection of many tests which are meant to be run simultaneously to validate and document specific aspects of the documentation. For instance, the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind UI.\n\nTest suites\n\nCollection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\n\nTemplates\n\nAn outline of the sections/sub-sections of a ValidMind document (model documentation or validation report) and how they are organized. Templates also contain boilerplates and documentation & test results placeholders for which content will be provided by the Developer Framework. Template requirements are typically provided by the model risk management team, and can be configured programmatically for each model use case, typically by an administrator."
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind overview",
    "section": "",
    "text": "ValidMind is a model risk management (MRM) solution designed for the specific needs of model developers and model validators alike. The platform automates key aspects of the MRM process, including model documentation, validation, and testing. In addition, the platform comes with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date.\nOur solution comprises two primary architectural components: the ValidMind Developer Framework and the cloud-based ValidMind MRM platform."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind overview",
    "section": "Related Topics",
    "text": "Related Topics\nReady to try out ValidMind? Try the Quickstart."
  },
  {
    "objectID": "guide/view-templates.html",
    "href": "guide/view-templates.html",
    "title": "View templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-templates.html#prerequisites",
    "href": "guide/view-templates.html#prerequisites",
    "title": "View templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-templates.html#steps",
    "href": "guide/view-templates.html#steps",
    "title": "View templates",
    "section": "Steps",
    "text": "Steps\n\nFrom the ValidMind Platform homepage, go to Templates on the left.\nClick on one of the available templates to view the YAML configuration file.\nIn the configuration file that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the Developer Framework that feed into the template\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTemplates can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-templates.html#related-topics",
    "href": "guide/view-templates.html#related-topics",
    "title": "View templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developers section."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe validation guidelines for each template can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s Next",
    "text": "What’s Next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "href": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/jupyter-notebooks.html",
    "href": "guide/jupyter-notebooks.html",
    "title": "Example notebooks",
    "section": "",
    "text": "Our example notebooks are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nTry the notebooks yourself:\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time, and download notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/jupyter-notebooks.html#related-topics",
    "href": "guide/jupyter-notebooks.html#related-topics",
    "title": "Example notebooks",
    "section": "Related topics",
    "text": "Related topics\nFor an introduction to how these notebooks get used with ValidMind, take a look at the Quickstart."
  },
  {
    "objectID": "guide/view-all-test-plans.html",
    "href": "guide/view-all-test-plans.html",
    "title": "View all test plans",
    "section": "",
    "text": "Learn how to use list_plans(), list_test(), and describe_plan() methods to view and describe test plans and tests available in the Developer Framework."
  },
  {
    "objectID": "guide/view-all-test-plans.html#prerequisites",
    "href": "guide/view-all-test-plans.html#prerequisites",
    "title": "View all test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are working on an active documentation project\nYou have already installed the ValidMind client library in your developer environment"
  },
  {
    "objectID": "guide/view-all-test-plans.html#steps",
    "href": "guide/view-all-test-plans.html#steps",
    "title": "View all test plans",
    "section": "Steps",
    "text": "Steps\n\nInitialize the client library.\nUse list_plans() and list_tests() to view the list of all available test plans and tests.\nExamples:\n\nList all available test plans currently available in the the Developer Framework:\nvm.test_plans.list_plans()\nList all available individual tests currently available in the Developer Framework:\nvm.test_plans.list_tests() \n\nUse describe_testplan() to list all the tests included in a specific test plan:\nExample: The following code will list tests included in the tabular_data_quality test plan:\nvm.test_plans.describe_plan(\"tabular_data_quality\")"
  },
  {
    "objectID": "guide/view-all-test-plans.html#related-topics",
    "href": "guide/view-all-test-plans.html#related-topics",
    "title": "View all test plans",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a solution designed to help simplify and automate key aspects of model risk management (MRM) activities for model developers and model validators alike. The platform helps automate model documentation, validation, and testing. In addition, the platform offers with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/get-started.html#welcome-to-validmind",
    "href": "guide/get-started.html#welcome-to-validmind",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a solution designed to help simplify and automate key aspects of model risk management (MRM) activities for model developers and model validators alike. The platform helps automate model documentation, validation, and testing. In addition, the platform offers with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/get-started.html#how-does-it-work",
    "href": "guide/get-started.html#how-does-it-work",
    "title": "Get started",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\nValidMind consists of two main products components:\n\nThe Developer Framework is a library of tools and methods designed to automate model documentation and validation. It is platform agnostic, and integrates with the model development environment.\nThe ValidMind Platform is an easy-to-use web-based UI that enables users to review and edit the documentation and test metrics generated by the Developer Framework online. It also enables collaboration and feedback capture between model developers and model validators, and offers workflow capabilities to manage the model documentation and validation process.\n\nFor more information about what ValidMind offers, check out our ValidMind overview."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "href": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "title": "Get started",
    "section": "How do I get access to ValidMind?",
    "text": "How do I get access to ValidMind?\nIf you are new to our products, you will need access: Request a trial."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nThe fastest way to explore what ValidMind can offer is with our Quickstart.\nThe Quickstart takes about 20 minutes to complete and walks you through the Developer Framework with a sample Jupyter notebook and introduces you to the ValidMind Platform.\nIf you have already tried the Quickstart, more how-to instructions and links to our FAQs can be found in Next steps."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html",
    "href": "guide/use-test-plans-and-tests.html",
    "title": "When to use test plans and tests",
    "section": "",
    "text": "This topic provides an overview about:"
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "href": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "title": "When to use test plans and tests",
    "section": "What Tests, Test plans, and Test suites are",
    "text": "What Tests, Test plans, and Test suites are\n\nTests are designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\nTest plans are collections of tests which are meant to be run simultaneously to address specific aspects of the documentation.\nExample: the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind Platform.\nTest suites are collection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\nExample: the binary_classifier_full_suite test suite runs the tabular_dataset and binary_classifier test plans to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "href": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "title": "When to use test plans and tests",
    "section": "When to use ValidMind Tests, Test plans, and Test suites",
    "text": "When to use ValidMind Tests, Test plans, and Test suites\nValidMind provides many built-in tests and test plans which make it easy for a model developer to document their work at any point during the model development lifecycle when they need to validate that their work satisfies model risk management requirements.\nWhile model developers have the flexibility to decide when to use ValidMind tests, we have identified a few typical scenarios which have their own characteristics and needs:\n\nWhen you want to document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test plan.\nFor time-series datasets: use the time_series_dataset test plan.\n\nWhen you want to document and validate about your model:\n\nFor binary classification models: use the binary_classifier test plan.\nFor time series models: use the timeseries test plan.\n\nWhen you want to document a binary classification model and the relevant dataset end-to-end: use the binary_classifier_full_suite test suite."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#api-reference",
    "href": "guide/use-test-plans-and-tests.html#api-reference",
    "title": "When to use test plans and tests",
    "section": "API Reference",
    "text": "API Reference\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/editions-and-features.html",
    "href": "guide/editions-and-features.html",
    "title": "Editions and features",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions-and-features.html#editions",
    "href": "guide/editions-and-features.html#editions",
    "title": "Editions and features",
    "section": "Editions",
    "text": "Editions\n\nDeveloper Edition\nThe Developer Edition is the ideal training ground for developers to play around with ValidMind’s automated model documentation and to test the robustness of our developer framework, documentation, and testing features. The Developer Edition is free, allowing developers who are new to model documentation and model risk management to build, implement, test, and maintain higher quality models and model documentation.\nThe Developer Edition is only for personal testing purposes and cannot be used as a commercial model documentation or model risk management solution.\n\n\nEssential Edition\nWith the Essential Edition, you get an advanced model risk management (MRM) solution. It offers your organization all the features and services of the Developer Edition, plus additional features tailored to the needs of larger-scale organizations.\n\n\nBusiness Critical\nProvides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. This edition encompasses all features and services of the Essential Edition but within a separate ValidMind environment, isolated from other ValidMind accounts via Virtual Private ValidMind (VPV). VPV accounts do not share resources with non-VPV accounts."
  },
  {
    "objectID": "guide/editions-and-features.html#features",
    "href": "guide/editions-and-features.html#features",
    "title": "Editions and features",
    "section": "Features",
    "text": "Features\n\n\n\n\nModel development & documentation\nDeveloper\nEssential\nBusiness Critical\n\n\n\n\nAutomated model documentation\n\n\n\n\n\nPlatform-independent developer framework\n\n\n\n\n\nOnline documentation editing\n\n\n\n\n\nAdvanced editing & readability assistance\n\n\n\n\n\nDocumentation quality measurement\n\n\n\n\n\nOffline document ingestion\n\n\n\n\n\nFeedback capture on online document\n\n\n\n\n\nDocumentation version history management\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nStandard tests & validation libraries\n\n\n\n\n\nConfigure / customize tests & validation libraries\n\n\n\n\n\nSupport for customer-provided tests\n\n\n\n\n\nDeveloper workflow management\n\n\n\n\n\nPre-configured documentation templates & boilerplates\n\n\n\n\n\nConfigurable documentation templates & boilerplates\n\n\n\n\n\nModel validation & audit\n\n\n\n\n\nModel validation report automation\n\n\n\n\n\nFindings / issues & remediation actions tracking\n\n\n\n\n\nConfigurable approval workflows\n\n\n\n\n\nMRM workflows & validation lifecycle tracking\n\n\n\n\n\nMRM resource & workflow management\n\n\n\n\n\nCentral model inventory\n\n\n\n\n\nHistorical documentation repository /documentation CMS\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nExecutive reporting\n\n\n\n\n\nPlatform integration & support\n\n\n\n\n\nData lake integration, such as Evidence Storeand monitoring data\n\n\n\n\n\nSSO integration\n\n\n\n\n\nCustomer managed encryption\n\n\n\n\n\nSupport 8/5 (one timezone)\n\n\n\n\n\nSupport 24/7 (global)\n\n\n\n\n\nPlatform deployment\n\n\n\n\n\nMulti-tenant SaaS\n\n\n\n\n\nVirtual private ValidMind (VPV)\n\n\n\n\n\nSelf-managed VPV\n\n\n\n\n\n\nContact Us\nContact Us\nContact Us"
  },
  {
    "objectID": "guide/register-model.html",
    "href": "guide/register-model.html",
    "title": "Register models",
    "section": "",
    "text": "Register a model you are documenting in the model inventory."
  },
  {
    "objectID": "guide/register-model.html#prerequisites",
    "href": "guide/register-model.html#prerequisites",
    "title": "Register models",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/register-model.html#steps",
    "href": "guide/register-model.html#steps",
    "title": "Register models",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/register-model.html#troubleshooting",
    "href": "guide/register-model.html#troubleshooting",
    "title": "Register models",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/register-model.html#whats-next",
    "href": "guide/register-model.html#whats-next",
    "title": "Register models",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html",
    "href": "guide/try-developer-framework-with-docker.html",
    "title": "Try the Developer Framework with Docker Desktop",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with the ValidMind Docker image."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#prerequisites",
    "href": "guide/try-developer-framework-with-docker.html#prerequisites",
    "title": "Try the Developer Framework with Docker Desktop",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have Docker Desktop installed on your machine."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#steps",
    "href": "guide/try-developer-framework-with-docker.html#steps",
    "title": "Try the Developer Framework with Docker Desktop",
    "section": "Steps",
    "text": "Steps\n\nFrom the command line, pull the latest ValidMind Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nRun the ValidMind Docker image:\ndocker run -it -p 8888:8888 validmind/validmind-jupyter-demo\nAfter the command completes, you should see a message that Jupyter Server is running similar to this:\n[I 2023-05-18 21:53:06.030 ServerApp] Serving notebooks from local directory: /app\n    1 active kernel\n    Jupyter Server 2.5.0 is running at:\n    http://032c824982aa:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\n        http://127.0.0.1:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\nCopy the browser URL that starts with http://127.0.0.1:8888 from the message and paste it into a new browser tab.\nAfter JupyterLab opens in your browser, you should see a link for our Quickstart_Customer Churn_full_suite.ipynb notebook.\nDouble click the notebook to open it:"
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#whats-next",
    "href": "guide/try-developer-framework-with-docker.html#whats-next",
    "title": "Try the Developer Framework with Docker Desktop",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore ValidMind in the Quickstart to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework."
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.prod.validmind.ai.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "href": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test plans and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html",
    "href": "guide/release-notes-2023-may-30.html",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#release-highlights",
    "href": "guide/release-notes-2023-may-30.html#release-highlights",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#bugfixes",
    "href": "guide/release-notes-2023-may-30.html#bugfixes",
    "title": "May 30, 2023",
    "section": "Bugfixes",
    "text": "Bugfixes\n\nFixed the display alignment in certain pages of the UI.\nFixed display issues related to Helvetica Neue font not available for Windows users.\nFixed an issue preventing users to drag & drop image files directly in the online editor.\nAdjusted filters for Model Inventory and Documentation Projects search boxes."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "href": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "title": "May 30, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, refresh your browser.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/edit-templates.html",
    "href": "guide/edit-templates.html",
    "title": "Edit templates",
    "section": "",
    "text": "Learn how to edit templates that get used for model documentation or for validation reports. This topic is relevant for administrators who need to configure templates for specific use cases or where the existing templates supplied by ValidMind need to be customized.\nDocumentation templates are stored as YAML files that you edit directly in the online editor. These templates are versioned and saving a documentation template after making changes or reverting to a previous version state always creates a new version."
  },
  {
    "objectID": "guide/edit-templates.html#prerequisites",
    "href": "guide/edit-templates.html#prerequisites",
    "title": "Edit templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe template you want to edit must have been added to the ValidMind Platform already.\nIf you are not sure which template or which version of a template a documentation project is using, check the Project Overview page of your documentation project. The Documentation Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/edit-templates.html#template-schema",
    "href": "guide/edit-templates.html#template-schema",
    "title": "Edit templates",
    "section": "Template schema",
    "text": "Template schema\n Schema Docs\n\n\n\n\n\n\n\n\n  Type: object     template_id Required     root    template_idType: string Unique identifier for the template.          template_name Required     root    template_nameType: string Name of the template.          version Required     root    versionType: string Version of the template.          description     root    descriptionType: string Description of the template.          sections Required     root    sectionsType: array Documentation sections of the template.  Each item of this array must be:   root    sections    sectionType: object     id Required     root    sections    sections items    idType: string Unique identifier for the section.          title Required     root    sections    sections items    titleType: string Title of the section.          description     root    sections    sections items    descriptionType: string Description of the section.          parent_section     root    sections    sections items    parent_sectionType: string ID of the parent section.          order     root    sections    sections items    orderType: integer Order of the section in the navigation menu. By default sections are ordered alphabetically. If order is specified, sections will be ordered by the order value, and then alphabetically.          default_text     root    sections    sections items    default_textType: string Default text for the section. If set, a metadata content row will be created with this text when installing the template on a given project          index_only     root    sections    sections items    index_onlyType: boolean If true, the section will be displayed in the navigation menu, but it will not be accessible via direct link.          condensed     root    sections    sections items    condensedType: boolean If true, the section will condense all of its subsections into a single section.          guidelines     root    sections    sections items    guidelinesType: array of string Documentation or validation guidelines for the section.  Each item of this array must be:   root    sections    sections items    guidelines    guidelines itemsType: string           contents     root    sections    sections items    contentsType: array Contents to be displayed on the section.  Each item of this array must be:   root    sections    sections items    contents    section_contentsType: object Single content block of the module.      content_type Required     root    sections    sections items    contents    contents items    content_typeType: enum (of string) Default: \"metadata_text\"  Must be one of: \"metadata_text\"\"dynamic\"\"metric\"\"test\"   Examples: \"metadata_text\"\n \"test\"\n          content_id     root    sections    sections items    contents    contents items    content_idType: string ID of the content to be displayed for the given content type (text, metric, testm, etc.).   Examples: \"sample_text\"\n \"section_intro\"\n          options     root    sections    sections items    contents    contents items    optionsType: object Options for the content block.   Examples: {\n    \"default_text\": \"This is a sample text block.\"\n}\n {\n    \"metric_id\": \"metric_1\",\n    \"title\": \"Custom Title for Metric 1\"\n}\n {\n    \"test_id\": \"adf_test\"\n}\n      default_text     root    sections    sections items    contents    contents items    options    default_textType: string Default text for the content block. Only applicable for metadata_text content blocks.          title     root    sections    sections items    contents    contents items    options    titleType: string Title of the content block. Only applicable for metric and test content blocks.                       Generated using json-schema-for-humans on 2023-06-08 at 14:44:40 -0700"
  },
  {
    "objectID": "guide/edit-templates.html#steps",
    "href": "guide/edit-templates.html#steps",
    "title": "Edit templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Templates.\nSelect one of the tabs for the type of template you want to edit:\n\nDocumentation Templates\nValidation Report Templates\n\nLocate the template to edit and, at the bottom of the template card, click Edit Template.\nIn the YAML editor that opens, make your changes.\n\nUse See changes to view a side-by-side comparison of your changes with the latest version of the template.\nUse Reset changes to delete your changes and return to the latest version of the template.\n\nClick Prepare new version to save your changes.\n\nAdd a description in Version notes to track the changes that were made once the version is saved.\n\n\nAfter you have saved a new version, it becomes available for use with model documentation or validation reports."
  },
  {
    "objectID": "guide/edit-templates.html#troubleshooting",
    "href": "guide/edit-templates.html#troubleshooting",
    "title": "Edit templates",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nThe documentation template editor validates the YAML changes you make and flags any errors that it finds. If you make a change that the editor cannot parse correctly, the editor will not let you save the changes until you correct the YAML.\nCommon issues with YAML include incorrect indenting, imbalanced quotes, or missing colons between keys and values. If you run into issues with incorrect YAML, check the error message provided by the template editor, as it might provide a line and column number where the error occurs."
  },
  {
    "objectID": "guide/edit-templates.html#whats-next",
    "href": "guide/edit-templates.html#whats-next",
    "title": "Edit templates",
    "section": "What’s Next",
    "text": "What’s Next\nYou can see which version of a template is used in the Project Overview page of each documentation project."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html",
    "href": "guide/try-developer-framework-with-colab.html",
    "title": "Try the Developer Framework with Google Colaboratory",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Google Colaboratory."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#prerequisites",
    "href": "guide/try-developer-framework-with-colab.html#prerequisites",
    "title": "Try the Developer Framework with Google Colaboratory",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to Google Colaboratory (Colab).\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#steps",
    "href": "guide/try-developer-framework-with-colab.html#steps",
    "title": "Try the Developer Framework with Google Colaboratory",
    "section": "Steps",
    "text": "Steps\n\n\n\n\n\n\n\n\nAbout our Jupyter notebooks\n\n\n\nNotebooks from ValidMind are safe to run — If you get a warning that this notebook was not authored by Google, we welcome you to inspect the notebook source.  Runtime errors — We recommend that you not use the Run all option. Run each cell individually to see what is happening in the notebook. If you do see errors, re-run the notebook cells.\n\n\n\nOpen the Quickstart notebook in Google Colaboratory: \nClick File &gt; Save a copy in Drive to make a copy of the Quickstart notebook so that you can modify it later.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#whats-next",
    "href": "guide/try-developer-framework-with-colab.html#whats-next",
    "title": "Try the Developer Framework with Google Colaboratory",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore ValidMind in the Quickstart to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/developer-framework.html",
    "href": "guide/developer-framework.html",
    "title": "Developers",
    "section": "",
    "text": "Geared towards model developers, this section includes information for:"
  },
  {
    "objectID": "guide/developer-framework.html#related-topics",
    "href": "guide/developer-framework.html#related-topics",
    "title": "Developers",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developer tasks related to documentation projects and collaborating with model validators and model owners, refer to our Guides."
  },
  {
    "objectID": "guide/explore-validmind.html",
    "href": "guide/explore-validmind.html",
    "title": "Explore ValidMind",
    "section": "",
    "text": "Our sample notebook shows you how to initialize the ValidMind Developer Framework and run functions from a sample dataset for a customer churn model that we trained for this demo.\n\nIn a web browser, go to https://jupyterhub.validmind.ai.\nClick Sign in with Auth0, enter your email address and password, and click Continue.\nIn the sidebar, double click the Quickstart_Customer Churn_full_suite.ipynb notebook.\nAfter the notebook opens, run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the Developer Framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the Platform UI."
  },
  {
    "objectID": "guide/explore-validmind.html#access-the-sample-jupyter-notebook",
    "href": "guide/explore-validmind.html#access-the-sample-jupyter-notebook",
    "title": "Explore ValidMind",
    "section": "",
    "text": "Our sample notebook shows you how to initialize the ValidMind Developer Framework and run functions from a sample dataset for a customer churn model that we trained for this demo.\n\nIn a web browser, go to https://jupyterhub.validmind.ai.\nClick Sign in with Auth0, enter your email address and password, and click Continue.\nIn the sidebar, double click the Quickstart_Customer Churn_full_suite.ipynb notebook.\nAfter the notebook opens, run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the Developer Framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the Platform UI."
  },
  {
    "objectID": "guide/explore-validmind.html#explore-the-validmind-platform",
    "href": "guide/explore-validmind.html#explore-the-validmind-platform",
    "title": "Explore ValidMind",
    "section": "Explore the ValidMind Platform",
    "text": "Explore the ValidMind Platform\nNext, let’s take a look at how the Developer Framework works hand-in-hand with the ValidMind Platform and how documentation and test results get uploaded.\nThe ValidMind Platform is the central place to:\n\nView results and documentation uploaded via the Developer Framework\nCollaborate with other model developers, model reviewers and validators, and other users involved in the documentation and validation workflow\n\n\n\n\n\n\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the [Quickstart] Customer Churn Model - Initial Validation and select it.\nOn the model details page that open, you can find important information about the model, such as:\n\nThe ID of the model and its specific use case\nThe owners, developers, validators, and business unit associated with the model\nThe risk tier and current version\nAnd more\n\nScroll down to Documentation Project History and select the model.\nOn the project overview page that opens, you can see what is included, such as model, project findings, recent activity, and project stakeholders, and more. In the left sidebar, you can find links to the documentation, project findings, validation report, audit trail, and client integration.\nFor this Quickstart, we will focus on the Documentation section to show you how content from the Developer Framework gets uploaded.\nNote that the model status is In Documentation. This is the status that a model starts in as part of a documentation project. You can click See workflow to look at what the full workflow is, from documentation, to validation, to review, and finally approval.\nFrom the left sidebar, select Documentation &gt; 2. Data preparation &gt; 2.1. Data description.\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically by the Developer Framework whenever a test result is above or below a certain threshold.\nFrom the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the Developer Framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nThe Documentation Guidelines in the ValidMind Insights right sidebar can tell you more about what these sections mean and help you with the task of documenting the model.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the Developer Framework, but they get added by the model developer in the Platform UI."
  },
  {
    "objectID": "guide/explore-validmind.html#create-a-new-documentation-project",
    "href": "guide/explore-validmind.html#create-a-new-documentation-project",
    "title": "Explore ValidMind",
    "section": "Create a new documentation project",
    "text": "Create a new documentation project\nNext, let’s learn how to create your own documentation project. You can use this project to upload tests and documentation and then add that to the Quickstart notebook you looked at earlier.\n\nNavigate to the landing page by clicking on the ValidMind logo or if you have to, Log in to the ValidMind UI.\nFrom the left sidebar, select Documentation Projects and on the page that opens, click the Create new Project button at top right of the screen.\nSelect the right options in the form:\n\nModel: [Quickstart] Customer Churn Model\nType: Initial validation (selected automatically) \nProject name: Enter your preferred name\n\nClick Create Project.\nValidMind will create an empty documentation project associated with the customer churn model.\nYou can now access this project from the UI on the Documentation Projects page or by navigating to the relevant model - [Quickstart] Customer Churn Model - in the Model Inventory page.\nFrom the left sidebar, select Client Integration.\nThe page that opens provides you with the credentials for the newly created project to use with the ValidMind Developer Framework.\nLocate the project identifier, API key, and secret:\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\n\n\n\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nTry this: Use the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/explore-validmind.html#modify-the-quickstart-notebook-to-upload-to-your-own-project",
    "href": "guide/explore-validmind.html#modify-the-quickstart-notebook-to-upload-to-your-own-project",
    "title": "Explore ValidMind",
    "section": "Modify the Quickstart notebook to upload to your own project",
    "text": "Modify the Quickstart notebook to upload to your own project\nAfter you have completed the above steps, you are ready to use your documentation project with your copy of the Quickstart notebook.\n\nReopen the Quickstart notebook you accessed earlier.\nIn the Quickstart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you created your new documentation project:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework."
  },
  {
    "objectID": "guide/explore-validmind.html#whats-next",
    "href": "guide/explore-validmind.html#whats-next",
    "title": "Explore ValidMind",
    "section": "What’s next",
    "text": "What’s next\nReady to learn more about how you can use ValidMind? Check out Next steps."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\nEmail support@validmind.com\nEmail support@validmind.com"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s Next",
    "text": "What’s Next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html",
    "href": "guide/review-data-streams-and-audit-trails.html",
    "title": "Review Audit Trail",
    "section": "",
    "text": "Learn how to access and use the audit trail functionality in the ValidMind Platform. This topic matters for for model developers, model validators, and auditors who are looking to track or audit all the information events associated with a specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "href": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "title": "Review Audit Trail",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#steps",
    "href": "guide/review-data-streams-and-audit-trails.html#steps",
    "title": "Review Audit Trail",
    "section": "Steps",
    "text": "Steps\n\nIn the ValidMind platform, navigate to the relevant model documentation project.\nFrom the Overview page, select Audit Trail on the left.\n\nThe table in this page shows a record of all activities generated from the Developer Framework and actions performed by users in the organization related to this specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "href": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "title": "Review Audit Trail",
    "section": "What’s Next",
    "text": "What’s Next\n\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "href": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documetnation"
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nView validation guidelines"
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "href": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/license-agreement.html",
    "href": "guide/license-agreement.html",
    "title": "License agreement",
    "section": "",
    "text": "SOFTWARE LICENSE AGREEMENT\nIMPORTANT - READ CAREFULLY:\nThis software and associated media, printed materials, and “online” or electronic documentation files (the “Software”), is theproprietary information of ValidMind Inc. and its licensors (collectively, “Licensor”), and is protected under copyright and other intellectual property laws.\nNo part of this Software may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means, including, but not limited to, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of ValidMind Inc. or the respective copyright owner.\nBy installing, copying, or otherwise using the Software, the undersigned (“you”) agrees to be bound by the terms of this Software License Agreement (this “Agreement”). If you do not agree to the terms of this Agreement, do not install or use the Software.\nLICENSE GRANT. Subject to the terms and conditions of this Agreement, Licensor grants you a limited, personal, non-exclusive, non-transferable license to use the Software solely for the duration of the 4-week testing phase (the “Testing Period”) of the Software - starting on May 15th, 2023. You may install and use the Software on a single computer or device. You further agree to use the Software solely for internal testing purposes.\nOWNERSHIP. The Software is owned by Licensor and is protected by copyright laws and international copyright treaties, as well as other intellectual property laws and treaties. Licensor retains all right, title, and interest in and to the Software, including all intellectual property rights.\nRESTRICTIONS. You may not modify, adapt, translate, reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code of the Software including (without limitation) for the purpose of obtaining unauthorized access to the Software. You may not distribute, sublicense, rent, lease, or lend the Software to any third party.\nSUPPORT. Licensor may, at its discretion, provide technical support for the Software. Technical support is provided on a best-effort basis and is subject to Licensor’s support policies.\nCONFIDENTIALITY. You agree to safeguard the Software and its related materials with that degree of normal due care commensurate with reasonable standards of industrial security for the protection of trade secrets and proprietary information so that no unauthorized use is made of them and no disclosure of any part of their contents is made to anyone other than your employees, agents or consultants whose duties reasonably require such disclosure, or as necessary in the ordinary course of business. You shall make all such persons fully aware of their responsibility to fulfill your obligations under this Agreement and agree to be responsible for any breach of this Agreement by such persons. You agree to promptly notify Licensor if you obtain information as to any unauthorized possession, use or disclosure of the Software by any person or entity, and further agrees to cooperate with Licensor in protecting Licensor’s proprietary rights.\nTERMINATION. This Agreement will terminate automatically after the Testing Period, or if you fail to comply with any of the terms and conditions of this Agreement. Upon termination, you must immediately cease all use of the Software and destroy all copies of the Software in your possession.\nDISCLAIMER OF WARRANTY. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.\nLIMITATION OF LIABILITY. IN NO EVENT SHALL LICENSOR BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR IN CONNECTION WITH THE USE OR INABILITY TO USE THE SOFTWARE, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. IN NO EVENT SHALL LICENSOR’S LIABILITY EXCEED THE AMOUNT PAID BY YOU FOR THE SOFTWARE.\nINDEMNIFICATION. You agree to indemnify and hold Licensor harmless from and against all loss, cost, expense or liability (including reasonable attorney’s fees) (i) arising out of a claim by a third party against Licensor based upon your use of the Software, or (ii) related to, or associated with your customizations, updates and/or corrections to the Software. Licensor shall have no liability to you for any damage sustained by you as a result of your use of the Software, whether such damages would arise as a result of breach of contract, tort or otherwise.\nGOVERNING LAW. This Agreement shall be governed by and construed in accordance with the laws of the State of California and the federal laws of the United States of America, without giving effect to any principles of conflicts of law.\nENTIRE AGREEMENT. This Agreement constitutes the entire agreement between you and Licensor with respect to the Software and supersedes all prior or contemporaneous communications and proposals, whether oral or written, between you and Licensor.\nBy installing or using the Software, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.\nCopyright © 2023 ValidMind Inc. All rights reserved."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/tutorials.html",
    "href": "guide/tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our tutorials provide a more targeted learning experience and cover specific scenarios or use cases."
  },
  {
    "objectID": "guide/tutorials.html#related-topics",
    "href": "guide/tutorials.html#related-topics",
    "title": "Tutorials",
    "section": "Related topics",
    "text": "Related topics\nBesides our tutorials, we also offer a Quickstart that walks you through the full experience from the Developer Framework to the Platform UI."
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "As of the current release (v1.13.9), the Developer Framework supports the following model types:\nThe following table presents an overview of libraries supported by each test plan, as well as the tests which comprise each test plan as of the current Developer Framework release."
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\n\nCheck out our Developer Framework documentation for more details on how to use our documentation and testing functions with supported models."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "href": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test plan execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test plans\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test plans and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\n\nLocating the project identifier, API key and secret:\nOn the Client Integration page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/create-documentation-project.html#related-topics",
    "href": "guide/create-documentation-project.html#related-topics",
    "title": "Create documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nInstall and initialize the Developer Framework\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enbale their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier\n\n\n\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick Client integration and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"&lt;project-identifier&gt;\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s Next",
    "text": "What’s Next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "Don’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "",
    "text": "Don’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#whats-next",
    "href": "guide/before-you-begin.html#whats-next",
    "title": "Before you begin",
    "section": "What’s Next",
    "text": "What’s Next\nContinue with Explore ValidMind in the Quickstart."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstart — 20 mins",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework in Jupyter Hub and to explore the ValidMind Platform UI online.\nThis Quickstart takes about 20 minutes of your time."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstart — 20 mins",
    "section": "Steps",
    "text": "Steps\n\nBefore you begin\nCheck the prerequisites for the Developer Framework and ValidMind Platform UI.\nExplore ValidMind\nTry our introductory Jupyter notebook to see the Developer Framework in action and explore our Platform UI to work with a documentation project and see the results of tests you run.\nNext steps\nTry some more advanced sample notebooks or set up ValidMind for production with your own use cases."
  },
  {
    "objectID": "guide/quickstart.html#related-topics",
    "href": "guide/quickstart.html#related-topics",
    "title": "Quickstart — 20 mins",
    "section": "Related topics",
    "text": "Related topics\nAs an alternative to exploring the Developer Framework in Jupyter Hub, you can also try the Developer Framework with:\n\nDocker Desktop\nGoogle Colaboratory"
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases? Our documentation includes how-to instructions for the following user roles:"
  },
  {
    "objectID": "guide/next-steps.html#have-more-questions",
    "href": "guide/next-steps.html#have-more-questions",
    "title": "Next steps",
    "section": "Have more questions?",
    "text": "Have more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/next-steps.html#need-help",
    "href": "guide/next-steps.html#need-help",
    "title": "Next steps",
    "section": "Need help?",
    "text": "Need help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/next-steps.html#related-topics",
    "href": "guide/next-steps.html#related-topics",
    "title": "Next steps",
    "section": "Related topics",
    "text": "Related topics\n\nAdditional Jupyter notebooks\nIntroduction to the ValidMind Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "href": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The guide to elevating your MRM workflow",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Documentation\n                            The guide to elevating your MRM workflow\n                            Need help? Find all the information you need to use our platform for model risk management (MRM).\n                            \n                            \n                                \n                                    \n                                        \n                                            \n                                        \n                                        \n                                                                \n                                        \n                                    \n                                \n                            \n\n                        \n                    \n                \n                \n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating key aspects of the model risk management process, ValidMind is an MRM solution designed for the unique needs documentation and validation needs of model developers and validators.\n                    Model Documentation Automation\n                    MRM Lifecycle and Workflow\n                    Communication & TrackingInstructional GuidesGet Started\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Automate your model documentation and testing tasks with our Developer Framework.Collaboration for Model Developers\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and documentation to ensure they comply with organizational & regulatory requirements.Collaboration for Model Validators\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "",
    "text": "This interactive notebook will guide you through documenting a model using the ValidMind Developer framework. We will use sample datasets provided by the library and train a simple classification model.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#before-starting-important",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#before-starting-important",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Before Starting (Important)",
    "text": "Before Starting (Important)\nClick File &gt; Save a copy in Drive &gt; to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment."
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#install-validmind-developer-framework",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#install-validmind-developer-framework",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Install ValidMind Developer Framework",
    "text": "Install ValidMind Developer Framework\n\n!pip install validmind\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue with the next cell.\n##Initializing the Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#initializing-the-validmind-client-library",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#initializing-the-validmind-client-library",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Initializing the ValidMind Client Library",
    "text": "Initializing the ValidMind Client Library\nLog in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n\nCreating a new Documentation Project\n(Note: if a documentation project has already been created, you can skip this section and head directly “Finding Project API key and secret”)\nClicking on “Create a new project” allows to you to register a new documentation project for our demo model.\nSelect “Customer Churn model” from the Model drop-down, and “Initial Validation” as Type. Finally, click on “Create Project”.\n\n\nFinding the project API key and secret\nIn the “Client Integration” page of the newly created project, you will find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\napi_host: Location of the ValidMind API.\napi_key: Account API key.\napi_secret: Account Secret key.\nproject: The project identifier. The project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nThe code snippet can be copied and pasted directly in the cell below to initialize the ValidMind Developer Framework when run:\n\n## Replace the code below with the code snippet from your project ## \n\n\n\n\n\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"d84fda1911a2cd3711b32d296e33f848\",\n  api_secret = \"e8f5ae23f6afc61368cdd64b3ef546af0b31cd7b78d7eedcd8a312f9c44b9dc2\",\n  project = \"clhowg73e001s1pk10uouvsde\"\n)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#load-the-demo-dataset",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#load-the-demo-dataset",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with: \n#from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#run-the-full-data-and-model-validation-test-suite",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#run-the-full-data-and-model-validation-test-suite",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Run the Full Data and Model Validation Test Suite",
    "text": "Run the Full Data and Model Validation Test Suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the Raw Dataset\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nWe can now initialize the training and test datasets into dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nWe also initialize a model object using vm.init_model():\n\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the Full Suite\nWe are now ready to run the test suite for binary classifier with tabular datasets. This function will run test plans on the dataset and model objects, and will document the results in the ValidMind UI.\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html",
    "href": "notebooks/Introduction_Customer_Churn.html",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "",
    "text": "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#validmind-python-library-introduction",
    "href": "notebooks/Introduction_Customer_Churn.html#validmind-python-library-introduction",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "",
    "text": "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#before-starting-important",
    "href": "notebooks/Introduction_Customer_Churn.html#before-starting-important",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Before Starting (Important)",
    "text": "Before Starting (Important)\nClick File &gt; Save a copy in Drive &gt; to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n##Install ValidMind Developer Framework\n\n!pip install validmind\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting validmind\n  Downloading validmind-1.11.7-py3-none-any.whl (1.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 11.9 MB/s eta 0:00:00\nCollecting arch&lt;6.0.0,&gt;=5.4.0 (from validmind)\n  Downloading arch-5.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (918 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 918.2/918.2 kB 67.3 MB/s eta 0:00:00\nCollecting catboost&lt;2.0,&gt;=1.2 (from validmind)\n  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 MB 8.2 MB/s eta 0:00:00\nRequirement already satisfied: click&lt;9.0.0,&gt;=8.0.4 in /usr/local/lib/python3.10/dist-packages (from validmind) (8.1.3)\nCollecting dython&lt;0.8.0,&gt;=0.7.1 (from validmind)\n  Downloading dython-0.7.4-py3-none-any.whl (24 kB)\nRequirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (7.34.0)\nRequirement already satisfied: markdown&lt;4.0.0,&gt;=3.4.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (3.4.3)\nCollecting myst-parser&lt;2.0.0,&gt;=1.0.0 (from validmind)\n  Downloading myst_parser-1.0.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.3/77.3 kB 8.5 MB/s eta 0:00:00\nCollecting numpy==1.22.3 (from validmind)\n  Downloading numpy-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 83.5 MB/s eta 0:00:00\nRequirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.5.3)\nCollecting pandas-profiling&lt;4.0.0,&gt;=3.6.6 (from validmind)\n  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.4/324.4 kB 24.5 MB/s eta 0:00:00\nRequirement already satisfied: pydantic&lt;2.0.0,&gt;=1.9.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.10.7)\nCollecting pypmml&lt;0.10.0,&gt;=0.9.17 (from validmind)\n  Downloading pypmml-0.9.17.tar.gz (14.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/14.2 MB 93.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nCollecting python-dotenv&lt;0.21.0,&gt;=0.20.0 (from validmind)\n  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.27.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (2.27.1)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.0.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.2.2)\nCollecting seaborn&lt;0.12.0,&gt;=0.11.2 (from validmind)\n  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 32.6 MB/s eta 0:00:00\nCollecting shap&lt;0.42.0,&gt;=0.41.0 (from validmind)\n  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 572.6/572.6 kB 53.1 MB/s eta 0:00:00\nCollecting sphinx&lt;7.0.0,&gt;=6.1.3 (from validmind)\n  Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 84.8 MB/s eta 0:00:00\nCollecting sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5 (from validmind)\n  Downloading sphinx_markdown_builder-0.5.5-py2.py3-none-any.whl (15 kB)\nCollecting sphinx-rtd-theme&lt;2.0.0,&gt;=1.2.0 (from validmind)\n  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 85.9 MB/s eta 0:00:00\nRequirement already satisfied: statsmodels&lt;0.14.0,&gt;=0.13.5 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.13.5)\nRequirement already satisfied: tabulate&lt;0.9.0,&gt;=0.8.9 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.8.10)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.64.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (4.65.0)\nRequirement already satisfied: xgboost&lt;2.0.0,&gt;=1.5.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.7.5)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (67.7.2)\nCollecting jedi&gt;=0.16 (from ipython==7.34.0-&gt;validmind)\n  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 87.6 MB/s eta 0:00:00\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (0.7.5)\nRequirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (3.0.38)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (2.14.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0-&gt;validmind) (4.8.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3-&gt;validmind) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3-&gt;validmind) (2022.7.1)\nRequirement already satisfied: scipy&gt;=1.3 in /usr/local/lib/python3.10/dist-packages (from arch&lt;6.0.0,&gt;=5.4.0-&gt;validmind) (1.10.1)\nCollecting property-cached&gt;=1.6.4 (from arch&lt;6.0.0,&gt;=5.4.0-&gt;validmind)\n  Downloading property_cached-1.6.4-py2.py3-none-any.whl (7.8 kB)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (0.20.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (3.7.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (5.13.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.16.0)\nINFO: pip is looking at multiple versions of dython to determine which version is compatible with other requirements. This could take a while.\nCollecting dython&lt;0.8.0,&gt;=0.7.1 (from validmind)\n  Downloading dython-0.7.3-py3-none-any.whl (23 kB)\n  Downloading dython-0.7.2-py3-none-any.whl (22 kB)\nCollecting scikit-plot&gt;=0.3.7 (from dython&lt;0.8.0,&gt;=0.7.1-&gt;validmind)\n  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\nRequirement already satisfied: psutil&gt;=5.9.1 in /usr/local/lib/python3.10/dist-packages (from dython&lt;0.8.0,&gt;=0.7.1-&gt;validmind) (5.9.5)\nRequirement already satisfied: docutils&lt;0.20,&gt;=0.15 in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (0.16)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (3.1.2)\nRequirement already satisfied: markdown-it-py&lt;3.0.0,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (2.2.0)\nCollecting mdit-py-plugins~=0.3.4 (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind)\n  Downloading mdit_py_plugins-0.3.5-py3-none-any.whl (52 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.1/52.1 kB 6.6 MB/s eta 0:00:00\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (6.0)\nCollecting ydata-profiling (from pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading ydata_profiling-4.1.2-py2.py3-none-any.whl (345 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.9/345.9 kB 39.2 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;2.0.0,&gt;=1.9.1-&gt;validmind) (4.5.0)\nRequirement already satisfied: py4j&gt;=0.10.7 in /usr/local/lib/python3.10/dist-packages (from pypmml&lt;0.10.0,&gt;=0.9.17-&gt;validmind) (0.10.9.7)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (1.26.15)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (2022.12.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (2.0.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.27.1-&gt;validmind) (3.4)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.2-&gt;validmind) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn&lt;2.0.0,&gt;=1.0.2-&gt;validmind) (3.1.0)\nRequirement already satisfied: packaging&gt;20.9 in /usr/local/lib/python3.10/dist-packages (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (23.1)\nCollecting slicer==0.0.7 (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind)\n  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (0.56.4)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (2.2.1)\nRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.4)\nRequirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.2)\nRequirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.1)\nRequirement already satisfied: sphinxcontrib-htmlhelp&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (2.0.1)\nRequirement already satisfied: sphinxcontrib-serializinghtml&gt;=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.1.5)\nRequirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.0.3)\nCollecting docutils&lt;0.20,&gt;=0.15 (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind)\n  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 570.5/570.5 kB 43.3 MB/s eta 0:00:00\nRequirement already satisfied: snowballstemmer&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (2.2.0)\nRequirement already satisfied: babel&gt;=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (2.12.1)\nRequirement already satisfied: alabaster&lt;0.8,&gt;=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (0.7.13)\nRequirement already satisfied: imagesize&gt;=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx&lt;7.0.0,&gt;=6.1.3-&gt;validmind) (1.4.1)\nCollecting html2text (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\nCollecting pydash (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading pydash-7.0.3-py3-none-any.whl (109 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109.5/109.5 kB 13.0 MB/s eta 0:00:00\nCollecting unify (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading unify-0.5.tar.gz (4.4 kB)\n  Preparing metadata (setup.py) ... done\nCollecting yapf (from sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading yapf-0.33.0-py2.py3-none-any.whl (200 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.9/200.9 kB 1.4 MB/s eta 0:00:00\nCollecting docutils&lt;0.20,&gt;=0.15 (from myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind)\n  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 570.0/570.0 kB 33.8 MB/s eta 0:00:00\nCollecting sphinxcontrib-jquery!=3.0.0,&gt;=2.0.0 (from sphinx-rtd-theme&lt;2.0.0,&gt;=1.2.0-&gt;validmind)\n  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 15.4 MB/s eta 0:00:00\nRequirement already satisfied: patsy&gt;=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels&lt;0.14.0,&gt;=0.13.5-&gt;validmind) (0.5.3)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;ipython==7.34.0-&gt;validmind) (0.8.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (2.1.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&lt;3.0.0,&gt;=1.0.0-&gt;myst-parser&lt;2.0.0,&gt;=1.0.0-&gt;validmind) (0.1.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (4.39.3)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (8.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (3.0.9)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;ipython==7.34.0-&gt;validmind) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython==7.34.0-&gt;validmind) (0.2.6)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba-&gt;shap&lt;0.42.0,&gt;=0.41.0-&gt;validmind) (0.39.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly-&gt;catboost&lt;2.0,&gt;=1.2-&gt;validmind) (8.2.2)\nCollecting untokenize (from unify-&gt;sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind)\n  Downloading untokenize-0.1.1.tar.gz (3.1 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: tomli&gt;=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf-&gt;sphinx-markdown-builder&lt;0.6.0,&gt;=0.5.5-&gt;validmind) (2.0.1)\nCollecting scipy&gt;=1.3 (from arch&lt;6.0.0,&gt;=5.4.0-&gt;validmind)\n  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.7/33.7 MB 43.5 MB/s eta 0:00:00\nCollecting matplotlib (from catboost&lt;2.0,&gt;=1.2-&gt;validmind)\n  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 93.9 MB/s eta 0:00:00\nCollecting visions[type_image_path]==0.7.5 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.7/102.7 kB 13.8 MB/s eta 0:00:00\nCollecting htmlmin==0.1.12 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... done\nCollecting phik&lt;0.13,&gt;=0.11.1 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading phik-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 679.5/679.5 kB 56.2 MB/s eta 0:00:00\nCollecting tqdm&lt;5.0.0,&gt;=4.64.0 (from validmind)\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 9.7 MB/s eta 0:00:00\nCollecting multimethod&lt;1.10,&gt;=1.4 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading multimethod-1.9.1-py3-none-any.whl (10 kB)\nCollecting typeguard&lt;2.14,&gt;=2.13.2 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nCollecting imagehash==4.3.1 (from ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.5/296.5 kB 35.5 MB/s eta 0:00:00\nRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind) (1.4.1)\nRequirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind) (23.1.0)\nRequirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind) (3.1)\nCollecting tangled-up-in-unicode&gt;=0.0.4 (from visions[type_image_path]==0.7.5-&gt;ydata-profiling-&gt;pandas-profiling&lt;4.0.0,&gt;=3.6.6-&gt;validmind)\n  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 102.3 MB/s eta 0:00:00\nBuilding wheels for collected packages: pypmml, unify, htmlmin, untokenize\n  Building wheel for pypmml (setup.py) ... done\n  Created wheel for pypmml: filename=pypmml-0.9.17-py3-none-any.whl size=14215036 sha256=b063498209ef70ccfff6e68ff2443587e5783d68ee64efa51b180fd989759977\n  Stored in directory: /root/.cache/pip/wheels/8c/74/f1/946a04acaa6de2e9df0f02739511aba5a7aac52383c52ac900\n  Building wheel for unify (setup.py) ... done\n  Created wheel for unify: filename=unify-0.5-py3-none-any.whl size=5224 sha256=e90e221dffdeb63d42f729d865afa7754af0586422ac5ec5e53e897e22e9f0cf\n  Stored in directory: /root/.cache/pip/wheels/f1/d3/32/7f86dc94d89d1775b69018f1cb94e1ff77691cd676b1d6e99a\n  Building wheel for htmlmin (setup.py) ... done\n  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27081 sha256=342eecb3263f09426ad33a7f390ace1b4947999829e532973402b9de2cac8507\n  Stored in directory: /root/.cache/pip/wheels/dd/91/29/a79cecb328d01739e64017b6fb9a1ab9d8cb1853098ec5966d\n  Building wheel for untokenize (setup.py) ... done\n  Created wheel for untokenize: filename=untokenize-0.1.1-py3-none-any.whl size=2874 sha256=7726ac802ce0b6b6f57dd42d85e9a125c96ee99d0b8e780ac96de3bc00065728\n  Stored in directory: /root/.cache/pip/wheels/dd/b6/d4/187059c19a28026b81e54afd260a63aab2e7ccddf2e05977eb\nSuccessfully built pypmml unify htmlmin untokenize\nInstalling collected packages: untokenize, htmlmin, yapf, unify, typeguard, tqdm, tangled-up-in-unicode, slicer, python-dotenv, pypmml, pydash, property-cached, numpy, multimethod, jedi, html2text, docutils, sphinx, scipy, mdit-py-plugins, visions, sphinxcontrib-jquery, sphinx-markdown-builder, myst-parser, matplotlib, imagehash, sphinx-rtd-theme, shap, seaborn, scikit-plot, phik, catboost, arch, ydata-profiling, dython, pandas-profiling, validmind\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.65.0\n    Uninstalling tqdm-4.65.0:\n      Successfully uninstalled tqdm-4.65.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.22.4\n    Uninstalling numpy-1.22.4:\n      Successfully uninstalled numpy-1.22.4\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.16\n    Uninstalling docutils-0.16:\n      Successfully uninstalled docutils-0.16\n  Attempting uninstall: sphinx\n    Found existing installation: Sphinx 3.5.4\n    Uninstalling Sphinx-3.5.4:\n      Successfully uninstalled Sphinx-3.5.4\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.10.1\n    Uninstalling scipy-1.10.1:\n      Successfully uninstalled scipy-1.10.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.1\n    Uninstalling matplotlib-3.7.1:\n      Successfully uninstalled matplotlib-3.7.1\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\nSuccessfully installed arch-5.5.0 catboost-1.2 docutils-0.18.1 dython-0.7.2 html2text-2020.1.16 htmlmin-0.1.12 imagehash-4.3.1 jedi-0.18.2 matplotlib-3.6.3 mdit-py-plugins-0.3.5 multimethod-1.9.1 myst-parser-1.0.0 numpy-1.22.3 pandas-profiling-3.6.6 phik-0.12.3 property-cached-1.6.4 pydash-7.0.3 pypmml-0.9.17 python-dotenv-0.20.0 scikit-plot-0.3.7 scipy-1.9.3 seaborn-0.11.2 shap-0.41.0 slicer-0.0.7 sphinx-6.2.1 sphinx-markdown-builder-0.5.5 sphinx-rtd-theme-1.2.0 sphinxcontrib-jquery-4.1 tangled-up-in-unicode-0.2.0 tqdm-4.64.1 typeguard-2.13.3 unify-0.5 untokenize-0.1.1 validmind-1.11.7 visions-0.7.5 yapf-0.33.0 ydata-profiling-4.1.2\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue on to the next cell.\n\nInitializing Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#create-a-new-document-project-and-initialize-the-validmind-client-library",
    "href": "notebooks/Introduction_Customer_Churn.html#create-a-new-document-project-and-initialize-the-validmind-client-library",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Create a new Document Project and Initialize the ValidMind Client Library",
    "text": "Create a new Document Project and Initialize the ValidMind Client Library\nLog in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n\nCreating a new Documentation Project\n(Note: if a documentation project has already been created, you can skip this section and head directly “Finding Project API key and secret”)\nClicking on “Create a new project” allows to you to register a new documentation project for our demo model.\nSelect “Customer Churn model” from the Model drop-down, and “Initial Validation” as Type. Finally, click on “Create Project”.\n\n\nFinding the project API key and secret\nIn the “Client Integration” page of the newly created project, you will find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\napi_host: Location of the ValidMind API.\napi_key: Account API key.\napi_secret: Account Secret key.\nproject: The project identifier. The project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nThe code snippet can be copied and pasted directly in the cell below to initialize the ValidMind Developer Framework when run:\n\n## Replace the code below with the code snippet from your project ## \n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"3b481c4935130773c825014129d62c02\",\n  api_secret = \"61ba7b6d80e1e7b162646d3a29374807dab6b3613757942f12ca11eab7530dcc\",\n  project = \"clhny5uil00071ojtaomas47g\"\n)\n  \n\nThe Developer Framework is now initialized and connected to the correct project on the platform."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#training-an-example-model",
    "href": "notebooks/Introduction_Customer_Churn.html#training-an-example-model",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Training an Example Model",
    "text": "Training an Example Model\nWe will now train an example model that will be used to demonstrate the ValidMind Developer Framework functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\nLoading demo dataset\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nPreparing the training dataset\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\nDropping irrelevant variables\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\nEncoding categorical variables\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\nDataset preparation\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\nModel training\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nNow that we are satisfied with our model, we can begin using the ValidMind Library to generate test and document it.\n\n\nViewing all test plans available in the developer framework\nWe can find all the test plans and tests available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nList all available tests: vm.test_plans.list_tests()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_data_quality\")\n\nHere is an example:\n\nvm.test_plans.list_plans()\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\n\nRunning a data quality test plan\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\n\n\nInitialize and run the TabularDataset test plan\nWe can now initialize the TabularDataset test suite. The primary method of doing this is with the run_test_suite function from the vm module. This function takes in a test suite name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Data Preparation” section of the model documentation.\n\n\n\nRunning a model evaluation test plan\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\nInitialize VM model object and train/test datasets\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nWe can now run the BinaryClassifierModelValidation test plan:\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html",
    "href": "notebooks/how_to/run_a_test_plan.html",
    "title": "Running an Individual Test Plan",
    "section": "",
    "text": "This notebook shows how to run an individual test plan and pass custom config parameters for the tests.\n%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\ncannot find .env file\nimport validmind as vm\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_plan.html#load-the-demo-dataset",
    "title": "Running an Individual Test Plan",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the Raw Dataset\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test_plan.html#train-a-model-for-testing",
    "title": "Running an Individual Test Plan",
    "section": "Train a Model for Testing",
    "text": "Train a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#import-and-run-the-individual-test-plan",
    "href": "notebooks/how_to/run_a_test_plan.html#import-and-run-the-individual-test-plan",
    "title": "Running an Individual Test Plan",
    "section": "Import and Run the Individual Test Plan",
    "text": "Import and Run the Individual Test Plan\n\nInitialize ValidMind objects\nWe initize the objects required to run test plans using the ValidMind framework\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nList of available Test Plans\nThe interface to show list of test plans available in the ValidMind development framework\n\nvm.test_plans.list_plans()\n\n\n\n\n\n\nID\nName\nDescription\n\n\n\n\nbinary_classifier_metrics\nBinaryClassifierMetrics\nTest plan for sklearn classifier metrics\n\n\nbinary_classifier_validation\nBinaryClassifierPerformance\nTest plan for sklearn classifier models\n\n\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nTest plan for sklearn classifier model diagnosis tests\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\ntabular_data_quality\nTabularDataQuality\nTest plan for data quality on tabular datasets\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest plan for data quality on time series datasets\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nTest plan to perform time series univariate analysis.\n\n\ntime_series_multivariate\nTimeSeriesMultivariate\nTest plan to perform time series multivariate analysis.\n\n\ntime_series_forecast\nTimeSeriesForecast\nTest plan to perform time series forecast tests.\n\n\nregression_model_description\nRegressionModelDescription\nTest plan for performance metric of regression model of statsmodels library\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\nDetail of an individual Test Plan\nThe interface will get detail of a specific test plan\n\nvm.test_plans.describe_plan(\"binary_classifier_model_diagnosis\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nTest plan for sklearn classifier model diagnosis tests\nmodel\nOverfitDiagnosis (ThresholdTest), WeakspotsDiagnosis (ThresholdTest), RobustnessDiagnosis (ThresholdTest)\n\n\n\n\n\n\n\nDefine the required config parameters\nThe config can be apply to specific test to override the default configuration parameters.\nThe format of a config is:\nconfig = {\n    \"&lt;test1_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n     \"&lt;test2_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n}\nUsers can input the configuration to test plan using config, allowing fine-tuning the suite according to their specific data requirements.\n\nconfig={\n    \"overfit_regions\": {\n        \"cut_off_percentage\": 3,\n        \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n    },\n    \"weak_spots\":{\n        \"features_columns\": [\"Age\", \"Balance\"],\n        \"accuracy_gap_threshold\": 85,\n    },\n    \"robustness\":{\n        \"features_columns\": [ \"Balance\", \"Tenure\"],\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        \"accuracy_decay_threshold\": 4,\n    }\n}\n\n\n\nRun the test plan and display results\n\nmodel_diagnosis_test_plan = vm.run_test_plan(\"binary_classifier_model_diagnosis\", \n                                             model=vm_model,\n                                             config=config)\n\n\n\n\n\n\n\n\n\nAccessing the test plan results\nWe can now access all the results of the test plan, including subtest plans using test_plan.get_results().\n\ntest_plan.get_results(): With no arguments, this returns a list of all results\ntest_plan.get_results(test_id): If provided with a test id, this returns the all results that match the given test id\n\nBy default, get_results() returns a list, in case there are multiple tests with the same id.\n\nmodel_diagnosis_test_plan.get_results()\n\n[TestPlanTestResult(result_id=\"overfit_regions\", test_results),\n TestPlanTestResult(result_id=\"weak_spots\", test_results),\n TestPlanTestResult(result_id=\"robustness\", test_results)]\n\n\n\nmodel_robustness = model_diagnosis_test_plan.get_results(\"robustness\")[0]\nmodel_robustness.show()"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html",
    "href": "notebooks/how_to/run_a_test_suite.html",
    "title": "Running an Individual Test Suite",
    "section": "",
    "text": "This notebook shows how to run an individual test suite"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#initialize-validmind",
    "href": "notebooks/how_to/run_a_test_suite.html#initialize-validmind",
    "title": "Running an Individual Test Suite",
    "section": "Initialize ValidMind",
    "text": "Initialize ValidMind\n\n%load_ext dotenv\n%dotenv .env\n%matplotlib inline\n\nimport validmind as vm\nimport xgboost as xgb\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "title": "Running an Individual Test Suite",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import taiwan_credit like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "href": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "title": "Running an Individual Test Suite",
    "section": "List available test suites",
    "text": "List available test suites\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\nbinary_classifier_model_validation\nBinaryClassifierModelValidation\nTest suite for binary classification models.\nbinary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\ntabular_dataset_description, tabular_data_quality\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the Data Validation Test suite",
    "text": "Run the Data Validation Test suite\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the Model Validation Test suite",
    "text": "Run the Model Validation Test suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the Raw Dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\n\nTrain a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Import and Run the Individual Test Suite",
    "text": "Import and Run the Individual Test Suite\n\nInitialize ValidMind objects\nWe initize the objects required to run test plans using the ValidMind framework\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nRun the Binary Classification Test Suite\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "",
    "text": "This notebook aim to demostrate the list of interfaces available to get details of test suites, test plans and tests"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#initialize-validmind",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#initialize-validmind",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Initialize ValidMind",
    "text": "Initialize ValidMind\n\n%load_ext dotenv\n%dotenv .env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Available test suites",
    "text": "Available test suites\nThe interface will provide the list of test suites available in the ValidMind framework\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\nbinary_classifier_model_validation\nBinaryClassifierModelValidation\nTest suite for binary classification models.\nbinary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\ntabular_dataset_description, tabular_data_quality\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#test-plans",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#test-plans",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Test plans",
    "text": "Test plans\nThe list of test plans available in a given test suite\n\nvm.test_suites.describe_test_suite(\"binary_classifier_full_suite\")\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\n\n\n\n\nTest plan description and list of tests\nThe list of tests avaiable in a specific test plan\n\nvm.test_plans.describe_plan(\"tabular_dataset_description\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest plan to extract metadata and descriptive statistics from a tabular dataset\ndataset\nDatasetMetadata (None), DatasetDescription (Metric), DescriptiveStatistics (Metric), DatasetCorrelations (Metric)\n\n\n\n\n\n\n\nTest detail\n\nvm.test_plans.describe_test('DescriptiveStatistics')\n\n\n\n\n\n\nTest Type\nID\nName\nDescription\n\n\n\n\nMetric\ndescriptive_statistics\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\n\n\n\n\n\nDetails of test suites, test plans and tests\nThis interface provide comprehensive details of test suites, test plans and tests\n\nvm.test_suites.describe_test_suites_plans_tests()\n\n\n\n\n\n\nTest Suite\nTest Plan\nTest Type\nTest ID\nTest Name\nTest Description\n\n\n\n\nbinary_classifier_full_suite\ntabular_dataset_description\nNone\ndataset_metadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\nbinary_classifier_full_suite\ntabular_dataset_description\nMetric\ndataset_description\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\n\n\nbinary_classifier_full_suite\ntabular_dataset_description\nMetric\ndescriptive_statistics\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\nbinary_classifier_full_suite\ntabular_dataset_description\nMetric\ndataset_correlations\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\nclass_imbalance\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\nduplicates\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\ncardinality\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\npearson_correlation\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\nmissing\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\nskewness\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\nunique\nUniqueRows\nTest that the number of unique rows is greater than a threshold\n\n\nbinary_classifier_full_suite\ntabular_data_quality\nThresholdTest\nzeros\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\nmodel_metadata\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\ndataset_split\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\nconfusion_matrix\nConfusionMatrix\nConfusion Matrix\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\nclassifier_in_sample_performance\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\nclassifier_out_of_sample_performance\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\npfi\nPermutationFeatureImportance\nPermutation Feature Importance\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\npr_curve\nPrecisionRecallCurve\nPrecision Recall Curve\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\nroc_curve\nROCCurve\nROC Curve\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\ncsi\nCharacteristicStabilityIndex\nCharacteristic Stability Index between two datasets\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\npsi\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\n\n\nbinary_classifier_full_suite\nbinary_classifier_metrics\nMetric\nshap\nSHAPGlobalImportance\nSHAP Global Importance\n\n\nbinary_classifier_full_suite\nbinary_classifier_validation\nThresholdTest\naccuracy_score\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\nbinary_classifier_full_suite\nbinary_classifier_validation\nThresholdTest\nf1_score\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\nbinary_classifier_full_suite\nbinary_classifier_validation\nThresholdTest\nroc_auc_score\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\nbinary_classifier_full_suite\nbinary_classifier_validation\nThresholdTest\ntraining_test_degradation\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\nbinary_classifier_full_suite\nbinary_classifier_model_diagnosis\nThresholdTest\noverfit_regions\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\nbinary_classifier_full_suite\nbinary_classifier_model_diagnosis\nThresholdTest\nweak_spots\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques.\n\n\nbinary_classifier_full_suite\nbinary_classifier_model_diagnosis\nThresholdTest\nrobustness\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\nmodel_metadata\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\ndataset_split\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\nconfusion_matrix\nConfusionMatrix\nConfusion Matrix\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\nclassifier_in_sample_performance\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\nclassifier_out_of_sample_performance\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\npfi\nPermutationFeatureImportance\nPermutation Feature Importance\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\npr_curve\nPrecisionRecallCurve\nPrecision Recall Curve\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\nroc_curve\nROCCurve\nROC Curve\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\ncsi\nCharacteristicStabilityIndex\nCharacteristic Stability Index between two datasets\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\npsi\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\n\n\nbinary_classifier_model_validation\nbinary_classifier_metrics\nMetric\nshap\nSHAPGlobalImportance\nSHAP Global Importance\n\n\nbinary_classifier_model_validation\nbinary_classifier_validation\nThresholdTest\naccuracy_score\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\nbinary_classifier_model_validation\nbinary_classifier_validation\nThresholdTest\nf1_score\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\nbinary_classifier_model_validation\nbinary_classifier_validation\nThresholdTest\nroc_auc_score\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\nbinary_classifier_model_validation\nbinary_classifier_validation\nThresholdTest\ntraining_test_degradation\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\nbinary_classifier_model_validation\nbinary_classifier_model_diagnosis\nThresholdTest\noverfit_regions\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\nbinary_classifier_model_validation\nbinary_classifier_model_diagnosis\nThresholdTest\nweak_spots\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques.\n\n\nbinary_classifier_model_validation\nbinary_classifier_model_diagnosis\nThresholdTest\nrobustness\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\n\n\ntabular_dataset\ntabular_dataset_description\nNone\ndataset_metadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\ntabular_dataset\ntabular_dataset_description\nMetric\ndataset_description\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\n\n\ntabular_dataset\ntabular_dataset_description\nMetric\ndescriptive_statistics\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\ntabular_dataset\ntabular_dataset_description\nMetric\ndataset_correlations\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\nclass_imbalance\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\nduplicates\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\ncardinality\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\npearson_correlation\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\nmissing\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\nskewness\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\nunique\nUniqueRows\nTest that the number of unique rows is greater than a threshold\n\n\ntabular_dataset\ntabular_data_quality\nThresholdTest\nzeros\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\n\n\ntime_series_dataset\ntime_series_data_quality\nThresholdTest\ntime_series_outliers\nTimeSeriesOutliers\nTest that find outliers for time series data using the z-score method\n\n\ntime_series_dataset\ntime_series_data_quality\nThresholdTest\ntime_series_missing_values\nTimeSeriesMissingValues\nTest that the number of missing values is less than a threshold\n\n\ntime_series_dataset\ntime_series_data_quality\nThresholdTest\ntime_series_frequency\nTimeSeriesFrequency\nTest that detect frequencies in the data\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\ntime_series_line_plot\nTimeSeriesLinePlot\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\ntime_series_histogram\nTimeSeriesHistogram\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nacf_pacf_plot\nACFandPACFPlot\nPlots ACF and PACF for a given time series dataset.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nseasonal_decompose\nSeasonalDecompose\nCalculates seasonal_decompose metric for each of the dataset features\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nauto_seasonality\nAutoSeasonality\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nauto_stationarity\nAutoStationarity\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nrolling_stats_plot\nRollingStatsPlot\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nauto_ar\nAutoAR\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\ntime_series_dataset\ntime_series_univariate\nMetric\nauto_ma\nAutoMA\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\ntime_series_dataset\ntime_series_multivariate\nMetric\nscatter_plot\nScatterPlot\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\ntime_series_dataset\ntime_series_multivariate\nMetric\nlagged_correlation_heatmap\nLaggedCorrelationHeatmap\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\ntime_series_dataset\ntime_series_multivariate\nMetric\nengle_granger_coint\nEngleGrangerCoint\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\ntime_series_dataset\ntime_series_multivariate\nMetric\nspread_plot\nSpreadPlot\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\ntime_series_model_validation\nregression_model_description\nMetric\ndataset_split\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\ntime_series_model_validation\nregression_model_description\nMetric\nmodel_metadata\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\ntime_series_model_validation\nregression_models_evaluation\nMetric\nregression_models_coefficients\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\n\n\ntime_series_model_validation\nregression_models_evaluation\nMetric\nregression_models_performance\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\n\n\ntime_series_model_validation\nregression_models_evaluation\nMetric\nregression_models_coefficients\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\n\n\ntime_series_model_validation\nregression_models_evaluation\nMetric\nregression_models_performance\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\n\n\ntime_series_model_validation\ntime_series_forecast\nMetric\nregression_forecast_plot_levels\nRegressionModelForecastPlotLevels\nThis metric creates a plot of forecast vs observed for each model in the list."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites-1",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites-1",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Available test suites",
    "text": "Available test suites\n\nvm.test_plans.list_tests()\n\n\n\n\n\n\nTest Type\nID\nName\nDescription\n\n\n\n\nMetric\nacf_pacf_plot\nACFandPACFPlot\nPlots ACF and PACF for a given time series dataset.\n\n\nMetric\nauto_ar\nAutoAR\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\nMetric\nauto_ma\nAutoMA\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\nMetric\nauto_seasonality\nAutoSeasonality\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\nMetric\nauto_stationarity\nAutoStationarity\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\nMetric\ncsi\nCharacteristicStabilityIndex\nCharacteristic Stability Index between two datasets\n\n\nMetric\nclassifier_in_sample_performance\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\n\n\nMetric\nclassifier_out_of_sample_performance\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\n\n\nMetric\nconfusion_matrix\nConfusionMatrix\nConfusion Matrix\n\n\nMetric\ndataset_correlations\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\nMetric\ndataset_description\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\n\n\nMetric\ndataset_split\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\nMetric\ndescriptive_statistics\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\nMetric\nengle_granger_coint\nEngleGrangerCoint\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\nMetric\nlagged_correlation_heatmap\nLaggedCorrelationHeatmap\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\nMetric\nmodel_metadata\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\nMetric\npfi\nPermutationFeatureImportance\nPermutation Feature Importance\n\n\nMetric\npsi\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\n\n\nMetric\npr_curve\nPrecisionRecallCurve\nPrecision Recall Curve\n\n\nMetric\nroc_curve\nROCCurve\nROC Curve\n\n\nMetric\nregression_forecast_plot_levels\nRegressionModelForecastPlotLevels\nThis metric creates a plot of forecast vs observed for each model in the list.\n\n\nMetric\nregression_models_coefficients\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\n\n\nMetric\nregression_models_performance\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\n\n\nMetric\nrolling_stats_plot\nRollingStatsPlot\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\nMetric\nshap\nSHAPGlobalImportance\nSHAP Global Importance\n\n\nMetric\nscatter_plot\nScatterPlot\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\nMetric\nseasonal_decompose\nSeasonalDecompose\nCalculates seasonal_decompose metric for each of the dataset features\n\n\nMetric\nspread_plot\nSpreadPlot\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\nMetric\ntime_series_histogram\nTimeSeriesHistogram\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\nMetric\ntime_series_line_plot\nTimeSeriesLinePlot\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\nNone\ndataset_metadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\nThresholdTest\nclass_imbalance\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\nThresholdTest\nduplicates\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\nThresholdTest\ncardinality\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\nThresholdTest\npearson_correlation\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\nThresholdTest\naccuracy_score\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nf1_score\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nroc_auc_score\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nmissing\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\nThresholdTest\noverfit_regions\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\nThresholdTest\nrobustness\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\n\n\nThresholdTest\nskewness\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\nThresholdTest\ntime_series_frequency\nTimeSeriesFrequency\nTest that detect frequencies in the data\n\n\nThresholdTest\ntime_series_missing_values\nTimeSeriesMissingValues\nTest that the number of missing values is less than a threshold\n\n\nThresholdTest\ntime_series_outliers\nTimeSeriesOutliers\nTest that find outliers for time series data using the z-score method\n\n\nThresholdTest\nzeros\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\n\n\nThresholdTest\ntraining_test_degradation\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\nThresholdTest\nunique\nUniqueRows\nTest that the number of unique rows is greater than a threshold\n\n\nThresholdTest\nweak_spots\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques."
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html",
    "href": "notebooks/how_to/run_a_test.html",
    "title": "Running an Individual Test",
    "section": "",
    "text": "This notebook shows how to run individual metrics or thresholds tests.\n%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\ncannot find .env file\nimport validmind as vm\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhsvn7va0000kgrl0nwybdff\"\n)\n  \n  \n\nConnected to ValidMind. Project: Customer Churn Model dev - Initial Validation (clhsvn7va0000kgrl0nwybdff)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "title": "Running an Individual Test",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the Raw Dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "title": "Running an Individual Test",
    "section": "Train a Model for Testing",
    "text": "Train a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#import-and-run-the-individual-test",
    "href": "notebooks/how_to/run_a_test.html#import-and-run-the-individual-test",
    "title": "Running an Individual Test",
    "section": "Import and Run the Individual Test",
    "text": "Import and Run the Individual Test\n\nInitialize ValidMind objects\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nImport the individual test\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.model_validation.sklearn.threshold_tests import TrainingTestDegradation\n\n\n\nPass the required context and config parameters\n\ntest_context = TestContext(model=vm_model)\nws_diagnostic = TrainingTestDegradation(test_context)\n\n\n\nRun the test\n\nws_diagnostic.run()\n\nTestPlanTestResult(result_id=\"training_test_degradation\", test_results)\n\n\n\n\nDisplay results\n\nws_diagnostic.result.show()"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This tutorial shows Model Developers on how to use and configure the Developer Framework and the MRM ValidMind Platform. The following steps will guide you to automatically document and test Time Series Forecasting models:\n\nStep 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Run Data Validation Test Suite on Raw Data\nStep 4: Preprocess Data\nStep 5: Run Data Validation Test Suite on Processed Data\nStep 6: Load Pre-Trained Models\nStep 7: Run Model Validation Test Suite on Models"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#introduction",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#introduction",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This tutorial shows Model Developers on how to use and configure the Developer Framework and the MRM ValidMind Platform. The following steps will guide you to automatically document and test Time Series Forecasting models:\n\nStep 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Run Data Validation Test Suite on Raw Data\nStep 4: Preprocess Data\nStep 5: Run Data Validation Test Suite on Processed Data\nStep 6: Load Pre-Trained Models\nStep 7: Run Model Validation Test Suite on Models"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-1-connect-notebook-to-validmind-project",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-1-connect-notebook-to-validmind-project",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 1: Connect Notebook to ValidMind Project",
    "text": "Step 1: Connect Notebook to ValidMind Project\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\nImport Libraries\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n\n\nConnect Notebook to ValidMind Project\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"...\",\n  api_secret = \"...\",\n  project = \"...\"\n)\n\nConnected to ValidMind. Project: [3] FRED Loan Rates Model - Periodic Review (cli4fylps0000s5y6oi5z06xy)\n\n\n\n\nExplore Test Suites, Test Plans and Tests\n\nvm.test_suites.list_suites()\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\nbinary_classifier_full_suite\nBinaryClassifierFullSuite\nFull test suite for binary classification models.\ntabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\nbinary_classifier_model_validation\nBinaryClassifierModelValidation\nTest suite for binary classification models.\nbinary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n\n\ntabular_dataset\nTabularDataset\nTest suite for tabular datasets.\ntabular_dataset_description, tabular_data_quality\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast, time_series_sensitivity\n\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n\n\nID\nName\nDescription\n\n\n\n\nbinary_classifier_metrics\nBinaryClassifierMetrics\nTest plan for sklearn classifier metrics\n\n\nbinary_classifier_validation\nBinaryClassifierPerformance\nTest plan for sklearn classifier models\n\n\nbinary_classifier_model_diagnosis\nBinaryClassifierDiagnosis\nTest plan for sklearn classifier model diagnosis tests\n\n\ntabular_dataset_description\nTabularDatasetDescription\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\ntabular_data_quality\nTabularDataQuality\nTest plan for data quality on tabular datasets\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest plan for data quality on time series datasets\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nTest plan to perform time series univariate analysis.\n\n\ntime_series_multivariate\nTimeSeriesMultivariate\nTest plan to perform time series multivariate analysis.\n\n\ntime_series_forecast\nTimeSeriesForecast\nTest plan to perform time series forecast tests.\n\n\ntime_series_sensitivity\nTimeSeriesSensitivity\nTest plan to perform time series forecast tests.\n\n\nregression_model_description\nRegressionModelDescription\nTest plan for performance metric of regression model of statsmodels library\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\n\n\nTest Type\nID\nName\nDescription\n\n\n\n\nMetric\nacf_pacf_plot\nACFandPACFPlot\nPlots ACF and PACF for a given time series dataset.\n\n\nMetric\nauto_ar\nAutoAR\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\nMetric\nauto_ma\nAutoMA\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\nMetric\nauto_seasonality\nAutoSeasonality\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\nMetric\nauto_stationarity\nAutoStationarity\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\nMetric\nclassifier_in_sample_performance\nClassifierInSamplePerformance\nTest that outputs the performance of the model on the training data.\n\n\nMetric\nclassifier_out_of_sample_performance\nClassifierOutOfSamplePerformance\nTest that outputs the performance of the model on the test data.\n\n\nMetric\nconfusion_matrix\nConfusionMatrix\nConfusion Matrix\n\n\nMetric\ndataset_correlations\nDatasetCorrelations\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson's R for numerical variables - Cramer's V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\nMetric\ndataset_description\nDatasetDescription\nCollects a set of descriptive statistics for a dataset\n\n\nMetric\ndataset_split\nDatasetSplit\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\nMetric\ndescriptive_statistics\nDescriptiveStatistics\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\nMetric\nengle_granger_coint\nEngleGrangerCoint\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\nMetric\nlagged_correlation_heatmap\nLaggedCorrelationHeatmap\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\nMetric\nmodel_metadata\nModelMetadata\nCustom class to collect the following metadata for a model: - Model architecture - Model hyperparameters - Model task type\n\n\nMetric\npearson_correlation_matrix\nPearsonCorrelationMatrix\nExtracts the Pearson correlation coefficient for all pairs of numerical variables in the dataset. This metric is useful to identify highly correlated variables that can be removed from the dataset to reduce dimensionality.\n\n\nMetric\npfi\nPermutationFeatureImportance\nPermutation Feature Importance\n\n\nMetric\npsi\nPopulationStabilityIndex\nPopulation Stability Index between two datasets\n\n\nMetric\npr_curve\nPrecisionRecallCurve\nPrecision Recall Curve\n\n\nMetric\nroc_curve\nROCCurve\nROC Curve\n\n\nMetric\nregression_forecast_plot_levels\nRegressionModelForecastPlotLevels\nThis metric creates a plot of forecast vs observed for each model in the list.\n\n\nMetric\nregression_sensitivity_plot\nRegressionModelSensitivityPlot\nThis metric performs sensitivity analysis applying shocks to one variable at a time.\n\n\nMetric\nregression_models_coefficients\nRegressionModelsCoeffs\nTest that outputs the coefficients of stats library regression models.\n\n\nMetric\nregression_models_performance\nRegressionModelsPerformance\nTest that outputs the comparison of stats library regression models.\n\n\nMetric\nrolling_stats_plot\nRollingStatsPlot\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\nMetric\nshap\nSHAPGlobalImportance\nSHAP Global Importance\n\n\nMetric\nscatter_plot\nScatterPlot\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\nMetric\nseasonal_decompose\nSeasonalDecompose\nCalculates seasonal_decompose metric for each of the dataset features\n\n\nMetric\nspread_plot\nSpreadPlot\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\nMetric\ntime_series_histogram\nTimeSeriesHistogram\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\nMetric\ntime_series_line_plot\nTimeSeriesLinePlot\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\nNone\ndataset_metadata\nDatasetMetadata\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via `log_dataset` instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\nThresholdTest\nclass_imbalance\nClassImbalance\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\nThresholdTest\nduplicates\nDuplicates\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\nThresholdTest\ncardinality\nHighCardinality\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\nThresholdTest\npearson_correlation\nHighPearsonCorrelation\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\nThresholdTest\naccuracy_score\nMinimumAccuracy\nTest that the model's prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nf1_score\nMinimumF1Score\nTest that the model's F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nroc_auc_score\nMinimumROCAUCScore\nTest that the model's ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\nThresholdTest\nmissing\nMissingValues\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\nThresholdTest\noverfit_regions\nOverfitDiagnosis\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\nThresholdTest\nrobustness\nRobustnessDiagnosis\nTest robustness of model by perturbing the features column values by adding noise within scale stardard deviation.\n\n\nThresholdTest\nskewness\nSkewness\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\nThresholdTest\ntime_series_frequency\nTimeSeriesFrequency\nTest that detect frequencies in the data\n\n\nThresholdTest\ntime_series_missing_values\nTimeSeriesMissingValues\nTest that the number of missing values is less than a threshold\n\n\nThresholdTest\ntime_series_outliers\nTimeSeriesOutliers\nTest that find outliers for time series data using the z-score method\n\n\nThresholdTest\nzeros\nTooManyZeroValues\nThe zeros test finds columns that have too many zero values.\n\n\nThresholdTest\ntraining_test_degradation\nTrainingTestDegradation\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\nThresholdTest\nunique\nUniqueRows\nTest that the number of unique rows is greater than a threshold\n\n\nThresholdTest\nweak_spots\nWeakspotsDiagnosis\nTest that identify weak regions with high residuals by histogram slicing techniques."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-2-import-raw-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-2-import-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 2: Import Raw Data",
    "text": "Step 2: Import Raw Data\n\nImport FRED Dataset\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\ndf = demo_dataset.load_data()\ndf.tail(10)\n\n\n\n\n\n\n\n\nMORTGAGE30US\nFEDFUNDS\nGS10\nUNRATE\n\n\nDATE\n\n\n\n\n\n\n\n\n2023-03-02\n6.65\nNaN\nNaN\nNaN\n\n\n2023-03-09\n6.73\nNaN\nNaN\nNaN\n\n\n2023-03-16\n6.60\nNaN\nNaN\nNaN\n\n\n2023-03-23\n6.42\nNaN\nNaN\nNaN\n\n\n2023-03-30\n6.32\nNaN\nNaN\nNaN\n\n\n2023-04-01\nNaN\nNaN\n3.46\nNaN\n\n\n2023-04-06\n6.28\nNaN\nNaN\nNaN\n\n\n2023-04-13\n6.27\nNaN\nNaN\nNaN\n\n\n2023-04-20\n6.39\nNaN\nNaN\nNaN\n\n\n2023-04-27\n6.43\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-3-run-data-validation-test-suite-on-raw-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-3-run-data-validation-test-suite-on-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 3: Run Data Validation Test Suite on Raw Data",
    "text": "Step 3: Run Data Validation Test Suite on Raw Data\n\nExplore the Time Series Dataset Test Suite\n\nvm.test_suites.describe_test_suite(\"time_series_dataset\")\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\ntime_series_dataset\nTimeSeriesDataset\nTest suite for time series datasets.\ntime_series_data_quality, time_series_univariate, time_series_multivariate\n\n\n\n\n\n\n\nExplore Test Plans\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntime_series_data_quality\nTimeSeriesDataQuality\nTest plan for data quality on time series datasets\ndataset\nTimeSeriesOutliers (ThresholdTest), TimeSeriesMissingValues (ThresholdTest), TimeSeriesFrequency (ThresholdTest)\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntime_series_univariate\nTimeSeriesUnivariate\nTest plan to perform time series univariate analysis.\ndataset\nTimeSeriesLinePlot (Metric), TimeSeriesHistogram (Metric), ACFandPACFPlot (Metric), SeasonalDecompose (Metric), AutoSeasonality (Metric), AutoStationarity (Metric), RollingStatsPlot (Metric), AutoAR (Metric), AutoMA (Metric)\n\n\n\n\n\n\nConnect Raw Dataset to ValidMind Platform\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nRun Time Series Dataset Test Suite on Raw Dataset\n\nconfig={\n    \n    # TIME SERIES DATA QUALITY PARAMS\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    },\n    \n    # TIME SERIES UNIVARIATE PARAMS \n    \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n     \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive'\n    },\n     \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 2\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 2\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS \n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)\n\n\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nNo frequency could be inferred for variable 'MORTGAGE30US'. Skipping seasonal decomposition and plots for this variable.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\n\n\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-4-preprocess-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-4-preprocess-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 4: Preprocess Data",
    "text": "Step 4: Preprocess Data\n\nHandle Frequencies, Missing Values and Stationairty\n\n# Sample frequencies to Monthly\nresampled_df = df.resample(\"MS\").last()\n\n# Remove all missing values\nnona_df = resampled_df.dropna()\n\n# Take the first different across all variables\npreprocessed_df = nona_df.diff().dropna()"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-5-run-data-validation-test-suite-on-processed-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-5-run-data-validation-test-suite-on-processed-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 5: Run Data Validation Test Suite on Processed Data",
    "text": "Step 5: Run Data Validation Test Suite on Processed Data\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\n\n\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-6-load-pre-trained-models",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-6-load-pre-trained-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 6: Load Pre-Trained Models",
    "text": "Step 6: Load Pre-Trained Models\n\nLoad Pre-Trained Models\n\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\n\n\nConnect a List of Models To the ValidMind Platform\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n\nlist_of_models = [vm_model_A, vm_model_B]\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-7-run-model-validation-test-suite-on-models",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-7-run-model-validation-test-suite-on-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 7: Run Model Validation Test Suite on Models",
    "text": "Step 7: Run Model Validation Test Suite on Models\n\nExplore the Time Series Model Validation Test Suite\n\nvm.test_suites.describe_test_suite(\"time_series_model_validation\")\n\n\n\n\n\n\nID\nName\nDescription\nTest Plans\n\n\n\n\ntime_series_model_validation\nTimeSeriesModelValidation\nTest suite for time series model validation.\nregression_model_description, regression_models_evaluation, time_series_forecast, time_series_sensitivity\n\n\n\n\n\n\n\nExplore Test Plans\n\nvm.test_plans.describe_plan(\"regression_model_description\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\nregression_model_description\nRegressionModelDescription\nTest plan for performance metric of regression model of statsmodels library\nmodel\nDatasetSplit (Metric), ModelMetadata (Metric)\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"regression_models_evaluation\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest plan for metrics comparison of regression model of statsmodels library\nmodels\nRegressionModelsCoeffs (Metric), RegressionModelsPerformance (Metric)\n\n\nregression_models_evaluation\nRegressionModelsEvaluation\nTest plan for metrics comparison of regression model of statsmodels library\nmodel\nRegressionModelsCoeffs (Metric), RegressionModelsPerformance (Metric)\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_forecast\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntime_series_forecast\nTimeSeriesForecast\nTest plan to perform time series forecast tests.\nmodels\nRegressionModelForecastPlotLevels (Metric)\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_sensitivity\")\n\n\n\n\n\n\nID\nName\nDescription\nRequired Context\nTests\n\n\n\n\ntime_series_sensitivity\nTimeSeriesSensitivity\nTest plan to perform time series forecast tests.\nmodels\nRegressionModelSensitivityPlot (Metric)\n\n\n\n\n\n\n\nRun Model Validation Test Suite on a List of Models\n\nconfig= {\n    \"regression_forecast_plot_levels\": {\n        \"transformation\": \"integrate\",\n    },\n    \"regression_sensitivity_plot\": {\n        \"transformation\": \"integrate\",\n        \"shocks\": [0.3],\n    }\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model = vm_model_B,\n    models = list_of_models,\n    config = config,\n)\n\n\n\n\n{'regression_forecast_plot_levels': {'transformation': 'integrate'}, 'transformation': 'integrate', 'shocks': [0.3]}\nregression_sensitivity_plot:0\nregression_sensitivity_plot:1"
  }
]