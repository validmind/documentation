[
  {
    "objectID": "guide/developer-framework-introduction.html",
    "href": "guide/developer-framework-introduction.html",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "",
    "text": "This page provides an introduction for:\n\nWhat the ValidMind Developer Framework is, key concepts, and what functionality it provides\nHow ValidMind documentation projects are structured"
  },
  {
    "objectID": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "href": "guide/developer-framework-introduction.html#what-validminds-developer-framework-is",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "What ValidMind’s Developer Framework is",
    "text": "What ValidMind’s Developer Framework is\n\nValidMind’s Python Developer Framework is a library of developer tools and methods designed to automate the documentation and validation of your models.\nThe Developer Framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python library will provide all the standard functionality without requiring your developers to rewrite any functions.\nThe Developer Framework provides a rich suite of documentation tools and test plans, from documenting descriptions of your dataset to testing your models for weak spots and overfit areas. The Developer Framework helps you automate the generation of model documentation by feeding the ValidMind platform with documentation artifacts and test results to the ValidMind platform."
  },
  {
    "objectID": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "href": "guide/developer-framework-introduction.html#validmind-documentation-project-structure",
    "title": "Introduction to the ValidMind Developer Framework",
    "section": "ValidMind Documentation Project Structure",
    "text": "ValidMind Documentation Project Structure\n\n\nProjects\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s MRM lifecycle will constitute a new project, and may be configured with its own templates and workflows.\n\nModel documentation\n\nA comprehensive record and description of a quantitative model. It should encompass all relevant information about the model in accordance with regulatory requirements (set by regulatory bodies) and model risk policies (set by an institution’s MRM team), assumptions, methodologies, data and inputs, model performance evaluation, limitations, and intended use. The purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\n\nTest plans\n\nA collection of many tests which are meant to be run simultaneously to validate and document specific aspects of the documentation. For instance, the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind UI.\n\nTest suites\n\nCollection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\n\nTemplates\n\nAn outline of the sections/sub-sections of a ValidMind document (model documentation or validation report) and how they are organized. Templates also contain boilerplates and documentation & test results placeholders for which content will be provided by the Developer Framework. Template requirements are typically provided by the model risk management team, and can be configured programmatically for each model use case, typically by an administrator."
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind overview",
    "section": "",
    "text": "ValidMind is a model risk management (MRM) solution designed for the specific needs of model developers and model validators alike. The platform automates key aspects of the MRM process, including model documentation, validation, and testing. In addition, the platform comes with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date.\nOur solution comprises two primary architectural components: the ValidMind Developer Framework and the cloud-based ValidMind MRM platform."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind overview",
    "section": "Related Topics",
    "text": "Related Topics\nReady to try out ValidMind? Try the Quickstart."
  },
  {
    "objectID": "guide/view-templates.html",
    "href": "guide/view-templates.html",
    "title": "View templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-templates.html#prerequisites",
    "href": "guide/view-templates.html#prerequisites",
    "title": "View templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-templates.html#steps",
    "href": "guide/view-templates.html#steps",
    "title": "View templates",
    "section": "Steps",
    "text": "Steps\n\nFrom the ValidMind Platform homepage, go to Templates on the left.\nClick on one of the available templates to view the YAML configuration file.\nIn the configuration file that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the Developer Framework that feed into the template\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTemplates can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-templates.html#related-topics",
    "href": "guide/view-templates.html#related-topics",
    "title": "View templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developers section."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe validation guidelines for each template can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s Next",
    "text": "What’s Next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/jupyter-notebooks.html",
    "href": "guide/jupyter-notebooks.html",
    "title": "Example notebooks",
    "section": "",
    "text": "Our example notebooks are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nTry the notebook source yourself:\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time, and download notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/jupyter-notebooks.html#related-topics",
    "href": "guide/jupyter-notebooks.html#related-topics",
    "title": "Example notebooks",
    "section": "Related topics",
    "text": "Related topics\nFor an introduction to how these notebooks get used with ValidMind, take a look at the Quickstart."
  },
  {
    "objectID": "guide/view-all-test-plans.html",
    "href": "guide/view-all-test-plans.html",
    "title": "View all test plans",
    "section": "",
    "text": "<<<<<<< HEAD This topic describes how to use list_plans(), list_test(), and describe_plan() methods to view and describe test plans and tests available in the Developer Framework. ======= Learn how to use list_plans(), list_test(), and describe_plan() methods to view and describe test plans and tests available in the Developer Framework. >>>>>>> 874af753c9bacc9440a5837f643a5f84cc6d73ea"
  },
  {
    "objectID": "guide/view-all-test-plans.html#prerequisites",
    "href": "guide/view-all-test-plans.html#prerequisites",
    "title": "View all test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are working on an active documentation project\nYou have already installed the ValidMind client library in your developer environment"
  },
  {
    "objectID": "guide/view-all-test-plans.html#steps",
    "href": "guide/view-all-test-plans.html#steps",
    "title": "View all test plans",
    "section": "Steps",
    "text": "Steps\n\nInitialize the client library.\nUse list_plans() and list_tests() to view the list of all available test plans and tests.\nExamples:\n\nList all available test plans currently available in the the Developer Framework:\nvm.test_plans.list_plans()\nList all available individual tests currently available in the Developer Framework:\nvm.test_plans.list_tests() \n\nUse describe_testplan() to list all the tests included in a specific test plan:\nExample: The following code will list tests included in the tabular_data_quality test plan:\nvm.test_plans.describe_plan(\"tabular_data_quality\")"
  },
  {
    "objectID": "guide/view-all-test-plans.html#related-topics",
    "href": "guide/view-all-test-plans.html#related-topics",
    "title": "View all test plans",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a solution designed to help simplify and automate key aspects of model risk management (MRM) activities for model developers and model validators alike. The platform helps automate model documentation, validation, and testing. In addition, the platform offers with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/get-started.html#how-does-it-work",
    "href": "guide/get-started.html#how-does-it-work",
    "title": "Get started",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\nValidMind consists of two main products components:\n\nThe Developer Framework is a library of tools and methods designed to automate model documentation and validation. It is platform agnostic, and integrates with the model development environment.\nThe ValidMind Platform is an easy-to-use web-based UI that enables users to review and edit the documentation generated by the Developer Framework. It also enables collaboration and feedback capture between model developers and model validators, and offers workflow capabilities to manage the model documentation and validation process.\n\nFor more information about what ValidMind offers, check out our ValidMind overview"
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "href": "guide/get-started.html#how-do-i-get-access-to-validmind",
    "title": "Get started",
    "section": "How do I get access to ValidMind?",
    "text": "How do I get access to ValidMind?\nIf you are new to our products, you will need access. You can request it."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nThe fastest way to explore what ValidMind can offer is with our Quickstart.\nThe Quickstart takes about 20 minutes to complete and walks you through the Developer Framework with a sample Jupyter notebook and introduces you to the ValidMind Platform.\nIf you have already tried the Quickstart, how-to instructions for different users are in our Guides:\n\nFor platform administrators — Learn how to configure the platform, from setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your exisiting workflows, and more.\nFor model developers — Find information for ValidMind test plans and tests, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nAlso check the Guides for how you integrate the Developer Framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers."
  },
  {
    "objectID": "guide/get-started.html#have-more-questions",
    "href": "guide/get-started.html#have-more-questions",
    "title": "Get started",
    "section": "Have more questions?",
    "text": "Have more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html",
    "href": "guide/use-test-plans-and-tests.html",
    "title": "When to use test plans and tests",
    "section": "",
    "text": "This topic provides an overview about:"
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "href": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "title": "When to use test plans and tests",
    "section": "What Tests, Test plans, and Test suites are",
    "text": "What Tests, Test plans, and Test suites are\n\nTests are designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\nTest plans are collections of tests which are meant to be run simultaneously to address specific aspects of the documentation.\nExample: the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind Platform.\nTest suites are collection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\nExample: the binary_classifier_full_suite test suite runs the tabular_dataset and binary_classifier test plans to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "href": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "title": "When to use test plans and tests",
    "section": "When to use ValidMind Tests, Test plans, and Test suites",
    "text": "When to use ValidMind Tests, Test plans, and Test suites\nValidMind provides many built-in tests and test plans which make it easy for a model developer to document their work at any point during the model development lifecycle when they need to validate that their work satisfies model risk management requirements.\nWhile model developers have the flexibility to decide when to use ValidMind tests, we have identified a few typical scenarios which have their own characteristics and needs:\n\nWhen you want to document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test plan.\nFor time-series datasets: use the time_series_dataset test plan.\n\nWhen you want to document and validate about your model:\n\nFor binary classification models: use the binary_classifier test plan.\nFor time series models: use the timeseries test plan.\n\nWhen you want to document a binary classification model and the relevant dataset end-to-end: use the binary_classifier_full_suite test suite."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#api-reference",
    "href": "guide/use-test-plans-and-tests.html#api-reference",
    "title": "When to use test plans and tests",
    "section": "API Reference",
    "text": "API Reference\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/editions-and-features.html",
    "href": "guide/editions-and-features.html",
    "title": "Editions and features",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions-and-features.html#editions",
    "href": "guide/editions-and-features.html#editions",
    "title": "Editions and features",
    "section": "Editions",
    "text": "Editions\n\nDeveloper Edition\nThe Developer Edition is the ideal training ground for developers to play around with ValidMind’s automated model documentation and to test the robustness of our developer framework, documentation, and testing features. The Developer Edition is free, allowing developers who are new to model documentation and model risk management to build, implement, test, and maintain higher quality models and model documentation.\nThe Developer Edition is only for personal testing purposes and cannot be used as a commercial model documentation or model risk management solution.\n\n\nEssential Edition\nWith the Essential Edition, you get an advanced model risk management (MRM) solution. It offers your organization all the features and services of the Developer Edition, plus additional features tailored to the needs of larger-scale organizations.\n\n\nBusiness Critical\nProvides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. This edition encompasses all features and services of the Essential Edition but within a separate ValidMind environment, isolated from other ValidMind accounts via Virtual Private ValidMind (VPV). VPV accounts do not share resources with non-VPV accounts."
  },
  {
    "objectID": "guide/editions-and-features.html#features",
    "href": "guide/editions-and-features.html#features",
    "title": "Editions and features",
    "section": "Features",
    "text": "Features\n\n\n\n\nModel development & documentation\nDeveloper\nEssential\nBusiness Critical\n\n\n\n\nAutomated model documentation\n\n\n\n\n\nPlatform-independent developer framework\n\n\n\n\n\nOnline documentation editing\n\n\n\n\n\nAdvanced editing & readability assistance\n\n\n\n\n\nDocumentation quality measurement\n\n\n\n\n\nOffline document ingestion\n\n\n\n\n\nFeedback capture on online document\n\n\n\n\n\nDocumentation version history management\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nStandard tests & validation libraries\n\n\n\n\n\nConfigure / customize tests & validation libraries\n\n\n\n\n\nSupport for customer-provided tests\n\n\n\n\n\nDeveloper workflow management\n\n\n\n\n\nPre-configured documentation templates & boilerplates\n\n\n\n\n\nConfigurable documentation templates & boilerplates\n\n\n\n\n\nModel validation & audit\n\n\n\n\n\nModel validation report automation\n\n\n\n\n\nFindings / issues & remediation actions tracking\n\n\n\n\n\nConfigurable approval workflows\n\n\n\n\n\nMRM workflows & validation lifecycle tracking\n\n\n\n\n\nMRM resource & workflow management\n\n\n\n\n\nCentral model inventory\n\n\n\n\n\nHistorical documentation repository /documentation CMS\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nExecutive reporting\n\n\n\n\n\nPlatform integration & support\n\n\n\n\n\nData lake integration, such as Evidence Storeand monitoring data\n\n\n\n\n\nSSO integration\n\n\n\n\n\nCustomer managed encryption\n\n\n\n\n\nSupport 8/5 (one timezone)\n\n\n\n\n\nSupport 24/7 (global)\n\n\n\n\n\nPlatform deployment\n\n\n\n\n\nMulti-tenant SaaS\n\n\n\n\n\nVirtual private ValidMind (VPV)\n\n\n\n\n\nSelf-managed VPV\n\n\n\n\n\n\nContact Us\nContact Us\nContact Us"
  },
  {
    "objectID": "guide/register-model.html",
    "href": "guide/register-model.html",
    "title": "Register models",
    "section": "",
    "text": "Register a model you are documenting in the model inventory."
  },
  {
    "objectID": "guide/register-model.html#prerequisites",
    "href": "guide/register-model.html#prerequisites",
    "title": "Register models",
    "section": "Prerequisites",
    "text": "Prerequisites\n[Include a list of any prerequisites the user needs to complete before starting the task, such as having certain software installed, access to certain resources, or completing previous tasks.]"
  },
  {
    "objectID": "guide/register-model.html#steps",
    "href": "guide/register-model.html#steps",
    "title": "Register models",
    "section": "Steps",
    "text": "Steps\n\n[Step 1]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 2]\n\n[Include any sub-steps or details needed to complete this step]\n\n[Step 3]\n\n[Include any sub-steps or details needed to complete this step]\n[If applicable, include screenshots or images to illustrate the step]\n\n[Step 4]\n\n[Include any sub-steps or details needed to complete this step]"
  },
  {
    "objectID": "guide/register-model.html#troubleshooting",
    "href": "guide/register-model.html#troubleshooting",
    "title": "Register models",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n[Include any common issues or errors that may arise during the task and how to resolve them.]"
  },
  {
    "objectID": "guide/register-model.html#whats-next",
    "href": "guide/register-model.html#whats-next",
    "title": "Register models",
    "section": "What’s Next",
    "text": "What’s Next\n[Summarize the task and provide any next steps or resources for the user to continue their learning or work.]"
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework."
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.prod.validmind.ai.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test plans and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/developer-framework.html",
    "href": "guide/developer-framework.html",
    "title": "Developers",
    "section": "",
    "text": "Geared towards model developers, this section includes information for:"
  },
  {
    "objectID": "guide/developer-framework.html#related-topics",
    "href": "guide/developer-framework.html#related-topics",
    "title": "Developers",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developer tasks related to documentation projects and collaborating with model validators and model owners, refer to our Guides."
  },
  {
    "objectID": "guide/explore-validmind.html",
    "href": "guide/explore-validmind.html",
    "title": "Explore ValidMind",
    "section": "",
    "text": "To see how the ValidMind Developer Framework works, try our introductory Jupyter notebook.\nThis notebook shows you how to initialize the ValidMind Developer Framework and run functions from a sample dataset for a customer churn model that we trained for this demo.\n\n\n\nOpen the Quickstart notebook: \nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there.\nMake a copy of the Quickstart notebook for yourself:\n\nIn Colab, click File > Save a copy in Drive > make your own copy in Google Drive so that you can modify the notebook.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n\n\n\n\n\n\n\n\n\n“Warning: This notebook was not authored by Google”\n\n\n\nNotebooks from ValidMind are safe to run. You can inspect the notebook source yourself.\n\n\n\n\n\n\n\n\n\nGetting runtime errors in Google Colaboratory?\n\n\n\nWe recommend that you NOT use the Run all option. Stepping through each cell individually enables you to see what is happening in the notebook. If you run into errors, re-run the notebook cells.\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework.\nYou should see a message like this near the bottom of the Initialize ValidMind section:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)"
  },
  {
    "objectID": "guide/explore-validmind.html#explore-the-validmind-platform",
    "href": "guide/explore-validmind.html#explore-the-validmind-platform",
    "title": "Explore ValidMind",
    "section": "Explore the ValidMind Platform",
    "text": "Explore the ValidMind Platform\nNext, let’s take a look at how the Developer Framework works hand-in-hand with the ValidMind Platform and how documentation and test results get uploaded.\nThe ValidMind Platform is the central place to:\n\nView results and documentation uploaded via the Developer Framework\nCollaborate with other model developers, model reviewers and validators, and other users involved in the documentation and validation workflow"
  },
  {
    "objectID": "guide/explore-validmind.html#steps-1",
    "href": "guide/explore-validmind.html#steps-1",
    "title": "Explore ValidMind",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the [Quickstart] Customer Churn Model - Initial Validation and select it.\nOn the model details page that open, you can find important information about the model, such as:\n\nThe ID of the model and its specific use case\nThe owners, developers, validators, and business unit associated with the model\nThe risk tier and current version\nAnd more\n\nScroll down to Project History and select the model.\nOn the project overview page that opens, you can see what is included, such as model, project findings, recent activity, and project stakeholders, and more. In the left sidebar, you can find links to the documentation, project findings, validation report, audit trail, and client integration.\nFor this Quickstart, we will focus on the Documentation section to show you how content from the Developer Framework gets uploaded into it.\nNote that the model status is In Documentation. This is the status that a model starts in as part of a documentation project. You can click See workflow to look at what the full workflow is, from documentation, to validation, to review, and finally approval.\nFrom the left sidebar, select Documentation > 2. Data preparation > 2.1. Data description.\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically by the Developer Framework whenever a test result is above or below a certain threshold.\nFrom the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the Developer Framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nThe Documentation Guidelines in the ValidMind Insights right sidebar can tell you more about what these sections mean and help you with the task of documenting the model.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the Developer Framework, but they get added by the model developer in the Platform UI."
  },
  {
    "objectID": "guide/explore-validmind.html#create-a-new-documentation-project",
    "href": "guide/explore-validmind.html#create-a-new-documentation-project",
    "title": "Explore ValidMind",
    "section": "Create a new documentation project",
    "text": "Create a new documentation project\nNext, let’s learn how to create your own documentation project. You can use this project to upload tests and documentation and then add that to your own copy of the Quickstart notebook you looked at earlier.\n\nSteps\n\nNavigate to the landing page by clicking on the ValidMind logo or if you have to, Log in to the ValidMind UI.\nFrom the left sidebar, select Documentation Projects and on the page that opens, click the Create new Project button at top right of the screen.\nSelect the right options in the form:\n\nModel: [Quickstart] Customer Churn Model\nType: Initial validation (selected automatically) \nProject name: Enter your preferred name\n\nClick Create Project.\nValidMind will create an empty documentation project associated with the customer churn model.\nYou can now access this project from the UI on the Documentation Projects page or by navigating to the relevant model - [Quickstart] Customer Churn Model - in the Model Inventory page.\nFrom the left sidebar, select Client Integration.\nThe page that opens provides you with the credentials for the newly created project to use with the ValidMind Developer Framework.\nLocate the project identifier, API key, and secret:\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\n\n\n\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nTry this: Use the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/explore-validmind.html#modify-the-quickstart-notebook-to-upload-to-your-own-project",
    "href": "guide/explore-validmind.html#modify-the-quickstart-notebook-to-upload-to-your-own-project",
    "title": "Explore ValidMind",
    "section": "Modify the Quickstart notebook to upload to your own project",
    "text": "Modify the Quickstart notebook to upload to your own project\nAfter you have completed the above steps, you are ready to use your documentation project with your copy of the Quickstart notebook.\n\nSteps\n\nReopen your copy of the Quickstart notebook in Google Colaboratory.\nIn the Quickstart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you created your new project:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework.\n\n\n\n\n\n\n\n\nGetting runtime errors in Google Colaboratory?\n\n\n\nWe recommend that you NOT use the Run all option. Stepping through each cell individually enables you to see what is happening in the notebook. If you run into errors, re-run the notebook cells."
  },
  {
    "objectID": "guide/explore-validmind.html#whats-next",
    "href": "guide/explore-validmind.html#whats-next",
    "title": "Explore ValidMind",
    "section": "What’s next",
    "text": "What’s next\nReady to use ValidMind for production with your own use cases? Follow our how-to guides."
  },
  {
    "objectID": "guide/explore-validmind.html#related-topics",
    "href": "guide/explore-validmind.html#related-topics",
    "title": "Explore ValidMind",
    "section": "Related topics",
    "text": "Related topics\n\nIntroduction to the ValidMind Developer Framework.\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/tabular-data-tests.html",
    "href": "guide/tabular-data-tests.html",
    "title": "Tabular data tests",
    "section": "",
    "text": "This article provides an example of how to run a Test Plan to validate a tabular dataset, and provides a reference to other dataset tests and test plans available in the Developer Framework."
  },
  {
    "objectID": "guide/tabular-data-tests.html#prerequisites",
    "href": "guide/tabular-data-tests.html#prerequisites",
    "title": "Tabular data tests",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nThe model is already registered in the model inventory\nThere is an active documentation project for the model\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/tabular-data-tests.html#examples",
    "href": "guide/tabular-data-tests.html#examples",
    "title": "Tabular data tests",
    "section": "Examples",
    "text": "Examples\nPlease refer to the following notebooks for examples of how to run dataset test plans:\n\nTabular dataset: Customer Churn Model \nTime series dataset: Time Series Dataset Test Suite"
  },
  {
    "objectID": "guide/tabular-data-tests.html#list-of-all-dataset-test-plans-and-tests",
    "href": "guide/tabular-data-tests.html#list-of-all-dataset-test-plans-and-tests",
    "title": "Tabular data tests",
    "section": "List of all dataset test plans and tests:",
    "text": "List of all dataset test plans and tests:\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\ncom.amazonaws.vpce.us-west-2.vpce-svc-0b956fa3e03afa538\nhttps://private.prod.vm.validmind.ai"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s Next",
    "text": "What’s Next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html",
    "href": "guide/review-data-streams-and-audit-trails.html",
    "title": "Review Audit Trail",
    "section": "",
    "text": "<<<<<<< HEAD This topic describes how to access and use the Audit Trail functionality in the ValidMind Platform. It is relevant for model developers, model validators, and auditors who are looking to track and/or audit all the information events associated with a specific project. ======= Learn how to access and use the audit trail functionality in the ValidMind Platform. This topic matters for for model developers, model validators, and auditors who are looking to track or audit all the information events associated with a specific project. >>>>>>> 874af753c9bacc9440a5837f643a5f84cc6d73ea"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "href": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "title": "Review Audit Trail",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#steps",
    "href": "guide/review-data-streams-and-audit-trails.html#steps",
    "title": "Review Audit Trail",
    "section": "Steps",
    "text": "Steps\n\nIn the ValidMind platform, navigate to the relevant model documentation project.\nFrom the Overview page, select Audit Trail on the left.\n\nThe table in this page shows a record of all activities generated from the Developer Framework and actions performed by users in the organization related to this specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "href": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "title": "Review Audit Trail",
    "section": "What’s Next",
    "text": "What’s Next\n\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Client Integration page.\nRun the {…} test plan.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documetnation"
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nView validation guidelines"
  },
  {
    "objectID": "guide/model-evaluation-tests.html",
    "href": "guide/model-evaluation-tests.html",
    "title": "Model evaluation tests",
    "section": "",
    "text": "This article provides an examples of how to run model evaluation test plans, and provides a reference to other model evaluation tests and test plans available in the Developer Framework."
  },
  {
    "objectID": "guide/model-evaluation-tests.html#prerequisites",
    "href": "guide/model-evaluation-tests.html#prerequisites",
    "title": "Model evaluation tests",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nThe model is already registered in the model inventory\nThere is an active documentation project for the model\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/model-evaluation-tests.html#examples",
    "href": "guide/model-evaluation-tests.html#examples",
    "title": "Model evaluation tests",
    "section": "Examples",
    "text": "Examples\nPlease refer to the following notebooks for examples of how to run model evaluation test plans:\n\nBinary Classifier model example: Customer Churn Model \nTime Series Forecasting model example: Time Series Dataset Test Suite"
  },
  {
    "objectID": "guide/model-evaluation-tests.html#list-of-all-model-evaluation-test-plans-and-tests",
    "href": "guide/model-evaluation-tests.html#list-of-all-model-evaluation-test-plans-and-tests",
    "title": "Model evaluation tests",
    "section": "List of all model evaluation test plans and tests:",
    "text": "List of all model evaluation test plans and tests:\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/license-agreement.html",
    "href": "guide/license-agreement.html",
    "title": "License agreement",
    "section": "",
    "text": "SOFTWARE LICENSE AGREEMENT\nIMPORTANT - READ CAREFULLY:\nThis software and associated media, printed materials, and “online” or electronic documentation files (the “Software”), is theproprietary information of ValidMind Inc. and its licensors (collectively, “Licensor”), and is protected under copyright and other intellectual property laws.\nNo part of this Software may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means, including, but not limited to, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of ValidMind Inc. or the respective copyright owner.\nBy installing, copying, or otherwise using the Software, the undersigned (“you”) agrees to be bound by the terms of this Software License Agreement (this “Agreement”). If you do not agree to the terms of this Agreement, do not install or use the Software.\nLICENSE GRANT. Subject to the terms and conditions of this Agreement, Licensor grants you a limited, personal, non-exclusive, non-transferable license to use the Software solely for the duration of the 4-week testing phase (the “Testing Period”) of the Software - starting on May 15th, 2023. You may install and use the Software on a single computer or device. You further agree to use the Software solely for internal testing purposes.\nOWNERSHIP. The Software is owned by Licensor and is protected by copyright laws and international copyright treaties, as well as other intellectual property laws and treaties. Licensor retains all right, title, and interest in and to the Software, including all intellectual property rights.\nRESTRICTIONS. You may not modify, adapt, translate, reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code of the Software including (without limitation) for the purpose of obtaining unauthorized access to the Software. You may not distribute, sublicense, rent, lease, or lend the Software to any third party.\nSUPPORT. Licensor may, at its discretion, provide technical support for the Software. Technical support is provided on a best-effort basis and is subject to Licensor’s support policies.\nCONFIDENTIALITY. You agree to safeguard the Software and its related materials with that degree of normal due care commensurate with reasonable standards of industrial security for the protection of trade secrets and proprietary information so that no unauthorized use is made of them and no disclosure of any part of their contents is made to anyone other than your employees, agents or consultants whose duties reasonably require such disclosure, or as necessary in the ordinary course of business. You shall make all such persons fully aware of their responsibility to fulfill your obligations under this Agreement and agree to be responsible for any breach of this Agreement by such persons. You agree to promptly notify Licensor if you obtain information as to any unauthorized possession, use or disclosure of the Software by any person or entity, and further agrees to cooperate with Licensor in protecting Licensor’s proprietary rights.\nTERMINATION. This Agreement will terminate automatically after the Testing Period, or if you fail to comply with any of the terms and conditions of this Agreement. Upon termination, you must immediately cease all use of the Software and destroy all copies of the Software in your possession.\nDISCLAIMER OF WARRANTY. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.\nLIMITATION OF LIABILITY. IN NO EVENT SHALL LICENSOR BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF OR IN CONNECTION WITH THE USE OR INABILITY TO USE THE SOFTWARE, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. IN NO EVENT SHALL LICENSOR’S LIABILITY EXCEED THE AMOUNT PAID BY YOU FOR THE SOFTWARE.\nINDEMNIFICATION. You agree to indemnify and hold Licensor harmless from and against all loss, cost, expense or liability (including reasonable attorney’s fees) (i) arising out of a claim by a third party against Licensor based upon your use of the Software, or (ii) related to, or associated with your customizations, updates and/or corrections to the Software. Licensor shall have no liability to you for any damage sustained by you as a result of your use of the Software, whether such damages would arise as a result of breach of contract, tort or otherwise.\nGOVERNING LAW. This Agreement shall be governed by and construed in accordance with the laws of the State of California and the federal laws of the United States of America, without giving effect to any principles of conflicts of law.\nENTIRE AGREEMENT. This Agreement constitutes the entire agreement between you and Licensor with respect to the Software and supersedes all prior or contemporaneous communications and proposals, whether oral or written, between you and Licensor.\nBy installing or using the Software, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.\nCopyright © 2023 ValidMind Inc. All rights reserved."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "As of the current release (v1.9.3), the Developer Framework supports the following model types:\nThe following table presents an overview of libraries supported by each test plan, as well as the tests which comprise each test plan as of the current Developer Framework release."
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\n\nCheck out our Developer Framework documentation for more details on how to use our documentation and testing functions with supported models."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\nCheck Include comment threads to include comment threads in the exported file.\nCheck Section activity logs to include a history of changes in each section of the documentation.\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\nCheck Include comment threads to include comment threads in the exported file.\nCheck Section activity logs to include a history of changes in each section of the documentation.\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test plan execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test plans\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test plans and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\n\nLocating the project identifier, API key and secret:\nOn the Client Integration page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/create-documentation-project.html#related-topics",
    "href": "guide/create-documentation-project.html#related-topics",
    "title": "Create documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nInstall and initialize the Developer Framework\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enbale their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier\n\n\n\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick Client integration and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"<project-identifier>\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s Next",
    "text": "What’s Next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "To try out ValidMind, you need to be a registered user on the ValidMind Platform. If you don’t already have access, you can request it."
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo connect ValidMind’s Developer Framework to the ValidMind platform and to access the ValidMind platform User Interface (Web UI), you must be able to access our domain - validmind.ai . If necessary, ask a network administrator to add this domain to your company’s firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\nQuickStart Requirements\nTo follow the Quickstart, you must be able to access Google Colaboratory (Colab) where you can run our sample notebooks for free.\nNote: If you would like to run our sample notebooks locally, you need to ensure your developer environment is setup with Python 3.8+. \n\n\nAccess to Validmind’s Web UI\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstart — 20 mins",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework and explore the ValidMind Platform.\nThis Quickstart takes about 20 minutes of your time."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstart — 20 mins",
    "section": "Steps",
    "text": "Steps\n\nBefore you begin\nCheck the prerequisites for the Developer Framework and ValidMind Platform UI.\nExplore ValidMind\nTry our introductory Jupyter notebook to see the Developer Framework in action and explore our Platform UI to work with a documentation projects.\nNext steps\nTry some more advanced sample notebooks or set up ValidMind for production with your own use cases."
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases?"
  },
  {
    "objectID": "guide/next-steps.html#additional-resources",
    "href": "guide/next-steps.html#additional-resources",
    "title": "Next steps",
    "section": "Additional resources",
    "text": "Additional resources\nSee our FAQ for a curated list of frequently asked questions about what ValidMind offers."
  },
  {
    "objectID": "guide/next-steps.html#need-help",
    "href": "guide/next-steps.html#need-help",
    "title": "Next steps",
    "section": "Need help?",
    "text": "Need help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "validmind/api.html",
    "href": "validmind/api.html",
    "title": "ValidMind",
    "section": "",
    "text": "Python Library API\nMain entrypoint to the ValidMind Python Library\n\nvalidmind.init\nInitializes the API client instances and calls the /ping endpoint to ensure the provided credentials are valid and we can connect to the ValidMind API.\nIf the API key and secret are not provided, the client will attempt to retrieve them from the environment variables VM_API_KEY and VM_API_SECRET.\n\nParameters\n\nproject (str) – The project CUID\napi_key (str, optional) – The API key. Defaults to None.\napi_secret (str, optional) – The API secret. Defaults to None.\napi_host (str, optional) – The API host. Defaults to None.\n\nRaises\nValueError – If the API key and secret are not provided\nReturns\nTrue if the ping was successful\nReturn type\nbool\n\n\n\nvalidmind.init_dataset\nInitializes a VM Dataset, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a valid dataset type. We only support Pandas DataFrames at the moment.\n\nParameters\n\ndataset (pd.DataFrame) – We only support Pandas DataFrames at the moment\ntype (str) – The dataset split type is necessary for mapping and relating multiple datasets together. Can be one of training, validation, test or generic\noptions (dict) – A dictionary of options for the dataset\ntargets (vm.vm.DatasetTargets) – A list of target variables\ntarget_column (str) – The name of the target column in the dataset\nclass_labels (dict) – A list of class labels for classification problems\n\nRaises\nValueError – If the dataset type is not supported\nReturns\nA VM Dataset instance\nReturn type\nvm.vm.Dataset\n\n\n\nvalidmind.init_model\nInitializes a VM Model, which can then be passed to other functions that can perform additional analysis and tests on the data. This function also ensures we are reading a supported model type.\n\nParameters\nmodel – A trained sklearn model\nRaises\nValueError – If the model type is not supported\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.init_r_model\nInitializes a VM Model for an R model\nR models must be saved to disk and the filetype depends on the model type… Currently we support the following model types:\n\nLogisticRegression glm model in R: saved as an RDS file with saveRDS\nLinearRegression lm model in R: saved as an RDS file with saveRDS\nXGBClassifier: saved as a .json or .bin file with xgb.save\nXGBRegressor: saved as a .json or .bin file with xgb.save\n\nLogisticRegression and LinearRegression models are converted to sklearn models by extracting the coefficients and intercept from the R model. XGB models are loaded using the xgboost since xgb models saved in .json or .bin format can be loaded directly with either Python or R\n\nParameters\n\nmodel_path (str) – The path to the R model saved as an RDS or XGB file\nmodel_type (str) – The type of the model (one of R_MODEL_TYPES)\n\nReturns\nA VM Model instance\nReturn type\nvm.vm.Model\n\n\n\nvalidmind.run_test_plan\nHigh Level function for running a test plan\nThis function provides a high level interface for running a test plan. It removes the need to manually initialize a TestPlan instance and run it. This function will automatically find the correct test plan class based on the test_plan_name, initialize the test plan, and run it.\n\nParameters\n\ntest_plan_name (str) – The test plan name (e.g. ‘binary_classifier’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test plan. These will provide the TestPlan instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan for more details.\n\nRaises\nValueError – If the test plan name is not found or if there is an error initializing the test plan\nReturns\nA dictionary of test results\nReturn type\ndict\n\n\n\nvalidmind.run_test_suite\nHigh Level function for running a test suite\nThis function provides a high level interface for running a test suite. A test suite is a collection of test plans. This function will automatically find the correct test suite class based on the test_suite_name, initialize each of the test plans, and run them.\n\nParameters\n\ntest_suite_name (str) – The test suite name (e.g. ‘binary_classifier_full_suite’)\nsend (bool, optional) – Whether to post the test results to the API. send=False is useful for testing. Defaults to True.\n**kwargs – Additional keyword arguments to pass to the test suite. These will provide the TestSuite instance with the necessary context to run the tests. e.g. dataset, model etc. See the documentation for the specific test plan, metric or threshold test for more details.\n\nRaises\nValueError – If the test suite name is not found or if there is an error initializing the test suite\nReturns\nthe TestSuite instance\nReturn type\nTestSuite\n\n\n\nvalidmind.log_dataset\nLogs metadata and statistics about a dataset to ValidMind API.\n\nParameters\n\nvm_dataset (validmind.VMDataset) – A VM dataset object\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\ndataset_options (dict, optional) – Additional dataset options for analysis. Defaults to None.\ndataset_targets (validmind.DatasetTargets, optional) – A list of targets for the dataset. Defaults to None.\nfeatures (list, optional) – Optional. A list of features metadata. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nThe VMDataset object\nReturn type\nvalidmind.VMDataset\n\n\n\nvalidmind.log_figure\nLogs a figure\n\nParameters\n\ndata_or_path (str or matplotlib.figure.Figure) – The path of the image or the data of the plot\nkey (str) – Identifier of the figure\nmetadata (dict) – Python data structure\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_metrics\nLogs metrics to ValidMind API.\n\nParameters\n\nmetrics (list) – A list of Metric objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_model\nLogs model metadata and hyperparameters to ValidMind API.\n\nParameters\nvm_model (validmind.VMModel) – A VM model object\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nvalidmind.log_test_results\nLogs test results information. This method will be called automatically be any function running tests but can also be called directly if the user wants to run tests on their own.\n\nParameters\n\nresults (list) – A list of TestResults objects\nrun_cuid (str, optional) – The run CUID. If not provided, a new run will be created. Defaults to None.\ndataset_type (str, optional) – The type of dataset. Can be one of “training”, “test”, or “validation”. Defaults to “training”.\n\nRaises\nException – If the API call fails\nReturns\nTrue if the API call was successful\nReturn type\nbool\n\n\n\nclass validmind.Dataset\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nproperty index()\nReturns the dataset’s index.\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nget_numeric_features_columns()\nReturns list of numeric features columns\n\nReturns\nThe list of numberic features columns\nReturn type\nlist\n\n\n\nget_categorical_features_columns()\nReturns list of categorical features columns\n\nReturns\nThe list of categorical features columns\nReturn type\nlist\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.DatasetTargets\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.Figure\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.Metric\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nTODO: Metric should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\ndescription()\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(metric_value: dict | list | DataFrame | None = None)\nReturn the metric summary. Should be overridden by subclasses. Defaults to None. The metric summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the metric results.\nWe return None here because the metric summary is optional.\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\n\nclass validmind.Model\nBases: object\nA class that wraps a trained model instance and its associated data.\n\nattributes()\nThe attributes of the model. Defaults to None.\n\nType\nModelAttributes, optional\n\n\n\ntask()\nThe task that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nsubtask()\nThe subtask that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nparams()\nThe parameters of the model. Defaults to None.\n\nType\ndict, optional\n\n\n\nmodel_id()\nThe identifier of the model. Defaults to “main”.\n\nType\nstr\n\n\n\nmodel()\nThe trained model instance. Defaults to None.\n\nType\nobject, optional\n\n\n\ntrain_ds()\nThe training dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ntest_ds()\nThe test dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\nvalidation_ds()\nThe validation dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ny_train_predict()\nThe predicted outputs for the training dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_test_predict()\nThe predicted outputs for the test dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_validation_predict()\nThe predicted outputs for the validation dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\nvalidation_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\ny_validation_predict(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic model_library(model)\nReturns the model library name\n\n\nstatic model_class(model)\nReturns the model class name\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod init_vm_model(model, train_ds, test_ds, validation_ds, attributes)\nInitializes a model instance from the provided data.\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.ModelAttributes\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.TestResult\nBases: object\nTestResult model\n\nvalues(: dic )\n\n\ntest_name(: str | Non _ = Non_ )\n\n\ncolumn(: str | Non _ = Non_ )\n\n\npassed(: bool | Non _ = Non_ )\n\n\nserialize()\nSerializes the TestResult to a dictionary so it can be sent to the API\n\n\n\nclass validmind.TestResults\nBases: object\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\nsummary(: ResultSummary | Non )\n\n\nserialize()\nSerializes the TestResults to a dictionary so it can be sent to the API\n\n\n\nclass validmind.ThresholdTest\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nTODO: ThresholdTest should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\ndescription()\nReturn the test description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(test_results: TestResults | None = None)\nReturn the threshold test summary. Should be overridden by subclasses. Defaults to None. The test summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the test results.\nWe return None here because the test summary is optional.\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult"
  },
  {
    "objectID": "validmind/vm_models.html",
    "href": "validmind/vm_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Models\nModels entrypoint\n\nclass validmind.vm_models.Dataset\nBases: object\nModel class wrapper\n\nraw_dataset(: objec )\n\n\nfields(: lis )\n\n\nsample(: lis )\n\n\nshape(: dic )\n\n\ncorrelation_matrix(: objec _ = Non_ )\n\n\ncorrelations(: dic _ = Non_ )\n\n\ntype(: st _ = Non_ )\n\n\noptions(: dic _ = Non_ )\n\n\nstatistics(: dic _ = Non_ )\n\n\ntargets(: dic _ = Non_ )\n\n\ntarget_column(: st _ = ’_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\nproperty df()\nReturns the raw Pandas DataFrame\n\n\nproperty x()\nReturns the dataset’s features\n\n\nproperty y()\nReturns the dataset’s target column\n\n\nproperty index()\nReturns the dataset’s index.\n\n\nget_feature_by_id(feature_id)\nReturns the feature with the given id. We also build a lazy lookup cache in case the same feature is requested multiple times.\n\nParameters\nfeature_id (str) – The id of the feature to return\nRaises\nValueError – If the feature with the given id does not exist\nReturns\nThe feature with the given id\nReturn type\ndict\n\n\n\nget_feature_type(feature_id)\nReturns the type of the feature with the given id\n\nParameters\nfeature_id (str) – The id of the feature to return\nReturns\nThe type of the feature with the given id\nReturn type\nstr\n\n\n\nget_numeric_features_columns()\nReturns list of numeric features columns\n\nReturns\nThe list of numberic features columns\nReturn type\nlist\n\n\n\nget_categorical_features_columns()\nReturns list of categorical features columns\n\nReturns\nThe list of categorical features columns\nReturn type\nlist\n\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\ndescribe()\nExtracts descriptive statistics for each field in the dataset\n\n\nget_correlations()\nExtracts correlations for each field in the dataset\n\n\nget_correlation_plots(n_top=15)\nExtracts correlation plots for the n_top correlations in the dataset\n\nParameters\nn_top (int, optional) – The number of top correlations to extract. Defaults to 15.\nReturns\nA list of correlation plots\nReturn type\nlist\n\n\n\nproperty transformed_dataset()\nReturns a transformed dataset that uses the features from vm_dataset. Some of the features in vm_dataset are of type Dummy so we need to reverse the one hot encoding and drop the individual dummy columns\n\nParameters\nforce_refresh (bool, optional) – Whether to force a refresh of the transformed dataset. Defaults to False.\nReturns\nThe transformed dataset\nReturn type\npd.DataFrame\n\n\n\nclassmethod create_from_dict(dict_)\nCreates a Dataset object from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Dataset object from\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\nclassmethod init_from_pd_dataset(df, options=None, targets=None, target_column=None, class_labels=None)\nInitializes a Dataset object from a pandas DataFrame\n\nParameters\n\ndf (pd.DataFrame) – The pandas DataFrame to initialize the Dataset object from\noptions (dict, optional) – The options to use when initializing the Dataset object. Defaults to None.\ntargets (list, optional) – The targets to use when initializing the Dataset object. Defaults to None.\ntarget_column (str, optional) – The target column to use when initializing the Dataset object. Defaults to None.\nclass_labels (list, optional) – The class labels to use when initializing the Dataset object. Defaults to None.\n\nReturns\nThe Dataset object\nReturn type\nDataset\n\n\n\n\nclass validmind.vm_models.DatasetTargets\nBases: object\nDataset targets definition\n\ntarget_column(: st )\n\n\ndescription(: st _ = Non_ )\n\n\nclass_labels(: dic _ = Non_ )\n\n\n\nclass validmind.vm_models.Figure\nBases: object\nFigure objects track the schema supported by the ValidMind API\n\nkey(: st )\n\n\nmetadata(: dic )\n\n\nfigure(: objec )\n\n\nextras(: dict | Non _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Metric\nBases: TestContextUtils\nMetric objects track the schema supported by the ValidMind API\nTODO: Metric should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’Metric_ )\n\n\ntype(: ClassVar[str _ = ’_ )\n\n\nscope(: ClassVar[str _ = ’_ )\n\n\nkey(: ClassVar[str _ = ’_ )\n\n\nvalue_formatter(: ClassVar[str | None _ = Non_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\nresult(: TestPlanMetricResul _ = Non_ )\n\n\nproperty name()\n\n\ndescription()\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(metric_value: dict | list | DataFrame | None = None)\nReturn the metric summary. Should be overridden by subclasses. Defaults to None. The metric summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the metric results.\nWe return None here because the metric summary is optional.\n\n\nrun(*args, **kwargs)\nRun the metric calculation and cache its results\n\n\ncache_results(metric_value: dict | list | DataFrame | None = None, figures: List[Figure] | None = None)\nCache the results of the metric calculation and do any post-processing if needed\n\nParameters\n\nmetric_value (*Union**[dict, list, pd.DataFrame]*) – The value of the metric\nfigures (*Optional**[object]*) – Any figures to attach to the test plan result\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\nrequired_context(: ClassVar[List[str] )\n\n\n\nclass validmind.vm_models.MetricResult\nBases: object\nMetricResult class definition. A MetricResult is returned by any internal method that extracts metrics from a dataset or model, and returns 1) Metric and Figure objects that can be sent to the API and 2) and plots and metadata for display purposes\n\ntype(: st )\n\n\nscope(: st )\n\n\nkey(: dic )\n\n\nvalue(: dict | list | DataFram )\n\n\nsummary(: ResultSummary | Non _ = Non_ )\n\n\nvalue_formatter(: str | Non _ = Non_ )\n\n\nserialize()\nSerializes the Metric to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.Model\nBases: object\nA class that wraps a trained model instance and its associated data.\n\nattributes()\nThe attributes of the model. Defaults to None.\n\nType\nModelAttributes, optional\n\n\n\ntask()\nThe task that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nsubtask()\nThe subtask that the model is intended to solve. Defaults to None.\n\nType\nstr, optional\n\n\n\nparams()\nThe parameters of the model. Defaults to None.\n\nType\ndict, optional\n\n\n\nmodel_id()\nThe identifier of the model. Defaults to “main”.\n\nType\nstr\n\n\n\nmodel()\nThe trained model instance. Defaults to None.\n\nType\nobject, optional\n\n\n\ntrain_ds()\nThe training dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ntest_ds()\nThe test dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\nvalidation_ds()\nThe validation dataset. Defaults to None.\n\nType\nDataset, optional\n\n\n\ny_train_predict()\nThe predicted outputs for the training dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_test_predict()\nThe predicted outputs for the test dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\ny_validation_predict()\nThe predicted outputs for the validation dataset. Defaults to None.\n\nType\nobject, optional\n\n\n\nattributes(: ModelAttribute _ = Non_ )\n\n\ntask(: st _ = Non_ )\n\n\nsubtask(: st _ = Non_ )\n\n\nparams(: dic _ = Non_ )\n\n\nmodel_id(: st _ = ’main_ )\n\n\nmodel(: objec _ = Non_ )\n\n\ntrain_ds(: Datase _ = Non_ )\n\n\ntest_ds(: Datase _ = Non_ )\n\n\nvalidation_ds(: Datase _ = Non_ )\n\n\ny_train_predict(: objec _ = Non_ )\n\n\ny_test_predict(: objec _ = Non_ )\n\n\ny_validation_predict(: objec _ = Non_ )\n\n\nserialize()\nSerializes the model to a dictionary so it can be sent to the API\n\n\nclass_predictions(y_predict)\nConverts a set of probability predictions to class predictions\n\nParameters\ny_predict (np.array, pd.DataFrame) – Predictions to convert\nReturns\nClass predictions\nReturn type\n(np.array, pd.DataFrame)\n\n\n\npredict(*args, **kwargs)\nPredict method for the model. This is a wrapper around the model’s predict_proba (for classification) or predict (for regression) method\nNOTE: This only works for sklearn or xgboost models at the moment\n\n\nstatic model_library(model)\nReturns the model library name\n\n\nstatic model_class(model)\nReturns the model class name\n\n\nstatic is_supported_model(model)\nChecks if the model is supported by the API\n\nParameters\nmodel (object) – The trained model instance to check\nReturns\nTrue if the model is supported, False otherwise\nReturn type\nbool\n\n\n\nclassmethod init_vm_model(model, train_ds, test_ds, validation_ds, attributes)\nInitializes a model instance from the provided data.\n\n\nclassmethod create_from_dict(dict_)\nCreates a Model instance from a dictionary\n\nParameters\ndict (dict) – The dictionary to create the Model instance from\nReturns\nThe Model instance created from the dictionary\nReturn type\nModel\n\n\n\n\nclass validmind.vm_models.ModelAttributes\nBases: object\nModel attributes definition\n\narchitecture(: st _ = Non_ )\n\n\nframework(: st _ = Non_ )\n\n\nframework_version(: st _ = Non_ )\n\n\n\nclass validmind.vm_models.ResultSummary\nBases: object\nA dataclass that holds the summary of a metric or threshold test results\n\nresults(: List[ResultTable )\n\n\nadd_result(result: ResultTable)\nAdds a result to the list of results\n\n\nserialize()\nSerializes the ResultSummary to a list of results\n\n\n\nclass validmind.vm_models.ResultTable\nBases: object\nA dataclass that holds the table summary of result\n\ndata(: Dict[str, Any] | DataFram )\n\n\ntype(: st _ = ’table_ )\n\n\nmetadata(: ResultTableMetadat _ = Non_ )\n\n\nserialize()\nSerializes the Figure to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.ResultTableMetadata\nBases: object\nA dataclass that holds the metadata of a table summary\n\ntitle(: st )\n\n\n\nclass validmind.vm_models.TestContext\nBases: object\nHolds context that can be used by tests to run. Allows us to store data that needs to be reused across different tests/metrics such as shared dataset metrics, etc.\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\nmodels(: List[Model _ = Non_ )\n\n\ncontext_data(: dic _ = Non_ )\n\n\nset_context_data(key, value)\n\n\nget_context_data(key)\n\n\n\nclass validmind.vm_models.TestContextUtils\nBases: object\nUtility methods for classes that receive a TestContext\nTODO: more validation\n\ntest_context(: TestContex )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\nproperty dataset()\n\n\nproperty model()\n\n\nproperty models()\n\n\nproperty df()\nReturns a Pandas DataFrame for the dataset, first checking if we passed in a Dataset or a DataFrame\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\n\nclass validmind.vm_models.TestPlan\nBases: object\nBase class for test plans. Test plans are used to define any arbitrary grouping of tests that will be run on a dataset or model.\n\nname(: ClassVar[str )\n\n\nrequired_context(: ClassVar[List[str] )\n\n\ntests(: ClassVar[List[object] _ = [_ )\n\n\ntest_plans(: ClassVar[List[object] _ = [_ )\n\n\nresults(: ClassVar[List[TestPlanResult] _ = [_ )\n\n\nconfig(: { _ = Non_ )\n\n\ntest_context(: TestContex _ = Non_ )\n\n\ndataset(: Datase _ = Non_ )\n\n\nmodel(: Mode _ = Non_ )\n\n\nmodels(: List[Model _ = Non_ )\n\n\npbar(: tqd _ = Non_ )\n\n\ntitle()\nReturns the title of the test plan. Defaults to the title version of the test plan name\n\n\ndescription()\nReturns the description of the test plan. Defaults to the docstring of the test plan\n\n\nvalidate_context()\nValidates that the context elements are present in the instance so that the test plan can be run\n\n\nget_config_params_for_test(test_name)\nReturns the config for a given test, if it exists. The config attribute is a dictionary where the keys are the test names and the values are dictionaries of config values for that test.\nThe key in the config must match the name of the test, i.e. for a test called “time_series_univariate_inspection_raw” we could pass a config like this:\n{\n“time_series_univariate_inspection_raw”: {\n\n    “columns”: [“col1”, “col2”]\n\n}\n}\n\n\nrun(send=True)\nRuns the test plan\n\n\nlog_results()\nLogs the results of the test plan to ValidMind\nThis method will be called after the test plan has been run and all results have been collected. This method will log the results to ValidMind.\n\n\nsummarize()\nSummarizes the results of the test plan\nThis method will be called after the test plan has been run and all results have been logged to ValidMind. It will summarize the results of the test plan by creating an html table with the results of each test. This html table will be displayed in an VS Code, Jupyter or other notebook environment.\n\n\nget_results(result_id: str | None = None)\nReturns one or more results of the test plan. Includes results from sub test plans.\n\n\n\nclass validmind.vm_models.TestPlanDatasetResult\nBases: TestPlanResult\nResult wrapper for datasets that run as part of a test plan\n\ndataset(: Datase _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanMetricResult\nBases: TestPlanResult\nResult wrapper for metrics that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\nmetric(: MetricResult | Non _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanModelResult\nBases: TestPlanResult\nResult wrapper for models that run as part of a test plan\n\nmodel(: Mode _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestPlanTestResult\nBases: TestPlanResult\nResult wrapper for test results produced by the tests that run as part of a test plan\n\nfigures(: List[Figure] | Non _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\nlog()\nLog the result… Must be overridden by subclasses\n\n\n\nclass validmind.vm_models.TestResult\nBases: object\nTestResult model\n\nvalues(: dic )\n\n\ntest_name(: str | Non _ = Non_ )\n\n\ncolumn(: str | Non _ = Non_ )\n\n\npassed(: bool | Non _ = Non_ )\n\n\nserialize()\nSerializes the TestResult to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.TestResults\nBases: object\nTestResults model\n\ncategory(: st )\n\n\ntest_name(: st )\n\n\nparams(: dic )\n\n\npassed(: boo )\n\n\nresults(: List[TestResult )\n\n\nsummary(: ResultSummary | Non )\n\n\nserialize()\nSerializes the TestResults to a dictionary so it can be sent to the API\n\n\n\nclass validmind.vm_models.TestSuite\nBases: TestPlan\nBase class for test suites. Test suites are used to define any arbitrary grouping of test plans that will be run on a dataset and/or model.\n\ntest_plans(: ClassVar[List[str] _ = [_ )\n\n\nrun(send=True)\nRuns the test suite.\n\n\nproperty results()\nReturns the results of the test suite.\n\n\n\nclass validmind.vm_models.ThresholdTest\nBases: TestContextUtils\nA threshold test is a combination of a metric/plot we track and a corresponding set of parameters and thresholds values that allow us to determine whether the metric/plot passes or fails.\nTODO: ThresholdTest should validate required context too\n\ntest_context(: TestContex )\n\n\ntest_type(: ClassVar[str _ = ’ThresholdTest_ )\n\n\ncategory(: ClassVar[str _ = ’_ )\n\n\nname(: ClassVar[str _ = ’_ )\n\n\ndefault_params(: ClassVar[dict _ = {_ )\n\n\nparams(: dic _ = Non_ )\n\n\ntest_results(: TestResult _ = Non_ )\n\n\ndescription()\nReturn the test description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\nsummary(test_results: TestResults | None = None)\nReturn the threshold test summary. Should be overridden by subclasses. Defaults to None. The test summary allows renderers (e.g. Word and ValidMind UI) to display a short summary of the test results.\nWe return None here because the test summary is optional.\n\n\nrun(*args, **kwargs)\nRun the test and cache its results\n\n\ncache_results(results: List[TestResult], passed: bool, figures: List[Figure] | None = None)\nCache the individual results of the threshold test as a list of TestResult objects\n\nParameters\n\nresults (*List**[TestResult]*) – The results of the threshold test\npassed (bool) – Whether the threshold test passed or failed\n\nReturns\nThe test plan result object\nReturn type\nTestPlanResult\n\n\n\nrequired_context(: ClassVar[List[str] )"
  },
  {
    "objectID": "validmind/model_validation_tests_sklearn.html",
    "href": "validmind/model_validation_tests_sklearn.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions models trained with sklearn or that provide a sklearn-like API\n\n\n\nBases: Metric\nAccuracy Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCharacteristic Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculates PSI for each of the dataset features\n\n\n\n\nBases: Metric\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nF1 Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPermutation Feature Importance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPrecision Recall Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPrecision Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nRecall Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nROC AUC Score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nROC Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nSHAP Global Importance\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPopulation Stability Index between two datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nTest that the model’s prediction accuracy on a dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the model’s F1 score on the validation dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the model’s ROC AUC score on the validation dataset meets or exceeds a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the degradation in performance between the training and test datasets does not exceed a predefined threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that identify overfit regions with high residuals by histogram slicing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that identify weak regions with high residuals by histogram slicing techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest robustness of model by perturbing the features column values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\nAdds Gaussian noise to a list of values.\n\nParameters\n\nvalues (*list**[float]*) – A list of numerical values to which noise is added.\nx_std_dev (float) – A scaling factor for the standard deviation of the noise.\n\nReturns\nA tuple containing:\n  * A list of noisy values, where each value is the sum of the corresponding value\n\n  in the input list and a randomly generated value sampled from a Gaussian distribution\n  with mean 0 and standard deviation x_std_dev times the standard deviation of the input list.\n  - The standard deviation of the input list of values.\nReturn type\ntuple[list[float], float]"
  },
  {
    "objectID": "validmind/readme.html",
    "href": "validmind/readme.html",
    "title": "ValidMind",
    "section": "",
    "text": "pip install validmind\n\n\npip install validmind[r-support]\n\n\n\n\n\n\n\n\nEnsure you have poetry installed: https://python-poetry.org/\nAfter cloning this repo run:\n\npoetry shell\npoetry install\n\n\n\nIf you want to use the R support that is provided by the ValidMind Developer Framework, you must have R installed on your machine. You can download R from https://cran.r-project.org/. If you are on a Mac, you can install R using Homebrew:\nbrew install r\nOnce you have R installed, you can install the r-support extra to install the necessary dependencies for R by running:\npoetry install --extras r-support\n\n\n\nMake sure you bump the package version before merging a PR with the following command:\nmake version tag=patch\nThe value of tag corresponds to one of the options provided by Poetry: https://python-poetry.org/docs/cli/#version\n\n\n\n\nIf you want to integate the validmind package to your development environment, you must build the package wheel first, since we have not pushed the package to a public PyPI repository yet. Steps:\n\nRun make build to build a new Python package for the developer framework\nThis will create a new wheel file in the dist folder\nRun pip install <path-to-wheel> to install the newly built package in your environment\n\n\n\n\nAPI documentation can be generated in Markdown or HTML format. Our documentation pipeline uses Markdown documentation before generating the final HTML assets for the documentation site.\nFor local testing, HTML docs can be generated with Sphinx. Note that the output template is different since the documentation pipeline uses the source Markdown files for the final HTML output.\nMarkdown and HTML docs can be generated with the following commands:\n# Navigate to the docs folder\ncd docs/\n\n# Generate HTML and Markdown docs\nmake docs\n\n# Generate Markdown docs only\nmake docs-markdown\n\n# Generate HTML docs only\nmake docs-html\nThe resulting markdown and html under docs/_build folders will contain the generated documentation.\n\n\n\n\n\nIf you run into an error related to the ValidMind wheel, try:\npoetry add wheel\npoetry update wheel\npoetry install\nIf there are lightgbm errors partway through, run remove lightgbm, followed by poetry update wheel and poetry install."
  },
  {
    "objectID": "validmind/test_plans.html",
    "href": "validmind/test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "Test Plans entry point\n\n\nReturns a list of all available test plans\n\n\n\nReturns a list of all available tests.\n\n\n\nReturns the test plan by name\n\n\n\nReturns a description of the test plan\n\n\n\nRegisters a custom test plan\n\n\n\n\n\n\nTest plan for tabular datasets\nIdeal setup is to have the API client to read a custom test plan from the project’s configuration\n\n\nBases: TestPlan\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on tabular datasets\n\n\n\n\n\n\n\n\n\n\n\n\nBases: TestPlan\nTest plan for data quality on time series datasets"
  },
  {
    "objectID": "validmind/index.html",
    "href": "validmind/index.html",
    "title": "ValidMind",
    "section": "",
    "text": "ValidMind Developer Framework\n\nValidMind Python Client\n\nInstallation\nContributing to ValidMind Developer Framework\nIntegrating the ValidMind Developer Framework to your development environment\nGenerating Docs\nKnown Issues\n\nPython Library API\nCore Library Tests\n\nData Validation Tests\n\nCore Library Tests\n\nModel Validation Tests for SKLearn-Compatible Models\n\nTest Plans\n\nlist_plans()\nlist_tests()\nget_by_name()\ndescribe_plan()\nregister_test_plan()\nTest Plans for SKLearn-Compatible Classifiers\nTest Plans for Tabular Datasets\n\nValidMind Models"
  },
  {
    "objectID": "validmind/data_validation_tests.html",
    "href": "validmind/data_validation_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "Metrics functions for any Pandas-compatible datasets\n\n\n\nBases: TestContextUtils\nCustom class to collect a set of descriptive statistics for a dataset. This class will log dataset metadata via log_dataset instead of a metric. Dataset metadata is necessary to initialize dataset object that can be related to different metrics and test results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust set the dataset to the result attribute of the test plan result and it will be logged via the log_dataset function\n\n\n\n\nBases: Metric\nExtracts the correlation matrix for a dataset. The following coefficients are calculated: - Pearson’s R for numerical variables - Cramer’s V for categorical variables - Correlation ratios for categorical-numerical variables\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCollects a set of descriptive statistics for a dataset, both for numerical and categorical variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild two tables: one for summarizing numerical variables and one for categorical variables\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAttempts to extract information about the dataset split from the provided training, test and validation datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn the metric description. Should be overridden by subclasses. Defaults to returning the class’ docstring\n\n\n\nReturns a summarized representation of the dataset split information\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the raw time series. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of time series data by plotting the histogram. The input dataset can have multiple time series if necessary. In this case we produce a separate plot for each time series.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a visual analysis of data by plotting a scatter plot matrix for all columns in the dataset. The input dataset can have multiple columns (features) if necessary.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nGenerates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the AR order of a time series using both BIC and AIC.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the MA order of a time series using both BIC and AIC.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nCalculates seasonal_decompose metric for each of the dataset features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStores the seasonal decomposition results in the test context so they can be re-used by other tests. Note we store one sd at a time for every column in the dataset.\n\n\n\nSerializes the seasonal decomposition results for one column into a JSON serializable format that can be sent to the API.\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects the optimal seasonal order for a time series dataset using the seasonal_decompose method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nPlots ACF and PACF for a given time series dataset.\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nAutomatically detects stationarity for each time series in a DataFrame using the Augmented Dickey-Fuller (ADF) test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nThis class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot rolling mean and rolling standard deviation in different subplots for a given series.\n\nParameters\n\nseries – Pandas Series with time-series data\nwindow_size – Window size for the rolling calculations\nax1 – Axis object for the rolling mean plot\nax2 – Axis object for the rolling standard deviation plot\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nTest for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nBases: Metric\nThis class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n\n\n\n\n\n\n\n\n\n\n\nPlot the spread between two time series variables.\n\nParameters\n\nseries1 – Pandas Series with time-series data for the first variable\nseries2 – Pandas Series with time-series data for the second variable\nax – Axis object for the spread plot\n\n\n\n\n\nRun the metric calculation and cache its results\n\n\n\n\nThreshold based tests\n\n\n\nBases: ThresholdTest\nThe class imbalance test measures the disparity between the majority class and the minority class in the target column.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe duplicates test measures the number of duplicate rows found in the dataset. If a primary key column is specified, the dataset is checked for duplicate primary keys as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe high cardinality test measures the number of unique values found in categorical columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the pairwise Pearson correlation coefficients between the features in the dataset do not exceed a specified threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values in the dataset across all features is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe skewness test measures the extent to which a distribution of values differs from a normal distribution. A positive skew describes a longer tail of values in the right and a negative skew describes a longer tail of values in the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of unique rows is greater than a threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nThe zeros test finds columns that have too many zero values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that find outliers for time series data using the z-score method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that the number of missing values is less than a threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results\n\n\n\n\nBases: ThresholdTest\nTest that detect frequencies in the data\n\n\n\n\n\n\n\n\n\n\n\nRun the test and cache its results"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to our documentation",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Documentation\n                            The guide to elevating your MRM workflow\n                            Need help? Find all the information you need to use our platform for model risk management (MRM).\n                            \n                            \n                                \n                                    \n                                        \n                                            \n                                        \n                                        \n                                                                \n                                        \n                                    \n                                \n                            \n\n                        \n                    \n                \n                \n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating the key aspects of the model risk management process, ValidMind is a MRM solution designed for the unique needs of model developers and validators.\n                    Model Documentation Automation\n                    MRM Lifecycle and Workflow\n                    Communication & TrackingInstructional GuidesGet Started\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Collect, manage, and automate your model documentation and testing with our Developer Framework.Collaboration for Model Developers\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and documentation to ensure they comply with organizational & regulatory requirements.Collaboration for Model Validators\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "notebooks/passing_data.html",
    "href": "notebooks/passing_data.html",
    "title": "ValidMind",
    "section": "",
    "text": "In this notebook we will demonstrate how to make use of the test context to pass data between tests. Any object can be passed to the context, but we will use a simple dictionary to demonstrate the concept.\n\n%load_ext dotenv\n%dotenv dev.env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  # Use your project ID\n  project = \"...\"\n)\n  \n\nTrue\n\n\n\n\nWe will build two simple metrics to demonstrate the concept. The first metric will create a sample dataframe with mock data, and the second will add one more column to it.\n\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MyFirstMetric(Metric):\n    type = \"dataset\"\n    key = \"my_first_metric\"\n\n    def run(self):\n        df = pd.DataFrame({\"column_a\": [1, 2, 3, 4, 5]})\n        # Store the dataframe in the test context\n        self.test_context.set_context_data(\"some_dataset\", df)\n\n        return self.cache_results(df.to_dict(\"records\"))\n\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MySecondMetric(Metric):\n    type = \"dataset\"\n    key = \"my_second_metric\"\n\n    def run(self):\n        # Get the dataframe from the test context. We can\n        # throw an error if it doesn't exist just to be sure\n        df = self.test_context.get_context_data(\"some_dataset\")\n        if df is None:\n            raise ValueError(\"'some_dataset' not found in test context\")\n        \n        new_df = df.copy()\n        new_df[\"column_b\"] = [5, 4, 3, 2, 1]\n\n        return self.cache_results(new_df.to_dict(\"records\"))\n\nNow let’s define a test plan that will run the two metrics in sequence. We need to make sure that the first metric is run before the second one.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MyFirstMetric, MySecondMetric]\n\nmy_custom_test_plan = MyCustomTestPlan()\nmy_custom_test_plan.run()\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        my_first_metric\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'column_a': 1}, {'column_a': 2}, {'column_a': 3}, {'column_a': 4}, {'column_a': 5}]\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        my_second_metric\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'column_a': 1, 'column_b': 5}, {'column_a': 2, 'column_b': 4}, {'column_a': 3, 'column_b': 3}, {'column_a': 4, 'column_b': 2}, {'column_a': 5, 'column_b': 1}]"
  },
  {
    "objectID": "notebooks/accessing_test_plan_results.html",
    "href": "notebooks/accessing_test_plan_results.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load dotenv extension for environment variables\n%load_ext dotenv\n# Tell dotenv to read the dev.env file\n%dotenv dev.env\n# Disable test plan summarization for this notebook \n%env VM_SUMMARIZE_TEST_PLANS False\n\nenv: VM_SUMMARIZE_TEST_PLANS=False\n\n\nIt is possible to access all the test results for a test plan when it has finished executing. This allows inspecting the results of each test, whether it is a metric, a threshold test or a figure, and the results of subtest plans if they exist.\nIn this notebook we’ll run the timeseries test plan on a demo dataset and inspect the results.\n\n# ML libraries\nimport pandas as pd\n\n# ValidMind libraries \nimport validmind as vm\n\n%matplotlib inline\n\nWe load the dataset as usual:\n\ndf = pd.read_csv(\"./datasets/lending_club_loan_rates.csv\", sep='\\t')\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\n\n# Remove diff columns\ncolumns_to_remove = [col for col in df.columns if col.startswith(\"diff\")]\ndf = df.drop(columns=columns_to_remove)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n    \n  \n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clgyd137o0000pi8hcn9oukz7\"\n)\n\nTrue\n\n\n\n\nInitialize the VM dataset:\n\ntarget_variables = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column = target_variables   \n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nRun the “timeseries” test plan:\n\nloan_rate_columns = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\n\ntest_plan_config = {\n    \"time_series_univariate_inspection_raw\": {\n        \"columns\": loan_rate_columns\n    },\n    \"time_series_univariate_inspection_histogram\": {\n        \"columns\": loan_rate_columns\n    }\n}\n\ntimeseries_plan = vm.run_test_plan(\n    \"timeseries\",\n    config=test_plan_config,    \n    test_ds=vm_dataset,\n    train_ds=vm_dataset,\n    dataset=vm_dataset,\n)\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n\n\n\n\n\nWe can now access all the results of the test plan, including subtest plans using test_plan.get_results().\n\ntest_plan.get_results(): With no arguments, this returns a list of all results\ntest_plan.get_results(test_id): If provided with a test id, this returns the all results that match the given test id\n\nBy default, get_results() returns a list, in case there are multiple tests with the same id.\n\ntimeseries_plan.get_results()\n\n[TestPlanMetricResult(result_id=\"time_series_univariate_inspection_histogram\", figures),\n TestPlanMetricResult(result_id=\"time_series_univariate_inspection_raw\", figures),\n TestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures),\n TestPlanMetricResult(result_id=\"seasonality_detection_with_acf_and_pacf\", metric, figures),\n TestPlanMetricResult(result_id=\"residuals_visual_inspection\", figures),\n TestPlanMetricResult(result_id=\"ljung_box\", metric, figures),\n TestPlanMetricResult(result_id=\"box_pierce\", metric, figures),\n TestPlanMetricResult(result_id=\"runs_test\", metric, figures),\n TestPlanMetricResult(result_id=\"jarque_bera\", metric, figures),\n TestPlanMetricResult(result_id=\"kolmogorov_smirnov\", metric, figures),\n TestPlanMetricResult(result_id=\"shapiro_wilk\", metric, figures),\n TestPlanMetricResult(result_id=\"lilliefors_test\", metric, figures),\n TestPlanMetricResult(result_id=\"adf\", metric, figures),\n TestPlanMetricResult(result_id=\"kpss\", metric, figures),\n TestPlanMetricResult(result_id=\"phillips_perron\", metric, figures),\n TestPlanMetricResult(result_id=\"zivot_andrews\", metric, figures),\n TestPlanMetricResult(result_id=\"dickey_fuller_gls\", metric, figures),\n TestPlanMetricResult(result_id=\"adf\", metric, figures),\n TestPlanMetricResult(result_id=\"kpss\", metric, figures),\n TestPlanMetricResult(result_id=\"phillips_perron\", metric, figures),\n TestPlanMetricResult(result_id=\"zivot_andrews\", metric, figures),\n TestPlanMetricResult(result_id=\"dickey_fuller_gls\", metric, figures)]\n\n\n\nseasonal_decomposition = timeseries_plan.get_results(\"seasonal_decompose\")[0]\nseasonal_decomposition\n\nTestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures)\n\n\n\nseasonal_decomposition.show()\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        seasonal_decompose\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'loan_rate_A': [{'Date': '2007-08-01', 'loan_rate_A': 7.7666666666666675, 'trend': nan, 'seasonal': 0.010868499397109575, 'resid': nan}, {'Date': '2007-09-01', 'loan_rate_A': 7.841428571428572, 'trend': nan, 'seasonal': 0.0013181145386968443, 'resid': nan}, {'Date': '2007-10-01', 'loan_rate_A': 7.83, 'trend': nan, 'seasonal': 0.04951104761291364, 'resid': nan}, {'Date': '2007-11-01', 'loan_rate_A': 7.779090909090908, 'trend': nan, 'seasonal': -0.034519089222105774, 'resid': nan}, {'Date': '2007-12-01', 'loan_rate_A': 7.695833333333333, 'trend': nan, 'seasonal': 0.06051880355383553, 'resid': nan}, {'Date': '2008-01-01', 'loan_rate_A': 7.961333333333333, 'trend': nan, 'seasonal': 0.029051115578029918, 'resid': nan}, {'Date': '2008-02-01', 'loan_rate_A': 8.130333333333333, 'trend': 8.005048767959094, 'seasonal': -0.010261300106122759, 'resid': 0.13554586548036146}, {'Date': '2008-03-01', 'loan_rate_A': 8.126285714285714, 'trend': 8.036669799705125, 'seasonal': -0.06065012545155052, 'resid': 0.15026604003213978}...\n                    \n                \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n\n\n\nseasonal_decomposition.metric.value[\"loan_rate_A\"][0:5]\n\n[{'Date': '2007-08-01',\n  'loan_rate_A': 7.7666666666666675,\n  'trend': nan,\n  'seasonal': 0.010868499397109575,\n  'resid': nan},\n {'Date': '2007-09-01',\n  'loan_rate_A': 7.841428571428572,\n  'trend': nan,\n  'seasonal': 0.0013181145386968443,\n  'resid': nan},\n {'Date': '2007-10-01',\n  'loan_rate_A': 7.83,\n  'trend': nan,\n  'seasonal': 0.04951104761291364,\n  'resid': nan},\n {'Date': '2007-11-01',\n  'loan_rate_A': 7.779090909090908,\n  'trend': nan,\n  'seasonal': -0.034519089222105774,\n  'resid': nan},\n {'Date': '2007-12-01',\n  'loan_rate_A': 7.695833333333333,\n  'trend': nan,\n  'seasonal': 0.06051880355383553,\n  'resid': nan}]"
  },
  {
    "objectID": "notebooks/quickstart_classification.html",
    "href": "notebooks/quickstart_classification.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebooks provides a quick introduction to documenting a model using the ValidMind developer framework. We will use sample datasets provided by the library and train a simple classification model.\n\n\n\n%load_ext dotenv\n%dotenv .env\n%matplotlib inline\n\nimport validmind as vm\nimport xgboost as xgb\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhmm220k0006558hv20npy8z\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhmm220k0006558hv20npy8z)\n\n\n\n\n\n\n# You can also import taiwan_credit like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)\n\n\n\n\n\n\n\n\n\n\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "This interactive notebook will guide you through documenting a model using the ValidMind Developer framework. We will use sample datasets provided by the library and train a simple classification model.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nInitializing the ValidMind Developer Framework\nUsing a sample datasets provided by the library to train a simple classification model\nRunning a test suite to quickly generate document about the data and model\n\n\n\nClick File > Save a copy in Drive > to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n\n\n\n\n!pip install validmind\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: validmind in /usr/local/lib/python3.10/dist-packages (1.11.6)\nRequirement already satisfied: arch<6.0.0,>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (5.5.0)\nRequirement already satisfied: catboost<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.2)\nRequirement already satisfied: click<9.0.0,>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from validmind) (8.1.3)\nRequirement already satisfied: dython<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.7.2)\nRequirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (7.34.0)\nRequirement already satisfied: markdown<4.0.0,>=3.4.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (3.4.3)\nRequirement already satisfied: myst-parser<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.0.0)\nRequirement already satisfied: numpy==1.22.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.22.3)\nRequirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.5.3)\nRequirement already satisfied: pandas-profiling<4.0.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from validmind) (3.6.6)\nRequirement already satisfied: pydantic<2.0.0,>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.10.7)\nRequirement already satisfied: pypmml<0.10.0,>=0.9.17 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.9.17)\nRequirement already satisfied: python-dotenv<0.21.0,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.20.0)\nRequirement already satisfied: requests<3.0.0,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from validmind) (2.27.1)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.2.2)\nRequirement already satisfied: seaborn<0.12.0,>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.11.2)\nRequirement already satisfied: shap<0.42.0,>=0.41.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.41.0)\nRequirement already satisfied: sphinx<7.0.0,>=6.1.3 in /usr/local/lib/python3.10/dist-packages (from validmind) (6.2.1)\nRequirement already satisfied: sphinx-markdown-builder<0.6.0,>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.5.5)\nRequirement already satisfied: sphinx-rtd-theme<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.2.0)\nRequirement already satisfied: statsmodels<0.14.0,>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.13.5)\nRequirement already satisfied: tabulate<0.9.0,>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from validmind) (0.8.10)\nRequirement already satisfied: tqdm<5.0.0,>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from validmind) (4.64.1)\nRequirement already satisfied: xgboost<2.0.0,>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from validmind) (1.7.5)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (67.7.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (0.18.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (3.0.38)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (2.14.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (0.1.6)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->validmind) (4.8.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->validmind) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->validmind) (2022.7.1)\nRequirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.10/dist-packages (from arch<6.0.0,>=5.4.0->validmind) (1.9.3)\nRequirement already satisfied: property-cached>=1.6.4 in /usr/local/lib/python3.10/dist-packages (from arch<6.0.0,>=5.4.0->validmind) (1.6.4)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<2.0,>=1.2->validmind) (0.20.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost<2.0,>=1.2->validmind) (3.6.3)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<2.0,>=1.2->validmind) (5.13.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<2.0,>=1.2->validmind) (1.16.0)\nRequirement already satisfied: scikit-plot>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from dython<0.8.0,>=0.7.1->validmind) (0.3.7)\nRequirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.10/dist-packages (from dython<0.8.0,>=0.7.1->validmind) (5.9.5)\nRequirement already satisfied: docutils<0.20,>=0.15 in /usr/local/lib/python3.10/dist-packages (from myst-parser<2.0.0,>=1.0.0->validmind) (0.18.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from myst-parser<2.0.0,>=1.0.0->validmind) (3.1.2)\nRequirement already satisfied: markdown-it-py<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from myst-parser<2.0.0,>=1.0.0->validmind) (2.2.0)\nRequirement already satisfied: mdit-py-plugins~=0.3.4 in /usr/local/lib/python3.10/dist-packages (from myst-parser<2.0.0,>=1.0.0->validmind) (0.3.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from myst-parser<2.0.0,>=1.0.0->validmind) (6.0)\nRequirement already satisfied: ydata-profiling in /usr/local/lib/python3.10/dist-packages (from pandas-profiling<4.0.0,>=3.6.6->validmind) (4.1.2)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.9.1->validmind) (4.5.0)\nRequirement already satisfied: py4j>=0.10.7 in /usr/local/lib/python3.10/dist-packages (from pypmml<0.10.0,>=0.9.17->validmind) (0.10.9.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->validmind) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->validmind) (2022.12.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->validmind) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.27.1->validmind) (3.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.2->validmind) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.2->validmind) (3.1.0)\nRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.41.0->validmind) (23.1)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.41.0->validmind) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.41.0->validmind) (0.56.4)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.41.0->validmind) (2.2.1)\nRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (1.0.4)\nRequirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (1.0.2)\nRequirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (1.0.1)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (2.0.1)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (1.1.5)\nRequirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (1.0.3)\nRequirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (2.12.1)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (0.7.13)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0.0,>=6.1.3->validmind) (1.4.1)\nRequirement already satisfied: html2text in /usr/local/lib/python3.10/dist-packages (from sphinx-markdown-builder<0.6.0,>=0.5.5->validmind) (2020.1.16)\nRequirement already satisfied: pydash in /usr/local/lib/python3.10/dist-packages (from sphinx-markdown-builder<0.6.0,>=0.5.5->validmind) (7.0.3)\nRequirement already satisfied: unify in /usr/local/lib/python3.10/dist-packages (from sphinx-markdown-builder<0.6.0,>=0.5.5->validmind) (0.5)\nRequirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from sphinx-markdown-builder<0.6.0,>=0.5.5->validmind) (0.33.0)\nRequirement already satisfied: sphinxcontrib-jquery!=3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme<2.0.0,>=1.2.0->validmind) (4.1)\nRequirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels<0.14.0,>=0.13.5->validmind) (0.5.3)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->validmind) (0.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->myst-parser<2.0.0,>=1.0.0->validmind) (2.1.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=1.0.0->myst-parser<2.0.0,>=1.0.0->validmind) (0.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost<2.0,>=1.2->validmind) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost<2.0,>=1.2->validmind) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost<2.0,>=1.2->validmind) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost<2.0,>=1.2->validmind) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost<2.0,>=1.2->validmind) (8.4.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost<2.0,>=1.2->validmind) (3.0.9)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->validmind) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->validmind) (0.2.6)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap<0.42.0,>=0.41.0->validmind) (0.39.1)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<2.0,>=1.2->validmind) (8.2.2)\nRequirement already satisfied: untokenize in /usr/local/lib/python3.10/dist-packages (from unify->sphinx-markdown-builder<0.6.0,>=0.5.5->validmind) (0.1.1)\nRequirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->sphinx-markdown-builder<0.6.0,>=0.5.5->validmind) (2.0.1)\nRequirement already satisfied: visions[type_image_path]==0.7.5 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (0.7.5)\nRequirement already satisfied: htmlmin==0.1.12 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (0.1.12)\nRequirement already satisfied: phik<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (0.12.3)\nRequirement already satisfied: multimethod<1.10,>=1.4 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (1.9.1)\nRequirement already satisfied: typeguard<2.14,>=2.13.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (2.13.3)\nRequirement already satisfied: imagehash==4.3.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (4.3.1)\nRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (1.4.1)\nRequirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (23.1.0)\nRequirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (3.1)\nRequirement already satisfied: tangled-up-in-unicode>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling<4.0.0,>=3.6.6->validmind) (0.2.0)\n\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue with the next cell.\n##Initializing the Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nLog in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n\n\n(Note: if a documentation project has already been created, you can skip this section and head directly “Finding Project API key and secret”)\nClicking on “Create a new project” allows to you to register a new documentation project for our demo model.\nSelect “Customer Churn model” from the Model drop-down, and “Initial Validation” as Type. Finally, click on “Create Project”.\n\n\n\nIn the “Client Integration” page of the newly created project, you will find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\napi_host: Location of the ValidMind API.\napi_key: Account API key.\napi_secret: Account Secret key.\nproject: The project identifier. The project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nThe code snippet can be copied and pasted directly in the cell below to initialize the ValidMind Developer Framework when run:\n\n## Replace the code below with the code snippet from your project ## \n\n\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"b21d0d2e5bdaaaa550a7996e89a2354e\",\n  api_secret = \"3b56efe570096f7d3dc60d42c66a56fa92da94ad0e280a58cb6e8ccca035c664\",\n  project = \"clhowg73e001s1pk10uouvsde\"\n)\n  \n  \n  \n\nConnected to ValidMind. Project: [Quickstart] Customer Churn Model - Initial Validation (clhowg73e001s1pk10uouvsde)\n\n\n\n\n\n\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with: \n#from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\n\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\nWe can now initialize the training and test datasets into dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe also initialize a model object using vm.init_model():\n\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\n\nWe are now ready to run the test suite for binary classifier with tabular datasets. This function will run test plans on the dataset and model objects, and will document the results in the ValidMind UI.\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\n\n\n\n\n\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/test_suites_test_plans.html",
    "href": "notebooks/test_suites_test_plans.html",
    "title": "ValidMind",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv dev.env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clho9pot100005n8hz1574d8w\"\n)\n\nConnected to ValidMind. Project: Custom Name (clho9pot100005n8hz1574d8w)\n\n\n\nvm.test_suites.list_suites()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_full_suite\n      BinaryClassifierFullSuite\n      Full test suite for binary classification models.\n      tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      binary_classifier_model_validation\n      BinaryClassifierModelValidation\n      Test suite for binary classification models.\n      binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      tabular_dataset\n      TabularDataset\n      Test suite for tabular datasets.\n      tabular_dataset_description, tabular_data_quality\n    \n    \n      time_series_dataset\n      TimeSeriesDataset\n      Test suite for time series datasets.\n      time_series_data_quality, time_series_univariate, time_series_multivariate\n    \n    \n      time_series_model_validation\n      TimeSeriesModelValidation\n      Test suite for time series model validation.\n      regression_model_performance, regression_models_comparison, time_series_forecast\n    \n  \n\n\n\n\nvm.test_plans.describe_plan(\"binary_classifier_validation\")\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Required Context\n      Tests\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n      model\n      MinimumAccuracy (ThresholdTest), MinimumF1Score (ThresholdTest), MinimumROCAUCScore (ThresholdTest), TrainingTestDegradation (ThresholdTest)\n      \n    \n  \n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\n  \n    \n      Test Type\n      ID\n      Name\n      Description\n    \n  \n  \n    \n      Custom Test\n      classifier_in_sample_performance\n      ClassifierInSamplePerformance\n      Test that outputs the performance of the model on the training data.\n    \n    \n      Custom Test\n      classifier_out_of_sample_performance\n      ClassifierOutOfSamplePerformance\n      Test that outputs the performance of the model on the test data.\n    \n    \n      Custom Test\n      dataset_metadata\n      DatasetMetadata\n      Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadata is necessary to initialize dataset object that can be related\n    to different metrics and test results\n    \n    \n      Metric\n      \n      ACFandPACFPlot\n      Plots ACF and PACF for a given time series dataset.\n    \n    \n      Metric\n      \n      AutoAR\n      Automatically detects the AR order of a time series using both BIC and AIC.\n    \n    \n      Metric\n      \n      AutoMA\n      Automatically detects the MA order of a time series using both BIC and AIC.\n    \n    \n      Metric\n      \n      AutoSeasonality\n      Automatically detects the optimal seasonal order for a time series dataset\n    using the seasonal_decompose method.\n    \n    \n      Metric\n      \n      AutoStationarity\n      Automatically detects stationarity for each time series in a DataFrame\n    using the Augmented Dickey-Fuller (ADF) test.\n    \n    \n      Metric\n      \n      CharacteristicStabilityIndex\n      Characteristic Stability Index between two datasets\n    \n    \n      Metric\n      \n      ConfusionMatrix\n      Confusion Matrix\n    \n    \n      Metric\n      \n      DatasetCorrelations\n      Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables\n    \n    \n      Metric\n      \n      DatasetDescription\n      Collects a set of descriptive statistics for a dataset\n    \n    \n      Metric\n      \n      DatasetSplit\n      Attempts to extract information about the dataset split from the\n    provided training, test and validation datasets.\n    \n    \n      Metric\n      \n      DescriptiveStatistics\n      Collects a set of descriptive statistics for a dataset, both for\n    numerical and categorical variables\n    \n    \n      Metric\n      \n      EngleGrangerCoint\n      Test for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.\n    \n    \n      Metric\n      \n      LaggedCorrelationHeatmap\n      Generates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.\n    \n    \n      Metric\n      \n      ModelMetadata\n      Custom class to collect the following metadata for a model:\n    - Model architecture\n    - Model hyperparameters\n    - Model task type\n    \n    \n      Metric\n      \n      PermutationFeatureImportance\n      Permutation Feature Importance\n    \n    \n      Metric\n      \n      PopulationStabilityIndex\n      Population Stability Index between two datasets\n    \n    \n      Metric\n      \n      PrecisionRecallCurve\n      Precision Recall Curve\n    \n    \n      Metric\n      \n      ROCCurve\n      ROC Curve\n    \n    \n      Metric\n      \n      RegressionModelForecastPlot\n      This metric creates a plot of forecast vs observed for each model in the list.\n    \n    \n      Metric\n      \n      RegressionModelInsampleComparison\n      Test that output the comparison of stats library regression models.\n    \n    \n      Metric\n      \n      RegressionModelOutsampleComparison\n      Test that evaluates the performance of different regression models on a separate test dataset\n    that was not used to train the models.\n    \n    \n      Metric\n      \n      RegressionModelSummary\n      Test that output the summary of regression models of statsmodel library.\n    \n    \n      Metric\n      \n      RollingStatsPlot\n      This class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\n    \n    \n      Metric\n      \n      SHAPGlobalImportance\n      SHAP Global Importance\n    \n    \n      Metric\n      \n      ScatterPlot\n      Generates a visual analysis of data by plotting a scatter plot matrix for all columns\n    in the dataset. The input dataset can have multiple columns (features) if necessary.\n    \n    \n      Metric\n      \n      SeasonalDecompose\n      Calculates seasonal_decompose metric for each of the dataset features\n    \n    \n      Metric\n      \n      SpreadPlot\n      This class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.\n    \n    \n      Metric\n      \n      TimeSeriesHistogram\n      Generates a visual analysis of time series data by plotting the\n    histogram. The input dataset can have multiple time series if\n    necessary. In this case we produce a separate plot for each time series.\n    \n    \n      Metric\n      \n      TimeSeriesLinePlot\n      Generates a visual analysis of time series data by plotting the\n    raw time series. The input dataset can have multiple time series\n    if necessary. In this case we produce a separate plot for each time series.\n    \n    \n      ThresholdTest\n      class_imbalance\n      ClassImbalance\n      The class imbalance test measures the disparity between the majority\n    class and the minority class in the target column.\n    \n    \n      ThresholdTest\n      duplicates\n      Duplicates\n      The duplicates test measures the number of duplicate rows found in\n    the dataset. If a primary key column is specified, the dataset is\n    checked for duplicate primary keys as well.\n    \n    \n      ThresholdTest\n      cardinality\n      HighCardinality\n      The high cardinality test measures the number of unique\n    values found in categorical columns.\n    \n    \n      ThresholdTest\n      pearson_correlation\n      HighPearsonCorrelation\n      Test that the pairwise Pearson correlation coefficients between the\n    features in the dataset do not exceed a specified threshold.\n    \n    \n      ThresholdTest\n      accuracy_score\n      MinimumAccuracy\n      Test that the model's prediction accuracy on a dataset meets or\n    exceeds a predefined threshold.\n    \n    \n      ThresholdTest\n      f1_score\n      MinimumF1Score\n      Test that the model's F1 score on the validation dataset meets or\n    exceeds a predefined threshold.\n    \n    \n      ThresholdTest\n      roc_auc_score\n      MinimumROCAUCScore\n      Test that the model's ROC AUC score on the validation dataset meets or\n    exceeds a predefined threshold.\n    \n    \n      ThresholdTest\n      missing\n      MissingValues\n      Test that the number of missing values in the dataset across all features\n    is less than a threshold\n    \n    \n      ThresholdTest\n      overfit_regions\n      OverfitDiagnosis\n      Test that identify overfit regions with high residuals by histogram slicing techniques.\n    \n    \n      ThresholdTest\n      robustness\n      RobustnessDiagnosis\n      Test robustness of model by perturbing the features column values by adding noise within scale\n    stardard deviation.\n    \n    \n      ThresholdTest\n      skewness\n      Skewness\n      The skewness test measures the extent to which a distribution of\n    values differs from a normal distribution. A positive skew describes\n    a longer tail of values in the right and a negative skew describes a\n    longer tail of values in the left.\n    \n    \n      ThresholdTest\n      time_series_frequency\n      TimeSeriesFrequency\n      Test that detect frequencies in the data\n    \n    \n      ThresholdTest\n      time_series_missing_values\n      TimeSeriesMissingValues\n      Test that the number of missing values is less than a threshold\n    \n    \n      ThresholdTest\n      time_series_outliers\n      TimeSeriesOutliers\n      Test that find outliers for time series data using the z-score method\n    \n    \n      ThresholdTest\n      zeros\n      TooManyZeroValues\n      The zeros test finds columns that have too many zero values.\n    \n    \n      ThresholdTest\n      training_test_degradation\n      TrainingTestDegradation\n      Test that the degradation in performance between the training and test datasets\n    does not exceed a predefined threshold.\n    \n    \n      ThresholdTest\n      unique\n      UniqueRows\n      Test that the number of unique rows is greater than a threshold\n    \n    \n      ThresholdTest\n      weak_spots\n      WeakspotsDiagnosis\n      Test that identify weak regions with high residuals by histogram slicing techniques."
  },
  {
    "objectID": "notebooks/model_diagnosis_test_plan.html",
    "href": "notebooks/model_diagnosis_test_plan.html",
    "title": "ValidMind",
    "section": "",
    "text": "Dataset: bank customer churn dataset: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data\nTwo models: we want to allow the model diagnosis functions to work for statsmodels and sklearn model interfaces since they have different predict() signatures.\n\nXGBoost/SKLearn classifier\nLogistic Regression with statsmodels\n\nTest plans\n\nModel weak spots\n\nSingle variable only\n\nModel overfit\n\nSingle variable only\n\nModel robustness\n\nAll features and single feature"
  },
  {
    "objectID": "notebooks/model_diagnosis_test_plan.html#initializing-the-validmind-library",
    "href": "notebooks/model_diagnosis_test_plan.html#initializing-the-validmind-library",
    "title": "ValidMind",
    "section": "Initializing the ValidMind Library",
    "text": "Initializing the ValidMind Library\n\n%load_ext dotenv\n%dotenv dev.env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhqvlen0005b0i8h6plie0yo\"\n)\n\nConnected to ValidMind. Project: Demo 2 (clhqvlen0005b0i8h6plie0yo)\n\n\n\nImport libraries\n\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n\n\nRunning a data quality test plan\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\nLoad our demo dataset\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nPreparing the dataset for training\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\nDropping irrelevant variables\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\nEncoding categorical variables\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\nDataset preparation\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_df.drop(\"Exited\", axis=1)\ny_train = train_df.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\nModel training\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.955625\n\n\n\nInitialize VM model object and train/test datasets\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\n\n\nFinding all test plans available in the developer framework\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"sklearn_classifier\")\nList available diagnosis tests: vm.test_plans.describe_plan(\"sklearn_classifier_model_diagnosis\")\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\nvm.test_plans.describe_plan(\"binary_classifier_model_diagnosis\")\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Required Context\n      Tests\n    \n  \n  \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n      model\n      OverfitDiagnosis (ThresholdTest), WeakspotsDiagnosis (ThresholdTest), RobustnessDiagnosis (ThresholdTest)\n    \n  \n\n\n\n\n\nRun model diagnosis test plan\nWe can now run the SKLearnClassifier->SKLearnClassifierDiagnosis test plan: #### Define config\n\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")\nvm_model = vm.init_model(model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\nconfig={\n    \"overfit_regions\": {\n        \"cut_off_percentage\": 3,\n        \"feature_columns\": None #[\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n    },\n    \"weak_spots\":{\n        \"features_columns\": None, # [\"Age\", \"Balance\"],\n        \"accuracy_gap_threshold\": 85,\n    },\n    \"robustness\":{\n        \"features_columns\": None, #[ \"Balance\", \"Tenure\", \"NumOfProducts\"],\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        \"accuracy_decay_threshold\": 4,\n    }\n}\nmodel_diagnosis_test_plan = vm.run_test_plan(\"binary_classifier_model_diagnosis\", \n                                             model=vm_model,\n                                             config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html",
    "href": "notebooks/Introduction_Customer_Churn.html",
    "title": "ValidMind",
    "section": "",
    "text": "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\n\nClick File > Save a copy in Drive > to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\n##Install ValidMind Developer Framework\n\n!pip install validmind\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue on to the next cell.\n\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\n\nLog in to the ValidMind platform with your registered email address, and navigate to the Documentation Projects page.\n\n\n(Note: if a documentation project has already been created, you can skip this section and head directly “Finding Project API key and secret”)\nClicking on “Create a new project” allows to you to register a new documentation project for our demo model.\nSelect “Customer Churn model” from the Model drop-down, and “Initial Validation” as Type. Finally, click on “Create Project”.\n\n\n\nIn the “Client Integration” page of the newly created project, you will find the initialization code that allows the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\napi_host: Location of the ValidMind API.\napi_key: Account API key.\napi_secret: Account Secret key.\nproject: The project identifier. The project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nThe code snippet can be copied and pasted directly in the cell below to initialize the ValidMind Developer Framework when run:\n\n## Replace the code below with the code snippet from your project ## \n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"3b481c4935130773c825014129d62c02\",\n  api_secret = \"61ba7b6d80e1e7b162646d3a29374807dab6b3613757942f12ca11eab7530dcc\",\n  project = \"clhny5uil00071ojtaomas47g\"\n)\n  \n\nThe Developer Framework is now initialized and connected to the correct project on the platform.\n\n\n\n\nWe will now train an example model that will be used to demonstrate the ValidMind Developer Framework functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\n\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nNow that we are satisfied with our model, we can begin using the ValidMind Library to generate test and document it.\n\n\n\nWe can find all the test plans and tests available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nList all available tests: vm.test_plans.list_tests()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_data_quality\")\n\nHere is an example:\n\nvm.test_plans.list_plans()\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\n\n\n\nWe can now initialize the TabularDataset test suite. The primary method of doing this is with the run_test_suite function from the vm module. This function takes in a test suite name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Data Preparation” section of the model documentation.\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nWe can now run the BinaryClassifierModelValidation test plan:\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "ValidMind",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\n\n\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard.\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhdxzbb700020a8hpu126rq0\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhdxzbb700020a8hpu126rq0)\n\n\n\n\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\n\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\n\n\nBefore running the test plan, we must first load the dataset into a Pandas DataFrame and initialize a ValidMind dataset object:\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\nWe can now initialize the TabularDataset test plan. The primary method of doing this is with the run_test_plan function from the vm module. This function takes in a test plan name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\ntabular_plan = vm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset)\n\n                                                                                                                                        \n\n\n\nTest plan to extract metadata and descriptive statistics from a tabular dataset\n  \n    \n      \n      RowNumber\n      CustomerId\n      CreditScore\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.000000\n      8.000000e+03\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      5020.520000\n      1.569047e+07\n      650.159625\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      2885.718516\n      7.190247e+04\n      96.846230\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      1.000000\n      1.556570e+07\n      350.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      2518.750000\n      1.562816e+07\n      583.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      5036.500000\n      1.569014e+07\n      651.500000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      7512.250000\n      1.575238e+07\n      717.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      10000.000000\n      1.581566e+07\n      850.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        descriptive_statistics\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'numerical': [{'Name': 'RowNumber', 'Count': 8000.0, 'Mean': 5020.52, 'Std': 2885.7185155986554, 'Min': 1.0, '25%': 2518.75, '50%': 5036.5, '75%': 7512.25, '90%': 9015.1, '95%': 9516.05, 'Max': 10000.0}, {'Name': 'CustomerId', 'Count': 8000.0, 'Mean': 15690474.465625, 'Std': 71902.473335347, 'Min': 15565701.0, '25%': 15628163.75, '50%': 15690143.5, '75%': 15752378.25, '90%': 15790809.1, '95%': 15802760.55, 'Max': 15815660.0}, {'Name': 'CreditScore', 'Count': 8000.0, 'Mean': 650.159625, 'Std': 96.84623014808636, 'Min': 350.0, '25%': 583.0, '50%': 651.5, '75%': 717.0, '90%': 778.0, '95%': 813.0, 'Max': 850.0}, {'Name': 'Age', 'Count': 8000.0, 'Mean': 38.948875, 'Std': 10.458952382767269, 'Min': 18.0, '25%': 32.0, '50%': 37.0, '75%': 44.0, '90%': 53.0, '95%': 60.0, 'Max': 92.0}, {'Name': 'Tenure', 'Count': 8000.0, 'Mean': 5.033875, 'Std': 2.885267419215253, 'Min': 0.0, '25%': 3.0, '50%': 5.0, '75%': 8.0, '90%': 9.0, '95%': 9.0, 'Max': 10.0}, {'Name': 'Balance', 'Count': 8000.0, 'Mean': 76434.09651125, 'Std': 62...\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        training\n                    \n                \n                \n                    Metric Value\n                    \n                        [[{'field': 'CreditScore', 'value': 1.0}, {'field': 'Geography', 'value': 0.010103440458197478}, {'field': 'Gender', 'value': 0.008251776778083898}, {'field': 'Age', 'value': -0.007269780957496768}, {'field': 'Tenure', 'value': -0.006914675142663373}, {'field': 'NumOfProducts', 'value': 0.005677094521946256}, {'field': 'HasCrCard', 'value': -0.009291152528707963}, {'field': 'IsActiveMember', 'value': 0.030554141043824444}, {'field': 'Exited', 'value': -0.025533166369817405}], [{'field': 'CreditScore', 'value': 0.010103440458197478}, {'field': 'Geography', 'value': 1.0}, {'field': 'Gender', 'value': 0.035023152881466464}, {'field': 'Age', 'value': 0.053602289473512775}, {'field': 'Tenure', 'value': 0.015510338111733172}, {'field': 'NumOfProducts', 'value': 0.011118424429054087}, {'field': 'HasCrCard', 'value': 0.021747611293409512}, {'field': 'IsActiveMember', 'value': 0.02017951122934769}, {'field': 'Exited', 'value': 0.1784101181767361}], [{'field': 'CreditScore', 'value': 0.008251776778083898}, {'field': 'G...\n                    \n                \n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n        \n        \n\n\nTest plan for data quality on tabular datasets\n        \n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(values={0: 0.798, 1: 0.202}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'n_duplicates': 0, 'p_duplicates': 0.0}, test_name=None, column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(values={'n_distinct': 2616, 'p_distinct': 0.327}, test_name=None, column='Surname', passed=False), TestResult(values={'n_distinct': 3, 'p_distinct': 0.000375}, test_name=None, column='Geography', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='Gender', passed=True), TestResult(values={'n_distinct': 4, 'p_distinct': 0.0005}, test_name=None, column='NumOfProducts', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='HasCrCard', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='IsActiveMember', passed=True), TestResult(values={'n_distinct': 2, 'p_distinct': 0.00025}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389459}]}, test_name=None, column='Balance', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='RowNumber', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='CustomerId', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Surname', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='CreditScore', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Geography', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Gender', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Age', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Tenure', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Balance', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='NumOfProducts', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='HasCrCard', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='IsActiveMember', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='EstimatedSalary', passed=True), TestResult(values={'n_missing': 0, 'p_missing': 0.0}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'skewness': -0.005920679739677088}, test_name=None, column='RowNumber', passed=True), TestResult(values={'skewness': 0.010032280260684402}, test_name=None, column='CustomerId', passed=True), TestResult(values={'skewness': -0.06195161237091896}, test_name=None, column='CreditScore', passed=True), TestResult(values={'skewness': 1.0245221429799511}, test_name=None, column='Age', passed=False), TestResult(values={'skewness': 0.007692043774702702}, test_name=None, column='Tenure', passed=True), TestResult(values={'skewness': -0.13527693543111804}, test_name=None, column='Balance', passed=True), TestResult(values={'skewness': 0.009510428002077728}, test_name=None, column='EstimatedSalary', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(values={'n_unique': 8000, 'p_unique': 1.0}, test_name=None, column='RowNumber', passed=False), TestResult(values={'n_unique': 8000, 'p_unique': 1.0}, test_name=None, column='CustomerId', passed=False), TestResult(values={'n_unique': 2616, 'p_unique': 0.327}, test_name=None, column='Surname', passed=True), TestResult(values={'n_unique': 452, 'p_unique': 0.0565}, test_name=None, column='CreditScore', passed=True), TestResult(values={'n_unique': 3, 'p_unique': 0.000375}, test_name=None, column='Geography', passed=True), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='Gender', passed=True), TestResult(values={'n_unique': 69, 'p_unique': 0.008625}, test_name=None, column='Age', passed=True), TestResult(values={'n_unique': 11, 'p_unique': 0.001375}, test_name=None, column='Tenure', passed=True), TestResult(values={'n_unique': 5088, 'p_unique': 0.636}, test_name=None, column='Balance', passed=True), TestResult(values={'n_unique': 4, 'p_unique': 0.0005}, test_name=None, column='NumOfProducts', passed=True), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='HasCrCard', passed=True), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='IsActiveMember', passed=True), TestResult(values={'n_unique': 8000, 'p_unique': 1.0}, test_name=None, column='EstimatedSalary', passed=False), TestResult(values={'n_unique': 2, 'p_unique': 0.00025}, test_name=None, column='Exited', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(values={'n_zeros': 323, 'p_zeros': 0.040375}, test_name=None, column='Tenure', passed=False), TestResult(values={'n_zeros': 2912, 'p_zeros': 0.364}, test_name=None, column='Balance', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_dataset\")\nList all available tests: vm.test_plans.list_tests()\n\nAs an example, here’s the outpout list_plans() and list_tests():\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \nbinary_classifier                BinaryClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                                            \ntabular_dataset                  TabularDataset             Test plan for generic tabular datasets                                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \nnormality_test_plan              NormalityTestPlan          Test plan to perform normality tests.                                      \nautocorrelation_test_plan        AutocorrelationTestPlan    Test plan to perform autocorrelation tests.                                \nseasonality_test_plan            SesonalityTestPlan         Test plan to perform seasonality tests.                                    \nunit_root                        UnitRoot                   Test plan to perform unit root tests.                                      \nstationarity_test_plan           StationarityTestPlan       Test plan to perform stationarity tests.                                   \ntimeseries                       TimeSeries                 Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                                            \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_dataset              TimeSeriesDataset          Test plan for time series  datasets                                        \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\nvm.test_plans.list_tests()\n\n\n\n\nTest Type    ID                         Name                              Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n\n\nCustom Test  dataset_metadata           DatasetMetadata                   Custom class to collect a set of descriptive statistics for a dataset.\n    This class will log dataset metadata via `log_dataset` instead of a metric.\n    Dataset metadata is necessary to initialize dataset object that can be related\n    to different metrics and test results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       acf_pacf_plot              ACFandPACFPlot                    Plots ACF and PACF for a given time series dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nMetric       adf                        ADF                               Augmented Dickey-Fuller unit root test for establishing the order of integration of\n    time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       accuracy                   AccuracyScore                     Accuracy Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \nMetric       auto_ar                    AutoAR                            Automatically detects the AR order of a time series using both BIC and AIC.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \nMetric       auto_ma                    AutoMA                            Automatically detects the MA order of a time series using both BIC and AIC.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \nMetric       auto_seasonality           AutoSeasonality                   Automatically detects the optimal seasonal order for a time series dataset\n    using the seasonal_decompose method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       auto_stationarity          AutoStationarity                  Automatically detects stationarity for each time series in a DataFrame\n    using the Augmented Dickey-Fuller (ADF) test.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       box_pierce                 BoxPierce                         The Box-Pierce test is a statistical test used to determine\n    whether a given set of data has autocorrelations\n    that are different from zero.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       csi                        CharacteristicStabilityIndex      Characteristic Stability Index between two datasets                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nMetric       confusion_matrix           ConfusionMatrix                   Confusion Matrix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \nMetric       dickey_fuller_gls          DFGLSArch                         Dickey-Fuller GLS unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       dataset_correlations       DatasetCorrelations               Extracts the correlation matrix for a dataset. The following coefficients\n    are calculated:\n    - Pearson's R for numerical variables\n    - Cramer's V for categorical variables\n    - Correlation ratios for categorical-numerical variables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       dataset_description        DatasetDescription                Collects a set of descriptive statistics for a dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nMetric       dataset_split              DatasetSplit                      Attempts to extract information about the dataset split from the\n    provided training, test and validation datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       descriptive_statistics     DescriptiveStatistics             Collects a set of descriptive statistics for a dataset, both for\n    numerical and categorical variables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       engle_granger_coint        EngleGrangerCoint                 Test for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \nMetric       f1_score                   F1Score                           F1 Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nMetric       jarque_bera                JarqueBera                        The Jarque-Bera test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       kpss                       KPSS                              Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       kolmogorov_smirnov         KolmogorovSmirnov                 The Kolmogorov-Smirnov metric is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       ljung_box                  LJungBox                          The Ljung-Box test is a statistical test used to determine\n    whether a given set of data has autocorrelations\n    that are different from zero.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       lagged_correlation_heatmap LaggedCorrelationHeatmap          Generates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nMetric       lilliefors_test            Lilliefors                        The Lilliefors test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       model_metadata             ModelMetadata                     Custom class to collect the following metadata for a model:\n    - Model architecture\n    - Model hyperparameters\n    - Model task type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       model_prediction_ols       ModelPredictionOLS                Calculates and plots the model predictions for each of the models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \nMetric       pfi                        PermutationFeatureImportance      Permutation Feature Importance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \nMetric       phillips_perron            PhillipsPerronArch                Phillips-Perron (PP) unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       psi                        PopulationStabilityIndex          Population Stability Index between two datasets                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       pr_curve                   PrecisionRecallCurve              Precision Recall Curve                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nMetric       precision                  PrecisionScore                    Precision Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       roc_auc                    ROCAUCScore                       ROC AUC Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \nMetric       roc_curve                  ROCCurve                          ROC Curve                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \nMetric       recall                     RecallScore                       Recall Score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \nMetric                                  RegressionModelInsampleComparison Test that output the comparison of stats library regression models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nMetric                                  RegressionModelOutsampleComparisonTest that evaluates the performance of different regression models on a separate test dataset\n    that was not used to train the models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric                                  RegressionModelSummary            Test that output the summary of regression models of statsmodel library.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nMetric       residuals_visual_inspectionResidualsVisualInspection         Log plots for visual inspection of residuals                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \nMetric       rolling_stats_plot         RollingStatsPlot                  This class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.\nMetric       runs_test                  RunsTest                          The runs test is a statistical test used to determine whether a given set\n    of data has runs of positive and negative values that are longer than expected\n    under the null hypothesis of randomness.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric                                  SHAPGlobalImportance              SHAP Global Importance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nMetric       scatter_plot               ScatterPlot                       Generates a visual analysis of data by plotting a scatter plot matrix for all columns\n    in the dataset. The input dataset can have multiple columns (features) if necessary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       seasonal_decompose         SeasonalDecompose                 Calculates seasonal_decompose metric for each of the dataset features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nMetric       shapiro_wilk               ShapiroWilk                       The Shapiro-Wilk test is a statistical test used to determine\n    whether a given set of data follows a normal distribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       spread_plot                SpreadPlot                        This class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.                                                                                                                                                                                                                                                                                                                             \nMetric       time_series_histogram      TimeSeriesHistogram               Generates a visual analysis of time series data by plotting the\n    histogram. The input dataset can have multiple time series if\n    necessary. In this case we produce a separate plot for each time series.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       time_series_line_plot      TimeSeriesLinePlot                Generates a visual analysis of time series data by plotting the\n    raw time series. The input dataset can have multiple time series\n    if necessary. In this case we produce a separate plot for each time series.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nMetric       zivot_andrews              ZivotAndrewsArch                  Zivot-Andrews unit root test for\n    establishing the order of integration of time series                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestclass_imbalance            ClassImbalance                    The class imbalance test measures the disparity between the majority\n    class and the minority class in the target column.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestduplicates                 Duplicates                        The duplicates test measures the number of duplicate rows found in\n    the dataset. If a primary key column is specified, the dataset is\n    checked for duplicate primary keys as well.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestcardinality                HighCardinality                   The high cardinality test measures the number of unique\n    values found in categorical columns.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestpearson_correlation        HighPearsonCorrelation            Test that the pairwise Pearson correlation coefficients between the\n    features in the dataset do not exceed a specified threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestaccuracy_score             MinimumAccuracy                   Test that the model's prediction accuracy on a dataset meets or\n    exceeds a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestf1_score                   MinimumF1Score                    Test that the model's F1 score on the validation dataset meets or\n    exceeds a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestroc_auc_score              MinimumROCAUCScore                Test that the model's ROC AUC score on the validation dataset meets or\n    exceeds a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestmissing                    MissingValues                     Test that the number of missing values in the dataset across all features\n    is less than a threshold                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestoverfit_regions            OverfitDiagnosis                  Test that identify overfit regions with high residuals by histogram slicing techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \nThresholdTestrobustness                 RobustnessDiagnosis               Test robustness of model by perturbing the features column values                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \nThresholdTestskewness                   Skewness                          The skewness test measures the extent to which a distribution of\n    values differs from a normal distribution. A positive skew describes\n    a longer tail of values in the right and a negative skew describes a\n    longer tail of values in the left.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTesttime_series_frequency      TimeSeriesFrequency               Test that detect frequencies in the data                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \nThresholdTesttime_series_missing_values TimeSeriesMissingValues           Test that the number of missing values is less than a threshold                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTesttime_series_outliers       TimeSeriesOutliers                Test that find outliers for time series data using the z-score method                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nThresholdTestzeros                      TooManyZeroValues                 The zeros test finds columns that have too many zero values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \nThresholdTesttraining_test_degradation  TrainingTestDegradation           Test that the degradation in performance between the training and test datasets\n    does not exceed a predefined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestunique                     UniqueRows                        Test that the number of unique rows is greater than a threshold                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nThresholdTestweak_spots                 WeakspotsDiagnosis                Test that identify weak regions with high residuals by histogram slicing techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\n\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\n\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n      Geography_France\n      Geography_Germany\n      Geography_Spain\n    \n  \n  \n    \n      0\n      1\n      42\n      2\n      0.00\n      1\n      1\n      1\n      101348.88\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      41\n      1\n      83807.86\n      1\n      0\n      1\n      112542.58\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      42\n      8\n      159660.80\n      3\n      1\n      0\n      113931.57\n      1\n      1\n      0\n      0\n    \n    \n      3\n      1\n      39\n      1\n      0.00\n      2\n      0\n      0\n      93826.63\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      43\n      2\n      125510.82\n      1\n      1\n      1\n      79084.10\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\n\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.883125\n\n\n\n\n\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\n\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nWe can now run the SKLearnClassifier test plan:\n\nmodel_plan = vm.run_test_plan(\"binary_classifier\", model=vm_model)\n\n                                                                                                                                   \n\n\nTest plan for sklearn classifier metrics\n            \n            \n            \n                \n                    \n                        Metric Name\n                        model_metadata\n                    \n                    \n                        Metric Type\n                        model\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'architecture': 'Extreme Gradient Boosting', 'task': 'classification', 'subtask': 'binary', 'framework': 'XGBoost', 'framework_version': '1.7.5', 'language': 'Python 3.8.13', 'params': {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'eval_metric': ['error', 'logloss', 'auc'], 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'monotone_constraints': None, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}}\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        dataset_split\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'total_size': 8000}\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.85375\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'tn': 1221, 'fp': 54, 'fn': 180, 'tp': 145}\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.5534351145038168\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n                \n                    Metric Value\n                    \n                        {'Gender': ([0.0031562499999999495], [0.0008466385740090111]), 'Age': ([0.09284374999999996], [0.004049739574960349]), 'Tenure': ([0.006312499999999943], [0.0010670856924352328]), 'Balance': ([0.032624999999999946], [0.0028391102259334882]), 'NumOfProducts': ([0.07334374999999997], [0.0030500384218891353]), 'HasCrCard': ([0.0011249999999999316], [0.000387802301437223]), 'IsActiveMember': ([0.039249999999999986], [0.0016488632447841187]), 'EstimatedSalary': ([0.00868749999999996], [0.0006745947857788526]), 'Geography_France': ([0.001687499999999953], [0.0004571480886977634]), 'Geography_Germany': ([0.021437499999999977], [0.0017382417481466934]), 'Geography_Spain': ([0.0008437499999999875], [0.0003217384419058682])}\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'precision': array([0.203125  , 0.20325203, 0.20337922, ..., 1.        , 1.        ,\n       1.        ]), 'recall': array([1.        , 1.        , 1.        , ..., 0.00615385, 0.00307692,\n       0.        ]), 'thresholds': array([0.00810315, 0.00885704, 0.00898217, ..., 0.980356  , 0.9822782 ,\n       0.9835564 ], dtype=float32)}\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.7286432160804021\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.4461538461538462\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        0.7019004524886877\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'auc': 0.7019004524886877, 'fpr': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.84313725e-04,\n       7.84313725e-04, 1.56862745e-03, 1.56862745e-03, 2.35294118e-03,\n       2.35294118e-03, 3.92156863e-03, 3.92156863e-03, 4.70588235e-03,\n       4.70588235e-03, 5.49019608e-03, 5.49019608e-03, 6.27450980e-03,\n       6.27450980e-03, 7.05882353e-03, 7.05882353e-03, 8.62745098e-03,\n       8.62745098e-03, 9.41176471e-03, 9.41176471e-03, 1.09803922e-02,\n       1.09803922e-02, 1.17647059e-02, 1.33333333e-02, 1.33333333e-02,\n       1.41176471e-02, 1.41176471e-02, 1.56862745e-02, 1.56862745e-02,\n       1.64705882e-02, 1.64705882e-02, 1.88235294e-02, 1.88235294e-02,\n       1.96078431e-02, 1.96078431e-02, 2.11764706e-02, 2.11764706e-02,\n       2.19607843e-02, 2.19607843e-02, 2.27450980e-02, 2.27450980e-02,\n       2.50980392e-02, 2.50980392e-02, 2.58823529e-02, 2.58823529e-02,\n       2.66666667e-02, 2.66666667e-02, 2.74509804e-02, 2.74509804e-02,\n       2.90196078e-02, 2.90196078e-02, 2.98039216e-02, 2.98039216e...\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        {'Gender': 7.3e-05, 'Age': 0.000503, 'Tenure': 0.000993, 'Balance': 0.000763, 'NumOfProducts': 6.3e-05, 'HasCrCard': 8.6e-05, 'IsActiveMember': 5.4e-05, 'EstimatedSalary': 0.00066, 'Geography_France': 0.0, 'Geography_Germany': 3.4e-05, 'Geography_Spain': 4.1e-05}\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                             initial  percent_initial  new  percent_new       psi\nbin                                                      \n1       3494         0.545938  861     0.538125  0.000113\n2       1018         0.159062  278     0.173750  0.001297\n3        467         0.072969  124     0.077500  0.000273\n4        300         0.046875   73     0.045625  0.000034\n5        235         0.036719   62     0.038750  0.000109\n6        156         0.024375   36     0.022500  0.000150\n7        168         0.026250   44     0.027500  0.000058\n8        161         0.025156   29     0.018125  0.002305\n9        142         0.022188   34     0.021250  0.000040\n10       259         0.040469   59     0.036875  0.000334\n                    \n                \n            \n        \n        \n        \n            \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n\n\nTest plan for sklearn classifier models\n        \n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(values={'score': 0.72, 'threshold': 0.7}, test_name=None, column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(values={'score': 0.1450381679389313, 'threshold': 0.5}, test_name=None, column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(values={'score': 0.4953242835595777, 'threshold': 0.5}, test_name=None, column=None, passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(values={'test_score': 0.72, 'train_score': 0.71390625, 'degradation': -0.008535784635587669}, test_name='accuracy', column=None, passed=False), TestResult(values={'test_score': 0.19095477386934673, 'train_score': 0.19318181818181818, 'degradation': 0.011528229382205149}, test_name='precision', column=None, passed=True), TestResult(values={'test_score': 0.11692307692307692, 'train_score': 0.13168086754453912, 'degradation': 0.11207239819004525}, test_name='recall', column=None, passed=True), TestResult(values={'test_score': 0.1450381679389313, 'train_score': 0.1566098572086596, 'degradation': 0.07388863942523582}, test_name='f1', column=None, passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\nTest plan for sklearn classifier model diagnosis tests\n        \n        \n            \n                \n                    \n                        Overfit Regions\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    overfit_regions\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'cut_off_percentage': 4}\n                \n            \n            \n                Results\n                [TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Gender', passed=True), TestResult(values={'slice': ['(77.2, 84.6]'], 'training records': [14], 'training accuracy': [85.71428571428571], 'test accuracy': [0.0], 'gap': [85.71428571428571]}, test_name='accuracy', column='Age', passed=False), TestResult(values={'slice': ['(3.0, 4.0]', '(9.0, 10.0]'], 'training records': [620, 327], 'training accuracy': [71.45161290322581, 72.47706422018348], 'test accuracy': [65.2439024390244, 65.71428571428571], 'gap': [6.207710464201412, 6.762778505897771]}, test_name='accuracy', column='Tenure', passed=False), TestResult(values={'slice': ['(25089.809, 50179.618]'], 'training records': [46], 'training accuracy': [69.56521739130434], 'test accuracy': [50.0], 'gap': [19.565217391304344]}, test_name='accuracy', column='Balance', passed=False), TestResult(values={'slice': ['(3.7, 4.0]'], 'training records': [36], 'training accuracy': [19.444444444444446], 'test accuracy': [12.5], 'gap': [6.944444444444446]}, test_name='accuracy', column='NumOfProducts', passed=False), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='HasCrCard', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='IsActiveMember', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='EstimatedSalary', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Geography_France', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Geography_Germany', passed=True), TestResult(values={'slice': [], 'training records': [], 'training accuracy': [], 'test accuracy': [], 'gap': []}, test_name='accuracy', column='Geography_Spain', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Weak Spots\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    weak_spots\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'thresholds': {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.5, 'f1': 0.7}}\n                \n            \n            \n                Results\n                [TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3534, 0, 0, 0, 0, 0, 0, 0, 0, 2866, 862, 0, 0, 0, 0, 0, 0, 0, 0, 738], 'accuracy': [0.7362761743067345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6863224005582693, 0.7563805104408353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6775067750677507], 'precision': [0.15860735009671179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24242424242424243, 0.1650485436893204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21875], 'recall': [0.14162348877374784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359550561797752, 0.12056737588652482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11413043478260869], 'f1': [0.14963503649635032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16372093023255815, 0.13934426229508198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15]}, test_name='accuracy', column='Gender', passed=False), TestResult(values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [381, 1378, 2330, 1225, 579, 283, 129, 79, 14, 2, 95, 354, 588, 295, 135, 74, 31, 25, 1, 2], 'accuracy': [0.8241469816272966, 0.806966618287373, 0.7630901287553649, 0.6481632653061224, 0.44905008635578586, 0.5371024734982333, 0.627906976744186, 0.810126582278481, 0.8571428571428571, 1.0, 0.7894736842105263, 0.7909604519774012, 0.7874149659863946, 0.6372881355932203, 0.45185185185185184, 0.5540540540540541, 0.6774193548387096, 0.84, 0.0, 1.0], 'precision': [0.1, 0.05319148936170213, 0.11490683229813664, 0.2830188679245283, 0.46153846153846156, 0.6410256410256411, 0.23809523809523808, 0.125, 0.0, 0.0, 0.13333333333333333, 0.047619047619047616, 0.11940298507462686, 0.2222222222222222, 0.45, 0.6666666666666666, 0.2, 0.0, 0.0, 0.0], 'recall': [0.18518518518518517, 0.10204081632653061, 0.12171052631578948, 0.12430939226519337, 0.1346153846153846, 0.176056338028169, 0.13513513513513514, 0.1111111111111111, 0.0, 0.0, 0.2222222222222222, 0.05555555555555555, 0.10810810810810811, 0.09195402298850575, 0.125, 0.21621621621621623, 0.14285714285714285, 0.0, 0.0, 0.0], 'f1': [0.12987012987012986, 0.06993006993006994, 0.1182108626198083, 0.1727447216890595, 0.20843672456575682, 0.27624309392265195, 0.1724137931034483, 0.11764705882352941, 0.0, 0.0, 0.16666666666666669, 0.05128205128205128, 0.11347517730496452, 0.13008130081300812, 0.1956521739130435, 0.326530612244898, 0.16666666666666666, 0.0, 0.0, 0.0]}, test_name='accuracy', column='Age', passed=False), TestResult(values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]', '(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'shape': [874, 665, 674, 620, 651, 618, 664, 683, 624, 327, 248, 176, 155, 164, 158, 162, 160, 139, 168, 70], 'accuracy': [0.6853546910755148, 0.7383458646616541, 0.6899109792284867, 0.714516129032258, 0.7327188940092166, 0.7038834951456311, 0.7515060240963856, 0.7115666178623719, 0.7003205128205128, 0.7247706422018348, 0.6895161290322581, 0.7215909090909091, 0.7225806451612903, 0.6524390243902439, 0.7215189873417721, 0.7716049382716049, 0.73125, 0.7050359712230215, 0.8035714285714286, 0.6571428571428571], 'precision': [0.16806722689075632, 0.21739130434782608, 0.18446601941747573, 0.1927710843373494, 0.22093023255813954, 0.20202020202020202, 0.15476190476190477, 0.16842105263157894, 0.18181818181818182, 0.30952380952380953, 0.2, 0.11764705882352941, 0.24, 0.25, 0.25, 0.26666666666666666, 0.06666666666666667, 0.058823529411764705, 0.29411764705882354, 0.07142857142857142], 'recall': [0.10204081632653061, 0.16393442622950818, 0.13194444444444445, 0.12698412698412698, 0.15079365079365079, 0.16129032258064516, 0.12149532710280374, 0.11940298507462686, 0.10144927536231885, 0.17567567567567569, 0.125, 0.05555555555555555, 0.2, 0.13333333333333333, 0.14705882352941177, 0.13333333333333333, 0.03333333333333333, 0.038461538461538464, 0.19230769230769232, 0.08333333333333333], 'f1': [0.12698412698412698, 0.18691588785046728, 0.15384615384615385, 0.15311004784688995, 0.1792452830188679, 0.17937219730941703, 0.13612565445026178, 0.13973799126637554, 0.1302325581395349, 0.2241379310344828, 0.15384615384615385, 0.07547169811320754, 0.2181818181818182, 0.1739130434782609, 0.18518518518518517, 0.17777777777777776, 0.04444444444444444, 0.04651162790697675, 0.23255813953488372, 0.07692307692307691]}, test_name='accuracy', column='Tenure', passed=False), TestResult(values={'slice': ['(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]', '(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]'], 'shape': [2352, 46, 218, 734, 1292, 1128, 488, 118, 22, 2, 565, 8, 63, 188, 352, 288, 99, 33, 4, 0], 'accuracy': [0.7644557823129252, 0.6956521739130435, 0.7155963302752294, 0.7111716621253406, 0.6679566563467493, 0.6737588652482269, 0.7090163934426229, 0.7033898305084746, 0.4090909090909091, 0.0, 0.7699115044247787, 0.5, 0.6984126984126984, 0.75, 0.6647727272727273, 0.7013888888888888, 0.6767676767676768, 0.696969696969697, 0.5, 0.0], 'precision': [0.14285714285714285, 0.0, 0.26666666666666666, 0.17307692307692307, 0.2542372881355932, 0.1958041958041958, 0.24390243902439024, 0.2777777777777778, 0.0, 0.0, 0.11764705882352941, 0.0, 0.2, 0.2727272727272727, 0.2978723404255319, 0.175, 0.09090909090909091, 0.0, 1.0, 0.0], 'recall': [0.1419753086419753, 0.0, 0.16666666666666666, 0.125, 0.13157894736842105, 0.099644128113879, 0.2, 0.18518518518518517, 0.0, 0.0, 0.10256410256410256, 0.0, 0.0625, 0.16216216216216217, 0.1414141414141414, 0.11666666666666667, 0.043478260869565216, 0.0, 0.3333333333333333, 0.0], 'f1': [0.14241486068111453, 0.0, 0.2051282051282051, 0.14516129032258066, 0.17341040462427745, 0.1320754716981132, 0.21978021978021978, 0.22222222222222224, 0.0, 0.0, 0.1095890410958904, 0.0, 0.09523809523809523, 0.20338983050847456, 0.1917808219178082, 0.14, 0.0588235294117647, 0.0, 0.5, 0.0]}, test_name='accuracy', column='Balance', passed=False), TestResult(values={'slice': ['(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]', '(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]'], 'shape': [3224, 0, 0, 2965, 0, 0, 175, 0, 0, 36, 824, 0, 0, 723, 0, 0, 45, 0, 0, 8], 'accuracy': [0.6550868486352357, 0.0, 0.0, 0.8114671163575042, 0.0, 0.0, 0.25142857142857145, 0.0, 0.0, 0.19444444444444445, 0.6650485436893204, 0.0, 0.0, 0.8160442600276625, 0.0, 0.0, 0.28888888888888886, 0.0, 0.0, 0.125], 'precision': [0.25565610859728505, 0.0, 0.0, 0.08029197080291971, 0.0, 0.0, 0.85, 0.0, 0.0, 1.0, 0.2524271844660194, 0.0, 0.0, 0.07777777777777778, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0], 'recall': [0.12611607142857142, 0.0, 0.0, 0.1542056074766355, 0.0, 0.0, 0.11724137931034483, 0.0, 0.0, 0.19444444444444445, 0.11555555555555555, 0.0, 0.0, 0.12280701754385964, 0.0, 0.0, 0.11428571428571428, 0.0, 0.0, 0.125], 'f1': [0.16890881913303438, 0.0, 0.0, 0.1056, 0.0, 0.0, 0.20606060606060606, 0.0, 0.0, 0.32558139534883723, 0.15853658536585366, 0.0, 0.0, 0.09523809523809523, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.2222222222222222]}, test_name='accuracy', column='NumOfProducts', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [1886, 0, 0, 0, 0, 0, 0, 0, 0, 4514, 493, 0, 0, 0, 0, 0, 0, 0, 0, 1107], 'accuracy': [0.7158006362672322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7131147540983607, 0.7342799188640974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7136404697380307], 'precision': [0.23106060606060605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17694805194805194, 0.21818181818181817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18055555555555555], 'recall': [0.1548223350253807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12151616499442586, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11555555555555555], 'f1': [0.18541033434650456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14408460013218768, 0.15483870967741936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14092140921409213]}, test_name='accuracy', column='HasCrCard', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3058, 0, 0, 0, 0, 0, 0, 0, 0, 3342, 783, 0, 0, 0, 0, 0, 0, 0, 0, 817], 'accuracy': [0.6677567037279267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.756134051466188, 0.6896551724137931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7490820073439413], 'precision': [0.26744186046511625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12222222222222222, 0.2727272727272727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12612612612612611], 'recall': [0.1409313725490196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11578947368421053, 0.11822660098522167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11475409836065574], 'f1': [0.18459069020866772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11891891891891893, 0.16494845360824742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12017167381974247]}, test_name='accuracy', column='IsActiveMember', passed=False), TestResult(values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]', '(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'shape': [636, 602, 653, 655, 648, 662, 641, 629, 649, 625, 155, 179, 152, 185, 154, 154, 155, 149, 166, 151], 'accuracy': [0.7091194968553459, 0.7159468438538206, 0.7044410413476263, 0.7206106870229008, 0.6975308641975309, 0.7311178247734139, 0.7020280811232449, 0.7313195548489666, 0.7010785824345146, 0.7264, 0.7096774193548387, 0.6927374301675978, 0.7105263157894737, 0.7081081081081081, 0.7792207792207793, 0.7337662337662337, 0.7225806451612903, 0.7516778523489933, 0.6927710843373494, 0.7086092715231788], 'precision': [0.18181818181818182, 0.17333333333333334, 0.08888888888888889, 0.18666666666666668, 0.14705882352941177, 0.25510204081632654, 0.1919191919191919, 0.24719101123595505, 0.25510204081632654, 0.19480519480519481, 0.1111111111111111, 0.11538461538461539, 0.125, 0.2, 0.391304347826087, 0.2631578947368421, 0.23529411764705882, 0.2222222222222222, 0.05263157894736842, 0.16666666666666666], 'recall': [0.10294117647058823, 0.10655737704918032, 0.06722689075630252, 0.10294117647058823, 0.12096774193548387, 0.19230769230769232, 0.14615384615384616, 0.1774193548387097, 0.17123287671232876, 0.12096774193548387, 0.06451612903225806, 0.08571428571428572, 0.0625, 0.1282051282051282, 0.3103448275862069, 0.15625, 0.11764705882352941, 0.14814814814814814, 0.029411764705882353, 0.09375], 'f1': [0.13145539906103285, 0.13197969543147206, 0.07655502392344497, 0.13270142180094785, 0.13274336283185842, 0.21929824561403508, 0.16593886462882096, 0.20657276995305163, 0.20491803278688528, 0.1492537313432836, 0.08163265306122448, 0.09836065573770493, 0.08333333333333333, 0.15625, 0.34615384615384615, 0.19607843137254902, 0.15686274509803924, 0.17777777777777776, 0.03773584905660377, 0.12000000000000002]}, test_name='accuracy', column='EstimatedSalary', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3193, 0, 0, 0, 0, 0, 0, 0, 0, 3207, 797, 0, 0, 0, 0, 0, 0, 0, 0, 803], 'accuracy': [0.6830566865017225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.744621141253508, 0.6913425345043914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7484433374844334], 'precision': [0.24604966139954854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13958810068649885, 0.2621359223300971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11458333333333333], 'recall': [0.13850063532401524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12103174603174603, 0.13705583756345177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0859375], 'f1': [0.17723577235772356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1296493092454835, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09821428571428571]}, test_name='accuracy', column='Geography_France', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [4803, 0, 0, 0, 0, 0, 0, 0, 0, 1597, 1188, 0, 0, 0, 0, 0, 0, 0, 0, 412], 'accuracy': [0.7432854465958776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6255479023168441, 0.7457912457912458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6456310679611651], 'precision': [0.14864864864864866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3317757009345794, 0.13793103448275862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333], 'recall': [0.12941176470588237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13498098859315588, 0.10152284263959391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.140625], 'f1': [0.13836477987421386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19189189189189187, 0.11695906432748539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19780219780219782]}, test_name='accuracy', column='Geography_Germany', passed=False), TestResult(values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [4804, 0, 0, 0, 0, 0, 0, 0, 0, 1596, 1215, 0, 0, 0, 0, 0, 0, 0, 0, 385], 'accuracy': [0.70503746877602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7406015037593985, 0.7135802469135802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7402597402597403], 'precision': [0.20276497695852536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16593886462882096, 0.19333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1836734693877551], 'recall': [0.12815533980582525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14559386973180077, 0.11328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13043478260869565], 'f1': [0.1570493753718025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15510204081632653, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15254237288135594]}, test_name='accuracy', column='Geography_Spain', passed=False)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Robustness\n                    \n                    \n                        ✅\n                    \n                \n            \n            \n                \n                    Test Name\n                    robustness\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'features_columns': None, 'scaling_factor_std_dev_list': [0.01, 0.02]}\n                \n            \n            \n                Results\n                [TestResult(values={'Perturbation Size': [0.01, 0.01, 0.02, 0.02], 'Dataset Type': ['Traning', 'Test', 'Traning', 'Test'], 'Records': [6400, 1600, 6400, 1600], 'accuracy': [88.828125, 85.5, 88.75, 85.375]}, test_name='accuracy', column='Age', passed=True)]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "href": "notebooks/insurance_mortality/validmind_insurance_POC.html",
    "title": "ValidMind",
    "section": "",
    "text": "Introduction\n\nExecutive Summary\nBeing able to make accurate and timely estimates of future claims is a fundamental task for actuaries. Questions of profitability, product competitiveness, and insurer solvency depend on understanding future claims, with mortality being one of the central issues facing a life insurer.\nIn this demo, we show an example of a machine learning application on mortality assumption setting, a classic life insurance problem. Using real mortality data collected by the Society of Actuaries, we will walk you through the process of model building and validation.\n\n\nOverview of Mortality Case Study\n\n Case Study Data \nOur dataset is the composite mortality experience data at policy level from 2012 to 2016. This dataset is used to published the 2016 Individual Life Experience Report by SOA’s Individual Life Experience Committee (ILEC).\nFor the case study, the data was restricted to term life insurance policies that were within the initial policy term, issued after 1980, and the issue age was at least 18 years old.\nMore details on this dataset can be found in Section 2 of the data report https://www.soa.org/49957f/globalassets/assets/files/resources/research-report/2021/2016-individual-life-report.pdf\n\n\n Case Study Model \nFor the case study in this paper, we used the statsmodel’s implementation of the GLM family models. Our main model is using Poisson distribution with log link function that is often used for mortality prediction.\nThe  response variable used in this case study is the number of deaths. Policies exposed was used as a weight in the model. We also tried to fit the mortality rate, which is number of deaths/ policies exposed using Gaussian distribution with log link, that can be found in the Appendix\nThe features used in the mortality model are:\n\nAttained Age – the sum of the policyholder’s age at policy issue and the number of years they have held the policy.\nDuration – the number of years (starting with a value of one) the policyholder has had the policy.\nSmoking Status – if the policyholder is considered a smoker or not.\nPreferred Class – an underwriting structure used by insurers to classify and price policyholders. Different companies have different structures with the number of classes ranging from two to four. The lower the class designation, the healthier the policyholders who are put into that class. Thus, someone in class 1 of 3 (displayed as 1_3 in this paper) is considered healthier at time of issue than someone in class 3 of 3.\nGender – A categorical feature in the model with two levels, male and female.\nGuaranteed Term Period – the length of the policy at issue during which the premium will remain constant regardless of policyholder behavior or health status. The shortest term period in the data is five years with increasing lengths by five years up to 30 years. Term period is used as a categorical feature with six levels.\nFace_Amount_Band\nObservation Year\n\n\n\n\n\nSet Up\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport sklearn \nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport os\nimport xgboost as xgb\n\nFirst, let’s download data directly from the SOA website and unzip. This might take 5-10 minutes due to the large size of the file.\n\n# directly curl from the SOA website and unzip\n! echo Working Directory = $(pwd)\n! if [ -d \"./Data\" ]; then echo \"Data folder already exists\"; else echo \"Create Data folder\"; mkdir Data; fi\n! if [ -f \"./Data/ILEC 2009-16 20200123.csv\" ]; then echo \"File already exists\";  else echo \"Download data ..\"; curl https://cdn-files.soa.org/web/ilec-2016/ilec-data-set.zip --output ./Data/ilec-data-set.zip; echo \"Unzip data ..\";  unzip ./Data/ilec-data-set.zip -d ./Data;  fi\n! echo \"Done\"\n\nWorking Directory = /Users/andres/code/validmind-sdk/notebooks/insurance_mortality\nData folder already exists\nFile already exists\nDone\n\n\nSecond, sample 5% from the giant file. Another 10 minutes or so the first time you run it :)\n\n#sample 5% and save it out to a sample file\nif not os.path.exists('./Data/ILEC 2009-16 20200123 sample.csv'):\n    p = 0.05\n    random.seed(42)\n    sample = pd.read_csv('./Data/ILEC 2009-16 20200123.csv', \n                        skiprows = lambda i: i>0 and random.random() >p)\n    sample.to_csv('./Data/ILEC 2009-16 20200123 sample.csv', index = False)\n\n\n\nEDA\n\n# load sample file \nsample_df = pd.read_csv('./Data/ILEC 2009-16 20200123 sample.csv',\n                    usecols = ['Observation_Year', 'Gender', 'Smoker_Status',\n                               'Insurance_Plan',  'Duration', 'Attained_Age', 'SOA_Guaranteed_Level_Term_Period',\n                               'Face_Amount_Band', 'Preferred_Class', \n                               'Number_Of_Deaths','Policies_Exposed', \n                               'SOA_Anticipated_Level_Term_Period','SOA_Post_level_Term_Indicator', \n                               'Expected_Death_QX2015VBT_by_Policy',\n                               'Issue_Age', 'Issue_Year'])\n\n# target variable\nsample_df['mort'] = sample_df['Number_Of_Deaths'] / sample_df['Policies_Exposed']\n\nsample_df.head()\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Gender\n      Smoker_Status\n      Insurance_Plan\n      Issue_Age\n      Duration\n      Attained_Age\n      Face_Amount_Band\n      Issue_Year\n      Preferred_Class\n      SOA_Anticipated_Level_Term_Period\n      SOA_Guaranteed_Level_Term_Period\n      SOA_Post_level_Term_Indicator\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      0\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      4.882191\n      0.001074\n      0.0\n    \n    \n      1\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      1\n      0\n      500000-999999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      25.795943\n      0.006449\n      0.0\n    \n    \n      2\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      10000-24999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      1.117809\n      0.000134\n      0.0\n    \n    \n      3\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      2\n      1\n      250000-499999\n      2008\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      70.098636\n      0.009814\n      0.0\n    \n    \n      4\n      2009\n      Female\n      NonSmoker\n      Perm\n      0\n      4\n      3\n      50000-99999\n      2006\n      NaN\n      N/A (Not Term)\n      N/A (Not Term)\n      N/A (Not Term)\n      0\n      493.523281\n      0.034547\n      0.0\n    \n  \n\n\n\n\n\n# filter pipeline\nsample_df = sample_df[(sample_df.Expected_Death_QX2015VBT_by_Policy != 0)\n               & (sample_df.Smoker_Status != 'Unknown') \n               & (sample_df.Insurance_Plan == ' Term')\n               & (-sample_df.Preferred_Class.isna())\n               & (sample_df.Attained_Age >= 18)\n               & (sample_df.Issue_Year >= 1980)\n               & (sample_df.SOA_Post_level_Term_Indicator == \"Within Level Term\")\n               & (sample_df.SOA_Anticipated_Level_Term_Period != \"Unknown\")\n               & (sample_df.mort < 1)]\n\nprint(f'Count: {sample_df.shape[0]}')\nprint()\n\n# describe data\nsample_df.describe()\n\nCount: 307233\n\n\n\n\n\n\n\n  \n    \n      \n      Observation_Year\n      Issue_Age\n      Duration\n      Attained_Age\n      Issue_Year\n      Preferred_Class\n      Number_Of_Deaths\n      Policies_Exposed\n      Expected_Death_QX2015VBT_by_Policy\n      mort\n    \n  \n  \n    \n      count\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      307233.000000\n      3.072330e+05\n      307233.000000\n    \n    \n      mean\n      2014.084001\n      42.248505\n      7.951434\n      49.199939\n      2006.640537\n      2.035013\n      0.018514\n      12.504679\n      1.932158e-02\n      0.001627\n    \n    \n      std\n      1.413654\n      12.777574\n      4.793230\n      13.340539\n      4.888334\n      0.962332\n      0.147063\n      29.112019\n      5.412559e-02\n      0.023061\n    \n    \n      min\n      2012.000000\n      18.000000\n      1.000000\n      18.000000\n      1984.000000\n      1.000000\n      0.000000\n      0.002732\n      1.918000e-07\n      0.000000\n    \n    \n      25%\n      2013.000000\n      32.000000\n      4.000000\n      39.000000\n      2003.000000\n      1.000000\n      0.000000\n      0.838356\n      7.766577e-04\n      0.000000\n    \n    \n      50%\n      2014.000000\n      42.000000\n      7.000000\n      49.000000\n      2007.000000\n      2.000000\n      0.000000\n      2.612022\n      3.316641e-03\n      0.000000\n    \n    \n      75%\n      2015.000000\n      52.000000\n      12.000000\n      59.000000\n      2011.000000\n      3.000000\n      0.000000\n      10.680379\n      1.470165e-02\n      0.000000\n    \n    \n      max\n      2016.000000\n      84.000000\n      30.000000\n      91.000000\n      2016.000000\n      4.000000\n      6.000000\n      655.938021\n      2.827005e+00\n      0.981233\n    \n  \n\n\n\n\n\n# Encode categorical variables\ncat_vars = ['Observation_Year', \n     'Gender', \n     'Smoker_Status',\n     'Face_Amount_Band', \n     'Preferred_Class',\n     'SOA_Anticipated_Level_Term_Period']\n\nonehot = preprocessing.OneHotEncoder()\nresults = onehot.fit_transform(sample_df[cat_vars]).toarray()\ncat_vars_encoded = list(onehot.get_feature_names_out())\nsample_df = pd.concat([sample_df,pd.DataFrame(data = results, columns = cat_vars_encoded, index = sample_df.index)], axis = 1)\n\n\n# categorical variables\nface_amount_order = ['    1-9999', '   10000-24999', '   25000-49999', '   50000-99999','  100000-249999' , '  250000-499999','  500000-999999',' 1000000-2499999', ' 2500000-4999999',' 5000000-9999999', '10000000+']\nterm_period_order = [' 5 yr guaranteed', '10 yr guaranteed',  '15 yr guaranteed', '20 yr guaranteed', '25 yr guaranteed','30 yr guaranteed']\nfig, ax = plt.subplots(4,2, figsize = (20,30))\nax = ax.flatten()\nfor i,column in enumerate(['Observation_Year', 'Gender', 'Smoker_Status', 'Insurance_Plan',\n       'Face_Amount_Band', 'Preferred_Class',\n       'SOA_Guaranteed_Level_Term_Period']):\n    if column == 'Face_Amount_Band':\n        order = face_amount_order\n    elif column == 'SOA_Guaranteed_Level_Term_Period':\n        order = term_period_order\n    else:\n        order = None\n    sns.countplot(y = sample_df[column], ax = ax[i], orient = 'h', order = order)\nplt.show()\n\n\n\n\n\n# age and duration variables\nfig, ax = plt.subplots(1,2, figsize = (20,5))\nsns.histplot(x = sample_df['Attained_Age'], ax = ax[0])\n\nsns.histplot(x = sample_df['Duration'], ax = ax[1])\nplt.show()\n\n\n\n\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(sample_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed']].corr(), annot=True)\nplt.show()\n\n\n\n\n\n# log mort by Attained Age\n\ndef stratify(field):\n    fig, ax = plt.subplots(figsize = (7,3))\n    temp = sample_df.groupby(['Attained_Age', field])[['Number_Of_Deaths', 'Policies_Exposed']].sum().reset_index()\n    temp['log_mort'] = (temp.Number_Of_Deaths / temp.Policies_Exposed).apply(np.log)\n    sns.lineplot(data = temp, x = 'Attained_Age', y = 'log_mort', hue = field, ax = ax)\n    plt.title(f'Log Mortality Rate by Attained Age and {field}')\n    plt.show()\n\nstratify('Smoker_Status')\nstratify('Preferred_Class')\nstratify('Gender')\nstratify('Observation_Year')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling\n\nTrain/test split\nFirst we split the data into 80% for training and 20% for testing.\nIn this context because we don’t really need to do hyperparameter tuning so it’s not necessary to create a validation set.\n\n# create training (80%), validation (5%) and test set (15%)\nrandom_seed = 0\ntrain_df = sample_df.sample(frac = 0.8, random_state = random_seed)\ntest_df = sample_df.loc[~sample_df.index.isin(train_df.index),:]\n\n# add constant variable\ntrain_df['Const'] = 1\ntest_df['Const'] = 1\n \nprint(f'Train size: {train_df.shape[0]}, test size: {test_df.shape[0]}')\n\nTrain size: 245786, test size: 61447\n\n\n\ntrain_df.to_csv('train_df.csv', index = False)\ntest_df.to_csv('test_df.csv', index = False)\n\n\n\nGLM modeling 101\nIn a generalized linear model (GLM), each outcome Y of the dependent variables is assumed to be generated from a particular distribution in an exponential family, a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others. The mean, \\(μ\\), of the distribution depends on the independent variables, X, through\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\n\n\\({\\displaystyle \\operatorname {E} (\\mathbf {Y} |\\mathbf {X} )={\\boldsymbol {\\mu }}=g^{-1}(\\mathbf {X} {\\boldsymbol {\\beta }})}\\)\nwhere:\n\n\\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on \\(X\\)\n\\(Xβ\\) is the linear predictor, a linear combination of unknown parameters \\(β\\)\n\\(g\\) is the link function.\n\n\n\nModel 1: Poisson distribution with log link on count\n Target Variable  = [Number_Of_Deaths]\n Input Variables  = [Observation_Year, Gender, Smoker_Status, Face_Amount_Band, Preferred_Class, Attained_Age, Duration, SOA_Anticipated_Level_Term_Period]\nAs the  target variable is a count measure, we will fit GLM with Poisson distribution and log link.\nThe target variable is count, what we really fit the Poisson model to is mortality rate (count/exposure) with the use of offset. This is a common practice according to https://en.wikipedia.org/wiki/Poisson_regression\n\nmodel1 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year)+ C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + C(SOA_Anticipated_Level_Term_Period) \\\n                                       + Attained_Age + Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres1 = model1.fit()\nres1.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:   Number_Of_Deaths   No. Observations:     245786   \n\n\n  Model:                  GLM         Df Residuals:       3076911.54 \n\n\n  Model Family:         Poisson       Df Model:                 26   \n\n\n  Link Function:          log         Scale:                 1.0000  \n\n\n  Method:                IRLS         Log-Likelihood:     -7.1471e+05\n\n\n  Date:            Mon, 05 Dec 2022   Deviance:           9.8740e+05 \n\n\n  Time:                22:28:25       Pearson chi2:        3.17e+06  \n\n\n  No. Iterations:         24          Pseudo R-squ. (CS):   0.6540   \n\n\n  Covariance Type:     nonrobust                                     \n\n\n\n\n                                                               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept                                                    -9.2794     0.158   -58.838  0.000    -9.589    -8.970\n\n\n  C(Observation_Year)[T.2013]                                  -0.0545     0.007    -8.190  0.000    -0.067    -0.041\n\n\n  C(Observation_Year)[T.2014]                                  -0.0051     0.006    -0.789  0.430    -0.018     0.008\n\n\n  C(Observation_Year)[T.2015]                                  -0.1405     0.007   -20.705  0.000    -0.154    -0.127\n\n\n  C(Observation_Year)[T.2016]                                  -0.0813     0.007   -12.377  0.000    -0.094    -0.068\n\n\n  C(Gender)[T.Male]                                             0.3527     0.005    74.784  0.000     0.343     0.362\n\n\n  C(Smoker_Status)[T.Smoker]                                    1.0350     0.015    67.166  0.000     1.005     1.065\n\n\n  C(Face_Amount_Band)[T.   10000-24999]                        -0.7187     0.118    -6.104  0.000    -0.949    -0.488\n\n\n  C(Face_Amount_Band)[T.   25000-49999]                        -0.7632     0.117    -6.500  0.000    -0.993    -0.533\n\n\n  C(Face_Amount_Band)[T.   50000-99999]                        -0.9776     0.117    -8.372  0.000    -1.206    -0.749\n\n\n  C(Face_Amount_Band)[T.  100000-249999]                       -1.6819     0.116   -14.452  0.000    -1.910    -1.454\n\n\n  C(Face_Amount_Band)[T.  250000-499999]                       -2.0061     0.116   -17.222  0.000    -2.234    -1.778\n\n\n  C(Face_Amount_Band)[T.  500000-999999]                       -2.0428     0.117   -17.521  0.000    -2.271    -1.814\n\n\n  C(Face_Amount_Band)[T. 1000000-2499999]                      -2.0690     0.117   -17.721  0.000    -2.298    -1.840\n\n\n  C(Face_Amount_Band)[T. 2500000-4999999]                      -2.0173     0.138   -14.656  0.000    -2.287    -1.747\n\n\n  C(Face_Amount_Band)[T. 5000000-9999999]                      -2.0177     0.229    -8.795  0.000    -2.467    -1.568\n\n\n  C(Face_Amount_Band)[T.10000000+]                            -23.7738  1.48e+04    -0.002  0.999 -2.89e+04  2.89e+04\n\n\n  C(Preferred_Class)[T.2.0]                                     0.4593     0.005    94.004  0.000     0.450     0.469\n\n\n  C(Preferred_Class)[T.3.0]                                     0.4168     0.007    60.272  0.000     0.403     0.430\n\n\n  C(Preferred_Class)[T.4.0]                                     0.5337     0.011    48.013  0.000     0.512     0.555\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.10 yr anticipated]    -0.1692     0.105    -1.607  0.108    -0.376     0.037\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.15 yr anticipated]    -0.2569     0.105    -2.438  0.015    -0.463    -0.050\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.20 yr anticipated]    -0.4042     0.105    -3.844  0.000    -0.610    -0.198\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.25 yr anticipated]     0.0217     0.106     0.205  0.838    -0.186     0.229\n\n\n  C(SOA_Anticipated_Level_Term_Period)[T.30 yr anticipated]    -0.2437     0.105    -2.314  0.021    -0.450    -0.037\n\n\n  Attained_Age                                                  0.0739     0.000   254.173  0.000     0.073     0.075\n\n\n  Duration                                                      0.0497     0.001    92.131  0.000     0.049     0.051\n\n\n\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nres1.save('res1.pkl')\n\n\nres1.predict(exog = train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nloaded = sm.load('res1.pkl')\n\n\nfitted = loaded.model.fit()\n\n\nfitted.predict(train_df)\n\n1283609    0.000487\n914790     0.000164\n1468496    0.004144\n1515604    0.000442\n1073383    0.001613\n             ...   \n1313854    0.010491\n1004151    0.000598\n1354488    0.000129\n1040410    0.001310\n1226199    0.000978\nLength: 245786, dtype: float64\n\n\n\nfitted.params[\"Intercept\"]\n\n-9.279412567322963\n\n\nFirst, we show the lift chart that breaks down the predicted mortality rates into deciles and show how the actual compares against the predicted rates for each decile. Looks like the predicted are not too far off on the test set, but then we’re only look at the high-level average for each decile.\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat1'] = res1.predict(exog = train_df)\ntrain_df['death_hat1'] = train_df['mort_hat1'] * train_df['Policies_Exposed']\ntest_df['mort_hat1'] = res1.predict(exog = test_df)\ntest_df['death_hat1'] = test_df['mort_hat1'] * test_df['Policies_Exposed']\n\n# groupby and aggregate by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat1'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat1\", wm), predicted = ('mort', wm))\ntemp\n\n# lift chart \nfig, ax = plt.subplots(figsize = (7,3))\ntemp.plot(ax = ax)\nplt.title('Actual vs predicted mortality rate by deciles')\nplt.show() \n\nSecond, we can plot the partial dependency chart between the log mortality rate and key covariates like Attained Age or Duration to see more granular comparisons between actual vs predicted.\nWe can immediately see that even on the train set, the model does not capture the dynamics near the two tails of the age distribution very well.\n\ndef pdp(df, agg_field, title, predict_col = 'death_hat1'):\n    agg = df.groupby(agg_field)['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort'] = (agg['Number_Of_Deaths']/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (7,3))\n    ax.plot(agg[agg_field], agg['log_mort'], color = 'r')\n    ax.plot(agg[agg_field], agg['log_mort_predicted'], color = 'b')\n    plt.legend(['actual','predicted']) \n    plt.xlabel(agg_field)\n    plt.ylabel('log_mort')\n    plt.title(title)\n    plt.show()\n    \npdp(train_df, 'Attained_Age', 'How well does the model fit the train set')\npdp(train_df, 'Duration', 'How well does the model fit the train set')\n\n\npdp(test_df, 'Attained_Age', 'How well does the model fit the test set')\npdp(test_df, 'Duration', 'How well does the model fit the test set')\n\nThird, we look at Prediction Error by taking the difference between the Number Of Deaths (actual) and Predicted Number of Deaths and then normalized by Policies Exposed. This tells the same story as the dependecy chart that we have a lot of errors near the two tails of the age distribution.\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\nagg = train_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\nagg = test_df.groupby('Attained_Age')['Err1', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']))\nplt.legend(['Model 1'])\nplt.ylabel('Error')\nplt.title('Testing error')\nplt.show()\n\n\n\n\nValidation\n\n1. Goodness of Fit\n\n Pseudo R-squared \nIn linear regression, the squared multiple correlation, R-squared is often used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.\nFor GLM, pseudo R-squared is the most analogous measure to the squared multiple correlations. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. Quantifiably, the higher is better.\n\n\\(R_{\\text{L}}^{2}={\\frac {{Deviance}_{\\text{null}}-Deviance_{\\text{fitted}}}{Deviance_{\\text{null}}}}\\)\n\n\nres1.pseudo_rsquared()\n\n\n\n Deviance \nThe (total) deviance for a model M with estimates \\({\\displaystyle {\\hat {\\mu }}=E[Y|{\\hat {\\theta }}_{0}]}\\), based on a dataset y, may be constructed by its likelihood as:\n\n\\({\\displaystyle D(y,{\\hat {\\mu }})=2\\left(\\log \\left[p(y\\mid {\\hat {\\theta }}_{s})\\right]-\\log \\left[p(y\\mid {\\hat {\\theta }}_{0})\\right]\\right)}\\)\n\nHere \\(\\hat \\theta_0\\) denotes the fitted values of the parameters in the model M, while \\(\\hat \\theta_s\\) denotes the fitted parameters for the saturated model: both sets of fitted values are implicitly functions of the observations y.\nIn large samples, deviance follows a chi-square distribution with n−p degrees of freedom, where n is the number of observations and p is the number of parameters in the model. The null hypothesis, H0, is that the model fits. The alternative hypothesis, H1, is that the model does not fit. A deviance much higher than n−p indicates the model is a poor fit to the data. Quantifiably, smaller is always better: The smaller the deviance, the better the fit of the model.\nHere we divided the deviance by the residual degree of freedom and observed a ratio much smaller than 1\n\nres1.deviance/res1.df_resid\n\n\n\n Pearson Statistic and dispersion \nSimilar to deviance test, the Pearson Statistic is approximately chi-square distributed with n – p degrees of freedom. A Pearson Statistic much higher than the degree of freedom indicates that the model is a poor fit.\nAdditionally, for a Poisson distribution, the mean and the variance are equal. In addition to testing goodness-of-fit, the Pearson statistic can also be used as a test of overdispersion. Overdispersion means that the actual covariance matrix for the observed data exceeds that for the specified model for Y|X.\nHere we divided the pearson statistic by the residual degree of freedom and observed a value very close to 1\n\nres1.pearson_chi2/res1.df_resid\n\n\n\n\n2. Feature importance\n\nConfidence intervals and p-values \nConfidence intervals and p-values quantifying the statistical significance of individual predictor variables. Unlike other models like XGBoost, the estimates for statistical significance of individual predictor variables are readily available.\n\nres1.summary()\n\nFrom the summary, we can see that all of the features other than SOA_Anticipated_Level_Term_Period are significant as all p-values are < 5%.\nDirectionally, the coeficients for the main features like Gender, Smoking Status, Attained_Age or Duration are all aligned with our intuition and the EDA charts that we created previously:\n\nMortality rate for Male is higher than Female\nMortality rate for Smoker is higher than non-Smoker\nMortality rate is higher as age is higher\nMortality rate is higher as duration is longer\n\n\n\n\n3. Main Effects\nWe want to understand the individual effects for each feature in the model. In a GLM context, the coefficient value of each feature already made it easy to understand the direction, magnitude, and shape of a feature’s effect on the predicted value. We can take this further by producing the partial dependence plots (PDP) that display partial dependencies of predicted mortality in terms of key covariates. Within each visualization, the projections are averaged over all covariates not included and over all predicted rows to provide an average representation of the full data set given.\n\ndef pdp2(df, x, hue, predict_col = 'death_hat1'):\n    agg = df.groupby([x, hue])['Number_Of_Deaths', predict_col, 'Policies_Exposed'].sum().reset_index()\n    agg['log_mort_predicted'] = (agg[predict_col]/agg['Policies_Exposed']).apply(lambda x: np.log(x))\n    \n    fig, ax = plt.subplots(figsize = (6,3))\n    sns.lineplot(data = agg, x = x, y = 'log_mort_predicted', hue = hue, ax = ax)\n    plt.xlabel(x)\n    plt.ylabel('log_mort')\n    plt.title(f'Log mortality by {x} and {hue}')\n    plt.show()\n    \npdp2(train_df, 'Attained_Age', 'Gender')\npdp2(train_df, 'Duration', 'Gender')\npdp2(train_df, 'Attained_Age', 'Smoker_Status')\npdp2(train_df, 'Duration', 'Smoker_Status')\npdp2(train_df, 'Attained_Age', 'Preferred_Class')\npdp2(train_df, 'Duration', 'Preferred_Class')\n\nWe can see that the partial dependency plots reconfirms the directional relationships between important covariates and the output that we have discussed in part 2. Feature Importances\nAdditionally, the charts reflect that fact that we have not included any interactions between the covariates. Look at the difference in mortality between smoking and non-smokingm for example, it’s almost constant regardless of ages.\n\n\n4. Interaction Effects\nOne of the key elements in understanding a predictive model is examining its interaction effects. Interaction effects occur when the impact of a change in a variable depends on the values of other features.\nHere we fit a model with all first-order interactions between variables and compare the results against our Vanilla model to evaluate the effect of interactions.\n\n Model 2: Poisson distribution with log link on Death Count with interactions \n\nmodel2 = smf.glm(formula = 'Number_Of_Deaths ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) +  Attained_Age + Duration\\\n                        + C(Observation_Year) * (C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Gender) * (C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Smoker_Status) * (C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration) + C(Face_Amount_Band) * (C(Preferred_Class) + Attained_Age + Duration) + C(Preferred_Class) * (Attained_Age + Duration) + Attained_Age * Duration',\n                data = train_df,\n                family=sm.families.Poisson(sm.families.links.log()),\n                freq_weights = train_df['Policies_Exposed'],\n                offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres2 = model2.fit() #_regularized(method='elastic_net', alpha=0.5)\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat2'] = res2.predict(exog = train_df)\ntrain_df['death_hat2'] = train_df['mort_hat2'] * train_df['Policies_Exposed']\ntest_df['mort_hat2'] = res2.predict(exog = test_df)\ntest_df['death_hat2'] = test_df['mort_hat2'] * test_df['Policies_Exposed']\n\nres2.summary()\n\n\n\n Compared to the vanilla model \nFirst, pearson and deviance are reasonable\n\nprint(f'Pearson_statistics/df = {res2.pearson_chi2/res2.df_resid}')\n\nprint(f'deviance/df = {res2.deviance/res2.df_resid}')\n\nCompared against model 1, we noticed a siginificant reduction on AIC so model 2 has a better fit, but the trade off is a more convoluted set of features.\n\nprint(f'AIC for Model 1 - No interaction: {res1.aic}')\nprint(f'AIC for Model 2 - With interactions: {res2.aic}')\n\nSide note on definition of AIC:  A collection of candidate models can be compared, and the selection criteria may be to choose the model with the highest log-likelihood. However, the log-likelihood of a model will almost always increase with the addition of more variables, even if those variables are insignificant and do little to increase the model’s predictive power. The Akaike information criterion, or AIC, is a penalized log-likelihood formula that charges a penalty for additional variables. It can be thought of as a measure of the relative quality of a model. When considering one or more models fit to the same dataset, the preferred model is the one with the minimum AIC value.\n\n\n\n5. Correlated Features\nFor GLMs and other variations of linear models, correlation, multicollinearity, and aliasing (perfect correlation) among predictor variables can cause standard deviations of coefficients to be large and coefficients to behave erratically, causing issues with interpretability.\nThis is usually assessed by looking at the correlation matrix, which we have seen during the EDA phase. Let’s show it again below. We don’t see severe correlation between any two features that requires dropping one from the feature set.\n\n# we quickly check for any collinearity\nfig, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(train_df[['Gender_Female','Gender_Male','Smoker_Status_NonSmoker','Smoker_Status_Smoker','Preferred_Class_1.0','Preferred_Class_2.0','Preferred_Class_3.0','Preferred_Class_4.0','Attained_Age', 'Duration', 'Policies_Exposed', 'Const']].corr(), annot=True)\nplt.show()\n\n\n\n\nConclusion\nIn this notebook, we walked through the process of building a GLM model for mortality prediction and the important validation exercises to confirm the correctness of the model. - We performed EDA on the ILEC dataset and created a simple GLM model with Poisson distribution and log link and achieved reasonable goodness of fit even with only a handful number of covariates. - We validated and confirmed the soundness of the feature importance and main efferts of important covariates. - We checked for any necessary inclusion of interactions and handling of correlated features.\nApparently, we are still limited by linear combination of covariates at the core of the Poisson GLM model, so certain non-linear dynamics near the two tails of the age distribution are not captured very well. In the Appendix, we show an example of how a more complex model like GBM has the potential to better capture those dynamics.\n\n\nAppendix\n\nModel 1 not using formula\nThis is the explicit setup where we don’t lean on R-like formula to set up the model. The output coefficients are in the same ballpark as model 1 using the formula in the main analysis.\n\n# Target Variable\nY = ['Number_Of_Deaths']\n\n# Predictors (aka Input Variables)\nX = cat_vars_encoded + ['Attained_Age', 'Duration',  'Const'] \n\n# Our choice for Link function is the Gaussian distribution for the nature of death frequency\nmodel = sm.GLM(endog = train_df[Y], \n               exog = train_df[X], \n               family=sm.families.Poisson(sm.families.links.log()),\n               freq_weights = train_df['Policies_Exposed'],\n               offset = train_df['Policies_Exposed'].apply(lambda x: np.log(x))\n              )\nres = model.fit()\nres.summary()\n\n\n\nModel 3: Gaussian distribution with log link on mortality rate\nThis is an experiment where we try to fit a GLM with Gaussian distribution and log link to the mortality rate. Pseudo R-squared is far worse than Model 1\n\nmodel2 = smf.glm(formula = 'mort ~ 1 + C(Observation_Year) + C(Gender) + C(Smoker_Status) + C(Face_Amount_Band) + C(Preferred_Class) + Attained_Age + Duration', \n                 data = train_df,\n                 family=sm.families.Gaussian(link = sm.families.links.log()),\n                 freq_weights = train_df['Policies_Exposed'])\nres2 = model2.fit()\nres2.summary()\n\n\n\nModel 4: XGBoost\nIn this experiment, we fit a Boosted Tree model to show how a more flexible can better fit the training data and generalize on test data.\nNote that a more thorough model building process with cross validation and regularization would be needed to find the best hyperparameters for the XGBRegressor model, we will save that for another time.\n\nX = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period', 'Attained_Age', 'Duration']#, 'Policies_Exposed']\nY = ['mort']#['Number_Of_Deaths']\n\nX_cat = ['Observation_Year', 'Gender', 'Smoker_Status', 'Face_Amount_Band', 'Preferred_Class', 'SOA_Anticipated_Level_Term_Period']\nfor x in X_cat:\n    train_df[x] = train_df[x].astype(\"category\")\n    test_df[x] = test_df[x].astype('category')\n\n\n# create model instance\nbst = xgb.XGBRegressor(n_estimators=50, \n                   max_depth=4, \n                   learning_rate=0.5, \n                   objective='count:poisson', \n                   enable_categorical = True, \n                   tree_method = 'approx', \n                   booster = 'gbtree', \n                   verbosity = 1)\n\n# fit model\nbst.fit(train_df[X], train_df[Y],sample_weight = train_df['Policies_Exposed'])\n\n# make predictions\npreds = bst.predict(test_df[X])\n\n# append fitted values for training and predicted values for testing\ntrain_df['mort_hat4'] = bst.predict(train_df[X])\ntrain_df['death_hat4'] = train_df['mort_hat4'] * train_df['Policies_Exposed']\ntest_df['mort_hat4'] = bst.predict(test_df[X])\ntest_df['death_hat4'] = test_df['mort_hat4'] * test_df['Policies_Exposed']\n\nLift chart does not show too much of a difference from Model 1\n\n# lift chart by deciles\ntest_df['deciles'] = pd.qcut(test_df['mort_hat4'], 10, labels=range(1, 11))\nwm = lambda x: np.average(x, weights=test_df.loc[x.index, \"Policies_Exposed\"])\n\n# groupby and aggregate\nfig, ax = plt.subplots(figsize = (7,3))\ntemp = test_df.groupby([\"deciles\"]).agg(actual=(\"mort_hat4\", wm), predicted = ('mort', wm))\ntemp.plot(ax = ax)\nplt.title('Actual vs Predicted by deciles')\nplt.show()\n\nPlotting actual vs predicted by age shows tighter fit on the training set, and the model seems to be able to capture the dynamics near the two tails of the age distribution better.\n\n# partial dependence plots\npdp(train_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Training', 'death_hat4')\npdp(train_df, 'Duration', 'Actual vs Predicted by Duration - Training', 'death_hat4')\npdp(test_df, 'Attained_Age', 'Actual vs Predicted by Attained_Age - Testing', 'death_hat4')\npdp(test_df, 'Duration', 'Actual vs Predicted by Duration - Testing', 'death_hat4')\n\nLooking at PDP charts and comparing against those of model 1, we see much more complex relationship between the covariates and the log mortality rates.\n\npdp2(train_df, 'Attained_Age', 'Gender', 'death_hat4')\npdp2(train_df, 'Duration', 'Gender','death_hat4')\npdp2(train_df, 'Attained_Age', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Duration', 'Smoker_Status','death_hat4')\npdp2(train_df, 'Attained_Age', 'Preferred_Class','death_hat4')\npdp2(train_df, 'Duration', 'Preferred_Class','death_hat4')\n\n\n\nCompare Model 1, Model 2 and Model 4\n\nfig, ax = plt.subplots(figsize = (7,3))\ntrain_df['Err1'] = (train_df['death_hat1'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat1']\ntrain_df['Err2'] = (train_df['death_hat2'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat2']\ntrain_df['Err4'] = (train_df['death_hat4'] - train_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ train_df['death_hat4']\n\nagg = train_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\n# plt.ylim(0,1)\n# plt.xlim(30,85)\nplt.ylabel('Error')\nplt.title('Training Error')\nplt.show()\n\nfig, ax = plt.subplots(figsize = (7,3))\ntest_df['Err1'] = (test_df['death_hat1'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat1']\ntest_df['Err2'] = (test_df['death_hat2'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat2']\ntest_df['Err4'] = (test_df['death_hat4'] - test_df['Number_Of_Deaths'].astype(float)).apply(lambda x: x**2)/ test_df['death_hat4']\n\nagg = test_df.groupby('Attained_Age')['Err1', 'Err2', 'Err4', 'Policies_Exposed'].sum().reset_index()\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err1']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err2']/agg['Policies_Exposed']), ax = ax)\nsns.lineplot(x = agg['Attained_Age'], y = np.sqrt(agg['Err4']/agg['Policies_Exposed']), ax = ax)\nplt.legend(['Model 1', 'Model 2', 'Model 4'])\nplt.ylabel('Error')\n# plt.ylim(0,1)\n# plt.xlim(30,85)\na = plt.title('Testing Error')\nplt.show()\n\n\nres1.save('mortality_model.pickle')"
  },
  {
    "objectID": "notebooks/send_custom_results.html",
    "href": "notebooks/send_custom_results.html",
    "title": "ValidMind",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv dev.env\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  # Use your project ID\n  project = \"...\"\n)\n  \n\nTrue\n\n\n\n\nIt is possible to send custom metrics to ValidMind without implementing a Metric class. The only requirement is to construct an instance of a MetricResult that can be sent to the ValidMind API.\n\nfrom validmind.vm_models import MetricResult\n\naccuracy_metric = MetricResult(\n    type=\"evaluation\",\n    scope=\"test_dataset\",    \n    key=\"my_custom_accuracy\",\n    value=0.666\n)\n\n\nvm.log_metrics([accuracy_metric])\n\nTrue\n\n\n\n\n\nIt is possible to send custom test results to ValidMind without implementing a ThresholdTest class. The only requirement is to construct an instance of a TestResults that can be sent to the ValidMind API.\n\nfrom validmind.vm_models import TestResult, TestResults\n\ncustom_params = {\n    \"min_percent_threshold\": 0.5\n}\n\ncustom_test_result = TestResults(\n    category=\"model_performance\",\n    test_name=\"accuracy_threshold\",\n    params=custom_params,\n    passed=False,\n    results=[\n        TestResult(\n            passed=False,\n            values={\n                \"score\": 0.15,\n                \"threshold\": custom_params[\"min_percent_threshold\"],\n            },\n        )\n    ],\n)\n\n\nvm.log_test_results([custom_test_result])\n\nTrue\n\n\n\n\n\nIt is possible to implement custom metrics or threshold test classes. The are only two requirements for getting this to work:\n\nWe need to build a TestPlan that can execute the custom metric or threshold test (or add it to an existing TestPlan, TBD).\nWe need to implement a run method on the custom metric or threshold test class.\n\n\n\nThe following example shows how to implement a custom metric that calculates the mean of a list of numbers.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MeanMetric(Metric):\n    type = \"dataset\"\n    key = \"mean_of_values\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        return self.cache_results(mean)\n\n\n\n\nIt is possible to run a custom metric without running an entire test plan. This is useful for testing the metric before integrating it into a test plan.\nThe key idea is to create a TestContext object and pass it to the metric initializer. When a test plan is executed, the TestContext is created by the TestPlan class and passed down to every associated metric and threshold test. However, when we want to test a metric in isolation, we need to create the TestContext ourselves.\nIn this example we don’t need to pass any arguments to the TestContext initializer, but it is possible to pass any arguments as required by required_context.\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))\n\n\nWe can also inspect the results of the metric by accessing the result variable:\n\nmean_metric.result\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))\n\n\n\nmean_metric.result.metric.value\n\n3.0\n\n\n\nmean_metric.result.show()\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n\n\n\n\n\nThe following example shows how to implement a custom threshold test that fails if the mean of a list of numbers is greater than 5.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import ThresholdTest\n\n@dataclass\nclass MeanThresholdTest(ThresholdTest):\n    category = \"data_quality\"\n    name = \"mean_threshold\"\n    default_params = {\"mean_threshold\": 5}    \n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)        \n\n        passed = mean <= self.params[\"mean_threshold\"]\n        results = [\n            TestResult(\n                passed=passed,\n                values={\n                    \"mean\": mean,\n                    \"values\": values,\n                },\n            )\n        ]\n\n        return self.cache_results(results, passed=passed)\n\n\n\n\nSimilarly to custom metrics, it is also possible to run a custom threshold test without running an entire test plan:\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_threshold_test = MeanThresholdTest(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_threshold_test.run()\n\nTestPlanTestResult(figures=None, test_results=TestResults(category='data_quality', test_name='mean_threshold', params={'mean_threshold': 5, 'values': [1, 2, 3, 4, 5]}, passed=True, results=[TestResult(test_name=None, column=None, passed=True, values={'mean': 3.0, 'values': [1, 2, 3, 4, 5]})]))\n\n\n\ntest_results = mean_threshold_test.test_results.test_results\ntest_results.passed\n\nTrue\n\n\n\nfor result in test_results.results:\n    print(f\"passed: {result.passed}, values: {result.values}\")\n\npassed: True, values: {'mean': 3.0, 'values': [1, 2, 3, 4, 5]}\n\n\n\n\n\nThe following example shows how to implement a custom test plan that executes the custom metric and threshold test.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MeanMetric, MeanThresholdTest]\n\nmy_custom_test_plan = MyCustomTestPlan(config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\nmy_custom_test_plan.run()\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\n\nIt is possible to register a custom test plan with ValidMind. This allows us to run the test plan using the same ValidMind Python API and combine it with other test plans as needed.\n\nvm.test_plans.register_test_plan(\"my_custom_test_plan\", MyCustomTestPlan)\n\nRegistered test plan: my_custom_test_plan\n\n\n\nvm.run_test_plan(\"my_custom_test_plan\", config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\n\n                                                                                                                   \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n            \n        \n        \n        \n        \n        \n        \n\n\n\n\n\nIt is possible to send figures with metrics and test results. The following example shows how to send a figure with a metric result.\n\n\nLet’s say we want to add a figure to our custom metric above. We can do this by adding a figures attribute to the cache_results method call. Let’s modify our custom metric code to do that.\n\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom validmind.vm_models import Figure, Metric\n\n@dataclass\nclass MeanMetricWithFigure(Metric):\n    type = \"dataset\"\n    key = \"mean_of_values_with_figure\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(key=self.key, figure=fig, metadata={})\n\n        return self.cache_results(mean, figures=[figure])\n\n\n\n\nSimilarly, we can add a figure to our custom threshold test by adding a figures attribute to the cache_results method call.\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Figure, ThresholdTest\n\n@dataclass\nclass MeanThresholdTestWithFigure(ThresholdTest):\n    category = \"data_quality\"\n    name = \"mean_threshold_with_figure\"\n    default_params = {\"mean_threshold\": 5}    \n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n\n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)        \n\n        passed = mean <= self.params[\"mean_threshold\"]\n        results = [\n            TestResult(\n                passed=passed,\n                values={\n                    \"mean\": mean,\n                    \"values\": values,\n                },\n            )\n        ]\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(key=self.name, figure=fig, metadata={})        \n\n        return self.cache_results(results, passed=passed, figures=[figure])\n\nWe can now run a new test plan that includes our two new custom metrics and threshold tests.\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlanWithFigures(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan_with_figures\"\n    required_context = []\n    tests = [MeanMetricWithFigure, MeanThresholdTestWithFigure]\n\nmy_custom_test_plan_with_figures = MyCustomTestPlanWithFigures(config={\n    \"mean_of_values_with_figure\": {\n        \"values\": [1, 2, 3, 4, 5]\n    },\n    \"mean_threshold_with_figure\": {\n        \"values\": [6, 7, 8, 9, 10]\n    }\n})\nmy_custom_test_plan_with_figures.run()\n\n                                                                                                                                \n\n\n\n            \n            \n            \n                \n                    \n                        Metric Name\n                        mean_of_values_with_figure\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        3.0\n                    \n                \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n            \n                \n                    \n                        Mean Threshold With Figure\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    mean_threshold_with_figure\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'mean_threshold': 5, 'values': [6, 7, 8, 9, 10]}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'mean': 8.0, 'values': [6, 7, 8, 9, 10]})]\n            \n            \n        \n        \n            Metric Plots"
  },
  {
    "objectID": "notebooks/run_individual_tests.html",
    "href": "notebooks/run_individual_tests.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebook shows how to run individual metrics or thresholds tests, and how to pass custom parameters to them.\n\n%load_ext dotenv\n%dotenv dev.env\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clh0yyhg700825x8h4ocp5i3u\"\n)\n\nTrue\n\n\n\n\nWe train a simple customer churn model for our test.\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nWe can now import the individual test and pass the required context and config parameters to it.\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.model_validation.sklearn.threshold_tests import WeakspotsDiagnosis\n\n\ntest_context = TestContext(model=vm_model)\nws_diagnostic = WeakspotsDiagnosis(test_context)\n\n\nws_diagnostic.run()\n\nTestPlanTestResult(result_id=\"weak_spots\", test_results)\n\n\n\nws_diagnostic.test_results.show()\n\n\n        \n        \n            \n                \n                    \n                        Weak Spots\n                    \n                    \n                        ❌\n                    \n                \n            \n            \n                \n                    Test Name\n                    weak_spots\n                \n                \n                    Category\n                    model_diagnosis\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'features_columns': None, 'thresholds': {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.5, 'f1': 0.7}}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column='Gender', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2674, 0, 0, 0, 0, 0, 0, 0, 0, 2126, 856, 0, 0, 0, 0, 0, 0, 0, 0, 744], 'accuracy': [0.9199700822737472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8857008466603951, 0.8820093457943925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8373655913978495], 'precision': [0.8969465648854962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8825214899713467, 0.7448979591836735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7279411764705882], 'recall': [0.556872037914692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6039215686274509, 0.4899328859060403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5409836065573771], 'f1': [0.6871345029239767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7171129220023281, 0.5910931174089069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6206896551724138]}), TestResult(test_name='accuracy', column='Age', passed=False, values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [265, 1042, 1795, 906, 426, 207, 93, 51, 12, 3, 105, 350, 567, 290, 144, 83, 33, 25, 3, 0], 'accuracy': [0.9433962264150944, 0.9299424184261037, 0.9153203342618385, 0.8509933774834437, 0.8661971830985915, 0.9082125603864735, 0.956989247311828, 0.9607843137254902, 1.0, 1.0, 0.9238095238095239, 0.94, 0.8694885361552028, 0.803448275862069, 0.6944444444444444, 0.8433734939759037, 0.9393939393939394, 0.88, 1.0, 0.0], 'precision': [1.0, 0.9285714285714286, 0.9830508474576272, 0.8212290502793296, 0.8854625550660793, 0.9292929292929293, 0.9565217391304348, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 0.5454545454545454, 0.7076923076923077, 0.7222222222222222, 0.8666666666666667, 1.0, 0.0, 0.0, 0.0], 'recall': [0.25, 0.15294117647058825, 0.27751196172248804, 0.588, 0.8663793103448276, 0.8846153846153846, 0.88, 0.6666666666666666, 1.0, 0.0, 0.0, 0.20833333333333334, 0.15789473684210525, 0.5476190476190477, 0.7738095238095238, 0.8478260869565217, 0.7142857142857143, 0.0, 0.0, 0.0], 'f1': [0.4, 0.26262626262626265, 0.4328358208955224, 0.6853146853146853, 0.8758169934640523, 0.9064039408866995, 0.9166666666666666, 0.8, 1.0, 0.0, 0.0, 0.3225806451612903, 0.24489795918367344, 0.6174496644295303, 0.7471264367816092, 0.8571428571428571, 0.8333333333333333, 0.0, 0.0, 0.0]}), TestResult(test_name='accuracy', column='Tenure', passed=False, values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]', '(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'shape': [662, 531, 494, 470, 485, 461, 481, 508, 481, 227, 241, 167, 182, 151, 157, 153, 173, 141, 138, 97], 'accuracy': [0.9063444108761329, 0.8983050847457628, 0.9149797570850202, 0.8936170212765957, 0.9278350515463918, 0.8828633405639913, 0.9147609147609148, 0.90748031496063, 0.8814968814968815, 0.933920704845815, 0.8464730290456431, 0.8982035928143712, 0.8516483516483516, 0.8211920529801324, 0.8789808917197452, 0.8758169934640523, 0.9075144508670521, 0.8156028368794326, 0.8768115942028986, 0.8247422680412371], 'precision': [0.8952380952380953, 0.8688524590163934, 0.8873239436619719, 0.8688524590163934, 0.9104477611940298, 0.8936170212765957, 0.8846153846153846, 0.8703703703703703, 0.8615384615384616, 1.0, 0.6976744186046512, 0.78125, 0.7083333333333334, 0.75, 0.8181818181818182, 0.6818181818181818, 0.7894736842105263, 0.6521739130434783, 0.6875, 0.8461538461538461], 'recall': [0.6482758620689655, 0.5353535353535354, 0.6494845360824743, 0.5578947368421052, 0.6777777777777778, 0.46153846153846156, 0.5679012345679012, 0.5402298850574713, 0.5384615384615384, 0.6511627906976745, 0.5555555555555556, 0.7142857142857143, 0.4594594594594595, 0.40540540540540543, 0.5454545454545454, 0.5555555555555556, 0.5555555555555556, 0.45454545454545453, 0.4782608695652174, 0.4230769230769231], 'f1': [0.752, 0.6625, 0.7500000000000001, 0.6794871794871795, 0.7770700636942676, 0.6086956521739131, 0.6917293233082706, 0.6666666666666666, 0.6627218934911243, 0.7887323943661972, 0.6185567010309277, 0.7462686567164178, 0.5573770491803279, 0.5263157894736842, 0.6545454545454545, 0.6122448979591836, 0.6521739130434783, 0.5357142857142857, 0.5641025641025642, 0.5641025641025641]}), TestResult(test_name='accuracy', column='Balance', passed=False, values={'slice': ['(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]', '(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]'], 'shape': [1771, 34, 165, 542, 994, 840, 352, 86, 15, 1, 565, 10, 57, 211, 321, 281, 119, 32, 3, 1], 'accuracy': [0.9305477131564088, 0.8823529411764706, 0.8787878787878788, 0.8948339483394834, 0.8812877263581489, 0.888095238095238, 0.9090909090909091, 0.8953488372093024, 1.0, 1.0, 0.9079646017699115, 0.8, 0.8947368421052632, 0.8625592417061612, 0.8161993769470405, 0.8398576512455516, 0.7899159663865546, 0.875, 1.0, 1.0], 'precision': [0.9050632911392406, 1.0, 0.9166666666666666, 0.8793103448275862, 0.864406779661017, 0.8702290076335878, 0.9696969696969697, 0.8333333333333334, 1.0, 1.0, 0.8181818181818182, 0.0, 0.6666666666666666, 0.7368421052631579, 0.6811594202898551, 0.7843137254901961, 0.7058823529411765, 0.5, 1.0, 1.0], 'recall': [0.5697211155378487, 0.6, 0.55, 0.504950495049505, 0.6194331983805668, 0.5968586387434555, 0.5079365079365079, 0.5882352941176471, 1.0, 1.0, 0.45, 0.0, 0.5, 0.5957446808510638, 0.5595238095238095, 0.5405405405405406, 0.375, 0.5, 1.0, 1.0], 'f1': [0.6992665036674817, 0.7499999999999999, 0.6874999999999999, 0.6415094339622641, 0.7216981132075471, 0.7080745341614907, 0.6666666666666666, 0.6896551724137931, 1.0, 1.0, 0.5806451612903226, 0.0, 0.5714285714285715, 0.6588235294117647, 0.6143790849673202, 0.6399999999999999, 0.48979591836734687, 0.5, 1.0, 1.0]}), TestResult(test_name='accuracy', column='NumOfProducts', passed=False, values={'slice': ['(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]', '(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]'], 'shape': [2433, 0, 0, 2223, 0, 0, 120, 0, 0, 24, 789, 0, 0, 745, 0, 0, 54, 0, 0, 12], 'accuracy': [0.8586107685984381, 0.0, 0.0, 0.9496176338281601, 0.0, 0.0, 0.9916666666666667, 0.0, 0.0, 1.0, 0.7858048162230672, 0.0, 0.0, 0.9409395973154362, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 1.0], 'precision': [0.8652482269503546, 0.0, 0.0, 0.8529411764705882, 0.0, 0.0, 0.9895833333333334, 0.0, 0.0, 1.0, 0.6790123456790124, 0.0, 0.0, 0.6470588235294118, 0.0, 0.0, 0.9069767441860465, 0.0, 0.0, 1.0], 'recall': [0.5604900459418071, 0.0, 0.0, 0.3625, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4845814977973568, 0.0, 0.0, 0.22448979591836735, 0.0, 0.0, 0.8863636363636364, 0.0, 0.0, 1.0], 'f1': [0.6802973977695167, 0.0, 0.0, 0.5087719298245613, 0.0, 0.0, 0.9947643979057591, 0.0, 0.0, 1.0, 0.5655526992287917, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.896551724137931, 0.0, 0.0, 1.0]}), TestResult(test_name='accuracy', column='HasCrCard', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [1420, 0, 0, 0, 0, 0, 0, 0, 0, 3380, 480, 0, 0, 0, 0, 0, 0, 0, 0, 1120], 'accuracy': [0.9063380281690141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9041420118343195, 0.8520833333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8651785714285715], 'precision': [0.8917525773195877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8872901678657075, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7294117647058823], 'recall': [0.6070175438596491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5718701700154559, 0.46601941747572817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5414847161572053], 'f1': [0.7223382045929019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6954887218045114, 0.5748502994011976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6215538847117795]}), TestResult(test_name='accuracy', column='IsActiveMember', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2326, 0, 0, 0, 0, 0, 0, 0, 0, 2474, 725, 0, 0, 0, 0, 0, 0, 0, 0, 875], 'accuracy': [0.8899398108340498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9187550525464834, 0.8386206896551724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], 'precision': [0.8857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8952879581151832, 0.782051282051282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6410256410256411], 'recall': [0.6413793103448275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48579545454545453, 0.5951219512195122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3937007874015748], 'f1': [0.7439999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6298342541436464, 0.6759002770083102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4878048780487805]}), TestResult(test_name='accuracy', column='EstimatedSalary', passed=False, values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]', '(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'shape': [471, 479, 477, 494, 461, 491, 499, 470, 477, 481, 139, 152, 165, 170, 178, 160, 153, 151, 181, 151], 'accuracy': [0.9171974522292994, 0.918580375782881, 0.8888888888888888, 0.902834008097166, 0.8980477223427332, 0.8879837067209776, 0.8897795591182365, 0.9042553191489362, 0.9119496855345912, 0.9293139293139293, 0.8848920863309353, 0.881578947368421, 0.8424242424242424, 0.8352941176470589, 0.848314606741573, 0.85625, 0.9150326797385621, 0.8807947019867549, 0.850828729281768, 0.8278145695364238], 'precision': [0.9156626506024096, 0.8870967741935484, 0.8717948717948718, 0.8448275862068966, 0.875, 0.8333333333333334, 0.8703703703703703, 0.9090909090909091, 0.92, 0.9365079365079365, 0.65, 0.7368421052631579, 0.7058823529411765, 0.7692307692307693, 0.7058823529411765, 0.6086956521739131, 0.8846153846153846, 0.8421052631578947, 0.7857142857142857, 0.6363636363636364], 'recall': [0.7037037037037037, 0.632183908045977, 0.4146341463414634, 0.5568181818181818, 0.550561797752809, 0.5555555555555556, 0.49473684210526314, 0.5555555555555556, 0.6571428571428571, 0.6629213483146067, 0.5909090909090909, 0.5185185185185185, 0.36363636363636365, 0.47619047619047616, 0.5853658536585366, 0.5, 0.696969696969697, 0.5161290322580645, 0.5116279069767442, 0.4375], 'f1': [0.7958115183246074, 0.7382550335570469, 0.5619834710743802, 0.6712328767123287, 0.6758620689655173, 0.6666666666666667, 0.6308724832214765, 0.6896551724137931, 0.7666666666666667, 0.7763157894736842, 0.6190476190476191, 0.6086956521739131, 0.48000000000000004, 0.588235294117647, 0.64, 0.5490196078431373, 0.7796610169491526, 0.6399999999999999, 0.619718309859155, 0.5185185185185185]}), TestResult(test_name='accuracy', column='Geography_France', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2409, 0, 0, 0, 0, 0, 0, 0, 0, 2391, 811, 0, 0, 0, 0, 0, 0, 0, 0, 789], 'accuracy': [0.8945620589456206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9150982852363028, 0.8310727496917386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8922686945500634], 'precision': [0.8875305623471883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8910891089108911, 0.7023809523809523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8181818181818182], 'recall': [0.6357267950963222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4986149584487535, 0.5756097560975609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4251968503937008], 'f1': [0.7408163265306122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6394316163410302, 0.6327077747989276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5595854922279793]}), TestResult(test_name='accuracy', column='Geography_Germany', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3606, 0, 0, 0, 0, 0, 0, 0, 0, 1194, 1181, 0, 0, 0, 0, 0, 0, 0, 0, 419], 'accuracy': [0.9143094841930116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8760469011725294, 0.8899237933954276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7804295942720764], 'precision': [0.9041533546325878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87248322147651, 0.7798165137614679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.696], 'recall': [0.50355871886121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7027027027027027, 0.44502617801047123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6170212765957447], 'f1': [0.6468571428571428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784431137724551, 0.5666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6541353383458647]}), TestResult(test_name='accuracy', column='Geography_Spain', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3585, 0, 0, 0, 0, 0, 0, 0, 0, 1215, 1208, 0, 0, 0, 0, 0, 0, 0, 0, 392], 'accuracy': [0.902092050209205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9127572016460905, 0.8534768211920529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8852040816326531], 'precision': [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9279279279279279, 0.7382198952879581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7209302325581395], 'recall': [0.6019151846785226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5124378109452736, 0.5261194029850746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375], 'f1': [0.7148659626320065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6602564102564101, 0.6143790849673203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5794392523364486]})]\n            \n            \n                \n                    See Result Details\n                \n            \n            \n        \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n            \n        \n        \n        \n        \n\n\n\nws_diagnostic.test_results.test_results.results\n\n[TestResult(test_name='accuracy', column='Gender', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2674, 0, 0, 0, 0, 0, 0, 0, 0, 2126, 856, 0, 0, 0, 0, 0, 0, 0, 0, 744], 'accuracy': [0.9199700822737472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8857008466603951, 0.8820093457943925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8373655913978495], 'precision': [0.8969465648854962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8825214899713467, 0.7448979591836735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7279411764705882], 'recall': [0.556872037914692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6039215686274509, 0.4899328859060403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5409836065573771], 'f1': [0.6871345029239767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7171129220023281, 0.5910931174089069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6206896551724138]}),\n TestResult(test_name='accuracy', column='Age', passed=False, values={'slice': ['(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]', '(17.926, 25.4]', '(25.4, 32.8]', '(32.8, 40.2]', '(40.2, 47.6]', '(47.6, 55.0]', '(55.0, 62.4]', '(62.4, 69.8]', '(69.8, 77.2]', '(77.2, 84.6]', '(84.6, 92.0]'], 'shape': [265, 1042, 1795, 906, 426, 207, 93, 51, 12, 3, 105, 350, 567, 290, 144, 83, 33, 25, 3, 0], 'accuracy': [0.9433962264150944, 0.9299424184261037, 0.9153203342618385, 0.8509933774834437, 0.8661971830985915, 0.9082125603864735, 0.956989247311828, 0.9607843137254902, 1.0, 1.0, 0.9238095238095239, 0.94, 0.8694885361552028, 0.803448275862069, 0.6944444444444444, 0.8433734939759037, 0.9393939393939394, 0.88, 1.0, 0.0], 'precision': [1.0, 0.9285714285714286, 0.9830508474576272, 0.8212290502793296, 0.8854625550660793, 0.9292929292929293, 0.9565217391304348, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 0.5454545454545454, 0.7076923076923077, 0.7222222222222222, 0.8666666666666667, 1.0, 0.0, 0.0, 0.0], 'recall': [0.25, 0.15294117647058825, 0.27751196172248804, 0.588, 0.8663793103448276, 0.8846153846153846, 0.88, 0.6666666666666666, 1.0, 0.0, 0.0, 0.20833333333333334, 0.15789473684210525, 0.5476190476190477, 0.7738095238095238, 0.8478260869565217, 0.7142857142857143, 0.0, 0.0, 0.0], 'f1': [0.4, 0.26262626262626265, 0.4328358208955224, 0.6853146853146853, 0.8758169934640523, 0.9064039408866995, 0.9166666666666666, 0.8, 1.0, 0.0, 0.0, 0.3225806451612903, 0.24489795918367344, 0.6174496644295303, 0.7471264367816092, 0.8571428571428571, 0.8333333333333333, 0.0, 0.0, 0.0]}),\n TestResult(test_name='accuracy', column='Tenure', passed=False, values={'slice': ['(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]', '(-0.01, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]', '(5.0, 6.0]', '(6.0, 7.0]', '(7.0, 8.0]', '(8.0, 9.0]', '(9.0, 10.0]'], 'shape': [662, 531, 494, 470, 485, 461, 481, 508, 481, 227, 241, 167, 182, 151, 157, 153, 173, 141, 138, 97], 'accuracy': [0.9063444108761329, 0.8983050847457628, 0.9149797570850202, 0.8936170212765957, 0.9278350515463918, 0.8828633405639913, 0.9147609147609148, 0.90748031496063, 0.8814968814968815, 0.933920704845815, 0.8464730290456431, 0.8982035928143712, 0.8516483516483516, 0.8211920529801324, 0.8789808917197452, 0.8758169934640523, 0.9075144508670521, 0.8156028368794326, 0.8768115942028986, 0.8247422680412371], 'precision': [0.8952380952380953, 0.8688524590163934, 0.8873239436619719, 0.8688524590163934, 0.9104477611940298, 0.8936170212765957, 0.8846153846153846, 0.8703703703703703, 0.8615384615384616, 1.0, 0.6976744186046512, 0.78125, 0.7083333333333334, 0.75, 0.8181818181818182, 0.6818181818181818, 0.7894736842105263, 0.6521739130434783, 0.6875, 0.8461538461538461], 'recall': [0.6482758620689655, 0.5353535353535354, 0.6494845360824743, 0.5578947368421052, 0.6777777777777778, 0.46153846153846156, 0.5679012345679012, 0.5402298850574713, 0.5384615384615384, 0.6511627906976745, 0.5555555555555556, 0.7142857142857143, 0.4594594594594595, 0.40540540540540543, 0.5454545454545454, 0.5555555555555556, 0.5555555555555556, 0.45454545454545453, 0.4782608695652174, 0.4230769230769231], 'f1': [0.752, 0.6625, 0.7500000000000001, 0.6794871794871795, 0.7770700636942676, 0.6086956521739131, 0.6917293233082706, 0.6666666666666666, 0.6627218934911243, 0.7887323943661972, 0.6185567010309277, 0.7462686567164178, 0.5573770491803279, 0.5263157894736842, 0.6545454545454545, 0.6122448979591836, 0.6521739130434783, 0.5357142857142857, 0.5641025641025642, 0.5641025641025641]}),\n TestResult(test_name='accuracy', column='Balance', passed=False, values={'slice': ['(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]', '(-250.898, 25089.809]', '(25089.809, 50179.618]', '(50179.618, 75269.427]', '(75269.427, 100359.236]', '(100359.236, 125449.045]', '(125449.045, 150538.854]', '(150538.854, 175628.663]', '(175628.663, 200718.472]', '(200718.472, 225808.281]', '(225808.281, 250898.09]'], 'shape': [1771, 34, 165, 542, 994, 840, 352, 86, 15, 1, 565, 10, 57, 211, 321, 281, 119, 32, 3, 1], 'accuracy': [0.9305477131564088, 0.8823529411764706, 0.8787878787878788, 0.8948339483394834, 0.8812877263581489, 0.888095238095238, 0.9090909090909091, 0.8953488372093024, 1.0, 1.0, 0.9079646017699115, 0.8, 0.8947368421052632, 0.8625592417061612, 0.8161993769470405, 0.8398576512455516, 0.7899159663865546, 0.875, 1.0, 1.0], 'precision': [0.9050632911392406, 1.0, 0.9166666666666666, 0.8793103448275862, 0.864406779661017, 0.8702290076335878, 0.9696969696969697, 0.8333333333333334, 1.0, 1.0, 0.8181818181818182, 0.0, 0.6666666666666666, 0.7368421052631579, 0.6811594202898551, 0.7843137254901961, 0.7058823529411765, 0.5, 1.0, 1.0], 'recall': [0.5697211155378487, 0.6, 0.55, 0.504950495049505, 0.6194331983805668, 0.5968586387434555, 0.5079365079365079, 0.5882352941176471, 1.0, 1.0, 0.45, 0.0, 0.5, 0.5957446808510638, 0.5595238095238095, 0.5405405405405406, 0.375, 0.5, 1.0, 1.0], 'f1': [0.6992665036674817, 0.7499999999999999, 0.6874999999999999, 0.6415094339622641, 0.7216981132075471, 0.7080745341614907, 0.6666666666666666, 0.6896551724137931, 1.0, 1.0, 0.5806451612903226, 0.0, 0.5714285714285715, 0.6588235294117647, 0.6143790849673202, 0.6399999999999999, 0.48979591836734687, 0.5, 1.0, 1.0]}),\n TestResult(test_name='accuracy', column='NumOfProducts', passed=False, values={'slice': ['(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]', '(0.997, 1.3]', '(1.3, 1.6]', '(1.6, 1.9]', '(1.9, 2.2]', '(2.2, 2.5]', '(2.5, 2.8]', '(2.8, 3.1]', '(3.1, 3.4]', '(3.4, 3.7]', '(3.7, 4.0]'], 'shape': [2433, 0, 0, 2223, 0, 0, 120, 0, 0, 24, 789, 0, 0, 745, 0, 0, 54, 0, 0, 12], 'accuracy': [0.8586107685984381, 0.0, 0.0, 0.9496176338281601, 0.0, 0.0, 0.9916666666666667, 0.0, 0.0, 1.0, 0.7858048162230672, 0.0, 0.0, 0.9409395973154362, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 1.0], 'precision': [0.8652482269503546, 0.0, 0.0, 0.8529411764705882, 0.0, 0.0, 0.9895833333333334, 0.0, 0.0, 1.0, 0.6790123456790124, 0.0, 0.0, 0.6470588235294118, 0.0, 0.0, 0.9069767441860465, 0.0, 0.0, 1.0], 'recall': [0.5604900459418071, 0.0, 0.0, 0.3625, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4845814977973568, 0.0, 0.0, 0.22448979591836735, 0.0, 0.0, 0.8863636363636364, 0.0, 0.0, 1.0], 'f1': [0.6802973977695167, 0.0, 0.0, 0.5087719298245613, 0.0, 0.0, 0.9947643979057591, 0.0, 0.0, 1.0, 0.5655526992287917, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.896551724137931, 0.0, 0.0, 1.0]}),\n TestResult(test_name='accuracy', column='HasCrCard', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [1420, 0, 0, 0, 0, 0, 0, 0, 0, 3380, 480, 0, 0, 0, 0, 0, 0, 0, 0, 1120], 'accuracy': [0.9063380281690141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9041420118343195, 0.8520833333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8651785714285715], 'precision': [0.8917525773195877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8872901678657075, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7294117647058823], 'recall': [0.6070175438596491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5718701700154559, 0.46601941747572817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5414847161572053], 'f1': [0.7223382045929019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6954887218045114, 0.5748502994011976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6215538847117795]}),\n TestResult(test_name='accuracy', column='IsActiveMember', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2326, 0, 0, 0, 0, 0, 0, 0, 0, 2474, 725, 0, 0, 0, 0, 0, 0, 0, 0, 875], 'accuracy': [0.8899398108340498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9187550525464834, 0.8386206896551724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88], 'precision': [0.8857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8952879581151832, 0.782051282051282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6410256410256411], 'recall': [0.6413793103448275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48579545454545453, 0.5951219512195122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3937007874015748], 'f1': [0.7439999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6298342541436464, 0.6759002770083102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4878048780487805]}),\n TestResult(test_name='accuracy', column='EstimatedSalary', passed=False, values={'slice': ['(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]', '(-188.401, 20009.67]', '(20009.67, 40007.76]', '(40007.76, 60005.85]', '(60005.85, 80003.94]', '(80003.94, 100002.03]', '(100002.03, 120000.12]', '(120000.12, 139998.21]', '(139998.21, 159996.3]', '(159996.3, 179994.39]', '(179994.39, 199992.48]'], 'shape': [471, 479, 477, 494, 461, 491, 499, 470, 477, 481, 139, 152, 165, 170, 178, 160, 153, 151, 181, 151], 'accuracy': [0.9171974522292994, 0.918580375782881, 0.8888888888888888, 0.902834008097166, 0.8980477223427332, 0.8879837067209776, 0.8897795591182365, 0.9042553191489362, 0.9119496855345912, 0.9293139293139293, 0.8848920863309353, 0.881578947368421, 0.8424242424242424, 0.8352941176470589, 0.848314606741573, 0.85625, 0.9150326797385621, 0.8807947019867549, 0.850828729281768, 0.8278145695364238], 'precision': [0.9156626506024096, 0.8870967741935484, 0.8717948717948718, 0.8448275862068966, 0.875, 0.8333333333333334, 0.8703703703703703, 0.9090909090909091, 0.92, 0.9365079365079365, 0.65, 0.7368421052631579, 0.7058823529411765, 0.7692307692307693, 0.7058823529411765, 0.6086956521739131, 0.8846153846153846, 0.8421052631578947, 0.7857142857142857, 0.6363636363636364], 'recall': [0.7037037037037037, 0.632183908045977, 0.4146341463414634, 0.5568181818181818, 0.550561797752809, 0.5555555555555556, 0.49473684210526314, 0.5555555555555556, 0.6571428571428571, 0.6629213483146067, 0.5909090909090909, 0.5185185185185185, 0.36363636363636365, 0.47619047619047616, 0.5853658536585366, 0.5, 0.696969696969697, 0.5161290322580645, 0.5116279069767442, 0.4375], 'f1': [0.7958115183246074, 0.7382550335570469, 0.5619834710743802, 0.6712328767123287, 0.6758620689655173, 0.6666666666666667, 0.6308724832214765, 0.6896551724137931, 0.7666666666666667, 0.7763157894736842, 0.6190476190476191, 0.6086956521739131, 0.48000000000000004, 0.588235294117647, 0.64, 0.5490196078431373, 0.7796610169491526, 0.6399999999999999, 0.619718309859155, 0.5185185185185185]}),\n TestResult(test_name='accuracy', column='Geography_France', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [2409, 0, 0, 0, 0, 0, 0, 0, 0, 2391, 811, 0, 0, 0, 0, 0, 0, 0, 0, 789], 'accuracy': [0.8945620589456206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9150982852363028, 0.8310727496917386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8922686945500634], 'precision': [0.8875305623471883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8910891089108911, 0.7023809523809523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8181818181818182], 'recall': [0.6357267950963222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4986149584487535, 0.5756097560975609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4251968503937008], 'f1': [0.7408163265306122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6394316163410302, 0.6327077747989276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5595854922279793]}),\n TestResult(test_name='accuracy', column='Geography_Germany', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3606, 0, 0, 0, 0, 0, 0, 0, 0, 1194, 1181, 0, 0, 0, 0, 0, 0, 0, 0, 419], 'accuracy': [0.9143094841930116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8760469011725294, 0.8899237933954276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7804295942720764], 'precision': [0.9041533546325878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87248322147651, 0.7798165137614679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.696], 'recall': [0.50355871886121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7027027027027027, 0.44502617801047123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6170212765957447], 'f1': [0.6468571428571428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784431137724551, 0.5666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6541353383458647]}),\n TestResult(test_name='accuracy', column='Geography_Spain', passed=False, values={'slice': ['(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]', '(-0.001, 0.1]', '(0.1, 0.2]', '(0.2, 0.3]', '(0.3, 0.4]', '(0.4, 0.5]', '(0.5, 0.6]', '(0.6, 0.7]', '(0.7, 0.8]', '(0.8, 0.9]', '(0.9, 1.0]'], 'shape': [3585, 0, 0, 0, 0, 0, 0, 0, 0, 1215, 1208, 0, 0, 0, 0, 0, 0, 0, 0, 392], 'accuracy': [0.902092050209205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9127572016460905, 0.8534768211920529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8852040816326531], 'precision': [0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9279279279279279, 0.7382198952879581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7209302325581395], 'recall': [0.6019151846785226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5124378109452736, 0.5261194029850746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375], 'f1': [0.7148659626320065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6602564102564101, 0.6143790849673203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5794392523364486]})]"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-demo.html",
    "href": "notebooks/r_demo/r-ecm-demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "We want to be able to load R models using Python and describe, test and evaluate them using the ValidMind framework like we do with Python models. This notebook demonstrates how we can load R models either from an RDS file or by building the model in R directly in the notebook with the rpy2 package. Either way, we can then use the ValidMind framework to run a TestPlan designed for the model (in this case, a simple ECM model).\n\n# lets import the required libraries\nimport os\nimport tempfile\n\nimport pandas as pd\nimport rpy2.robjects as robjects\nfrom IPython.display import display_png\nfrom PIL import Image as PILImage\nfrom rpy2.robjects.packages import importr\n\n# import the R packages\ntidyverse = importr(\"tidyverse\")\nbroom = importr(\"broom\")\ngraphics = importr(\"graphics\")\ngrdevices = importr(\"grDevices\")\n\n\n# Load the RDS model we created earlier (in r-ecm-model notebook)\n# alternatively, the model could be recreated in rpy2 from scratch\nr_model = robjects.r[\"readRDS\"](\"r-ecm-model.rds\")\n\n\n# lets run summary on the model\n# in pure R, this would be: `summary(model)`\n# for this, however we want to get a string representation of the summary\n# so we can use it in python\nsummary = robjects.r[\"summary\"](r_model)\nsummary_str = str(summary)\nprint(summary_str)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\n\n\n# now lets something similar to run tidy, augment, and glance\n# in pure R, this would be: `tidy(model)`, `augment(model)`, `glance(model)`\n# however, we want to end up with a pandas dataframe containing the data in the Tibble created by these functions\ntidy = robjects.r[\"tidy\"](r_model)\ntidy_df = pd.DataFrame(robjects.conversion.rpy2py(tidy))\n\naugment = robjects.r[\"augment\"](r_model)\naugment_df = pd.DataFrame(robjects.conversion.rpy2py(augment))\n\nglance = robjects.r[\"glance\"](r_model)\nglance_df = pd.DataFrame(robjects.conversion.rpy2py(glance))\n\n# lets display the dataframes\ndisplay(tidy_df)\ndisplay(augment_df)\ndisplay(glance_df)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n    \n  \n  \n    \n      0\n      (Intercept)\n      deltaCorpProfits\n      deltaFedFundsRate\n      deltaUnempRate\n      CorpProfitsLag1\n      FedFundsRateLag1\n      UnempRateLag1\n      yLag1\n    \n    \n      1\n      -0.416021\n      0.011985\n      -0.123162\n      -1.484146\n      0.002708\n      0.065564\n      -0.053275\n      -0.033703\n    \n    \n      2\n      0.823671\n      0.002033\n      0.154749\n      0.43898\n      0.000826\n      0.049471\n      0.104092\n      0.019268\n    \n    \n      3\n      -0.505082\n      5.894868\n      -0.795883\n      -3.380892\n      3.27874\n      1.325304\n      -0.51181\n      -1.74917\n    \n    \n      4\n      0.614155\n      0.0\n      0.42721\n      0.000896\n      0.001265\n      0.186849\n      0.609448\n      0.082066\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      168\n      169\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n    \n  \n  \n    \n      0\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      ...\n      170\n      171\n      172\n      173\n      174\n      175\n      176\n      177\n      178\n      179\n    \n    \n      1\n      -0.023333\n      0.0\n      0.126667\n      0.04\n      0.013333\n      0.063333\n      -0.05\n      -0.11\n      0.046667\n      -0.066667\n      ...\n      3.668281\n      4.628594\n      3.689168\n      2.55649\n      3.484343\n      1.330312\n      3.180822\n      2.360396\n      -3.238249\n      0.48375\n    \n    \n      2\n      4.012\n      2.183\n      4.194\n      1.068\n      3.195\n      6.352\n      11.655\n      3.034\n      1.692\n      4.836\n      ...\n      27.453\n      44.735\n      -79.877\n      86.058\n      49.698\n      15.633\n      -122.484\n      64.793\n      -58.055\n      -191.198\n    \n    \n      3\n      0.91\n      -0.723333\n      -1.21\n      0.76\n      0.44\n      0.403333\n      1.393333\n      1.28\n      2.743333\n      -0.563333\n      ...\n      -0.033333\n      0.003333\n      -0.013333\n      0.02\n      -0.003333\n      0.01\n      0.01\n      0.013333\n      0.013333\n      0.023333\n    \n    \n      4\n      0.133333\n      -0.1\n      -0.166667\n      -0.066667\n      -0.133333\n      -0.2\n      -0.433333\n      0.0\n      -0.133333\n      -0.033333\n      ...\n      -0.3\n      -0.3\n      -0.266667\n      -0.433333\n      -0.133333\n      -0.4\n      -0.133333\n      -0.166667\n      -0.3\n      -0.1\n    \n    \n      5\n      59.168\n      63.18\n      65.363\n      69.557\n      70.625\n      73.82\n      80.172\n      91.827\n      94.861\n      96.553\n      ...\n      1658.148\n      1685.601\n      1730.336\n      1650.459\n      1736.517\n      1786.215\n      1801.848\n      1679.364\n      1744.157\n      1686.102\n    \n    \n      6\n      4.563333\n      5.473333\n      4.75\n      3.54\n      4.3\n      4.74\n      5.143333\n      6.536667\n      7.816667\n      10.56\n      ...\n      0.116667\n      0.083333\n      0.086667\n      0.073333\n      0.093333\n      0.09\n      0.1\n      0.11\n      0.123333\n      0.136667\n    \n    \n      7\n      5.9\n      6.033333\n      5.933333\n      5.766667\n      5.7\n      5.566667\n      5.366667\n      4.933333\n      4.933333\n      4.8\n      ...\n      7.533333\n      7.233333\n      6.933333\n      6.666667\n      6.233333\n      6.1\n      5.7\n      5.566667\n      5.4\n      5.1\n    \n    \n      8\n      1.136667\n      1.113333\n      1.113333\n      1.24\n      1.28\n      1.293333\n      1.356667\n      1.306667\n      1.196667\n      1.243333\n      ...\n      68.969531\n      72.637812\n      77.266406\n      80.955574\n      83.512063\n      86.996406\n      88.326719\n      91.507541\n      93.867937\n      90.629688\n    \n    \n      9\n      -0.571137\n      0.018616\n      0.165415\n      -0.326461\n      -0.10769\n      0.07776\n      0.417852\n      -0.166964\n      -0.069546\n      0.416952\n      ...\n      2.133892\n      2.301005\n      0.741401\n      2.646158\n      1.939253\n      1.94912\n      -0.082572\n      1.779979\n      0.611129\n      -1.313894\n    \n    \n      10\n      0.547803\n      -0.018616\n      -0.038748\n      0.366461\n      0.121023\n      -0.014427\n      -0.467852\n      0.056964\n      0.116212\n      -0.483619\n      ...\n      1.534389\n      2.327588\n      2.947767\n      -0.089668\n      1.54509\n      -0.618807\n      3.263394\n      0.580416\n      -3.849378\n      1.797644\n    \n    \n      11\n      0.031876\n      0.026309\n      0.043736\n      0.032399\n      0.025897\n      0.024937\n      0.033407\n      0.029378\n      0.063377\n      0.031915\n      ...\n      0.035826\n      0.039586\n      0.050582\n      0.068121\n      0.053507\n      0.061846\n      0.075844\n      0.088754\n      0.087292\n      0.120518\n    \n    \n      12\n      1.715996\n      1.71653\n      1.716528\n      1.716291\n      1.716505\n      1.71653\n      1.71614\n      1.716525\n      1.716506\n      1.716114\n      ...\n      1.712317\n      1.70678\n      1.700683\n      1.716516\n      1.712178\n      1.715827\n      1.696552\n      1.715893\n      1.688317\n      1.710186\n    \n    \n      13\n      0.000436\n      0.0\n      0.000003\n      0.000198\n      0.000017\n      0.0\n      0.000334\n      0.000004\n      0.000042\n      0.00034\n      ...\n      0.003872\n      0.009922\n      0.020808\n      0.000027\n      0.006085\n      0.001148\n      0.040359\n      0.001537\n      0.066262\n      0.021487\n    \n    \n      14\n      0.325303\n      -0.011023\n      -0.023152\n      0.217675\n      0.071646\n      -0.008537\n      -0.278046\n      0.033784\n      0.070162\n      -0.287194\n      ...\n      0.913035\n      1.387735\n      1.76764\n      -0.054274\n      0.92795\n      -0.373291\n      1.983474\n      0.355264\n      -2.354259\n      1.120004\n    \n  \n\n15 rows × 178 columns\n\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      2.919125e-01\n    \n    \n      1\n      2.627560e-01\n    \n    \n      2\n      1.711475e+00\n    \n    \n      3\n      1.001190e+01\n    \n    \n      4\n      1.885342e-10\n    \n    \n      5\n      7.000000e+00\n    \n    \n      6\n      -3.441276e+02\n    \n    \n      7\n      7.062553e+02\n    \n    \n      8\n      7.348913e+02\n    \n    \n      9\n      4.979547e+02\n    \n    \n      10\n      1.700000e+02\n    \n    \n      11\n      1.780000e+02\n    \n  \n\n\n\n\n\n# finally, lets plot the model and somehow get the plots into python\n# in pure R, this would be: `plot(model)`\n# for this, however we want to get a png image of the plots\n\n# first of all, lets get a temporary file path that we can use to save the image\ntemp_file = tempfile.NamedTemporaryFile(suffix=\".png\")\n\n# now lets save the image to the temporary file using grDevices package\ngrdevices.png(temp_file.name, width=1200, height=800)\ngraphics.par(mfrow=robjects.IntVector([2, 2]))\nrobjects.r[\"plot\"](r_model) # creates 4 plots that will be combined into one image\ngrdevices.dev_off()\n\n# now we split the image into the 4 plots\nimage = PILImage.open(temp_file.name)\nwidth, height = image.size\nplot_width = width / 2\nplot_height = height / 2\nplots = [\n    image.crop((0, 0, plot_width, plot_height)),\n    image.crop((plot_width, 0, width, plot_height)),\n    image.crop((0, plot_height, plot_width, height)),\n    image.crop((plot_width, plot_height, width, height))\n]\n\n# display the plots\nfor plot in plots:\n    display_png(plot)\n\n# and finally, lets delete the temporary file\nos.remove(temp_file.name)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html",
    "href": "notebooks/r_demo/r-ecm-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "Install R with Homebrew on macOS:\nbrew install r"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "href": "notebooks/r_demo/r-ecm-model.html#step-1-load-the-required-libraries",
    "title": "ValidMind",
    "section": "Step 1: Load the Required Libraries",
    "text": "Step 1: Load the Required Libraries\nWe will start by loading the necessary libraries that we will use in this notebook. Here, we will use the ecm package to build the ECM model.\n\nlibrary(ecm)\nlibrary(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "href": "notebooks/r_demo/r-ecm-model.html#step-2-load-the-data",
    "title": "ValidMind",
    "section": "Step 2: Load the Data",
    "text": "Step 2: Load the Data\nNext, we will load the data that we will use to build the ECM model. For this example, we will use ecm to predict Wilshire 5000 index based on corporate profits, Federal Reserve funds rate, and unemployment rate\n\n# Load the data\ndata(Wilshire)\n# Use 2015-12-01 and earlier data to build models\ntrn <- Wilshire[Wilshire$date<='2015-12-01',]"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-3-build-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 3: Build the ECM Model",
    "text": "Step 3: Build the ECM Model\nNow, we will build the ECM model using the ecm package.\n\n# Assume all predictors are needed in the equilibrium and transient terms of ecm.\nxeq <- xtr <- trn[c('CorpProfits', 'FedFundsRate', 'UnempRate')]\nmodel <- ecm(trn$Wilshire5000, xeq, xtr, includeIntercept=TRUE)\n\nsummary(model)\n\n\nCall:\nlm(formula = dy ~ ., data = x, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9223 -0.6088  0.0210  0.6822  3.9381 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.4160213  0.8236705  -0.505 0.614155    \ndeltaCorpProfits   0.0119853  0.0020332   5.895 1.97e-08 ***\ndeltaFedFundsRate -0.1231619  0.1547487  -0.796 0.427210    \ndeltaUnempRate    -1.4841457  0.4389805  -3.381 0.000896 ***\nCorpProfitsLag1    0.0027077  0.0008258   3.279 0.001265 ** \nFedFundsRateLag1   0.0655636  0.0494706   1.325 0.186849    \nUnempRateLag1     -0.0532751  0.1040916  -0.512 0.609448    \nyLag1             -0.0337028  0.0192679  -1.749 0.082066 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.711 on 170 degrees of freedom\nMultiple R-squared:  0.2919,    Adjusted R-squared:  0.2628 \nF-statistic: 10.01 on 7 and 170 DF,  p-value: 1.885e-10\n\n\n\nsummary(model)$coefficients\n\n\n\nA matrix: 8 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits 0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate-1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1 0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1 0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1-0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1-0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\ntidy(model)\n\n\n\nA tibble: 8 × 5\n\n    termestimatestd.errorstatisticp.value\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    (Intercept)      -0.4160213030.8236705332-0.50508226.141553e-01\n    deltaCorpProfits  0.0119853370.0020331815 5.89486841.967203e-08\n    deltaFedFundsRate-0.1231619050.1547486740-0.79588344.272097e-01\n    deltaUnempRate   -1.4841457020.4389804803-3.38089228.963744e-04\n    CorpProfitsLag1   0.0027076520.0008258208 3.27874051.264620e-03\n    FedFundsRateLag1  0.0655636130.0494706252 1.32530391.868488e-01\n    UnempRateLag1    -0.0532751110.1040916078-0.51180996.094483e-01\n    yLag1            -0.0337028160.0192678856-1.74917048.206627e-02\n\n\n\n\n\naugment(model)\n\n\n\nA tibble: 178 × 15\n\n    .rownamesdydeltaCorpProfitsdeltaFedFundsRatedeltaUnempRateCorpProfitsLag1FedFundsRateLag1UnempRateLag1yLag1.fitted.resid.hat.sigma.cooksd.std.resid\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    2 -0.023333333  4.012 0.91000000 0.13333333 59.168 4.5633335.9000001.1366667-0.57113659 0.5478032600.031876201.7159964.355351e-04 0.325303481\n    3  0.000000000  2.183-0.72333333-0.10000000 63.180 5.4733336.0333331.1133333 0.01861637-0.0186163730.026308501.7165304.104050e-07-0.011023359\n    4  0.126666667  4.194-1.21000000-0.16666667 65.363 4.7500005.9333331.1133333 0.16541470-0.0387480300.043736251.7165283.064464e-06-0.023152097\n    5  0.040000000  1.068 0.76000000-0.06666667 69.557 3.5400005.7666671.2400000-0.32646091 0.3664609080.032399111.7162911.983193e-04 0.217675258\n    6  0.013333333  3.195 0.44000000-0.13333333 70.625 4.3000005.7000001.2800000-0.10768956 0.1210228920.025897371.7165051.705888e-05 0.071646457\n    7  0.063333333  6.352 0.40333333-0.20000000 73.820 4.7400005.5666671.2933333 0.07776005-0.0144267170.024936681.7165302.329577e-07-0.008536516\n    8 -0.050000000 11.655 1.39333333-0.43333333 80.172 5.1433335.3666671.3566667 0.41785218-0.4678521810.033407001.7161403.339924e-04-0.278045827\n    9 -0.110000000  3.034 1.28000000 0.00000000 91.827 6.5366674.9333331.3066667-0.16696419 0.0569641920.029377801.7165254.318091e-06 0.033783635\n    10 0.046666667  1.692 2.74333333-0.13333333 94.861 7.8166674.9333331.1966667-0.06954559 0.1162122590.063376581.7165064.163627e-05 0.070161519\n    11-0.066666667  4.836-0.56333333-0.03333333 96.55310.5600004.8000001.2433333 0.41695185-0.4836185150.031915461.7161143.398987e-04-0.287194305\n    12-0.046666667  6.127-0.67333333 0.36666667101.389 9.9966674.7666671.1766667-0.16750282 0.1208361570.028032821.7165051.848956e-05 0.071614448\n    13-0.110000000  2.535 1.92666667 0.06666667107.516 9.3233335.1333331.1300000-0.13104919 0.0210491890.043999891.7165309.102812e-07 0.012578705\n    14-0.186666667  5.197 0.84000000 0.43333333110.05111.2500005.2000001.0200000-0.37615628 0.1894896130.039931241.7164666.638164e-05 0.112996186\n    15 0.003333333-13.096-2.74333333 0.96666667115.24812.0900005.6333330.8333333-0.89326786 0.8966011900.077122691.7150293.106433e-03 0.545326840\n    16 0.140000000-12.925-3.04333333 1.66666667102.152 9.3466676.6000000.8366667-2.16010566 2.3001056600.149084931.7057814.648641e-02 1.456915037\n    17 0.143333333  4.811-0.88333333 0.60000000 89.227 6.3033338.2666670.9766667-0.95851326 1.1018465940.045981031.7143362.617436e-03 0.659131663\n    18-0.043333333 16.609 0.74000000-0.40000000 94.038 5.4200008.8666671.1200000 0.38541881-0.4287521470.039334411.7162013.343566e-04-0.255593460\n    19 0.040000000  7.948-0.74666667-0.16666667110.647 6.1600008.4666671.0766667 0.23467281-0.1946728100.029288701.7164635.026893e-05-0.115448901\n    20 0.160000000  8.331-0.58666667-0.56666667118.595 5.4133338.3000001.1166667 0.79331290-0.6333128990.047063801.7158058.870848e-04-0.379066991\n    21 0.023333333  3.825 0.37000000-0.16666667126.926 4.8266677.7333331.2766667 0.03671404-0.0133807080.022620991.7165301.809310e-07-0.007908191\n    22 0.036666667  2.177 0.08666667 0.16666667130.751 5.1966677.5666671.3000000-0.40014906 0.4368157300.023399561.7161941.997742e-04 0.258267261\n    23 0.026666667  0.171-0.41000000 0.03333333132.928 5.2833337.7333331.3366667-0.16367336 0.1903400260.021229221.7164673.426111e-05 0.112413682\n    24-0.010000000 10.848-0.21333333-0.26666667133.099 4.8733337.7666671.3633333 0.35622331-0.3662233060.024874771.7162941.497267e-04-0.216693223\n    25 0.006666667  8.840 0.49666667-0.36666667143.947 4.6600007.5000001.3533333 0.42305580-0.4163891310.024524301.7162241.906916e-04-0.246331926\n    26 0.006666667  6.448 0.66333333-0.23333333152.787 5.1566677.1333331.3600000 0.25178222-0.2451155550.019335271.7164255.154897e-05-0.144623924\n    27-0.013333333  1.004 0.69333333-0.23333333159.235 5.8200006.9000001.3666667 0.25599483-0.2693281660.017280481.7164035.538973e-05-0.158743710\n    28-0.040000000  5.396 0.24333333-0.33333333160.239 6.5133336.6666671.3533333 0.57352801-0.6135280130.018415451.7158703.070187e-04-0.361826262\n    29 0.153333333 18.499 0.52666667-0.33333333165.635 6.7566676.3333331.3133333 0.74534680-0.5920134700.018631561.7159152.893460e-04-0.349176568\n    30 0.126666667  6.325 0.81666667 0.03333333184.134 7.2833336.0000001.4666667 0.11674323 0.0099234380.017495501.7165307.616416e-08 0.005849577\n    31-0.116666667 11.520 1.48333333-0.13333333190.459 8.1000006.0333331.5933333 0.40888139-0.5255480600.025206401.7160423.126651e-04-0.311018089\n    ⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n    150 -3.93984375 -40.796-0.146666667 0.666666671229.6192.086666675.33333351.19484-0.41969615-3.520147600.049995301.6939002.929332e-02-2.11021907\n    151-13.21515625-517.379-1.433333333 0.866666671188.8231.940000006.00000047.25500-6.29287534-6.922280910.450834881.5589003.056885e+00-5.45792159\n    152 -3.41295850 372.812-0.323333333 1.40000000 671.4440.506666676.86666734.03984 2.35246941-5.765427910.316976911.6304949.638047e-01-4.07608658\n    153  3.81120999  76.362-0.003333333 1.033333331044.2560.183333338.26666730.62689 0.33287856 3.478331430.119783231.6926747.982295e-02 2.16623560\n    154  4.38612351 152.571-0.023333333 0.333333331120.6180.180000009.30000034.43810 2.31067811 2.075445400.070595941.7085241.502320e-02 1.25787816\n    155  3.78281250 101.178-0.036666667 0.300000001273.1890.156666679.63333338.82422 1.99182565 1.790986850.063379231.7106189.889477e-03 1.08128468\n    156  1.98280482  75.820 0.013333333-0.100000001374.3670.120000009.93333342.60703 2.40347852-0.420673710.052818351.7162094.446089e-04-0.25255634\n    157  0.99476711 -13.432 0.060000000-0.200000001450.1870.133333339.83333344.58984 1.62109998-0.626332870.054187501.7158161.014071e-03-0.37629827\n    158 -1.36429067  62.316-0.006666667-0.166666671436.7550.193333339.63333345.58460 2.43239779-3.796688470.048396961.6902203.287650e-02-2.27408451\n    159  5.05359375  -4.611 0.000000000 0.033333331499.0710.186666679.46666744.22031 1.55575748 3.497836270.056815211.6940253.334564e-02 2.10441132\n    160  4.37222278-144.215-0.030000000-0.466666671494.4600.186666679.50000049.27391 0.44374298 3.928479800.090026541.6870467.160301e-02 2.40624425\n    161  0.91371224  72.900-0.063333333 0.033333331350.2450.156666679.03333353.64613 1.79302659-0.879314350.043377471.7151371.564013e-03-0.52529517\n    162 -3.86734127   7.241-0.010000000-0.066666671423.1450.093333339.06666754.55984 1.30859223-5.175933500.039141481.6677824.846909e-02-3.08523686\n    163 -0.01948413  76.687-0.010000000-0.366666671430.3860.083333339.00000050.69250 2.73901212-2.758496250.042936741.7027721.522165e-02-1.64752351\n    164  5.57569380 220.314 0.030000000-0.366666671507.0730.073333338.63333350.67302 4.68268067 0.893013130.098256501.7150064.112255e-03 0.54947240\n    165  0.03144905 -72.595 0.050000000-0.066666671727.3870.103333338.26666756.24871 1.15447874-1.123029690.061144801.7142143.733480e-03-0.67720706\n    166  2.23761905  31.842-0.010000000-0.166666671654.7920.153333338.20000056.28016 2.37120363-0.133584590.041488471.7164993.438860e-05-0.07972366\n    167  1.14496416 -22.685 0.016666667-0.233333331686.6340.143333338.03333358.51778 1.83236442-0.687400260.042991361.7156799.465378e-04-0.41056442\n    168  4.96225806  15.537-0.016666667-0.066666671663.9490.160000007.80000059.66274 1.96072774 3.001530320.035880771.7003481.484071e-02 1.78610468\n    169  4.34453125 -21.338-0.026666667-0.200000001679.4860.143333337.73333364.62500 1.59517181 2.749359440.035211341.7029731.220254e-02 1.63547900\n    170  3.66828125  27.453-0.033333333-0.300000001658.1480.116666677.53333368.96953 2.13389190 1.534389350.035826041.7123173.871937e-03 0.91303500\n    171  4.62859375  44.735 0.003333333-0.300000001685.6010.083333337.23333372.63781 2.30100541 2.327588340.039586121.7067809.922189e-03 1.38773488\n    172  3.68916752 -79.877-0.013333333-0.266666671730.3360.086666676.93333377.26641 0.74140097 2.947766550.050581811.7006832.080820e-02 1.76764015\n    173  2.55648972  86.058 0.020000000-0.433333331650.4590.073333336.66666780.95557 2.64615821-0.089668480.068120881.7165162.691596e-05-0.05427372\n    174  3.48434276  49.698-0.003333333-0.133333331736.5170.093333336.23333383.51206 1.93925279 1.545089970.053506951.7121786.084879e-03 0.92795006\n    175  1.33031250  15.633 0.010000000-0.400000001786.2150.090000006.10000086.99641 1.94911984-0.618807340.061846411.7158271.148277e-03-0.37329144\n    176  3.18082223-122.484 0.010000000-0.133333331801.8480.100000005.70000088.32672-0.08257207 3.263394300.075844431.6965524.035912e-02 1.98347420\n    177  2.36039552  64.793 0.013333333-0.166666671679.3640.110000005.56666791.50754 1.77997920 0.580416330.088753711.7158931.536609e-03 0.35526406\n    178 -3.23824901 -58.055 0.013333333-0.300000001744.1570.123333335.40000093.86794 0.61112886-3.849377870.087292381.6883176.626180e-02-2.35425908\n    179  0.48375000-191.198 0.023333333-0.100000001686.1020.136666675.10000090.62969-1.31389361 1.797643610.120518221.7101862.148698e-02 1.12000437\n\n\n\n\n\nglance(model)\n\n\n\nA tibble: 1 × 12\n\n    r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobs\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><int><int>\n\n\n    0.29191250.2627561.71147510.01191.885342e-107-344.1276706.2553734.8913497.9547170178\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(model)"
  },
  {
    "objectID": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "href": "notebooks/r_demo/r-ecm-model.html#step-4-save-the-ecm-model",
    "title": "ValidMind",
    "section": "Step 4: Save the ECM Model",
    "text": "Step 4: Save the ECM Model\nFinally, we will save the ECM model to a file so that we can use it later.\n\n# save the model to an RDS file\nsaveRDS(model, 'r-ecm-model.rds')"
  },
  {
    "objectID": "notebooks/r_demo/rpy2.html",
    "href": "notebooks/r_demo/rpy2.html",
    "title": "ValidMind",
    "section": "",
    "text": "from rpy2.robjects.packages import importr\n\n\nbase = importr('base')\n\n\nfrom rpy2.robjects.packages import importr\n\ntidyr = importr('tidyr')\nggplot2 = importr('ggplot2')\npurrr = importr('purrr')\nprintr = importr('printr')\npROC = importr('pROC') \nROCR = importr('ROCR') \ncaret = importr('caret')\ncar = importr('car')\nrpart = importr('rpart')\nrpart_plot = importr('rpart.plot')\n\n\nfrom rpy2.robjects import r\n\ndata = r['read.csv']('./datasets/bank_customer_churn.csv', stringsAsFactors = True)\n\n\nr['str'](data)\n\n'data.frame':   8000 obs. of  14 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : Factor w/ 2616 levels \"Abazu\",\"Abbie\",..: 1002 1060 1832 258 1634 485 156 1793 1032 970 ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : Factor w/ 3 levels \"France\",\"Germany\",..: 1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num  0 83808 159661 0 125511 ...\n $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n<rpy2.rinterface_lib.sexp.NULLType object at 0x103d3c740> [RTYPES.NILSXP]\n\n\n\nr('''\n    knitr::kable(sapply(data, function(x) sum(is.na(x))), col.names = c(\"Missing Value Count\"))\n''')\n\n\n\n        StrVector with 10 elements.\n        \n        \n          \n          \n            \n            '|       ...\n            \n          \n            \n            '|:------...\n            \n          \n            \n            '|...    ...\n            \n          \n            \n            ...\n            \n          \n            \n            '|envir  ...\n            \n          \n            \n            '|overwri...\n            \n          \n            \n            '|       ...\n            \n          \n          \n        \n        \n        \n\n\n\nr(\"\"\"\n    # plot box plot\n    data[, names(data) %in% c('Age', 'Balance', 'CreditScore', 'EstimatedSalary')] %>%\n    gather() %>%\n    ggplot(aes(value)) +\n        facet_wrap(~ key, scales = \"free\") +\n        geom_boxplot() +\n        theme(axis.text.x = element_text(size = 7, angle=90), axis.text.y = element_text(size = 7))\n\"\"\")\n\nR[write to console]: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable\n\n\n\nRRuntimeError: Error in data[, names(data) %in% c(\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\")] : \n  object of type 'closure' is not subsettable"
  },
  {
    "objectID": "notebooks/r_demo/r-customer-churn-model.html",
    "href": "notebooks/r_demo/r-customer-churn-model.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebook demonstrates the process of creating a customer churn model using R. We will use the XGBoost and logistic regression algorithms to create two separate models and compare their performance.\nThe dataset used in this notebook is located at ../datasets/bank_customer_churn.csv.\n\n\n\n\nbrew install r\nYou additionally might need the following packages to run this notebook if you run into errors:\nbrew install harfbuzz fribidi libtiff libomp\n\n\n\ninstall.packages(\"xgboost\")\ninstall.packages(\"caret\")\ninstall.packages(\"pROC\")\n\n\n\n\nFirst, we load the required libraries.\n\n# load the required libraries\nlibrary(xgboost)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC)\n\n\n\n\nNow, we import the dataset and preprocess it by removing irrelevant columns, converting categorical variables, and one-hot encoding certain columns.\n\n# import the dataset\ndf <- read.csv(\"../datasets/bank_customer_churn.csv\", header = TRUE)\n\n# remove irrelevant columns\ndf <- df %>% select(-c(RowNumber, CustomerId, Surname, CreditScore))\n\n# Convert the 'Gender' column to 0 or 1 (assuming \"Female\" should be 0 and \"Male\" should be 1)\ndf$Gender <- ifelse(df$Gender == \"Female\", 0, 1)\n\n# one-hot encode categorical columns with caret\ndf <- dummyVars(\" ~ .\", data = df) %>% predict(df)\n\n# remove GeographySpain since it causes multicollinearity\ndf <- subset(df, select = -GeographySpain)\n\nsummary(df)\n\n GeographyFrance  GeographyGermany     Gender            Age       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :18.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:32.00  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :37.00  \n Mean   :0.5012   Mean   :0.2511   Mean   :0.5495   Mean   :38.95  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:44.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :92.00  \n     Tenure          Balance       NumOfProducts     HasCrCard     \n Min.   : 0.000   Min.   :     0   Min.   :1.000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:     0   1st Qu.:1.000   1st Qu.:0.0000  \n Median : 5.000   Median : 97264   Median :1.000   Median :1.0000  \n Mean   : 5.034   Mean   : 76434   Mean   :1.532   Mean   :0.7026  \n 3rd Qu.: 8.000   3rd Qu.:128045   3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :250898   Max.   :4.000   Max.   :1.0000  \n IsActiveMember   EstimatedSalary         Exited     \n Min.   :0.0000   Min.   :    11.58   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.: 50857.10   1st Qu.:0.000  \n Median :1.0000   Median : 99504.89   Median :0.000  \n Mean   :0.5199   Mean   : 99790.19   Mean   :0.202  \n 3rd Qu.:1.0000   3rd Qu.:149216.32   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :199992.48   Max.   :1.000  \n\n\n\n\n\nNext, we split the data into training and testing sets using a 70/30 ratio.\n\n# split data into training and testing sets\nset.seed(123)\ntrain_index <- sample(1:nrow(df), size = round(0.7*nrow(df)), replace = FALSE)\ndf_train <- df[train_index, ]\ndf_test <- df[-train_index, ]\n\n\n\n\nWe save the train and test datasets as CSV files.\n\n# save the train and test datasets as csv files\nwrite.csv(df_train, file = \"r_churn_train.csv\", row.names = FALSE)\nwrite.csv(df_test, file = \"r_churn_test.csv\", row.names = FALSE)\n\n\n\n\nWe convert the data into DMatrix format, which is required by the XGBoost library.\n\n# convert data into DMatrix format\ndtrain <- xgb.DMatrix(data = df_train[,-c(11)], label = df_train[,\"Exited\"])\ndtest <- xgb.DMatrix(data = df_test[,-c(11)], label = df_test[,\"Exited\"])\n\n\n\n\nWe set up the XGBoost parameters to be used during the training process.\n\n# set up XGBoost parameters\nparams <- list(\n  objective = \"binary:logistic\",\n  eval_metric = \"auc\",\n  max_depth = 3,\n  eta = 0.1,\n  gamma = 0.5,\n  subsample = 0.8,\n  colsample_bytree = 0.8,\n  min_child_weight = 1,\n  nthread = 4\n)\n\n\n\n\nWe train the XGBoost model using the parameters and data prepared earlier.\n\n# train the XGBoost model\nmodel <- xgb.train(\n  params = params,\n  data = dtrain,\n  nrounds = 100,\n  watchlist = list(train = dtrain, test = dtest),\n  early_stopping_rounds = 10\n)\n\n[1] train-auc:0.795105  test-auc:0.793822 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 10 rounds.\n\n[2] train-auc:0.820697  test-auc:0.808123 \n[3] train-auc:0.823965  test-auc:0.811294 \n[4] train-auc:0.837212  test-auc:0.823692 \n[5] train-auc:0.839206  test-auc:0.827146 \n[6] train-auc:0.843781  test-auc:0.832219 \n[7] train-auc:0.853531  test-auc:0.836494 \n[8] train-auc:0.857080  test-auc:0.838679 \n[9] train-auc:0.857191  test-auc:0.839732 \n[10]    train-auc:0.856166  test-auc:0.840575 \n[11]    train-auc:0.857386  test-auc:0.841168 \n[12]    train-auc:0.857084  test-auc:0.841385 \n[13]    train-auc:0.856794  test-auc:0.842336 \n[14]    train-auc:0.857827  test-auc:0.841208 \n[15]    train-auc:0.858503  test-auc:0.842312 \n[16]    train-auc:0.860074  test-auc:0.843007 \n[17]    train-auc:0.858916  test-auc:0.843317 \n[18]    train-auc:0.858676  test-auc:0.843113 \n[19]    train-auc:0.858821  test-auc:0.843309 \n[20]    train-auc:0.859816  test-auc:0.845118 \n[21]    train-auc:0.860960  test-auc:0.845355 \n[22]    train-auc:0.860677  test-auc:0.845800 \n[23]    train-auc:0.862110  test-auc:0.848165 \n[24]    train-auc:0.863008  test-auc:0.847703 \n[25]    train-auc:0.863292  test-auc:0.848459 \n[26]    train-auc:0.864104  test-auc:0.849124 \n[27]    train-auc:0.863918  test-auc:0.848931 \n[28]    train-auc:0.864840  test-auc:0.849988 \n[29]    train-auc:0.867052  test-auc:0.850861 \n[30]    train-auc:0.867307  test-auc:0.851330 \n[31]    train-auc:0.867691  test-auc:0.851321 \n[32]    train-auc:0.868384  test-auc:0.852436 \n[33]    train-auc:0.870037  test-auc:0.854785 \n[34]    train-auc:0.870912  test-auc:0.855056 \n[35]    train-auc:0.871229  test-auc:0.855566 \n[36]    train-auc:0.871868  test-auc:0.855823 \n[37]    train-auc:0.872831  test-auc:0.857390 \n[38]    train-auc:0.873618  test-auc:0.856942 \n[39]    train-auc:0.874207  test-auc:0.858250 \n[40]    train-auc:0.874417  test-auc:0.858442 \n[41]    train-auc:0.874423  test-auc:0.858399 \n[42]    train-auc:0.875082  test-auc:0.858565 \n[43]    train-auc:0.876418  test-auc:0.858693 \n[44]    train-auc:0.876752  test-auc:0.858042 \n[45]    train-auc:0.877068  test-auc:0.857847 \n[46]    train-auc:0.877774  test-auc:0.857960 \n[47]    train-auc:0.879001  test-auc:0.858793 \n[48]    train-auc:0.879490  test-auc:0.858593 \n[49]    train-auc:0.880067  test-auc:0.859837 \n[50]    train-auc:0.880651  test-auc:0.860296 \n[51]    train-auc:0.880768  test-auc:0.860439 \n[52]    train-auc:0.881091  test-auc:0.861333 \n[53]    train-auc:0.881637  test-auc:0.861322 \n[54]    train-auc:0.881942  test-auc:0.861473 \n[55]    train-auc:0.882016  test-auc:0.861432 \n[56]    train-auc:0.882721  test-auc:0.861169 \n[57]    train-auc:0.882949  test-auc:0.861621 \n[58]    train-auc:0.883306  test-auc:0.861810 \n[59]    train-auc:0.883506  test-auc:0.861506 \n[60]    train-auc:0.883632  test-auc:0.861516 \n[61]    train-auc:0.883968  test-auc:0.861404 \n[62]    train-auc:0.884255  test-auc:0.861261 \n[63]    train-auc:0.884719  test-auc:0.861073 \n[64]    train-auc:0.885060  test-auc:0.861043 \n[65]    train-auc:0.885252  test-auc:0.861357 \n[66]    train-auc:0.885285  test-auc:0.861367 \n[67]    train-auc:0.885594  test-auc:0.860972 \n[68]    train-auc:0.886119  test-auc:0.860755 \nStopping. Best iteration:\n[58]    train-auc:0.883306  test-auc:0.861810\n\n\n\n\n\n\nWe display a summary of the trained XGBoost model.\n\nsummary(model)\n\n                Length Class              Mode       \nhandle              1  xgb.Booster.handle externalptr\nraw             81624  -none-             raw        \nbest_iteration      1  -none-             numeric    \nbest_ntreelimit     1  -none-             numeric    \nbest_score          1  -none-             numeric    \nbest_msg            1  -none-             character  \nniter               1  -none-             numeric    \nevaluation_log      3  data.table         list       \ncall                6  -none-             call       \nparams             10  -none-             list       \ncallbacks           3  -none-             list       \nfeature_names      10  -none-             character  \nnfeatures           1  -none-             numeric    \n\n\n\n\n\nWe make predictions on the test data and calculate the accuracy of the model.\n\n# predict on test data\ntest_preds <- predict(model, dtest)\n\n# Convert predicted probabilities to binary predictions\ntest_preds_binary <- ifelse(test_preds > 0.5, 1, 0)\n\n# Calculate accuracy on test set\naccuracy <- sum(test_preds_binary == df_test[,\"Exited\"])/nrow(df_test)\naccuracy\n\n0.860416666666667\n\n\n\n\n\nWe calculate the confusion matrix, precision, recall, F1 score, and ROC AUC for the model.\n\n# Calculate the confusion matrix\ncm <- confusionMatrix(as.factor(test_preds_binary), as.factor(df_test[,\"Exited\"]))\n\n# Calculate precision, recall, and F1 score\nprecision <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[2, 1])\nrecall <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[1, 2])\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\ncat(\"Precision:\", precision, \"\\n\")\ncat(\"Recall:\", recall, \"\\n\")\ncat(\"F1 Score:\", f1_score, \"\\n\")\n\n# Calculate ROC AUC\nroc_obj <- roc(df_test[,\"Exited\"], test_preds)\nroc_auc <- auc(roc_obj)\ncat(\"ROC AUC:\", roc_auc, \"\\n\")\n\nPrecision: 0.7687075 \nRecall: 0.4584178 \nF1 Score: 0.5743329 \n\n\nSetting levels: control = 0, case = 1\n\nSetting direction: controls < cases\n\n\n\nROC AUC: 0.86181 \n\n\n\n\n\nWe save the trained XGBoost model as a JSON file.\n\n# save the model (notice the .json extension, we could also save it as .bin)\n# this ensures compatibility with the ValidMind sdk\nxgb.save(model, \"r_xgb_churn_model.json\")\n\nTRUE\n\n\n\n\n\nAs a comparison, we train a simple logistic regression model using the training data.\n\n# now lets train a simple logistic regression model\nlg_reg_model <- glm(Exited ~ ., data = as.data.frame(df_train), family = \"binomial\")\n\n\n\n\nWe display a summary of the trained logistic regression model.\n\nsummary(lg_reg_model)\n\n\nCall:\nglm(formula = Exited ~ ., family = \"binomial\", data = as.data.frame(df_train))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3284  -0.6470  -0.4563  -0.2781   2.8954  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      -3.732e+00  2.305e-01 -16.187  < 2e-16 ***\nGeographyFrance  -1.113e-01  9.474e-02  -1.174 0.240252    \nGeographyGermany  7.394e-01  1.054e-01   7.018 2.25e-12 ***\nGender           -4.974e-01  7.319e-02  -6.796 1.07e-11 ***\nAge               7.142e-02  3.433e-03  20.803  < 2e-16 ***\nTenure           -1.170e-02  1.263e-02  -0.926 0.354301    \nBalance           2.525e-06  6.995e-07   3.610 0.000306 ***\nNumOfProducts    -1.300e-01  6.475e-02  -2.008 0.044643 *  \nHasCrCard        -1.468e-02  8.025e-02  -0.183 0.854836    \nIsActiveMember   -9.979e-01  7.682e-02 -12.989  < 2e-16 ***\nEstimatedSalary   2.248e-07  6.337e-07   0.355 0.722854    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5612.8  on 5599  degrees of freedom\nResidual deviance: 4760.6  on 5589  degrees of freedom\nAIC: 4782.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\ncoef(lg_reg_model)\n\n(Intercept)-3.73163074444561GeographyFrance-0.111254692545161GeographyGermany0.73938562185064Gender-0.497380970370031Age0.0714194308317766Tenure-0.0116956480620419Balance2.52544411990314e-06NumOfProducts-0.130015233022276HasCrCard-0.0146825015464598IsActiveMember-0.997861450907317EstimatedSalary2.24751413745725e-07\n\n\n\n\n\nWe make predictions on the test data and calculate the accuracy of the logistic regression model.\n\n# Make predictions on test set\ntest_preds <- predict(lg_reg_model, newdata = as.data.frame(df_test), type = \"response\")\n\n# Convert predicted probabilities to binary predictions\ntest_preds_binary <- ifelse(test_preds > 0.5, 1, 0)\n\n# Calculate accuracy on test set\naccuracy <- sum(test_preds_binary == df_test[,\"Exited\"])/nrow(df_test)\naccuracy\n\n0.805416666666667\n\n\n\n\n\nWe calculate the confusion matrix, precision, recall, F1 score, and ROC AUC for the logistic regression model.\n\n# Calculate the confusion matrix\ncm <- confusionMatrix(as.factor(test_preds_binary), as.factor(df_test[,\"Exited\"]))\n\n# Calculate precision, recall, and F1 score\nprecision <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[2, 1])\nrecall <- cm$table[2, 2] / (cm$table[2, 2] + cm$table[1, 2])\nf1_score <- 2 * (precision * recall) / (precision + recall)\n\ncat(\"Precision:\", precision, \"\\n\")\ncat(\"Recall:\", recall, \"\\n\")\ncat(\"F1 Score:\", f1_score, \"\\n\")\n\n# Calculate ROC AUC\nroc_obj <- roc(df_test[,\"Exited\"], test_preds)\nroc_auc <- auc(roc_obj)\ncat(\"ROC AUC:\", roc_auc, \"\\n\")\n\nPrecision: 0.5890411 \nRecall: 0.1744422 \nF1 Score: 0.2691706 \n\n\nSetting levels: control = 0, case = 1\n\nSetting direction: controls < cases\n\n\n\nROC AUC: 0.7616043 \n\n\n\n\n\nWe save the trained logistic regression model as an RDS file.\n\n# save the model\nsaveRDS(lg_reg_model, \"r_log_reg_churn_model.rds\")"
  },
  {
    "objectID": "notebooks/r_demo/r-python.html",
    "href": "notebooks/r_demo/r-python.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd\nfrom pypmml import Model\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = Model.fromFile('./prune_dt.pmml')\n\n\ndf = pd.read_csv(\"./datasets/bank_customer_churn.csv\")\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\nmodel.predict({\n    \"CreditScore\": 0.64,\n    \"Geography\": 0,\n    \"Gender\": 0,\n    \"Age\": 0.51936320,\n    \"Tenure\": 2,\n    \"Balance\": 0.9118043,\n    \"NumOfProducts\": 1,\n    \"HasCrCard\": 1,\n    \"IsActiveMember\": 1,\n    \"EstimatedSalary\": 0.506734893\n})\n\n\nmodel.inputNames"
  },
  {
    "objectID": "notebooks/time_series/time_series_demo.html",
    "href": "notebooks/time_series/time_series_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "Time Series Test Plan Demo\n\n# This environment variable can be set to silence the summarized output of test results, for testing purposes.\n#\n# %env VM_SUMMARIZE_TEST_PLANS = False\n\nimport pandas as pd\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clg4hlb8d0046jn8hwqnes4ak\"\n)\n  \n\nTrue\n\n\n\ndf = pd.read_csv(\"../datasets/lending_club_loan_rates.csv\", sep='\\t')\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n  \n  \n    \n      0\n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      1\n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2\n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      3\n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      4\n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n  \n\n\n\n\n\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\ndf\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n      diff1_loan_rate_A\n      diff1_loan_rate_B\n      diff1_loan_rate_C\n      diff1_loan_rate_D\n      diff1_FEDFUNDS\n      diff2_FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n      0.060000\n      0.134359\n      0.207500\n      -0.467444\n      -0.24\n      -0.25\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n      0.074762\n      -0.221026\n      -0.118333\n      0.169667\n      -0.08\n      0.16\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n      -0.011429\n      0.156667\n      -0.003241\n      0.300702\n      -0.18\n      -0.10\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n      -0.050909\n      0.034444\n      0.141111\n      -0.127924\n      -0.27\n      -0.09\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n      -0.083258\n      -0.080278\n      -0.162037\n      -0.130556\n      -0.25\n      0.02\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2018-08-01\n      7.218997\n      11.161286\n      15.142618\n      19.857603\n      1.91\n      0.052118\n      0.045181\n      0.056796\n      0.088167\n      0.00\n      -0.09\n    \n    \n      2018-09-01\n      7.201281\n      11.191918\n      15.139769\n      19.748459\n      1.95\n      -0.017716\n      0.030632\n      -0.002849\n      -0.109144\n      0.04\n      0.04\n    \n    \n      2018-10-01\n      7.228498\n      11.208418\n      15.129105\n      19.792163\n      2.19\n      0.027218\n      0.016500\n      -0.010665\n      0.043704\n      0.24\n      0.20\n    \n    \n      2018-11-01\n      7.536897\n      11.390483\n      15.126869\n      19.632697\n      2.20\n      0.308399\n      0.182066\n      -0.002235\n      -0.159466\n      0.01\n      -0.23\n    \n    \n      2018-12-01\n      7.715209\n      11.459631\n      15.107476\n      19.558346\n      2.27\n      0.178312\n      0.069148\n      -0.019393\n      -0.074350\n      0.07\n      0.06\n    \n  \n\n137 rows × 11 columns\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                                Name                          Description                                           \n\n\nsklearn_classifier_metrics        SKLearnClassifierMetrics      Test plan for sklearn classifier metrics              \nsklearn_classifier_validation     SKLearnClassifierPerformance  Test plan for sklearn classifier models               \nsklearn_classifier_model_diagnosisSKLearnClassifierDiagnosis    Test plan for sklearn classifier model diagnosis tests\nsklearn_classifier                SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                       \ntabular_dataset                   TabularDataset                Test plan for generic tabular datasets                \ntabular_dataset_description       TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                       \ntabular_data_quality              TabularDataQuality            Test plan for data quality on tabular datasets        \nnormality_test_plan               NormalityTestPlan             Test plan to perform normality tests.                 \nautocorrelation_test_plan         AutocorrelationTestPlan       Test plan to perform autocorrelation tests.           \nseasonality_test_plan             SesonalityTestPlan            Test plan to perform seasonality tests.               \nunit_root                         UnitRoot                      Test plan to perform unit root tests.                 \nstationarity_test_plan            StationarityTestPlan          Test plan to perform stationarity tests.              \ntimeseries                        TimeSeries                    Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                       \ntimeseries_univariate_inspection  TimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.     \n\n\n\n\n\nloan_rate_columns = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\ndiff1_loan_rate_columns = [\"diff1_loan_rate_A\", \"diff1_loan_rate_B\", \"diff1_loan_rate_C\", \"diff1_loan_rate_D\"]\n\ntest_plan_config = {\n    \"time_series_univariate_inspection_raw\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    },\n    \"time_series_univariate_inspection_histogram\": {\n        \"columns\": loan_rate_columns + diff1_loan_rate_columns\n    }\n}\n\nplan = vm.run_test_plan(\"timeseries_univariate_inspection\", config=test_plan_config, dataset=vm_dataset)\n\n                                                                                                                                                                        \n\n\nResults for TimeSeriesUnivariateInspection Test Plan:\n        This section provides a preliminary understanding of the target variable(s)\n        used in the time series dataset. It visualizations that present the raw time\n        series data and a histogram of the target variable(s).\n\n        The raw time series data provides a visual inspection of the target variable's\n        behavior over time. This helps to identify any patterns or trends in the data,\n        as well as any potential outliers or anomalies. The histogram of the target\n        variable displays the distribution of values, providing insight into the range\n        and frequency of values observed in the data.\n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots\n                \n            \n            \n                \n        \n            \n        \n        \n                \n                    \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n                \n            \n        \n        \n        \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n                \n                    Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/time_series_forecast_wls_algo.html",
    "href": "notebooks/time_series/time_series_forecast_wls_algo.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhzo21s006wl9rl0swhv40h\"\n)\n\nConnected to ValidMind. Project: Stock Price Prediction Model - Initial Validation (clhhzo21s006wl9rl0swhv40h)\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import coint\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = '../datasets/time_series/raw/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\nHandling frequencies.\n\ndf = df.resample('MS').last()\n\n\n\nDrop missing values.\n\ndf = df.dropna()\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df\n)\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\ndata_quality_testplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": df.columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": df.columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": df.columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\nvm_dataset = vm.init_dataset(\n    dataset=df, target_column=\"MORTGAGE30US\"\n)\nunivariate_testplan = vm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of UNRATE: MS\nFrequency of GS10: MS\nFrequency of FEDFUNDS: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_multivariate\")\ntest_plan_config = {\n    \"scatter_plot\": {\n        \"columns\": df.columns\n    },\n    \"lagged_correlation_heatmap\": {\n        \"target_col\": 'MORTGAGE30US',\n        \"independent_vars\": [\"GS10\", \"FEDFUNDS\", \"UNRATE\"]\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nmultivariate_plan = vm.run_test_plan(\"time_series_multivariate\", config=test_plan_config, dataset=vm_dataset)\n\n\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()\n\n\n\n\n\n\n\nStep 1: Fit Model\n\nm1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\n\n# Add the independent variables with no intercept\nX_1 = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny_1 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.WLS(y_1, X_1).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 WLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            WLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Tue, 16 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        11:45:39   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\nm2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_training_dataset['const'] = 1.0\nm2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_test_dataset['const'] = 1.0\n\n# Add a constant to the independent variables for the linear regression model\nX_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny_2 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.WLS(y_2, X_2).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            WLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Tue, 16 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        11:45:39   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nStep 2: Reasoning\n\n\n\n\nm3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\nm3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\n\n# Add a constant to the independent variables for the linear regression model\nX_3 = df_train_diff['GS10']\n\n# Define the dependent variable \ny_3 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.WLS(y_3, X_3).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 WLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            WLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Tue, 16 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        11:45:39   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_4 = sm.WLS(y, X).fit()\n\n# Display the model summary\nprint(model_4.summary())\n\n                                 WLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            WLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Tue, 16 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        11:45:39   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_5 = sm.WLS(y, X).fit()\n\n# Display the model summary\nprint(model_5.summary())\n\n                                 WLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            WLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Tue, 16 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        11:45:39   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\n\n\n\n\nvm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\n\nmodel_performance_test_plan = vm.run_test_plan(\"regression_model_performance\", \n                                             model=vm_model_1\n                                            )\n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\nvm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\n\n\nvm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\n\n\nmodel_comparison_test_plan = vm.run_test_plan(\"regression_models_comparison\", \n                                             model = vm_model_1,\n                                             models= [vm_model_2, vm_model_3],\n                                            )\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\nconfig= {\n    \"regression_forecast_plot\": {\n        \"start_date\": '2010-01-01',\n        \"end_date\": '2022-01-01'\n    }\n}\n\nforcasting_testplan = vm.run_test_plan(\"time_series_forecast\",\n                                        models=[vm_model_1],\n                                        config=config)"
  },
  {
    "objectID": "notebooks/time_series/time_series_model_validation_demo.html",
    "href": "notebooks/time_series/time_series_model_validation_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "Prepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \nbinary_classifier                BinaryClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                                            \ntabular_dataset                  TabularDataset             Test plan for generic tabular datasets                                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \nnormality_test_plan              NormalityTestPlan          Test plan to perform normality tests.                                      \nautocorrelation_test_plan        AutocorrelationTestPlan    Test plan to perform autocorrelation tests.                                \nseasonality_test_plan            SesonalityTestPlan         Test plan to perform seasonality tests.                                    \nunit_root                        UnitRoot                   Test plan to perform unit root tests.                                      \nstationarity_test_plan           StationarityTestPlan       Test plan to perform stationarity tests.                                   \ntimeseries                       TimeSeries                 Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                                            \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_dataset              TimeSeriesDataset          Test plan for time series  datasets                                        \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\n\n\n\n\n# Currently only fred pre-trained models are available\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_1')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_3')\n\n\n\n\n\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\nlist_of_models = [vm_model_A, vm_model_B]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_forecast\")\n\n\n\n\nAttribute       Value                                           \n\n\nID              time_series_forecast                            \nName            TimeSeriesForecast                              \nDescription     Test plan to perform time series forecast tests.\nRequired Context['models']                                      \nTests           RegressionModelForecastPlot (Metric)            \nTest Plans      []                                              \n\n\n\n\n\nconfig= {\n    \"regression_forecast_plot\": {\n        \"start_date\": '2010-01-01',\n        \"end_date\": '2022-01-01'\n    }\n}\n\nvm.run_test_plan(\"time_series_forecast\",\n                                        models=list_of_models,\n                                        config=config)\n\nLogging result: TestPlanMetricResult(result_id=regression_forecast_plot, figures):   0%|          | 0/1 [00:00<?, ?it/s]\n\n\n{'start_date': '2010-01-01', 'end_date': '2022-01-01'}\nregression_forecast_plot\n{'start_date': '2010-01-01', 'end_date': '2022-01-01'}\n{'start_date': '2010-01-01', 'end_date': '2022-01-01'}\n0     1971-05-01\n1     1971-06-01\n2     1971-07-01\n3     1971-08-01\n4     1971-09-01\n         ...    \n119   2022-11-01\n120   2022-12-01\n121   2023-01-01\n122   2023-02-01\n123   2023-03-01\nName: DATE, Length: 622, dtype: datetime64[ns]\n0     1971-05-01\n1     1971-06-01\n2     1971-07-01\n3     1971-08-01\n4     1971-09-01\n         ...    \n119   2022-11-01\n120   2022-12-01\n121   2023-01-01\n122   2023-02-01\n123   2023-03-01\nName: DATE, Length: 622, dtype: datetime64[ns]\n\n\n                                                                                                                                \n\n\nThis test plan computes predictions from statsmodels OLS linear regression models against a list of models and plots the historical data alongside the forecasted data. The purpose of this test plan is to evaluate the performance of each model in predicting future values of a time series based on historical data. By comparing the historical values with the forecasted values, users can visually assess the accuracy of each model and determine which one best fits the data. In addition, this test plan can help users identify any discrepancies between the models and the actual data, allowing for potential improvements in model selection and parameter tuning.\n            \n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n        \n        \n\n\nTimeSeriesForecast(test_context=TestContext(dataset=None, model=None, models=[Model(attributes=ModelAttributes(architecture=None, framework=None, framework_version=None), task=None, subtask=None, params=None, model_id='main', model=<statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x28c37cfa0>, train_ds=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS\nDATE                              \n1971-05-01          0.17      0.47\n1971-06-01          0.08      0.28\n1971-07-01          0.15      0.40\n1971-08-01          0.00      0.26\n1971-09-01         -0.02     -0.02\n...                  ...       ...\n2012-06-01         -0.09      0.00\n2012-07-01         -0.17      0.00\n2012-08-01          0.10     -0.03\n2012-09-01         -0.19      0.01\n2012-10-01          0.01      0.02\n\n[498 rows x 2 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 0.16999999999999993, 'FEDFUNDS': 0.46999999999999975}, {'MORTGAGE30US': 0.08000000000000007, 'FEDFUNDS': 0.28000000000000025}, {'MORTGAGE30US': 0.15000000000000036, 'FEDFUNDS': 0.39999999999999947}, {'MORTGAGE30US': 0.0, 'FEDFUNDS': 0.2600000000000007}, {'MORTGAGE30US': -0.020000000000000462, 'FEDFUNDS': -0.020000000000000462}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': -0.08999999999999986, 'FEDFUNDS': 0.0}, {'MORTGAGE30US': -0.16999999999999993, 'FEDFUNDS': 0.0}, {'MORTGAGE30US': 0.09999999999999964, 'FEDFUNDS': -0.03}, {'MORTGAGE30US': -0.18999999999999995, 'FEDFUNDS': 0.010000000000000009}, {'MORTGAGE30US': 0.010000000000000231, 'FEDFUNDS': 0.01999999999999999}]}], shape={'rows': 498, 'columns': 2}, correlation_matrix=None, correlations=None, type='generic', options=None, statistics=None, targets=None, target_column='MORTGAGE30US', class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), test_ds=Dataset(raw_dataset=            MORTGAGE30US  UNRATE\nDATE                            \n2012-12-01          0.03     0.2\n2013-01-01          0.18     0.1\n2013-02-01         -0.02    -0.3\n2013-03-01          0.06    -0.2\n2013-04-01         -0.17     0.1\n...                  ...     ...\n2022-11-01         -0.50    -0.1\n2022-12-01         -0.16    -0.1\n2023-01-01         -0.29    -0.1\n2023-02-01          0.37     0.2\n2023-03-01         -0.18    -0.1\n\n[124 rows x 2 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 0.03000000000000025, 'UNRATE': 0.20000000000000018}, {'MORTGAGE30US': 0.17999999999999972, 'UNRATE': 0.09999999999999964}, {'MORTGAGE30US': -0.020000000000000018, 'UNRATE': -0.2999999999999998}, {'MORTGAGE30US': 0.06000000000000005, 'UNRATE': -0.20000000000000018}, {'MORTGAGE30US': -0.16999999999999993, 'UNRATE': 0.09999999999999964}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': -0.5, 'UNRATE': -0.10000000000000009}, {'MORTGAGE30US': -0.16000000000000014, 'UNRATE': -0.10000000000000009}, {'MORTGAGE30US': -0.29000000000000004, 'UNRATE': -0.10000000000000009}, {'MORTGAGE30US': 0.3700000000000001, 'UNRATE': 0.20000000000000018}, {'MORTGAGE30US': -0.17999999999999972, 'UNRATE': -0.10000000000000009}]}], shape={'rows': 124, 'columns': 2}, correlation_matrix=None, correlations=None, type='generic', options=None, statistics=None, targets=None, target_column='MORTGAGE30US', class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), validation_ds=None, y_train_predict=DATE\n1971-05-01    0.136325\n1971-06-01    0.081215\n1971-07-01    0.116021\n1971-08-01    0.075414\n1971-09-01   -0.005801\n                ...   \n2012-06-01    0.000000\n2012-07-01    0.000000\n2012-08-01   -0.008702\n2012-09-01    0.002901\n2012-10-01    0.005801\nFreq: MS, Length: 498, dtype: float64, y_test_predict=DATE\n2012-12-01    0.058011\n2013-01-01    0.029005\n2013-02-01   -0.087016\n2013-03-01   -0.058011\n2013-04-01    0.029005\n                ...   \n2022-11-01   -0.029005\n2022-12-01   -0.029005\n2023-01-01   -0.029005\n2023-02-01    0.058011\n2023-03-01   -0.029005\nLength: 124, dtype: float64, y_validation_predict=None), Model(attributes=ModelAttributes(architecture=None, framework=None, framework_version=None), task=None, subtask=None, params=None, model_id='main', model=<statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x107996f80>, train_ds=Dataset(raw_dataset=            MORTGAGE30US  GS10\nDATE                          \n1971-05-01          0.17  0.56\n1971-06-01          0.08  0.13\n1971-07-01          0.15  0.21\n1971-08-01          0.00 -0.15\n1971-09-01         -0.02 -0.44\n...                  ...   ...\n2012-06-01         -0.09 -0.18\n2012-07-01         -0.17 -0.09\n2012-08-01          0.10  0.15\n2012-09-01         -0.19  0.04\n2012-10-01          0.01  0.03\n\n[498 rows x 2 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 0.16999999999999993, 'GS10': 0.5599999999999996}, {'MORTGAGE30US': 0.08000000000000007, 'GS10': 0.1299999999999999}, {'MORTGAGE30US': 0.15000000000000036, 'GS10': 0.21000000000000085}, {'MORTGAGE30US': 0.0, 'GS10': -0.15000000000000036}, {'MORTGAGE30US': -0.020000000000000462, 'GS10': -0.4400000000000004}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': -0.08999999999999986, 'GS10': -0.17999999999999994}, {'MORTGAGE30US': -0.16999999999999993, 'GS10': -0.09000000000000008}, {'MORTGAGE30US': 0.09999999999999964, 'GS10': 0.1499999999999999}, {'MORTGAGE30US': -0.18999999999999995, 'GS10': 0.040000000000000036}, {'MORTGAGE30US': 0.010000000000000231, 'GS10': 0.030000000000000027}]}], shape={'rows': 498, 'columns': 2}, correlation_matrix=None, correlations=None, type='generic', options=None, statistics=None, targets=None, target_column='MORTGAGE30US', class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), test_ds=Dataset(raw_dataset=            MORTGAGE30US  GS10\nDATE                          \n2012-12-01          0.03  0.07\n2013-01-01          0.18  0.19\n2013-02-01         -0.02  0.07\n2013-03-01          0.06 -0.02\n2013-04-01         -0.17 -0.20\n...                  ...   ...\n2022-11-01         -0.50 -0.09\n2022-12-01         -0.16 -0.27\n2023-01-01         -0.29 -0.09\n2023-02-01          0.37  0.22\n2023-03-01         -0.18 -0.09\n\n[124 rows x 2 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 0.03000000000000025, 'GS10': 0.07000000000000006}, {'MORTGAGE30US': 0.17999999999999972, 'GS10': 0.18999999999999995}, {'MORTGAGE30US': -0.020000000000000018, 'GS10': 0.07000000000000006}, {'MORTGAGE30US': 0.06000000000000005, 'GS10': -0.020000000000000018}, {'MORTGAGE30US': -0.16999999999999993, 'GS10': -0.19999999999999996}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': -0.5, 'GS10': -0.08999999999999986}, {'MORTGAGE30US': -0.16000000000000014, 'GS10': -0.27}, {'MORTGAGE30US': -0.29000000000000004, 'GS10': -0.0900000000000003}, {'MORTGAGE30US': 0.3700000000000001, 'GS10': 0.2200000000000002}, {'MORTGAGE30US': -0.17999999999999972, 'GS10': -0.08999999999999986}]}], shape={'rows': 124, 'columns': 2}, correlation_matrix=None, correlations=None, type='generic', options=None, statistics=None, targets=None, target_column='MORTGAGE30US', class_labels=None, _Dataset__feature_lookup={}, _Dataset__transformed_df=None), validation_ds=None, y_train_predict=DATE\n1971-05-01    0.415991\n1971-06-01    0.096569\n1971-07-01    0.155996\n1971-08-01   -0.111426\n1971-09-01   -0.326850\n                ...   \n2012-06-01   -0.133711\n2012-07-01   -0.066856\n2012-08-01    0.111426\n2012-09-01    0.029714\n2012-10-01    0.022285\nFreq: MS, Length: 498, dtype: float64, y_test_predict=DATE\n2012-12-01    0.051999\n2013-01-01    0.141140\n2013-02-01    0.051999\n2013-03-01   -0.014857\n2013-04-01   -0.148568\n                ...   \n2022-11-01   -0.066856\n2022-12-01   -0.200567\n2023-01-01   -0.066856\n2023-02-01    0.163425\n2023-03-01   -0.066856\nLength: 124, dtype: float64, y_validation_predict=None)], context_data=None), config={...})"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#data-engineering",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#data-engineering",
    "title": "ValidMind",
    "section": "4.1. Data Engineering",
    "text": "4.1. Data Engineering\n\n4.1.1. Data Collection\n\nSetup\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import coint\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\nLoad FRED Data\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = '../datasets/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\n\n\n4.1.2. Data Description\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n4.1.3. Data Quality\n\nFrequency of Series\nDistribution of frequencies in the data.\n\ndef plot_time_difference_frequency(df):\n    # Calculate the time differences between consecutive entries\n    time_diff = df.index.to_series().diff().dropna()\n\n    # Convert the time differences to a suitable unit (e.g., days)\n    time_diff_days = time_diff.dt.total_seconds() / (60 * 60 * 24)\n\n    # Create a DataFrame with the time differences\n    time_diff_df = pd.DataFrame({'Time Differences (Days)': time_diff_days})\n\n    # Plot the frequency distribution of the time differences\n    sns.histplot(data=time_diff_df, x='Time Differences (Days)', bins=50, kde=False)\n    plt.xlabel('Time Differences (Days)')\n    plt.ylabel('Frequency')\n    plt.show()\n\n\nplot_time_difference_frequency(df)\n\n\n\n\nIdentify frequencies for each variable.\n\ndef identify_frequencies(df):\n    \"\"\"\n    Identify the frequency of each series in the DataFrame.\n\n    :param df: Time-series DataFrame\n    :return: DataFrame with two columns: 'Variable' and 'Frequency'\n    \"\"\"\n    frequencies = []\n    for column in df.columns:\n        series = df[column].dropna()\n        if not series.empty:\n            freq = pd.infer_freq(series.index)\n            if freq == 'MS' or freq == 'M':\n                label = 'Monthly'\n            elif freq == 'Q':\n                label = 'Quarterly'\n            elif freq == 'A':\n                label = 'Yearly'\n            else:\n                label = freq\n        else:\n            label = None\n\n        frequencies.append({'Variable': column, 'Frequency': label})\n\n    freq_df = pd.DataFrame(frequencies)\n\n    return freq_df\n\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      UNRATE\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      FEDFUNDS\n      Monthly\n    \n  \n\n\n\n\nHandling frequencies.\n\ndf = df.resample('MS').last()\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      Monthly\n    \n    \n      1\n      UNRATE\n      Monthly\n    \n    \n      2\n      GS10\n      Monthly\n    \n    \n      3\n      FEDFUNDS\n      Monthly\n    \n  \n\n\n\n\n\n\nMissing Values\nStep 1: Identify Missing Values\nTotal number of missing values.\n\ndef plot_missing_values_bar(df):\n    \"\"\"\n    Plot a bar chart displaying the total number of missing values per variable (column) in a time-series DataFrame using seaborn.\n    \n    :param df: Time-series DataFrame\n    \"\"\"\n    # Calculate the total number of missing values per column\n    missing_values = df.isnull().sum()\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the bar chart\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=missing_values.index, y=missing_values.values)\n    \n    plt.xticks(rotation=45, ha='right')\n    plt.xlabel('Variables (Columns)')\n    plt.ylabel('Number of Missing Values')\n    plt.title('Total Number of Missing Values per Variable')\n    plt.show()\n\n\nplot_missing_values_bar(df)\n\n\n\n\nHeatmap of missing values.\n\ndef plot_missing_values_heatmap(df, start_year=None, end_year=None):\n    \"\"\"\n    Plot a heatmap of missing values with actual years in rows using seaborn.\n\n    :param df: Time-series DataFrame\n    :param start_year: Start year for zooming in, defaults to None\n    :param end_year: End year for zooming in, defaults to None\n    \"\"\"\n    # Filter the DataFrame based on the specified start_year and end_year\n    if start_year:\n        df = df[df.index.year >= start_year]\n    if end_year:\n        df = df[df.index.year <= end_year]\n\n    # Create a boolean mask for missing values\n    missing_mask = df.isnull()\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the heatmap\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(missing_mask.T, cmap='viridis', cbar=False, xticklabels=False)\n\n    # Add actual years on the x-axis\n    years = df.index.year.unique()\n    xticks = [df.index.get_loc(df.index[df.index.year == year][0]) for year in years]\n    plt.xticks(xticks, years, rotation=45, ha='right')\n\n    plt.ylabel('Columns')\n    plt.xlabel('Rows (Years)')\n    plt.title('Missing Values Heatmap with Actual Years in Rows')\n    plt.show()\n\n\nplot_missing_values_heatmap(df)\n\n\n\n\nStep 2: Handling Missing Values\nDrop missing values.\n\ndf = df.dropna()\n\n\nplot_missing_values_bar(df)\n\n\n\n\n\nplot_missing_values_heatmap(df)\n\n\n\n\n\n\nOutliers\nStep 1: Identify Outliers\n\ndef identify_outliers(df, threshold=3):\n    z_scores = pd.DataFrame(stats.zscore(df), index=df.index, columns=df.columns)\n    outliers = z_scores[(z_scores.abs() > threshold).any(axis=1)]\n    \n    outlier_table = []\n    for idx, row in outliers.iterrows():\n        for col in df.columns:\n            if abs(row[col]) > threshold:\n                outlier_table.append({\"Variable\": col, \"z-score\": row[col], \"Threshold\": threshold, \"Date\": idx})\n                \n    return pd.DataFrame(outlier_table)\n\n\noutliers_table = identify_outliers(df, threshold=3)\ndisplay(outliers_table)\n\n\n\n\n\n  \n    \n      \n      Variable\n      z-score\n      Threshold\n      Date\n    \n  \n  \n    \n      0\n      FEDFUNDS\n      3.106442\n      3\n      1980-03-01\n    \n    \n      1\n      FEDFUNDS\n      3.212296\n      3\n      1980-04-01\n    \n    \n      2\n      FEDFUNDS\n      3.537417\n      3\n      1980-12-01\n    \n    \n      3\n      FEDFUNDS\n      3.582783\n      3\n      1981-01-01\n    \n    \n      4\n      FEDFUNDS\n      3.441645\n      3\n      1981-05-01\n    \n    \n      5\n      FEDFUNDS\n      3.587823\n      3\n      1981-06-01\n    \n    \n      6\n      FEDFUNDS\n      3.572701\n      3\n      1981-07-01\n    \n    \n      7\n      FEDFUNDS\n      3.265222\n      3\n      1981-08-01\n    \n    \n      8\n      MORTGAGE30US\n      3.246766\n      3\n      1981-09-01\n    \n    \n      9\n      MORTGAGE30US\n      3.271251\n      3\n      1981-10-01\n    \n    \n      10\n      MORTGAGE30US\n      3.011098\n      3\n      1982-01-01\n    \n    \n      11\n      UNRATE\n      5.011303\n      3\n      2020-04-01\n    \n    \n      12\n      UNRATE\n      4.128421\n      3\n      2020-05-01\n    \n  \n\n\n\n\nPlot outliers.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_outliers(df, outliers_table, use_subplots=False):\n    sns.set(style=\"darkgrid\")\n    \n    if use_subplots:\n        n_variables = len(df.columns)\n        fig, axes = plt.subplots(n_variables, 1, figsize=(12, 3 * n_variables), sharex=True)\n        \n        for i, col in enumerate(df.columns):\n            sns.lineplot(data=df, x=df.index, y=col, ax=axes[i], label=col)\n            \n            variable_outliers = outliers_table[outliers_table[\"Variable\"] == col]\n            for idx, row in variable_outliers.iterrows():\n                date = row[\"Date\"]\n                outlier_value = df.loc[date, col]\n                axes[i].scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\", label=\"Outlier\" if idx == 0 else \"\")\n            \n            axes[i].legend()\n            axes[i].set_ylabel(\"Value\")\n            axes[i].set_title(f\"Time Series with Outliers for {col}\")\n\n        plt.xlabel(\"Date\")\n        plt.tight_layout()\n\n    else:\n        plt.figure(figsize=(12, 3))\n        for col in df.columns:\n            sns.lineplot(data=df, x=df.index, y=col, label=col)\n        \n        plotted_outlier_variables = set()\n        for idx, row in outliers_table.iterrows():\n            date = row[\"Date\"]\n            variable = row[\"Variable\"]\n            outlier_value = df.loc[date, variable]\n            if variable not in plotted_outlier_variables:\n                plt.scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\", label=f\"Outlier ({variable})\")\n                plotted_outlier_variables.add(variable)\n            else:\n                plt.scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\")\n\n        plt.legend()\n        plt.xlabel(\"Date\")\n        plt.ylabel(\"Value\")\n        plt.title(\"Time Series with Outliers\")\n    \n    plt.show()\n\n\nplot_outliers(df, outliers_table, use_subplots=True)\n\n\n\n\nStep 2: Handling Outliers"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#exploratory-data-analysis",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#exploratory-data-analysis",
    "title": "ValidMind",
    "section": "4.2. Exploratory Data Analysis",
    "text": "4.2. Exploratory Data Analysis\n\n4.2.1. Univariate Analysis\n\nVisual Inspection\nLine plots.\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\n\nSeasonality\nStep 1: Compute Seasonal Decomposition\n\ndef compute_seasonal_decomposition(data, model='additive'):\n    \"\"\"\n    Compute seasonal decomposition for all time-series in a DataFrame and store all the components in a new DataFrame.\n    \n    :param data: DataFrame with time-series data\n    :param period: Number of observations in each seasonal period\n    :return: DataFrame with seasonal, trend, and residual components for all time-series in the input DataFrame\n    \"\"\"\n    # Initialize an empty DataFrame to store the components for each time-series\n    decomp_df = pd.DataFrame()\n\n    # Loop over each column in the input DataFrame and perform seasonal decomposition\n    for col in data.columns:\n        res = seasonal_decompose(data[col], model=model)\n        decomp_df[f'{col}_seasonal'] = res.seasonal\n        decomp_df[f'{col}_trend'] = res.trend\n        decomp_df[f'{col}_residual'] = res.resid\n\n    # Set the index of the decomposed DataFrame to be the same as the input DataFrame\n    decomp_df.index = data.index\n\n    return decomp_df\n\n\ndecomp_df = compute_seasonal_decomposition(df)\n\nStep 2: Visualize Seasonal Decomposition\n\ndef plot_seasonal_components(decomp_df):\n    \"\"\"\n    Plot all seasonal, trend, and residual components for each variable in a DataFrame.\n    \n    :param decomp_df: DataFrame with seasonal, trend, and residual components for each variable\n    \"\"\"\n    # Initialize a figure with subplots for each variable and component\n    fig, axs = plt.subplots(nrows=len(decomp_df.columns) // 3, ncols=3, figsize=(12, 4 * (len(decomp_df.columns) // 3)))\n\n    # Loop over each variable in the input DataFrame and plot the seasonal, trend, and residual components\n    for i, col in enumerate(decomp_df.columns[::3]):\n        axs[i, 0].plot(decomp_df.index, decomp_df[f'{col}'])\n        axs[i, 0].set_title(f'Seasonal: {col[:-9]}')\n        axs[i, 1].plot(decomp_df.index, decomp_df[f'{col[:-9]}_trend'])\n        axs[i, 1].set_title(f'Trend: {col[:-9]}')\n        axs[i, 2].plot(decomp_df.index, decomp_df[f'{col[:-9]}_residual'])\n        axs[i, 2].set_title(f'Residual: {col[:-9]}')\n\n    # Set the figure title\n    fig.suptitle('Seasonal Decomposition', fontsize=16)\n\n    # Adjust the spacing between subplots\n    fig.tight_layout()\n\n    # Show the plot\n    plt.show()\n\n\nplot_seasonal_components(decomp_df)\n\n\n\n\nStep 3: Residual Analysis\n\n\nStationarity\nStep 1: Auto Stationarity\n\ndef test_stationarity(data, threshold=0.05):\n    \"\"\"\n    Perform multiple stationarity tests on each time series in a DataFrame.\n    \n    :param data: DataFrame with time-series data\n    :return: DataFrame with test results (Variable, Test, p-value, Threshold, Pass/Fail, Decision)\n    \"\"\"\n    # Initialize an empty DataFrame to store the test results\n    test_results = pd.DataFrame(columns=['Variable', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n\n    # Loop over each column in the input DataFrame and perform stationarity tests\n    for col in data.columns:\n        # Perform the ADF test\n        adf_result = adfuller(data[col], autolag='AIC')\n        adf_pvalue = adf_result[1]\n        adf_pass_fail = adf_pvalue < threshold\n        adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'ADF',\n            'p-value': adf_pvalue,\n            'Threshold': threshold,\n            'Pass/Fail': adf_pass_fail,\n            'Decision': adf_decision\n        }, ignore_index=True)\n\n        # Perform the KPSS test\n        kpss_result = kpss(data[col], regression='c', nlags='auto')\n        kpss_pvalue = kpss_result[1]\n        kpss_pass_fail = kpss_pvalue > threshold\n        kpss_decision = 'Stationary' if kpss_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'KPSS',\n            'p-value': kpss_pvalue,\n            'Threshold': threshold,\n            'Pass/Fail': kpss_pass_fail,\n            'Decision': kpss_decision\n        }, ignore_index=True)\n\n        # Perform the Phillips-Perron test\n        pp_result = PhillipsPerron(data[col], trend='ct')\n        pp_pvalue = pp_result.pvalue\n        pp_threshold = threshold\n        pp_pass_fail = pp_pvalue < pp_threshold\n        pp_decision = 'Stationary' if pp_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'PhillipsPerron',\n            'p-value': pp_pvalue,\n            'Threshold': pp_threshold,\n            'Pass/Fail': pp_pass_fail,\n            'Decision': pp_decision\n        }, ignore_index=True)\n\n        # Perform the DF-GLS test\n        dfgls_result = DFGLS(data[col], trend='ct')\n        dfgls_pvalue = dfgls_result.pvalue\n        dfgls_threshold = threshold\n        dfgls_pass_fail = dfgls_pvalue < dfgls_threshold\n        dfgls_decision = 'Stationary' if dfgls_pass_fail else 'Non-stationary'\n        test_results = test_results.append({\n            'Variable': col,\n            'Test': 'DFGLS',\n            'p-value': dfgls_pvalue,\n            'Threshold': dfgls_threshold,\n            'Pass/Fail': dfgls_pass_fail,\n            'Decision': dfgls_decision\n        }, ignore_index=True)\n\n    return test_results\n\n\ndef auto_stationarity(data, max_order=5, threshold=0.05):\n    \"\"\"\n    Perform the Augmented Dickey-Fuller (ADF) stationarity test on each time series in a DataFrame,\n    testing for different integration orders until the series is stationary.\n    \n    :param data: DataFrame with time-series data\n    :param max_order: Maximum integration order to test\n    :param threshold: Significance level for the ADF test\n    :return: DataFrame with test results (Variable, Integration Order, Test, p-value, Threshold, Pass/Fail, Decision)\n    \"\"\"\n    # Initialize an empty DataFrame to store the test results\n    test_results = pd.DataFrame(columns=['Variable', 'Integration Order', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n\n    # Loop over each column in the input DataFrame and perform stationarity tests\n    for col in data.columns:\n        is_stationary = False\n        order = 0\n        \n        while not is_stationary and order <= max_order:\n            series = data[col]\n            \n            if order == 0:\n                adf_result = adfuller(series)\n            else:\n                adf_result = adfuller(np.diff(series, n=order-1))\n\n            adf_pvalue = adf_result[1]\n            adf_pass_fail = adf_pvalue < threshold\n            adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\n\n            test_results = test_results.append({\n                'Variable': col,\n                'Integration Order': order,\n                'Test': 'ADF',\n                'p-value': adf_pvalue,\n                'Threshold': threshold,\n                'Pass/Fail': 'Pass' if adf_pass_fail else 'Fail',\n                'Decision': adf_decision\n            }, ignore_index=True)\n\n            if adf_pass_fail:\n                is_stationary = True\n            \n            order += 1\n\n    return test_results\n\n\nauto_stationarity(df)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Integration Order\n      Test\n      p-value\n      Threshold\n      Pass/Fail\n      Decision\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      0\n      ADF\n      6.719476e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      1\n      MORTGAGE30US\n      1\n      ADF\n      6.719476e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      2\n      MORTGAGE30US\n      2\n      ADF\n      2.156453e-30\n      0.05\n      Pass\n      Stationary\n    \n    \n      3\n      UNRATE\n      0\n      ADF\n      1.939529e-02\n      0.05\n      Pass\n      Stationary\n    \n    \n      4\n      GS10\n      0\n      ADF\n      7.099537e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      5\n      GS10\n      1\n      ADF\n      7.099537e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      6\n      GS10\n      2\n      ADF\n      2.036674e-09\n      0.05\n      Pass\n      Stationary\n    \n    \n      7\n      FEDFUNDS\n      0\n      ADF\n      1.058010e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      8\n      FEDFUNDS\n      1\n      ADF\n      1.058010e-01\n      0.05\n      Fail\n      Non-stationary\n    \n    \n      9\n      FEDFUNDS\n      2\n      ADF\n      6.632874e-05\n      0.05\n      Pass\n      Stationary\n    \n  \n\n\n\n\nStep 2: Rolling Statistics\n\ndef plot_rolling_statistics(df, window_size=12):\n    \"\"\"\n    Plot rolling mean and rolling standard deviation in different subplots for each variable.\n    \n    :param df: DataFrame with time-series data\n    :param window_size: Window size for the rolling calculations\n    \"\"\"\n    for col_name in df.columns:\n        rolling_mean = df[col_name].rolling(window=window_size).mean()\n        rolling_std = df[col_name].rolling(window=window_size).std()\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))\n\n        ax1.plot(rolling_mean, label=f'{col_name} Rolling Mean')\n        ax1.legend()\n        ax1.set_ylabel('Value')\n        ax1.set_title(f'Rolling Mean for {col_name}')\n\n        ax2.plot(rolling_std, label=f'{col_name} Rolling Standard Deviation', color='orange')\n        ax2.legend()\n        ax2.set_xlabel('Time')\n        ax2.set_ylabel('Value')\n        ax2.set_title(f'Rolling Standard Deviation for {col_name}')\n\n        plt.show()\n\n\nplot_rolling_statistics(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\n\n\nAR Analysis\nStep 1: Calculate AR Orders\n\ndef calculate_ar_orders(dataset, max_order=3):\n    \"\"\"\n    This function calculates the autoregressive order of all time series in a dataset.\n    \n    Parameters:\n    dataset (pd.DataFrame): The dataset containing the time series.\n    max_order (int): The maximum order to consider for the autoregressive models.\n    \n    Returns:\n    pd.DataFrame: A table with the autoregressive order, AIC, and BIC for orders 0 up to max_order.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each column (time series) in the dataset\n    for col in dataset.columns:\n        time_series = dataset[col]\n        \n        # Test for stationarity using Augmented Dickey-Fuller test\n        adf_result = adfuller(time_series)\n        if adf_result[1] > 0.05:\n            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\n        \n        # Test different autoregressive orders and store the AIC and BIC values\n        for order in range(max_order + 1):\n            model = AutoReg(time_series, lags=order, old_names=False)\n            result = model.fit()\n            \n            # Add the current time series, order, AIC, and BIC to the results list\n            results.append({'Variable': col, 'AR order': order, 'AIC': result.aic, 'BIC': result.bic})\n\n    # Convert the results list to a DataFrame and return it\n    return pd.DataFrame(results)\n\n\ncalculate_ar_orders(df_diff)\n\n\n\n\n\n  \n    \n      \n      Variable\n      AR order\n      AIC\n      BIC\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      0\n      291.177506\n      300.046599\n    \n    \n      1\n      MORTGAGE30US\n      1\n      246.637029\n      259.935849\n    \n    \n      2\n      MORTGAGE30US\n      2\n      223.310101\n      241.035426\n    \n    \n      3\n      MORTGAGE30US\n      3\n      225.331792\n      247.480389\n    \n    \n      4\n      UNRATE\n      0\n      835.075578\n      843.944671\n    \n    \n      5\n      UNRATE\n      1\n      835.941726\n      849.240546\n    \n    \n      6\n      UNRATE\n      2\n      833.607234\n      851.332559\n    \n    \n      7\n      UNRATE\n      3\n      835.222318\n      857.370915\n    \n    \n      8\n      GS10\n      0\n      243.604950\n      252.474043\n    \n    \n      9\n      GS10\n      1\n      179.889575\n      193.188396\n    \n    \n      10\n      GS10\n      2\n      155.917382\n      173.642706\n    \n    \n      11\n      GS10\n      3\n      155.305036\n      177.453633\n    \n    \n      12\n      FEDFUNDS\n      0\n      992.528777\n      1001.397870\n    \n    \n      13\n      FEDFUNDS\n      1\n      879.316228\n      892.615048\n    \n    \n      14\n      FEDFUNDS\n      2\n      858.467020\n      876.192344\n    \n    \n      15\n      FEDFUNDS\n      3\n      858.142003\n      880.290601\n    \n  \n\n\n\n\nStep 2: Selection of AR Order\n\n\nMA Analysis\nStep 1: Calculate MA Orders\n\ndef calculate_ma_orders(dataset, max_order=3):\n    \"\"\"\n    This function calculates the moving average order of all time series in a dataset.\n    \n    Parameters:\n    dataset (pd.DataFrame): The dataset containing the time series.\n    max_order (int): The maximum order to consider for the moving average models.\n    \n    Returns:\n    pd.DataFrame: A table with the moving average order, AIC, and BIC for orders 0 up to max_order.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each column (time series) in the dataset\n    for col in dataset.columns:\n        time_series = dataset[col]\n        \n        # Test for stationarity using Augmented Dickey-Fuller test\n        adf_result = adfuller(time_series)\n        if adf_result[1] > 0.05:\n            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\n        \n        # Test different moving average orders and store the AIC and BIC values\n        for order in range(max_order + 1):\n            model = ARIMA(time_series, order=(0, 0, order))\n            result = model.fit()\n            \n            # Add the current time series, order, AIC, and BIC to the results list\n            results.append({'Variable': col, 'MA order': order, 'AIC': result.aic, 'BIC': result.bic})\n\n    # Convert the results list to a DataFrame and return it\n    return pd.DataFrame(results)\n\n\ncalculate_ma_orders(df_diff)\n\n\n\n\n\n  \n    \n      \n      Variable\n      MA order\n      AIC\n      BIC\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      0\n      291.177507\n      300.046600\n    \n    \n      1\n      MORTGAGE30US\n      1\n      228.313186\n      241.616825\n    \n    \n      2\n      MORTGAGE30US\n      2\n      227.394618\n      245.132804\n    \n    \n      3\n      MORTGAGE30US\n      3\n      225.782939\n      247.955672\n    \n    \n      4\n      UNRATE\n      0\n      835.075578\n      843.944671\n    \n    \n      5\n      UNRATE\n      1\n      836.126269\n      849.429909\n    \n    \n      6\n      UNRATE\n      2\n      833.512857\n      851.251043\n    \n    \n      7\n      UNRATE\n      3\n      835.509496\n      857.682228\n    \n    \n      8\n      GS10\n      0\n      243.604950\n      252.474043\n    \n    \n      9\n      GS10\n      1\n      155.604373\n      168.908013\n    \n    \n      10\n      GS10\n      2\n      152.550224\n      170.288410\n    \n    \n      11\n      GS10\n      3\n      154.471588\n      176.644321\n    \n    \n      12\n      FEDFUNDS\n      0\n      992.528777\n      1001.397870\n    \n    \n      13\n      FEDFUNDS\n      1\n      865.158571\n      878.462211\n    \n    \n      14\n      FEDFUNDS\n      2\n      864.192973\n      881.931159\n    \n    \n      15\n      FEDFUNDS\n      3\n      865.935637\n      888.108370\n    \n  \n\n\n\n\nStep 2: Selection of MA Order\n\n\n\n4.2.2. Multivariate Analysis\n\nCorrelations\nStep 1: Compute Correlation Matrix on Levels\n\ndef plot_corr_heatmap(df):\n    # Compute correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n\n    # Set plot title\n    plt.title('Correlation Matrix Heatmap')\n\n    # Show plot\n    plt.show()\n\nCorrrelations across Levels.\n\nplot_corr_heatmap(df)\n\n\n\n\nStep 2: Compute Correlation Matrix on First Difference\nCorrelations across First Differences.\n\nplot_corr_heatmap(df_diff)\n\n\n\n\nStep 3: Reasoning\n\n\nScatter Plots\nStep 1: Compute Scatter Plots on Levels\n\ndef plot_scatter_pairs(df):\n    # Compute pairwise scatter plots\n    sns.pairplot(df, kind='scatter')\n\n    # Show plot\n    plt.show()\n\n\nplot_scatter_pairs(df)\n\n\n\n\nStep 2: Compute Scatter Plots on First Difference\nCompute first difference.\n\nplot_scatter_pairs(df_diff)\n\n\n\n\nStep 3: Reasoning\n\n\nLag Analysis\nStep 1: Compute Correlations at Multiple Lags\n\ndef plot_heatmap_correlations(df, target_col, independent_vars, num_lags=10):\n    \"\"\"\n    Calculate the correlation between the target variable and the lags of independent variables in the dataset,\n    and plot a heatmap of these correlations.\n    :param df: DataFrame containing the target variable and independent variables of interest\n    :param target_col: Column name of the target variable in the DataFrame\n    :param independent_vars: List of column names of the independent variables in the DataFrame\n    :param num_lags: Number of lags to calculate (default is 10)\n    \"\"\"\n\n    correlations = np.zeros((len(independent_vars), num_lags + 1))\n\n    for i, ind_var_col in enumerate(independent_vars):\n        for lag in range(num_lags + 1):\n            # Create a new DataFrame with the original and lagged variable\n            temp_df = pd.DataFrame({target_col: df[target_col],\n                                    f'{ind_var_col}_lag{lag}': df[ind_var_col].shift(lag)})\n\n            # Drop NaN rows\n            temp_df = temp_df.dropna()\n\n            # Calculate the correlation between the target variable and the lagged independent variable\n            corr = temp_df[target_col].corr(temp_df[f'{ind_var_col}_lag{lag}'])\n\n            # Store the correlation in the correlations matrix\n            correlations[i, lag] = corr\n\n    # Create a DataFrame with the correlations matrix\n    correlation_df = pd.DataFrame(correlations, columns=[f'lag_{i}' for i in range(num_lags + 1)], index=independent_vars)\n\n    # Plot the heatmap\n    plt.figure(figsize=(12, 3))\n    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n    plt.title('Heatmap of Correlations between Target Variable and Lags of Independent Variables')\n    plt.xlabel('Lags')\n    plt.ylabel('Independent Variables')\n    plt.show()\n\n\ntarget_var = 'MORTGAGE30US'\nindependent_vars = ['GS10', 'UNRATE', 'FEDFUNDS']\nplot_heatmap_correlations(df_diff, target_col=target_var, independent_vars=independent_vars, num_lags=10)\n\n\n\n\nStep 2: Reasoning\n\n\nColinearity\n\n\nCointegration\nStep 1: Compute Cointegration Test\n\n# Function to calculate cointegration for each pair of variables in a DataFrame\ndef calculate_cointegration(dataframe, test=\"Engle-Granger\", threshold=0.05):\n    coint_df = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n    for i in range(len(dataframe.columns)):\n        for j in range(i+1, len(dataframe.columns)):\n            var1 = dataframe.columns[i]\n            var2 = dataframe.columns[j]\n            _, p_value, _ = coint(dataframe[var1], dataframe[var2])\n            pass_fail = \"Pass\" if p_value <= threshold else \"Fail\"\n            decision = \"Cointegrated\" if pass_fail == \"Pass\" else \"Not Cointegrated\"\n            coint_df = coint_df.append({\n                'Variable 1': var1,\n                'Variable 2': var2,\n                'Test': test,\n                'p-value': p_value,\n                'Threshold': threshold,\n                'Pass/Fail': pass_fail,\n                'Decision': decision\n            }, ignore_index=True)\n    return coint_df\n\n\n# Calculate cointegration for pairs of variables in the DataFrame\ncoint_results = calculate_cointegration(df)\ndisplay(coint_results)\n\n\n\n\n\n  \n    \n      \n      Variable 1\n      Variable 2\n      Test\n      p-value\n      Threshold\n      Pass/Fail\n      Decision\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      UNRATE\n      Engle-Granger\n      0.627876\n      0.05\n      Fail\n      Not Cointegrated\n    \n    \n      1\n      MORTGAGE30US\n      GS10\n      Engle-Granger\n      0.008688\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      2\n      MORTGAGE30US\n      FEDFUNDS\n      Engle-Granger\n      0.020417\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      3\n      UNRATE\n      GS10\n      Engle-Granger\n      0.013242\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      4\n      UNRATE\n      FEDFUNDS\n      Engle-Granger\n      0.027579\n      0.05\n      Pass\n      Cointegrated\n    \n    \n      5\n      GS10\n      FEDFUNDS\n      Engle-Granger\n      0.005832\n      0.05\n      Pass\n      Cointegrated\n    \n  \n\n\n\n\nStep 2: Plot Spread between Variables\n\n# Function to plot the spread between all pairs of variables in a DataFrame\ndef plot_spread(dataframe):\n    num_vars = len(dataframe.columns)\n    \n    for i in range(num_vars):\n        for j in range(i+1, num_vars):\n            var1 = dataframe.columns[i]\n            var2 = dataframe.columns[j]\n\n            # Calculate the spread between the two variables\n            dataframe['spread'] = dataframe[var1] - dataframe[var2]\n\n            # Plot the difference (spread) using seaborn\n            plt.figure(figsize=(10, 4))\n            sns.lineplot(data=dataframe['spread'], label=f'Spread ({var1} - {var2})')\n            plt.title(f'Spread ({var1} - {var2})')\n            plt.legend()\n\n            # Display the plot\n            plt.tight_layout()\n            plt.show()\n\n\nplot_spread(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.3. Feature Selection"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#model-methodology",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#model-methodology",
    "title": "ValidMind",
    "section": "4.3. Model Methodology",
    "text": "4.3. Model Methodology"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#training-data",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#training-data",
    "title": "ValidMind",
    "section": "4.4. Training Data",
    "text": "4.4. Training Data\n\n4.4.1. Sampling\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#model-training",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#model-training",
    "title": "ValidMind",
    "section": "4.5. Model Training",
    "text": "4.5. Model Training\n\nModel 1: Loan Rates and FEDFUNDS\nStep 1: Fit Model\n\n# Add the independent variables with no intercept\nX = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Sat, 06 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        16:41:11   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\nModel 2: Loan Rates, constant and FEDFUNDS\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Sat, 06 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        16:41:11   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\nModel 3: Loan Rates and GS10\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff['GS10']\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Sat, 06 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        16:41:12   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nModel 4: Loan Rates, FEDFUNDS and GS10\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_4 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_4.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            OLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Sat, 06 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        16:41:12   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\nModel 5: Loan Rates, FEDFUNDS, GS10 and UNRATE\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_5 = sm.OLS(y, X).fit()\n\n# Display the model summary\nprint(model_5.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            OLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Sat, 06 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        16:41:18   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nModel Selection\nStep 1: In-Sample Performance\n\ndef in_sample_performance_ols(models):\n    evaluation_results = []\n\n    for i, model in enumerate(models):\n        X = model.model.exog\n        X_columns = model.model.exog_names\n        y = model.model.endog\n\n        # Calculate the predicted values using the model\n        y_pred = model.predict(X)\n\n        # Calculate the residuals\n        residuals = y - y_pred\n\n        # Extract R-squared and Adjusted R-squared\n        r2 = model.rsquared\n        adj_r2 = model.rsquared_adj\n\n        # Calculate the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n        mse = model.mse_resid\n        rmse = mse ** 0.5\n\n        # Append the results to the evaluation_results list\n        evaluation_results.append({\n            'Model': f'Model_{i + 1}',\n            'Independent Variables': ', '.join(X_columns),\n            'R-Squared': r2,\n            'Adjusted R-Squared': adj_r2,\n            'MSE': mse,\n            'RMSE': rmse\n        })\n\n    # Convert the evaluation_results list to a DataFrame\n    results_df = pd.DataFrame(evaluation_results)\n    \n    return results_df\n\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\nresults_df = in_sample_performance_ols(models)\ndisplay(results_df)\n\n\n\n\n\n  \n    \n      \n      Model\n      Independent Variables\n      R-Squared\n      Adjusted R-Squared\n      MSE\n      RMSE\n    \n  \n  \n    \n      0\n      Model_1\n      FEDFUNDS\n      0.285734\n      0.284296\n      0.073824\n      0.271706\n    \n    \n      1\n      Model_2\n      const, FEDFUNDS\n      0.285602\n      0.284162\n      0.073943\n      0.271925\n    \n    \n      2\n      Model_3\n      GS10\n      0.528954\n      0.528007\n      0.048686\n      0.220649\n    \n    \n      3\n      Model_4\n      GS10, FEDFUNDS\n      0.621400\n      0.619873\n      0.039210\n      0.198015\n    \n    \n      4\n      Model_5\n      GS10, FEDFUNDS, UNRATE\n      0.621597\n      0.619304\n      0.039269\n      0.198163\n    \n  \n\n\n\n\nStep 2: In-Sample Forecast First Difference\n\ndef in_sample_forecast(models, observed_data, separate_subplots=False):\n    # Extract the observed data and dates\n    y = observed_data\n    x = observed_data.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data and in-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            X = model.model.exog\n            y_pred = model.predict(X)\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\n\n            # Get the independent variable names\n            ind_var_names = ', '.join(model.model.exog_names)\n\n            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i+1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data\n        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the in-sample predictions for each model\n        for i, model in enumerate(models):\n            X = model.model.exog\n            y_pred = model.predict(X)\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\n\n        # Customize the plot\n        plt.ylabel('Value')\n        plt.title('Observed Data and In-sample Predictions')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\nobserved_data = df_train_diff['MORTGAGE30US']\nin_sample_forecast(models, observed_data, separate_subplots=True)  # For separate subplots\n\n\n\n\nStep 3: In-Sample Forecast Levels\n\ndef in_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False):\n    # Extract the observed data (levels) and dates\n    y = original_data\n    x = original_data.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data (levels) and in-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            X = model.model.exog\n            y_diff_pred = model.predict(X)\n            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\n\n            # Get the independent variable names\n            ind_var_names = ', '.join(model.model.exog_names)\n\n            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i+1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.5, 0.04, 'DATE', ha='center')\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data (levels)\n        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the in-sample predictions for each model\n        for i, model in enumerate(models):\n            X = model.model.exog\n            y_diff_pred = model.predict(X)\n            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\n            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\n\n        # Customize the plot\n        plt.xlabel('DATE')\n        plt.ylabel('Value')\n        plt.title('Observed Data and In-sample Predictions (Levels)')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\noriginal_data = df_train['MORTGAGE30US']\ndiff_data = df_train_diff['MORTGAGE30US']\nin_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False)  # For a single plot with all series"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_poc.html#model-evaluation",
    "href": "notebooks/time_series/loan_rates_forecast_poc.html#model-evaluation",
    "title": "ValidMind",
    "section": "4.6. Model Evaluation",
    "text": "4.6. Model Evaluation\n\n4.6.1. Out-of-Sample Analysis\n\nOut-of-Sample Performance\n\ndef out_of_sample_performance(model_list, model_names, test_data, target_col):\n    # Initialize a list to store results\n    results = []\n\n    for fitted_model, model_name in zip(model_list, model_names):\n        # Extract the column names of the independent variables from the model\n        independent_vars = fitted_model.model.exog_names\n\n        # Separate the target variable and features in the test dataset\n        X_test = test_data[independent_vars]\n        y_test = test_data[target_col]\n\n        # Predict the test data\n        y_pred = fitted_model.predict(X_test)\n\n        # Calculate the residuals\n        residuals = y_test - y_pred\n\n        # Calculate the mean squared error and root mean squared error\n        mse = np.mean(residuals ** 2)\n        rmse_val = np.sqrt(mse)\n\n        # Store the results\n        model_name_with_vars = f\"{model_name} ({', '.join(independent_vars)})\"\n        results.append([model_name_with_vars, mse, rmse_val])\n\n    # Create a DataFrame to display the results\n    results_df = pd.DataFrame(results, columns=['Model', 'MSE', 'RMSE'])\n\n    return results_df\n\n\nmodel_list = [model_3, model_4]\nmodel_names = ['model_3', 'model_4']\nresults_df = out_of_sample_performance(model_list, model_names=model_names, test_data=df_test_diff, target_col='MORTGAGE30US')\ndisplay(results_df)\n\n\n\n\n\n  \n    \n      \n      Model\n      MSE\n      RMSE\n    \n  \n  \n    \n      0\n      model_3 (GS10)\n      0.024310\n      0.155916\n    \n    \n      1\n      model_4 (GS10, FEDFUNDS)\n      0.027384\n      0.165482\n    \n  \n\n\n\n\n\n\nOut-of-Sample Forecast\n\ndef out_of_sample_forecast(models, model_names, test_data, target_col, separate_subplots=False):\n    # Extract the observed data and dates\n    y = test_data[target_col]\n    x = test_data.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data and out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred = model.predict(X_test)\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\n\n            ax.set_title(f'{model_names[i]} ({\", \".join(exog_names)})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i + 1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data\n        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred = model.predict(X_test)\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\n\n        # Customize the plot\n        plt.ylabel('Value')\n        plt.title('Observed Data and Out-of-sample Predictions')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\nmodels = [model_3, model_4]\nout_of_sample_forecast(models, model_names=['model_3', 'model_4'], test_data=df_test_diff, target_col='MORTGAGE30US', separate_subplots=True)\n\n\n\n\n\ndef out_of_sample_forecast_levels(models, model_names, test_data, original_data, target_col, separate_subplots=False):\n    # Extract the observed data and dates\n    y_test = test_data[target_col]\n    y_orig = original_data[original_data.index.isin(test_data.index)][target_col]\n    x = y_orig.index\n\n    sns.set(style=\"darkgrid\")\n\n    if separate_subplots:\n        # Calculate the number of rows and columns for the subplots\n        n_models = len(models)\n        n_cols = 2\n        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n\n        # Set up the plot\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n        axes = axes.ravel()\n\n        # Plot the observed data and out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            ax = axes[i]\n            sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred_diff = model.predict(X_test)\n            y_pred = y_pred_diff + y_orig.shift(1).values\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\n\n            ax.set_title(f'{model_names[i]} ({\", \".join(exog_names)})')\n            ax.legend()\n\n        # Remove unused subplots\n        for j in range(i + 1, n_rows * n_cols):\n            fig.delaxes(axes[j])\n\n        # Customize the plot\n        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n\n    else:\n        # Set up the plot\n        plt.figure(figsize=(10, 6))\n\n        # Plot the observed data\n        sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey')\n\n        # Plot the out-of-sample predictions for each model\n        for i, model in enumerate(models):\n            exog_names = model.model.exog_names\n            if 'const' in exog_names and 'const' not in test_data.columns:\n                X_test = test_data[[name for name in exog_names if name != 'const']]\n                X_test.insert(0, 'const', 1)\n            else:\n                X_test = test_data[exog_names]\n\n            y_pred_diff = model.predict(X_test)\n            y_pred = y_pred_diff + y_orig.shift(1).values\n            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\n\n        # Customize the plot\n        plt.ylabel('Value')\n        plt.title('Observed Data and Out-of-sample Predictions')\n        plt.legend()\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n\n\nmodels = [model_3, model_4]\nmodel_names = ['model_3', 'model_4']\noriginal_data = df_test\nout_of_sample_forecast_levels(models, model_names, test_data=df_test_diff, original_data=original_data, target_col='MORTGAGE30US', separate_subplots=False)\n\n\n\n\n\n\n\n4.6.2. Forecast Performance\n\nOne-Step Ahead Forecast\n\n\nFive-Step Ahead Forecast\n\n\n\n4.6.3. Scenario Analysis\n\nParallel Interest Rates Shocks\n\n\n\n4.6.4. Stress Testing\n\n\n4.6.5. Uncertainty Analysis"
  },
  {
    "objectID": "notebooks/time_series/explore_metrics.html",
    "href": "notebooks/time_series/explore_metrics.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Data libraries \nimport pandas as pd\n\n# ML libraries\nfrom numpy import argmax\nimport scipy.stats as stats\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\nfrom arch.unitroot import PhillipsPerron\nfrom arch.unitroot import ZivotAndrews\nfrom arch.unitroot import DFGLS\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport xgboost as xgb\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# ValidMind libraries \nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nimport validmind as vm\nfrom validmind.vm_models.test_context import TestContext\n\n\n# Quick hack to load local library code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nTrue\n\n\n\n\n\n\ndf = pd.read_csv(\"/Users/juanvalidmind/Dev/github/validmind/validmind-python/notebooks/datasets/lending_club_loan_rates.csv\", sep='\\t')\ndf = df.rename(columns={'Unnamed: 0': 'Date'})\ndf = df.set_index(pd.to_datetime(df['Date']))\ndf.drop([\"Date\"], axis=1, inplace=True)\n\n# Remove diff columns\ncolumns_to_remove = [col for col in df.columns if col.startswith(\"diff\")]\ndf = df.drop(columns=columns_to_remove)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      loan_rate_A\n      loan_rate_B\n      loan_rate_C\n      loan_rate_D\n      FEDFUNDS\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2007-08-01\n      7.766667\n      9.497692\n      10.947500\n      12.267000\n      5.02\n    \n    \n      2007-09-01\n      7.841429\n      9.276667\n      10.829167\n      12.436667\n      4.94\n    \n    \n      2007-10-01\n      7.830000\n      9.433333\n      10.825926\n      12.737368\n      4.76\n    \n    \n      2007-11-01\n      7.779091\n      9.467778\n      10.967037\n      12.609444\n      4.49\n    \n    \n      2007-12-01\n      7.695833\n      9.387500\n      10.805000\n      12.478889\n      4.24\n    \n  \n\n\n\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgo0g0rt0000fjy6ozl9pb69\"\n)\n\nTrue\n\n\n\ntarget_variables = [\"loan_rate_A\", \"loan_rate_B\", \"loan_rate_C\", \"loan_rate_D\"]\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column = target_variables   \n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                              Name                          Description                                      \n\n\nsklearn_classifier_metrics      SKLearnClassifierMetrics      Test plan for sklearn classifier metrics         \nsklearn_classifier_validation   SKLearnClassifierPerformance  Test plan for sklearn classifier models          \nsklearn_classifier              SKLearnClassifier             Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                  \ntabular_dataset                 TabularDataset                Test plan for generic tabular datasets           \ntabular_dataset_description     TabularDatasetDescription     Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                  \ntabular_data_quality            TabularDataQuality            Test plan for data quality on tabular datasets   \nnormality_test_plan             NormalityTestPlan             Test plan to perform normality tests.            \nautocorrelation_test_plan       AutocorrelationTestPlan       Test plan to perform autocorrelation tests.      \nseasonality_test_plan           SesonalityTestPlan            Test plan to perform seasonality tests.          \nunit_root_test_plan             UnitRootTestPlan              Test plan to perform unit root tests.            \nstationarity_test_plan          StationarityTestPlan          Test plan to perform stationarity tests.         \ntimeseries                      TimeSeries                    Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                  \ntimeseries_univariate_inspectionTimeSeriesUnivariateInspectionTest plan to perform univariate inspection tests.\n\n\n\n\nCreate Train and Test Datasets\n\ntest_size = 30\ntrain_ds = df[:-test_size]\ntest_ds = df[-test_size:]\n\n\nvm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\")\nvm_test_ds = vm.init_dataset(dataset=test_ds, type=\"generic\")\nvm_dataset = vm.init_dataset(dataset=df, type=\"generic\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nOriginal\n\nadftest = adfuller(df['loan_rate_A'])\nadftest\n\n(-1.917289312690944,\n 0.32397189281015515,\n 1,\n 135,\n {'1%': -3.479742586699182,\n  '5%': -2.88319822181578,\n  '10%': -2.578319684499314},\n -71.08908853191068)\n\n\nValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import ADF\ntest_context = TestContext(dataset=vm_dataset)\nmetric = ADF(test_context=test_context)\nmetric.run()\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='evaluation', scope='test', key='adf', value={'loan_rate_A': {'stat': -1.917289312690944, 'pvalue': 0.32397189281015515, 'usedlag': 1, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -71.08908853191068}, 'loan_rate_B': {'stat': -3.1599303710498425, 'pvalue': 0.022424413263559147, 'usedlag': 9, 'nobs': 127, 'critical_values': {'1%': -3.482920063655088, '5%': -2.884580323367261, '10%': -2.5790575441750883}, 'icbest': -42.45027033820841}, 'loan_rate_C': {'stat': -2.530666699941385, 'pvalue': 0.10818994357289696, 'usedlag': 1, 'nobs': 135, 'critical_values': {'1%': -3.479742586699182, '5%': -2.88319822181578, '10%': -2.578319684499314}, 'icbest': -92.19465856666866}, 'loan_rate_D': {'stat': -1.617158531178829, 'pvalue': 0.47421928207593467, 'usedlag': 6, 'nobs': 130, 'critical_values': {'1%': -3.4816817173418295, '5%': -2.8840418343195267, '10%': -2.578770059171598}, 'icbest': -4.9426661983780775}, 'FEDFUNDS': {'stat': -0.16854321128256927, 'pvalue': 0.9421687822974046, 'usedlag': 13, 'nobs': 123, 'critical_values': {'1%': -3.4846672514209773, '5%': -2.8853397507076006, '10%': -2.5794629869786503}, 'icbest': -338.8629171086535}}, value_formatter=None))\n\n\n\n\n\nOriginal\n\nkpsstest = kpss(df['loan_rate_A'])\nkpsstest\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n\n\n(1.012356679488042,\n 0.01,\n 6,\n {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739})\n\n\nValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import KPSSTest\ntest_context = TestContext(train_ds=vm_train_ds)\nmetric = KPSSTest(test_context=test_context)\nmetric.run()\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is greater than the p-value returned.\n\n\n\nTestPlanMetricResult(figures=None, metric=MetricResult(type='evaluation', scope='test', key='kpss', value={'stat': 0.06862873540051123, 'pvalue': 0.1, 'usedlag': 8}, value_formatter='key_values'))\n\n\n\n\n\nOriginal\n\npp = PhillipsPerron(df['loan_rate_A'])\npp\n\n\n\nPhillips-Perron Test (Z-tau)\n\n  Test Statistic    -2.027\n\n\n  P-value            0.275\n\n\n  Lags                  13\n\nTrend: ConstantCritical Values: -3.48 (1%), -2.88 (5%), -2.58 (10%)Null Hypothesis: The process contains a unit root.Alternative Hypothesis: The process is weakly stationary.\n\n\n\npp.nobs\n\n136\n\n\nValidMind\n\n\n\nOriginal\n\nza = ZivotAndrews(df['loan_rate_A'])\nza\n\n\n\nZivot-Andrews Results\n\n  Test Statistic    -3.499\n\n\n  P-value            0.680\n\n\n  Lags                   1\n\nTrend: ConstantCritical Values: -5.28 (1%), -4.81 (5%), -4.57 (10%)Null Hypothesis: The process contains a unit root with a single structural break.Alternative Hypothesis: The process is trend and break stationary.\n\n\n\nza.nobs\n\n137\n\n\nValidMind\n\n\n\nOriginal\n\ndfgls = DFGLS(df['loan_rate_A'])\ndfgls\n\n\n\nDickey-Fuller GLS Results\n\n  Test Statistic    -1.798\n\n\n  P-value            0.071\n\n\n  Lags                   1\n\nTrend: ConstantCritical Values: -2.71 (1%), -2.09 (5%), -1.78 (10%)Null Hypothesis: The process contains a unit root.Alternative Hypothesis: The process is weakly stationary.\n\n\n\ndfgls.nobs\n\n135\n\n\nValidMind\n\n\n\nOff ValidMind\n\nsd = seasonal_decompose(df['loan_rate_A'])\nsd\n\n<statsmodels.tsa.seasonal.DecomposeResult at 0x28a928550>\n\n\n\nsd.trend\n\nDate\n2007-08-01   NaN\n2007-09-01   NaN\n2007-10-01   NaN\n2007-11-01   NaN\n2007-12-01   NaN\n              ..\n2018-08-01   NaN\n2018-09-01   NaN\n2018-10-01   NaN\n2018-11-01   NaN\n2018-12-01   NaN\nName: trend, Length: 137, dtype: float64\n\n\n\nfig, ax = plt.subplots()\n\n\n\n\nIn ValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import SeasonalDecomposeMetricWithFigure\ntest_context = TestContext(train_ds=vm_train_ds)\nsd_metric = SeasonalDecomposeMetricWithFigure(test_context=test_context)\nsd_metric.run()\n\nTestPlanMetricResult(figures=[Figure(key='seasonal_decomposition_with_figure', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)], metric=MetricResult(type='evaluation', scope='', key='seasonal_decomposition_with_figure', value={'loan_rate_A': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.050284773390468294, 'resid': nan, 'observed': 7.7666666666666675}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.06087962072801919, 'resid': nan, 'observed': 7.841428571428572}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.01749661199350142, 'resid': nan, 'observed': 7.83}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.047258378330469496, 'resid': nan, 'observed': 7.779090909090908}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.0850517814632488, 'resid': nan, 'observed': 7.695833333333333}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.06564185816692857, 'resid': nan, 'observed': 7.961333333333333}, {'date': '2008-02-01', 'trend': 8.005048767959094, 'seasonal': 0.008943337297934315, 'resid': 0.1163412280763044, 'observed': 8.130333333333333}, {'date': '2008-03-01', 'trend': 8.036669799705125, 'seasonal': -0.002099404440702747, 'resid': 0.09171531902129201, 'observed': 8.126285714285714}, {'date': '2008-04-01', 'trend': 8.079229323514648, 'seasonal': 0.020168623084867585, 'resid': -0.007314613266182244, 'observed': 8.092083333333333}, {'date': '2008-05-01', 'trend': 8.144335744871071, 'seasonal': -0.01070385076597299, 'resid': 0.017796677323472867, 'observed': 8.151428571428571}, {'date': '2008-06-01', 'trend': 8.239462007497334, 'seasonal': -0.028819584167133785, 'resid': -0.005142423330199462, 'observed': 8.2055}, {'date': '2008-07-01', 'trend': 8.343548073071103, 'seasonal': 0.0027433998162858285, 'resid': -0.12585669027869376, 'observed': 8.220434782608695}, {'date': '2008-08-01', 'trend': 8.417007001892738, 'seasonal': -0.050284773390468294, 'resid': -0.0797222285022692, 'observed': 8.287}, {'date': '2008-09-01', 'trend': 8.47796981223055, 'seasonal': -0.06087962072801919, 'resid': -0.3370901915025301, 'observed': 8.08}, {'date': '2008-10-01', 'trend': 8.531636932259564, 'seasonal': 0.01749661199350142, 'resid': 0.06372359860407714, 'observed': 8.612857142857143}, {'date': '2008-11-01', 'trend': 8.589412987821856, 'seasonal': -0.047258378330469496, 'resid': 0.016633269296492413, 'observed': 8.558787878787879}, {'date': '2008-12-01', 'trend': 8.652802325527823, 'seasonal': 0.0850517814632488, 'resid': 0.4613125596755969, 'observed': 9.199166666666668}, {'date': '2009-01-01', 'trend': 8.714159305191414, 'seasonal': 0.06564185816692857, 'resid': 0.17626441041215057, 'observed': 8.956065573770493}, {'date': '2009-02-01', 'trend': 8.752934265042391, 'seasonal': 0.008943337297934315, 'resid': 0.13673778227506048, 'observed': 8.898615384615386}, {'date': '2009-03-01', 'trend': 8.775809794329618, 'seasonal': -0.002099404440702747, 'resid': 0.04740072122219648, 'observed': 8.821111111111112}, {'date': '2009-04-01', 'trend': 8.783591261550063, 'seasonal': 0.020168623084867585, 'resid': -0.11849106743062912, 'observed': 8.685268817204301}, {'date': '2009-05-01', 'trend': 8.768108091755868, 'seasonal': -0.01070385076597299, 'resid': 0.18746418006273688, 'observed': 8.944868421052632}, {'date': '2009-06-01', 'trend': 8.727065734609644, 'seasonal': -0.028819584167133785, 'resid': 0.23515810487663832, 'observed': 8.933404255319148}, {'date': '2009-07-01', 'trend': 8.660325582011376, 'seasonal': 0.0027433998162858285, 'resid': 0.30202905738802394, 'observed': 8.965098039215686}, {'date': '2009-08-01', 'trend': 8.568932625411962, 'seasonal': -0.050284773390468294, 'resid': -0.04571207220498089, 'observed': 8.472935779816513}, {'date': '2009-09-01', 'trend': 8.454561132788863, 'seasonal': -0.06087962072801919, 'resid': 0.04939541101607955, 'observed': 8.443076923076923}, {'date': '2009-10-01', 'trend': 8.34828280363049, 'seasonal': 0.01749661199350142, 'resid': 0.0707560174468764, 'observed': 8.436535433070867}, {'date': '2009-11-01', 'trend': 8.23694393241163, 'seasonal': -0.047258378330469496, 'resid': 0.1738279594323525, 'observed': 8.363513513513514}, {'date': '2009-12-01', 'trend': 8.116444096407664, 'seasonal': 0.0850517814632488, 'resid': 0.20792858256074262, 'observed': 8.409424460431655}, {'date': '2010-01-01', 'trend': 7.995096023438102, 'seasonal': 0.06564185816692857, 'resid': 0.0833062360420266, 'observed': 8.144044117647057}, {'date': '2010-02-01', 'trend': 7.889837683206626, 'seasonal': 0.008943337297934315, 'resid': -0.3815751381516201, 'observed': 7.517205882352941}, {'date': '2010-03-01', 'trend': 7.807926812466593, 'seasonal': -0.002099404440702747, 'resid': -0.34822261760672807, 'observed': 7.457604790419162}, {'date': '2010-04-01', 'trend': 7.704196284836861, 'seasonal': 0.020168623084867585, 'resid': -0.22626966982649002, 'observed': 7.498095238095239}, {'date': '2010-05-01', 'trend': 7.556635938363027, 'seasonal': -0.01070385076597299, 'resid': -0.08602299668796279, 'observed': 7.4599090909090915}, {'date': '2010-06-01', 'trend': 7.386339108881637, 'seasonal': -0.028819584167133785, 'resid': 0.1688479966530175, 'observed': 7.52636752136752}, {'date': '2010-07-01', 'trend': 7.234176921822315, 'seasonal': 0.0027433998162858285, 'resid': 0.22286070025920932, 'observed': 7.45978102189781}, {'date': '2010-08-01', 'trend': 7.139619393217407, 'seasonal': -0.050284773390468294, 'resid': 0.3627180117520081, 'observed': 7.452052631578947}, {'date': '2010-09-01', 'trend': 7.084157646540273, 'seasonal': -0.06087962072801919, 'resid': 0.47482114774146483, 'observed': 7.498099173553719}, {'date': '2010-10-01', 'trend': 7.0310647217648095, 'seasonal': 0.01749661199350142, 'resid': -0.1565808142777916, 'observed': 6.891980519480519}, {'date': '2010-11-01', 'trend': 6.991663604498002, 'seasonal': -0.047258378330469496, 'resid': -0.5777851144356879, 'observed': 6.366620111731844}, {'date': '2010-12-01', 'trend': 6.958236325031823, 'seasonal': 0.0850517814632488, 'resid': -0.7240941518351232, 'observed': 6.319193954659949}, {'date': '2011-01-01', 'trend': 6.9214056863686135, 'seasonal': 0.06564185816692857, 'resid': -0.40466541054050575, 'observed': 6.582382133995036}, {'date': '2011-02-01', 'trend': 6.889921185335679, 'seasonal': 0.008943337297934315, 'resid': -0.08937734314643411, 'observed': 6.80948717948718}, {'date': '2011-03-01', 'trend': 6.856889534975067, 'seasonal': -0.002099404440702747, 'resid': -0.020548557500655016, 'observed': 6.834241573033709}, {'date': '2011-04-01', 'trend': 6.858796195374667, 'seasonal': 0.020168623084867585, 'resid': -0.03173655758996877, 'observed': 6.847228260869565}, {'date': '2011-05-01', 'trend': 6.921615774690598, 'seasonal': -0.01070385076597299, 'resid': 0.25423732980671804, 'observed': 7.165149253731343}, {'date': '2011-06-01', 'trend': 7.018217147459357, 'seasonal': -0.028819584167133785, 'resid': 0.029475088064770363, 'observed': 7.018872651356993}, {'date': '2011-07-01', 'trend': 7.11041588830481, 'seasonal': 0.0027433998162858285, 'resid': -0.029818724129772076, 'observed': 7.083340563991324}, {'date': '2011-08-01', 'trend': 7.1830792783975985, 'seasonal': -0.050284773390468294, 'resid': -0.0599294403121207, 'observed': 7.0728650646950095}, {'date': '2011-09-01', 'trend': 7.247497405208671, 'seasonal': -0.06087962072801919, 'resid': -0.10209065269770515, 'observed': 7.084527131782947}, {'date': '2011-10-01', 'trend': 7.309379082976638, 'seasonal': 0.01749661199350142, 'resid': 0.024436715871515715, 'observed': 7.351312410841655}, {'date': '2011-11-01', 'trend': 7.353215232472669, 'seasonal': -0.047258378330469496, 'resid': 0.10900126981090005, 'observed': 7.4149581239531}, {'date': '2011-12-01', 'trend': 7.387931350546878, 'seasonal': 0.0850517814632488, 'resid': 0.11630575687876285, 'observed': 7.589288888888889}, {'date': '2012-01-01', 'trend': 7.433640607004464, 'seasonal': 0.06564185816692857, 'resid': 0.025774514885588032, 'observed': 7.525056980056981}, {'date': '2012-02-01', 'trend': 7.478542797820363, 'seasonal': 0.008943337297934315, 'resid': 0.12324756053387684, 'observed': 7.610733695652174}, {'date': '2012-03-01', 'trend': 7.5208057544537, 'seasonal': -0.002099404440702747, 'resid': 0.06032375032145173, 'observed': 7.579030100334449}, {'date': '2012-04-01', 'trend': 7.555975713482307, 'seasonal': 0.020168623084867585, 'resid': 0.011455663432825416, 'observed': 7.5876}, {'date': '2012-05-01', 'trend': 7.57810750100383, 'seasonal': -0.01070385076597299, 'resid': -0.09055854773216213, 'observed': 7.476845102505695}, {'date': '2012-06-01', 'trend': 7.596137974654016, 'seasonal': -0.028819584167133785, 'resid': -0.02695475412324547, 'observed': 7.540363636363637}, {'date': '2012-07-01', 'trend': 7.6153264121992175, 'seasonal': 0.0027433998162858285, 'resid': 0.04080192195124229, 'observed': 7.658871733966746}, {'date': '2012-08-01', 'trend': 7.628451463506009, 'seasonal': -0.050284773390468294, 'resid': -0.0031802158143688156, 'observed': 7.574986474301172}, {'date': '2012-09-01', 'trend': 7.638002595789809, 'seasonal': -0.06087962072801919, 'resid': 0.019593706315086022, 'observed': 7.596716681376876}, {'date': '2012-10-01', 'trend': 7.658826993137152, 'seasonal': 0.01749661199350142, 'resid': 0.006878272803619381, 'observed': 7.683201877934273}, {'date': '2012-11-01', 'trend': 7.677789303574363, 'seasonal': -0.047258378330469496, 'resid': -0.01629936786684364, 'observed': 7.61423155737705}, {'date': '2012-12-01', 'trend': 7.68098745426593, 'seasonal': 0.0850517814632488, 'resid': 0.0567075873402256, 'observed': 7.822746823069404}, {'date': '2013-01-01', 'trend': 7.6779520344633525, 'seasonal': 0.06564185816692857, 'resid': 0.008527654331044757, 'observed': 7.752121546961326}, {'date': '2013-02-01', 'trend': 7.677057001432264, 'seasonal': 0.008943337297934315, 'resid': 0.012670021380605325, 'observed': 7.698670360110803}, {'date': '2013-03-01', 'trend': 7.682469578470097, 'seasonal': -0.002099404440702747, 'resid': 0.039950436657629505, 'observed': 7.720320610687024}, {'date': '2013-04-01', 'trend': 7.691712575890414, 'seasonal': 0.020168623084867585, 'resid': 0.2342138270083863, 'observed': 7.946095025983668}, {'date': '2013-05-01', 'trend': 7.706156831040658, 'seasonal': -0.01070385076597299, 'resid': -0.12200745325962684, 'observed': 7.573445527015058}, {'date': '2013-06-01', 'trend': 7.718205987083048, 'seasonal': -0.028819584167133785, 'resid': -0.16886757446403147, 'observed': 7.520518828451883}, {'date': '2013-07-01', 'trend': 7.72646306080784, 'seasonal': 0.0027433998162858285, 'resid': -0.12333999400747178, 'observed': 7.605866466616654}, {'date': '2013-08-01', 'trend': 7.738107999623397, 'seasonal': -0.050284773390468294, 'resid': -0.08131227732781923, 'observed': 7.606510948905109}, {'date': '2013-09-01', 'trend': 7.7509664574560295, 'seasonal': -0.06087962072801919, 'resid': 0.005007218952953606, 'observed': 7.695094055680964}, {'date': '2013-10-01', 'trend': 7.755204227116213, 'seasonal': 0.01749661199350142, 'resid': 0.03395560260807626, 'observed': 7.806656441717791}, {'date': '2013-11-01', 'trend': 7.750869193258667, 'seasonal': -0.047258378330469496, 'resid': 0.13382830227119422, 'observed': 7.837439117199391}, {'date': '2013-12-01', 'trend': 7.741988403538943, 'seasonal': 0.0850517814632488, 'resid': 0.06167882326227134, 'observed': 7.888719008264463}, {'date': '2014-01-01', 'trend': 7.724369241349729, 'seasonal': 0.06564185816692857, 'resid': 0.09430803164457958, 'observed': 7.884319131161237}, {'date': '2014-02-01', 'trend': 7.7012496300205235, 'seasonal': 0.008943337297934315, 'resid': 0.1357583401657619, 'observed': 7.84595130748422}, {'date': '2014-03-01', 'trend': 7.667279037619656, 'seasonal': -0.002099404440702747, 'resid': 0.21646301811787813, 'observed': 7.881642651296831}, {'date': '2014-04-01', 'trend': 7.6252937267207255, 'seasonal': 0.020168623084867585, 'resid': 0.24101710741265112, 'observed': 7.886479457218244}, {'date': '2014-05-01', 'trend': 7.584095565289032, 'seasonal': -0.01070385076597299, 'resid': -0.044371431323670635, 'observed': 7.529020283199388}, {'date': '2014-06-01', 'trend': 7.536200793048272, 'seasonal': -0.028819584167133785, 'resid': -0.15557608988697594, 'observed': 7.351805118994163}, {'date': '2014-07-01', 'trend': 7.483981914831136, 'seasonal': 0.0027433998162858285, 'resid': -0.13500503111416087, 'observed': 7.351720283533261}, {'date': '2014-08-01', 'trend': 7.429040668396031, 'seasonal': -0.050284773390468294, 'resid': -0.07296943491799038, 'observed': 7.305786460087572}, {'date': '2014-09-01', 'trend': 7.36455133491674, 'seasonal': -0.06087962072801919, 'resid': -0.12314738731106348, 'observed': 7.1805243268776575}, {'date': '2014-10-01', 'trend': 7.294261771259082, 'seasonal': 0.01749661199350142, 'resid': 0.0018203256941895195, 'observed': 7.313578708946773}, {'date': '2014-11-01', 'trend': 7.234071423791347, 'seasonal': -0.047258378330469496, 'resid': 0.15494793014887884, 'observed': 7.341760975609756}, {'date': '2014-12-01', 'trend': 7.191857825197275, 'seasonal': 0.0850517814632488, 'resid': -0.04198699058463887, 'observed': 7.234922616075885}, {'date': '2015-01-01', 'trend': 7.15300977164523, 'seasonal': 0.06564185816692857, 'resid': 0.06621081632638982, 'observed': 7.284862446138549}, {'date': '2015-02-01', 'trend': 7.114847188355677, 'seasonal': 0.008943337297934315, 'resid': 0.0030275524107579964, 'observed': 7.126818078064369}, {'date': '2015-03-01', 'trend': 7.08538334834315, 'seasonal': -0.002099404440702747, 'resid': -0.030252066688752053, 'observed': 7.053031877213695}, {'date': '2015-04-01', 'trend': 7.054016766629, 'seasonal': 0.020168623084867585, 'resid': -0.04604468619627991, 'observed': 7.028140703517588}, {'date': '2015-05-01', 'trend': 7.013202303085584, 'seasonal': -0.01070385076597299, 'resid': -0.05970775464519265, 'observed': 6.942790697674418}, {'date': '2015-06-01', 'trend': 6.9738382666741785, 'seasonal': -0.028819584167133785, 'resid': -0.02011034424566108, 'observed': 6.924908338261384}, {'date': '2015-07-01', 'trend': 6.938361615724768, 'seasonal': 0.0027433998162858285, 'resid': -0.09484123652406432, 'observed': 6.846263779016989}, {'date': '2015-08-01', 'trend': 6.908688078173169, 'seasonal': -0.050284773390468294, 'resid': 0.03693766087185387, 'observed': 6.895340965654555}, {'date': '2015-09-01', 'trend': 6.87437653070801, 'seasonal': -0.06087962072801919, 'resid': 0.07034075103002743, 'observed': 6.883837661010018}, {'date': '2015-10-01', 'trend': 6.828130516887571, 'seasonal': 0.01749661199350142, 'resid': 0.011840284793749958, 'observed': 6.857467413674822}, {'date': '2015-11-01', 'trend': 6.8029077692831725, 'seasonal': -0.047258378330469496, 'resid': 0.06267575488702634, 'observed': 6.818325145839729}, {'date': '2015-12-01', 'trend': 6.809591954262458, 'seasonal': 0.0850517814632488, 'resid': -0.08102216375352447, 'observed': 6.813621571972182}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.06564185816692857, 'resid': nan, 'observed': 6.854723867456379}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.008943337297934315, 'resid': nan, 'observed': 6.844791755508173}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.002099404440702747, 'resid': nan, 'observed': 6.51158106060606}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.020168623084867585, 'resid': nan, 'observed': 6.459687188434696}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.01070385076597299, 'resid': nan, 'observed': 6.905898270251756}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.028819584167133785, 'resid': nan, 'observed': 7.12222120518688}], 'loan_rate_B': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.004683903074718656, 'resid': nan, 'observed': 9.497692307692308}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.02878733626807229, 'resid': nan, 'observed': 9.276666666666666}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.07292104494645359, 'resid': nan, 'observed': 9.433333333333334}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.03883917943439751, 'resid': nan, 'observed': 9.467777777777778}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.020002635551053742, 'resid': nan, 'observed': 9.3875}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.057087759887233934, 'resid': nan, 'observed': 9.693125}, {'date': '2008-02-01', 'trend': 9.815614077323866, 'seasonal': -0.0026761200794264053, 'resid': 0.22279237983421207, 'observed': 10.035730337078652}, {'date': '2008-03-01', 'trend': 9.903222857432645, 'seasonal': -0.00016982951529819455, 'resid': 0.15404374627620265, 'observed': 10.05709677419355}, {'date': '2008-04-01', 'trend': 10.006583968543756, 'seasonal': 3.19587595681934e-05, 'resid': 0.15461214287211555, 'observed': 10.16122807017544}, {'date': '2008-05-01', 'trend': 10.121889524099311, 'seasonal': -0.021465549087508966, 'resid': -0.022507308345133377, 'observed': 10.077916666666669}, {'date': '2008-06-01', 'trend': 10.267257038529324, 'seasonal': 0.0048551821613346344, 'resid': -0.20393040250884076, 'observed': 10.068181818181818}, {'date': '2008-07-01', 'trend': 10.43272381322504, 'seasonal': -0.07584606528025915, 'resid': -0.19872959979663263, 'observed': 10.158148148148149}, {'date': '2008-08-01', 'trend': 10.595628603740092, 'seasonal': -0.004683903074718656, 'resid': -0.14730833702901014, 'observed': 10.443636363636363}, {'date': '2008-09-01', 'trend': 10.760741481659695, 'seasonal': 0.02878733626807229, 'resid': -0.3561954845944334, 'observed': 10.433333333333334}, {'date': '2008-10-01', 'trend': 10.92033892958057, 'seasonal': 0.07292104494645359, 'resid': -0.2359266411936919, 'observed': 10.757333333333332}, {'date': '2008-11-01', 'trend': 11.06172533831073, 'seasonal': -0.03883917943439751, 'resid': -0.11177504776522074, 'observed': 10.911111111111111}, {'date': '2008-12-01', 'trend': 11.192890459921573, 'seasonal': -0.020002635551053742, 'resid': 0.26009918861649195, 'observed': 11.432987012987011}, {'date': '2009-01-01', 'trend': 11.319548541808823, 'seasonal': 0.057087759887233934, 'resid': 0.2422042780140867, 'observed': 11.618840579710144}, {'date': '2009-02-01', 'trend': 11.435830739857689, 'seasonal': -0.0026761200794264053, 'resid': 0.5865751099514681, 'observed': 12.01972972972973}, {'date': '2009-03-01', 'trend': 11.551304435676638, 'seasonal': -0.00016982951529819455, 'resid': 0.4846718454515647, 'observed': 12.035806451612904}, {'date': '2009-04-01', 'trend': 11.656503680463528, 'seasonal': 3.19587595681934e-05, 'resid': 0.35632150363404713, 'observed': 12.012857142857143}, {'date': '2009-05-01', 'trend': 11.74012123279847, 'seasonal': -0.021465549087508966, 'resid': -0.09909428020218936, 'observed': 11.619561403508772}, {'date': '2009-06-01', 'trend': 11.79583062627412, 'seasonal': 0.0048551821613346344, 'resid': -0.1261858084354538, 'observed': 11.6745}, {'date': '2009-07-01', 'trend': 11.813709500580932, 'seasonal': -0.07584606528025915, 'resid': -0.14623950367674132, 'observed': 11.591623931623932}, {'date': '2009-08-01', 'trend': 11.7577233646958, 'seasonal': -0.004683903074718656, 'resid': 0.047893871712256, 'observed': 11.800933333333337}, {'date': '2009-09-01', 'trend': 11.645087849532663, 'seasonal': 0.02878733626807229, 'resid': 0.17352987749040427, 'observed': 11.84740506329114}, {'date': '2009-10-01', 'trend': 11.531122201859514, 'seasonal': 0.07292104494645359, 'resid': 0.2640002314549018, 'observed': 11.86804347826087}, {'date': '2009-11-01', 'trend': 11.436971193428466, 'seasonal': -0.03883917943439751, 'resid': 0.40909020822815567, 'observed': 11.807222222222224}, {'date': '2009-12-01', 'trend': 11.380949115633296, 'seasonal': -0.020002635551053742, 'resid': 0.5129548652092385, 'observed': 11.87390134529148}, {'date': '2010-01-01', 'trend': 11.346807882383448, 'seasonal': 0.057087759887233934, 'resid': 0.20312358849855025, 'observed': 11.607019230769232}, {'date': '2010-02-01', 'trend': 11.308341582352016, 'seasonal': -0.0026761200794264053, 'resid': -0.6177816448452021, 'observed': 10.687883817427387}, {'date': '2010-03-01', 'trend': 11.25711953661246, 'seasonal': -0.00016982951529819455, 'resid': -0.5925497070971618, 'observed': 10.6644}, {'date': '2010-04-01', 'trend': 11.175070289518446, 'seasonal': 3.19587595681934e-05, 'resid': -0.5260141979635501, 'observed': 10.649088050314464}, {'date': '2010-05-01', 'trend': 11.034948084615989, 'seasonal': -0.021465549087508966, 'resid': -0.28977624182218864, 'observed': 10.723706293706291}, {'date': '2010-06-01', 'trend': 10.85903782999656, 'seasonal': 0.0048551821613346344, 'resid': 0.36193223056055446, 'observed': 11.225825242718448}, {'date': '2010-07-01', 'trend': 10.708866411920479, 'seasonal': -0.07584606528025915, 'resid': 0.5878887442688713, 'observed': 11.22090909090909}, {'date': '2010-08-01', 'trend': 10.639141951735194, 'seasonal': -0.004683903074718656, 'resid': 0.6139989246332928, 'observed': 11.248456973293768}, {'date': '2010-09-01', 'trend': 10.622033529095082, 'seasonal': 0.02878733626807229, 'resid': 0.5197314602182407, 'observed': 11.170552325581395}, {'date': '2010-10-01', 'trend': 10.605833809461833, 'seasonal': 0.07292104494645359, 'resid': -0.10304056869400115, 'observed': 10.575714285714286}, {'date': '2010-11-01', 'trend': 10.613230598722458, 'seasonal': -0.03883917943439751, 'resid': -0.8377729221782316, 'observed': 9.736618497109829}, {'date': '2010-12-01', 'trend': 10.624008511489265, 'seasonal': -0.020002635551053742, 'resid': -0.8813469164006392, 'observed': 9.722658959537572}, {'date': '2011-01-01', 'trend': 10.61271994083742, 'seasonal': 0.057087759887233934, 'resid': -0.5156601180274538, 'observed': 10.1541475826972}, {'date': '2011-02-01', 'trend': 10.599581112470494, 'seasonal': -0.0026761200794264053, 'resid': -0.1295365713384361, 'observed': 10.467368421052631}, {'date': '2011-03-01', 'trend': 10.596316101806579, 'seasonal': -0.00016982951529819455, 'resid': -0.12183301927923353, 'observed': 10.474313253012047}, {'date': '2011-04-01', 'trend': 10.642422990347459, 'seasonal': 3.19587595681934e-05, 'resid': -0.19207342300261188, 'observed': 10.450381526104415}, {'date': '2011-05-01', 'trend': 10.760849905635485, 'seasonal': -0.021465549087508966, 'resid': 0.3605514036233296, 'observed': 11.099935760171306}, {'date': '2011-06-01', 'trend': 10.91443738038156, 'seasonal': 0.0048551821613346344, 'resid': 0.18897312011393141, 'observed': 11.108265682656826}, {'date': '2011-07-01', 'trend': 11.0539560569123, 'seasonal': -0.07584606528025915, 'resid': 0.08943296369441832, 'observed': 11.06754295532646}, {'date': '2011-08-01', 'trend': 11.162766577022664, 'seasonal': -0.004683903074718656, 'resid': -0.07159144587776857, 'observed': 11.086491228070177}, {'date': '2011-09-01', 'trend': 11.268679036144619, 'seasonal': 0.02878733626807229, 'resid': -0.043308557541674626, 'observed': 11.254157814871016}, {'date': '2011-10-01', 'trend': 11.388977923898196, 'seasonal': 0.07292104494645359, 'resid': 0.13677515256110262, 'observed': 11.598674121405752}, {'date': '2011-11-01', 'trend': 11.488625115138078, 'seasonal': -0.03883917943439751, 'resid': 0.10611869262731599, 'observed': 11.555904628330996}, {'date': '2011-12-01', 'trend': 11.570384807610292, 'seasonal': -0.020002635551053742, 'resid': 0.039090050162983274, 'observed': 11.589472222222222}, {'date': '2012-01-01', 'trend': 11.66228664035388, 'seasonal': 0.057087759887233934, 'resid': -0.08359184349081586, 'observed': 11.635782556750298}, {'date': '2012-02-01', 'trend': 11.76319166689917, 'seasonal': -0.0026761200794264053, 'resid': -0.16332961717150074, 'observed': 11.597185929648242}, {'date': '2012-03-01', 'trend': 11.860822521307425, 'seasonal': -0.00016982951529819455, 'resid': 0.025742071551277383, 'observed': 11.886394763343404}, {'date': '2012-04-01', 'trend': 11.934068361018747, 'seasonal': 3.19587595681934e-05, 'resid': -0.008626997919452104, 'observed': 11.925473321858863}, {'date': '2012-05-01', 'trend': 11.99482340535943, 'seasonal': -0.021465549087508966, 'resid': 0.04301869790214507, 'observed': 12.016376554174066}, {'date': '2012-06-01', 'trend': 12.048930270550104, 'seasonal': 0.0048551821613346344, 'resid': 0.10027205527578197, 'observed': 12.15405750798722}, {'date': '2012-07-01', 'trend': 12.091371298420814, 'seasonal': -0.07584606528025915, 'resid': 0.21186988270164997, 'observed': 12.227395115842205}, {'date': '2012-08-01', 'trend': 12.13362143172545, 'seasonal': -0.004683903074718656, 'resid': 0.21942217599061886, 'observed': 12.34835970464135}, {'date': '2012-09-01', 'trend': 12.161887323854964, 'seasonal': 0.02878733626807229, 'resid': 0.1447551839749598, 'observed': 12.335429844097996}, {'date': '2012-10-01', 'trend': 12.17420943046534, 'seasonal': 0.07292104494645359, 'resid': 0.028171769838638658, 'observed': 12.275302245250431}, {'date': '2012-11-01', 'trend': 12.181736211600096, 'seasonal': -0.03883917943439751, 'resid': 0.19450053649706522, 'observed': 12.337397568662764}, {'date': '2012-12-01', 'trend': 12.175118735274879, 'seasonal': -0.020002635551053742, 'resid': -0.04857205325722414, 'observed': 12.1065440464666}, {'date': '2013-01-01', 'trend': 12.130128219115964, 'seasonal': 0.057087759887233934, 'resid': -0.04992057760023518, 'observed': 12.137295401402962}, {'date': '2013-02-01', 'trend': 12.05328151565516, 'seasonal': -0.0026761200794264053, 'resid': 0.059070888731092334, 'observed': 12.109676284306826}, {'date': '2013-03-01', 'trend': 11.977871938153276, 'seasonal': -0.00016982951529819455, 'resid': 0.07458371115522532, 'observed': 12.052285819793203}, {'date': '2013-04-01', 'trend': 11.921326746462128, 'seasonal': 3.19587595681934e-05, 'resid': 0.1339541188363741, 'observed': 12.05531282405807}, {'date': '2013-05-01', 'trend': 11.881069792553062, 'seasonal': -0.021465549087508966, 'resid': 0.20757555574345343, 'observed': 12.067179799209006}, {'date': '2013-06-01', 'trend': 11.854568105629241, 'seasonal': 0.0048551821613346344, 'resid': 0.08501154335650916, 'observed': 11.944434831147085}, {'date': '2013-07-01', 'trend': 11.836843225112068, 'seasonal': -0.07584606528025915, 'resid': -0.40375175496345206, 'observed': 11.357245404868356}, {'date': '2013-08-01', 'trend': 11.814459446444463, 'seasonal': -0.004683903074718656, 'resid': -0.43558701081386386, 'observed': 11.37418853255588}, {'date': '2013-09-01', 'trend': 11.789602866611254, 'seasonal': 0.02878733626807229, 'resid': -0.318619046741066, 'observed': 11.49977115613826}, {'date': '2013-10-01', 'trend': 11.76503003465179, 'seasonal': 0.07292104494645359, 'resid': -0.08407474697564411, 'observed': 11.7538763326226}, {'date': '2013-11-01', 'trend': 11.719249496700483, 'seasonal': -0.03883917943439751, 'resid': 0.2122462702069169, 'observed': 11.892656587473002}, {'date': '2013-12-01', 'trend': 11.646817938065396, 'seasonal': -0.020002635551053742, 'resid': 0.2884292389703734, 'observed': 11.915244541484716}, {'date': '2014-01-01', 'trend': 11.592579216683331, 'seasonal': 0.057087759887233934, 'resid': 0.25353079740203666, 'observed': 11.903197773972602}, {'date': '2014-02-01', 'trend': 11.558608636178356, 'seasonal': -0.0026761200794264053, 'resid': 0.25063070761575534, 'observed': 11.806563223714685}, {'date': '2014-03-01', 'trend': 11.516112499599217, 'seasonal': -0.00016982951529819455, 'resid': 0.2428982943044916, 'observed': 11.75884096438841}, {'date': '2014-04-01', 'trend': 11.460639561456194, 'seasonal': 3.19587595681934e-05, 'resid': 0.2983381922199614, 'observed': 11.759009712435724}, {'date': '2014-05-01', 'trend': 11.375487965050318, 'seasonal': -0.021465549087508966, 'resid': -0.0892724159628101, 'observed': 11.26475}, {'date': '2014-06-01', 'trend': 11.261071150944403, 'seasonal': 0.0048551821613346344, 'resid': -0.2574191099917732, 'observed': 11.008507223113964}, {'date': '2014-07-01', 'trend': 11.142598337443784, 'seasonal': -0.07584606528025915, 'resid': -0.0753085724316206, 'observed': 10.991443699731905}, {'date': '2014-08-01', 'trend': 11.019433954700187, 'seasonal': -0.004683903074718656, 'resid': -0.09005374605251894, 'observed': 10.92469630557295}, {'date': '2014-09-01', 'trend': 10.88433692264195, 'seasonal': 0.02878733626807229, 'resid': 0.016231846311807734, 'observed': 10.92935610522183}, {'date': '2014-10-01', 'trend': 10.744093252808314, 'seasonal': 0.07292104494645359, 'resid': 0.17592657035175308, 'observed': 10.99294086810652}, {'date': '2014-11-01', 'trend': 10.623740180200944, 'seasonal': -0.03883917943439751, 'resid': 0.025052737481477398, 'observed': 10.609953738248024}, {'date': '2014-12-01', 'trend': 10.532876719632691, 'seasonal': -0.020002635551053742, 'resid': -0.06093023191390368, 'observed': 10.451943852167734}, {'date': '2015-01-01', 'trend': 10.449153446358014, 'seasonal': 0.057087759887233934, 'resid': 0.016909733029543995, 'observed': 10.523150939274792}, {'date': '2015-02-01', 'trend': 10.365652693622092, 'seasonal': -0.0026761200794264053, 'resid': -0.1323117009765743, 'observed': 10.230664872566091}, {'date': '2015-03-01', 'trend': 10.28402776621621, 'seasonal': -0.00016982951529819455, 'resid': -0.1914473905615518, 'observed': 10.09241054613936}, {'date': '2015-04-01', 'trend': 10.19996188279399, 'seasonal': 3.19587595681934e-05, 'resid': -0.14040178687606952, 'observed': 10.05959205467749}, {'date': '2015-05-01', 'trend': 10.131545323404618, 'seasonal': -0.021465549087508966, 'resid': -0.03438585913579207, 'observed': 10.075693915181317}, {'date': '2015-06-01', 'trend': 10.084529148703163, 'seasonal': 0.0048551821613346344, 'resid': -0.07254407656989471, 'observed': 10.016840254294603}, {'date': '2015-07-01', 'trend': 10.038650783234946, 'seasonal': -0.07584606528025915, 'resid': 0.010947392004321585, 'observed': 9.973752109959008}, {'date': '2015-08-01', 'trend': 10.003718613520608, 'seasonal': -0.004683903074718656, 'resid': -0.06066488076219157, 'observed': 9.938369829683698}, {'date': '2015-09-01', 'trend': 9.98791271441926, 'seasonal': 0.02878733626807229, 'resid': -0.06001572731742389, 'observed': 9.956684323369908}, {'date': '2015-10-01', 'trend': 9.98083261842099, 'seasonal': 0.07292104494645359, 'resid': -0.1057222155422445, 'observed': 9.9480314478252}, {'date': '2015-11-01', 'trend': 9.973055835915497, 'seasonal': -0.03883917943439751, 'resid': 0.07864907670333576, 'observed': 10.012865733184436}, {'date': '2015-12-01', 'trend': 9.974260889533937, 'seasonal': -0.020002635551053742, 'resid': -0.033614589586505546, 'observed': 9.920643664396378}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.057087759887233934, 'resid': nan, 'observed': 9.953370355808923}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.0026761200794264053, 'resid': nan, 'observed': 9.962073382887873}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.00016982951529819455, 'resid': nan, 'observed': 9.981660457385225}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 3.19587595681934e-05, 'resid': nan, 'observed': 10.000419839473144}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.021465549087508966, 'resid': nan, 'observed': 9.948223350253809}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.0048551821613346344, 'resid': nan, 'observed': 10.17323210606468}], 'loan_rate_C': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.015376141878352585, 'resid': nan, 'observed': 10.9475}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.04035308860321538, 'resid': nan, 'observed': 10.829166666666666}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.08058524115091258, 'resid': nan, 'observed': 10.825925925925926}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05290731070035628, 'resid': nan, 'observed': 10.967037037037038}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.02602059045270812, 'resid': nan, 'observed': 10.805}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.03449672569012794, 'resid': nan, 'observed': 11.288055555555555}, {'date': '2008-02-01', 'trend': 11.327619414408064, 'seasonal': 0.022574335467601885, 'resid': 0.17544261376069673, 'observed': 11.525636363636362}, {'date': '2008-03-01', 'trend': 11.40659510885251, 'seasonal': 0.010641559838160935, 'resid': 0.1864414922288681, 'observed': 11.60367816091954}, {'date': '2008-04-01', 'trend': 11.501532490144058, 'seasonal': -0.006495259466358668, 'resid': 0.1185565193223005, 'observed': 11.61359375}, {'date': '2008-05-01', 'trend': 11.613468328225728, 'seasonal': -0.02756787427454523, 'resid': 0.04891436086363189, 'observed': 11.634814814814815}, {'date': '2008-06-01', 'trend': 11.752703752291348, 'seasonal': -0.024120179891552147, 'resid': -0.07946592534097095, 'observed': 11.649117647058825}, {'date': '2008-07-01', 'trend': 11.912461763337886, 'seasonal': -0.03616359408614569, 'resid': -0.09501611796969031, 'observed': 11.78128205128205}, {'date': '2008-08-01', 'trend': 12.059014162327783, 'seasonal': -0.015376141878352585, 'resid': -0.17488802044942972, 'observed': 11.86875}, {'date': '2008-09-01', 'trend': 12.193218649730547, 'seasonal': 0.04035308860321538, 'resid': -0.43023840500042565, 'observed': 11.803333333333336}, {'date': '2008-10-01', 'trend': 12.322309884211464, 'seasonal': 0.08058524115091258, 'resid': -0.27263871510596693, 'observed': 12.13025641025641}, {'date': '2008-11-01', 'trend': 12.448988871382019, 'seasonal': -0.05290731070035628, 'resid': -0.0469148940149938, 'observed': 12.349166666666669}, {'date': '2008-12-01', 'trend': 12.57335810715044, 'seasonal': -0.02602059045270812, 'resid': 0.21718303124747265, 'observed': 12.764520547945205}, {'date': '2009-01-01', 'trend': 12.688007401986994, 'seasonal': 0.03449672569012794, 'resid': 0.4402231450501505, 'observed': 13.162727272727272}, {'date': '2009-02-01', 'trend': 12.803605690204769, 'seasonal': 0.022574335467601885, 'resid': 0.34204219654984847, 'observed': 13.16822222222222}, {'date': '2009-03-01', 'trend': 12.93876053448086, 'seasonal': 0.010641559838160935, 'resid': 0.23259790568097963, 'observed': 13.182}, {'date': '2009-04-01', 'trend': 13.068278952698869, 'seasonal': -0.006495259466358668, 'resid': 0.07167784522903088, 'observed': 13.13346153846154}, {'date': '2009-05-01', 'trend': 13.172533700281956, 'seasonal': -0.02756787427454523, 'resid': 0.010276892439188185, 'observed': 13.1552427184466}, {'date': '2009-06-01', 'trend': 13.251917774111583, 'seasonal': -0.024120179891552147, 'resid': -0.11424619235087095, 'observed': 13.11355140186916}, {'date': '2009-07-01', 'trend': 13.296720793031207, 'seasonal': -0.03616359408614569, 'resid': -0.19212582639604223, 'observed': 13.06843137254902}, {'date': '2009-08-01', 'trend': 13.317664880122448, 'seasonal': -0.015376141878352585, 'resid': 0.053670857715500855, 'observed': 13.355959595959597}, {'date': '2009-09-01', 'trend': 13.331744536272288, 'seasonal': 0.04035308860321538, 'resid': 0.18774237512449657, 'observed': 13.55984}, {'date': '2009-10-01', 'trend': 13.350328926794504, 'seasonal': 0.08058524115091258, 'resid': 0.05127761287650147, 'observed': 13.482191780821918}, {'date': '2009-11-01', 'trend': 13.373796709193448, 'seasonal': -0.05290731070035628, 'resid': 0.17845583960214642, 'observed': 13.499345238095238}, {'date': '2009-12-01', 'trend': 13.411937154210145, 'seasonal': -0.02602059045270812, 'resid': 0.13364318467023942, 'observed': 13.519559748427676}, {'date': '2010-01-01', 'trend': 13.47182703139438, 'seasonal': 0.03449672569012794, 'resid': -0.023363230768720024, 'observed': 13.482960526315788}, {'date': '2010-02-01', 'trend': 13.524905311746926, 'seasonal': 0.022574335467601885, 'resid': -0.1968325883909979, 'observed': 13.35064705882353}, {'date': '2010-03-01', 'trend': 13.554786033403438, 'seasonal': 0.010641559838160935, 'resid': -0.22794068224683492, 'observed': 13.337486910994764}, {'date': '2010-04-01', 'trend': 13.565108944517233, 'seasonal': -0.006495259466358668, 'resid': -0.13461368505087465, 'observed': 13.424}, {'date': '2010-05-01', 'trend': 13.542554509996647, 'seasonal': -0.02756787427454523, 'resid': -0.08705560123934074, 'observed': 13.42793103448276}, {'date': '2010-06-01', 'trend': 13.496382208505665, 'seasonal': -0.024120179891552147, 'resid': 0.28397173761965344, 'observed': 13.756233766233766}, {'date': '2010-07-01', 'trend': 13.45590861730361, 'seasonal': -0.03616359408614569, 'resid': 0.44336103738859645, 'observed': 13.86310606060606}, {'date': '2010-08-01', 'trend': 13.436416752050274, 'seasonal': -0.015376141878352585, 'resid': 0.4141230261917138, 'observed': 13.835163636363635}, {'date': '2010-09-01', 'trend': 13.43336842161868, 'seasonal': 0.04035308860321538, 'resid': 0.3240517691303332, 'observed': 13.797773279352228}, {'date': '2010-10-01', 'trend': 13.424416386656207, 'seasonal': 0.08058524115091258, 'resid': -0.01299325960628371, 'observed': 13.492008368200835}, {'date': '2010-11-01', 'trend': 13.432512703846879, 'seasonal': -0.05290731070035628, 'resid': -0.4313831709243, 'observed': 12.948222222222222}, {'date': '2010-12-01', 'trend': 13.453768023204553, 'seasonal': -0.02602059045270812, 'resid': -0.4651999042347351, 'observed': 12.96254752851711}, {'date': '2011-01-01', 'trend': 13.459765699171086, 'seasonal': 0.03449672569012794, 'resid': -0.42565586748416406, 'observed': 13.06860655737705}, {'date': '2011-02-01', 'trend': 13.460094510618998, 'seasonal': 0.022574335467601885, 'resid': -0.18547258440435596, 'observed': 13.297196261682243}, {'date': '2011-03-01', 'trend': 13.474198015666648, 'seasonal': 0.010641559838160935, 'resid': -0.16706179772703109, 'observed': 13.317777777777778}, {'date': '2011-04-01', 'trend': 13.527026690304142, 'seasonal': -0.006495259466358668, 'resid': -0.2916711367201349, 'observed': 13.228860294117649}, {'date': '2011-05-01', 'trend': 13.626640663310605, 'seasonal': -0.02756787427454523, 'resid': 0.21830956390511572, 'observed': 13.817382352941175}, {'date': '2011-06-01', 'trend': 13.752231546995366, 'seasonal': -0.024120179891552147, 'resid': 0.14879874525573725, 'observed': 13.876910112359552}, {'date': '2011-07-01', 'trend': 13.87357241867144, 'seasonal': -0.03616359408614569, 'resid': 0.048965113091762456, 'observed': 13.886373937677057}, {'date': '2011-08-01', 'trend': 13.98461653130225, 'seasonal': -0.015376141878352585, 'resid': -0.1494531553813426, 'observed': 13.819787234042554}, {'date': '2011-09-01', 'trend': 14.100343020217863, 'seasonal': 0.04035308860321538, 'resid': 0.010937693995820695, 'observed': 14.1516338028169}, {'date': '2011-10-01', 'trend': 14.23031804536025, 'seasonal': 0.08058524115091258, 'resid': 0.09513274952487473, 'observed': 14.406036036036037}, {'date': '2011-11-01', 'trend': 14.339700465210372, 'seasonal': -0.05290731070035628, 'resid': 0.13813675203204037, 'observed': 14.424929906542056}, {'date': '2011-12-01', 'trend': 14.423699396916051, 'seasonal': -0.02602059045270812, 'resid': 0.1023422461682363, 'observed': 14.50002105263158}, {'date': '2012-01-01', 'trend': 14.525801405280475, 'seasonal': 0.03449672569012794, 'resid': -0.11698417748223075, 'observed': 14.443313953488373}, {'date': '2012-02-01', 'trend': 14.653494230019042, 'seasonal': 0.022574335467601885, 'resid': -0.08852099677628465, 'observed': 14.58754756871036}, {'date': '2012-03-01', 'trend': 14.776156584948811, 'seasonal': 0.010641559838160935, 'resid': 0.018064059937437444, 'observed': 14.80486220472441}, {'date': '2012-04-01', 'trend': 14.874257215713214, 'seasonal': -0.006495259466358668, 'resid': -0.006585485658620364, 'observed': 14.861176470588235}, {'date': '2012-05-01', 'trend': 14.960668914357903, 'seasonal': -0.02756787427454523, 'resid': -0.1228567872097933, 'observed': 14.810244252873565}, {'date': '2012-06-01', 'trend': 15.055737285304788, 'seasonal': -0.024120179891552147, 'resid': -0.13159453204980495, 'observed': 14.900022573363431}, {'date': '2012-07-01', 'trend': 15.163703517893621, 'seasonal': -0.03616359408614569, 'resid': 0.18616975361187818, 'observed': 15.313709677419354}, {'date': '2012-08-01', 'trend': 15.269229382631286, 'seasonal': -0.015376141878352585, 'resid': 0.20322604727295804, 'observed': 15.457079288025891}, {'date': '2012-09-01', 'trend': 15.360602120013258, 'seasonal': 0.04035308860321538, 'resid': 0.057283058531540164, 'observed': 15.458238267148014}, {'date': '2012-10-01', 'trend': 15.441987157155793, 'seasonal': 0.08058524115091258, 'resid': -0.06872568825608992, 'observed': 15.453846710050616}, {'date': '2012-11-01', 'trend': 15.522336244532635, 'seasonal': -0.05290731070035628, 'resid': -0.018428933832278373, 'observed': 15.451}, {'date': '2012-12-01', 'trend': 15.600145184438405, 'seasonal': -0.02602059045270812, 'resid': 0.1814672679131936, 'observed': 15.75559186189889}, {'date': '2013-01-01', 'trend': 15.634210920934038, 'seasonal': 0.03449672569012794, 'resid': 0.11022507972889475, 'observed': 15.77893272635306}, {'date': '2013-02-01', 'trend': 15.620066570421177, 'seasonal': 0.022574335467601885, 'resid': 0.14190864366076897, 'observed': 15.784549549549547}, {'date': '2013-03-01', 'trend': 15.605915442823965, 'seasonal': 0.010641559838160935, 'resid': 0.18424891839050375, 'observed': 15.80080592105263}, {'date': '2013-04-01', 'trend': 15.611799860495061, 'seasonal': -0.006495259466358668, 'resid': 0.2131690446521172, 'observed': 15.81847364568082}, {'date': '2013-05-01', 'trend': 15.625247369996853, 'seasonal': -0.02756787427454523, 'resid': 0.18364567910286636, 'observed': 15.781325174825174}, {'date': '2013-06-01', 'trend': 15.604135339634787, 'seasonal': -0.024120179891552147, 'resid': 0.21634104940709334, 'observed': 15.796356209150328}, {'date': '2013-07-01', 'trend': 15.547031858857277, 'seasonal': -0.03616359408614569, 'resid': -0.2759145472435113, 'observed': 15.23495371752762}, {'date': '2013-08-01', 'trend': 15.47551431977326, 'seasonal': -0.015376141878352585, 'resid': -0.263767342285952, 'observed': 15.196370835608956}, {'date': '2013-09-01', 'trend': 15.388764348476775, 'seasonal': 0.04035308860321538, 'resid': -0.04979777984810272, 'observed': 15.379319657231887}, {'date': '2013-10-01', 'trend': 15.294595030065839, 'seasonal': 0.08058524115091258, 'resid': 0.298811072856343, 'observed': 15.673991344073094}, {'date': '2013-11-01', 'trend': 15.181142980694998, 'seasonal': -0.05290731070035628, 'resid': 0.42535992402581335, 'observed': 15.553595594020456}, {'date': '2013-12-01', 'trend': 15.037494206306443, 'seasonal': -0.02602059045270812, 'resid': 0.13483392333511915, 'observed': 15.146307539188854}, {'date': '2014-01-01', 'trend': 14.907041932227768, 'seasonal': 0.03449672569012794, 'resid': 0.07619485248493643, 'observed': 15.017733510402833}, {'date': '2014-02-01', 'trend': 14.801318577692147, 'seasonal': 0.022574335467601885, 'resid': 0.00543491432361955, 'observed': 14.829327827483368}, {'date': '2014-03-01', 'trend': 14.68834291186608, 'seasonal': 0.010641559838160935, 'resid': -0.024956139701048135, 'observed': 14.674028332003193}, {'date': '2014-04-01', 'trend': 14.55425093073973, 'seasonal': -0.006495259466358668, 'resid': 0.13743192159438591, 'observed': 14.685187592867758}, {'date': '2014-05-01', 'trend': 14.399014204506061, 'seasonal': -0.02756787427454523, 'resid': -0.17968428749337514, 'observed': 14.19176204273814}, {'date': '2014-06-01', 'trend': 14.251438055411992, 'seasonal': -0.024120179891552147, 'resid': -0.28896911960845106, 'observed': 13.938348755911989}, {'date': '2014-07-01', 'trend': 14.12541654679463, 'seasonal': -0.03616359408614569, 'resid': -0.12714635983071618, 'observed': 13.962106592877769}, {'date': '2014-08-01', 'trend': 14.00546444533696, 'seasonal': -0.015376141878352585, 'resid': -0.05823085205471999, 'observed': 13.931857451403888}, {'date': '2014-09-01', 'trend': 13.888750175213392, 'seasonal': 0.04035308860321538, 'resid': 0.0033137977947689454, 'observed': 13.932417061611376}, {'date': '2014-10-01', 'trend': 13.776028499486884, 'seasonal': 0.08058524115091258, 'resid': 0.04607265202341461, 'observed': 13.902686392661211}, {'date': '2014-11-01', 'trend': 13.683189812670596, 'seasonal': -0.05290731070035628, 'resid': -0.031063386145972684, 'observed': 13.599219115824267}, {'date': '2014-12-01', 'trend': 13.620293788031292, 'seasonal': -0.02602059045270812, 'resid': -0.03541675845120716, 'observed': 13.558856439127377}, {'date': '2015-01-01', 'trend': 13.567226974222327, 'seasonal': 0.03449672569012794, 'resid': -0.021055296264795055, 'observed': 13.58066840364766}, {'date': '2015-02-01', 'trend': 13.513730927922582, 'seasonal': 0.022574335467601885, 'resid': -0.14876276413578457, 'observed': 13.3875424992544}, {'date': '2015-03-01', 'trend': 13.460183939403779, 'seasonal': 0.010641559838160935, 'resid': -0.15615432197536414, 'observed': 13.314671177266575}, {'date': '2015-04-01', 'trend': 13.408445378415205, 'seasonal': -0.006495259466358668, 'resid': -0.062725588780694, 'observed': 13.339224530168153}, {'date': '2015-05-01', 'trend': 13.3634748819022, 'seasonal': -0.02756787427454523, 'resid': -0.026310385780782358, 'observed': 13.309596621846872}, {'date': '2015-06-01', 'trend': 13.324726093696405, 'seasonal': -0.024120179891552147, 'resid': 0.010403671655124475, 'observed': 13.311009585459978}, {'date': '2015-07-01', 'trend': 13.295059444065489, 'seasonal': -0.03616359408614569, 'resid': 0.05694638193523351, 'observed': 13.315842231914576}, {'date': '2015-08-01', 'trend': 13.28903396947279, 'seasonal': -0.015376141878352585, 'resid': 0.020558873578782184, 'observed': 13.29421670117322}, {'date': '2015-09-01', 'trend': 13.302630073928466, 'seasonal': 0.04035308860321538, 'resid': -0.0580530751409206, 'observed': 13.28493008739076}, {'date': '2015-10-01', 'trend': 13.319559651730444, 'seasonal': 0.08058524115091258, 'resid': -0.09169698972528272, 'observed': 13.308447903156074}, {'date': '2015-11-01', 'trend': 13.335995695872539, 'seasonal': -0.05290731070035628, 'resid': -0.16892269615494468, 'observed': 13.114165689017238}, {'date': '2015-12-01', 'trend': 13.36357309550888, 'seasonal': -0.02602059045270812, 'resid': -0.2236135560608082, 'observed': 13.113938948995363}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.03449672569012794, 'resid': nan, 'observed': 13.313586302637669}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.022574335467601885, 'resid': nan, 'observed': 13.51001321003963}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.010641559838160935, 'resid': nan, 'observed': 13.518506973417573}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.006495259466358668, 'resid': nan, 'observed': 13.54169860126461}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.02756787427454523, 'resid': nan, 'observed': 13.501587610160705}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.024120179891552147, 'resid': nan, 'observed': 13.780876188418324}], 'loan_rate_D': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.04677017162484018, 'resid': nan, 'observed': 12.267}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.050974187840284596, 'resid': nan, 'observed': 12.436666666666667}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.08472596878988434, 'resid': nan, 'observed': 12.737368421052633}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05303848060048236, 'resid': nan, 'observed': 12.609444444444444}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.11016855981499699, 'resid': nan, 'observed': 12.47888888888889}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.09055776109670126, 'resid': nan, 'observed': 13.0082}, {'date': '2008-02-01', 'trend': 12.946022013302766, 'seasonal': 0.014301916284665678, 'resid': 0.27560199633849425, 'observed': 13.235925925925926}, {'date': '2008-03-01', 'trend': 13.039515068858323, 'seasonal': 0.05402694293008556, 'resid': 0.05243359796768923, 'observed': 13.145975609756098}, {'date': '2008-04-01', 'trend': 13.122614914059561, 'seasonal': 0.02189789187409405, 'resid': 0.113748063631563, 'observed': 13.258260869565218}, {'date': '2008-05-01', 'trend': 13.21303629450572, 'seasonal': 0.00613243158641516, 'resid': -0.245168726092135, 'observed': 12.974}, {'date': '2008-06-01', 'trend': 13.345142221285036, 'seasonal': -0.030307932806885735, 'resid': -0.024417621811482743, 'observed': 13.290416666666667}, {'date': '2008-07-01', 'trend': 13.493951933332248, 'seasonal': 0.005243223388796764, 'resid': -0.18599515672104458, 'observed': 13.3132}, {'date': '2008-08-01', 'trend': 13.620062329512132, 'seasonal': 0.04677017162484018, 'resid': -0.20599916780364066, 'observed': 13.460833333333332}, {'date': '2008-09-01', 'trend': 13.739128813596224, 'seasonal': 0.050974187840284596, 'resid': -0.30343633476984244, 'observed': 13.486666666666666}, {'date': '2008-10-01', 'trend': 13.860800265690038, 'seasonal': 0.08472596878988434, 'resid': -0.26376152859756974, 'observed': 13.681764705882353}, {'date': '2008-11-01', 'trend': 13.987063226033499, 'seasonal': -0.05303848060048236, 'resid': -0.09886345511043668, 'observed': 13.83516129032258}, {'date': '2008-12-01', 'trend': 14.103115309366833, 'seasonal': -0.11016855981499699, 'resid': 0.43076753616244823, 'observed': 14.423714285714285}, {'date': '2009-01-01', 'trend': 14.210347506336527, 'seasonal': -0.09055776109670126, 'resid': 0.5150179470678666, 'observed': 14.634807692307692}, {'date': '2009-02-01', 'trend': 14.336290452098496, 'seasonal': 0.014301916284665678, 'resid': 0.28537537355232334, 'observed': 14.635967741935485}, {'date': '2009-03-01', 'trend': 14.475571637398703, 'seasonal': 0.05402694293008556, 'resid': 0.0739308314359172, 'observed': 14.603529411764706}, {'date': '2009-04-01', 'trend': 14.613243023661541, 'seasonal': 0.02189789187409405, 'resid': 0.08568100227258282, 'observed': 14.720821917808218}, {'date': '2009-05-01', 'trend': 14.738657673096528, 'seasonal': 0.00613243158641516, 'resid': -0.2030401046829425, 'observed': 14.54175}, {'date': '2009-06-01', 'trend': 14.82944765925895, 'seasonal': -0.030307932806885735, 'resid': -0.29122305978539564, 'observed': 14.507916666666668}, {'date': '2009-07-01', 'trend': 14.885117054715643, 'seasonal': 0.005243223388796764, 'resid': -0.22108755083171103, 'observed': 14.669272727272729}, {'date': '2009-08-01', 'trend': 14.9323876132182, 'seasonal': 0.04677017162484018, 'resid': 0.14823351950478605, 'observed': 15.127391304347826}, {'date': '2009-09-01', 'trend': 14.981868158243856, 'seasonal': 0.050974187840284596, 'resid': 0.13001479677300157, 'observed': 15.162857142857142}, {'date': '2009-10-01', 'trend': 15.024424507165973, 'seasonal': 0.08472596878988434, 'resid': 0.20053702404414345, 'observed': 15.3096875}, {'date': '2009-11-01', 'trend': 15.073536093923963, 'seasonal': -0.05303848060048236, 'resid': 0.19669246932114728, 'observed': 15.217190082644628}, {'date': '2009-12-01', 'trend': 15.147354517126578, 'seasonal': -0.11016855981499699, 'resid': 0.1834592039787434, 'observed': 15.220645161290324}, {'date': '2010-01-01', 'trend': 15.227469522958767, 'seasonal': -0.09055776109670126, 'resid': 0.03703054583024161, 'observed': 15.173942307692307}, {'date': '2010-02-01', 'trend': 15.281231545226769, 'seasonal': 0.014301916284665678, 'resid': -0.06420693089919113, 'observed': 15.231326530612243}, {'date': '2010-03-01', 'trend': 15.315594738321419, 'seasonal': 0.05402694293008556, 'resid': -0.17391797754780133, 'observed': 15.195703703703703}, {'date': '2010-04-01', 'trend': 15.333071365101393, 'seasonal': 0.02189789187409405, 'resid': -0.2049692569754867, 'observed': 15.15}, {'date': '2010-05-01', 'trend': 15.317257753325439, 'seasonal': 0.00613243158641516, 'resid': -0.032140184911854414, 'observed': 15.29125}, {'date': '2010-06-01', 'trend': 15.284795966826582, 'seasonal': -0.030307932806885735, 'resid': 0.2755707895097156, 'observed': 15.530058823529412}, {'date': '2010-07-01', 'trend': 15.264769830143157, 'seasonal': 0.005243223388796764, 'resid': 0.2998776568505589, 'observed': 15.569890710382513}, {'date': '2010-08-01', 'trend': 15.268900778305587, 'seasonal': 0.04677017162484018, 'resid': 0.20139090573967602, 'observed': 15.517061855670104}, {'date': '2010-09-01', 'trend': 15.281649843601524, 'seasonal': 0.050974187840284596, 'resid': 0.2652791943646423, 'observed': 15.597903225806451}, {'date': '2010-10-01', 'trend': 15.293769553052561, 'seasonal': 0.08472596878988434, 'resid': -0.08441506207232927, 'observed': 15.294080459770116}, {'date': '2010-11-01', 'trend': 15.349641612544133, 'seasonal': -0.05303848060048236, 'resid': -0.44333269169207823, 'observed': 14.853270440251572}, {'date': '2010-12-01', 'trend': 15.440148556758828, 'seasonal': -0.11016855981499699, 'resid': -0.5244980692329876, 'observed': 14.805481927710844}, {'date': '2011-01-01', 'trend': 15.519144828167612, 'seasonal': -0.09055776109670126, 'resid': -0.3201088062013446, 'observed': 15.108478260869566}, {'date': '2011-02-01', 'trend': 15.597932287226568, 'seasonal': 0.014301916284665678, 'resid': -0.21630087017790176, 'observed': 15.395933333333332}, {'date': '2011-03-01', 'trend': 15.683697841260013, 'seasonal': 0.05402694293008556, 'resid': -0.4006503161049927, 'observed': 15.337074468085106}, {'date': '2011-04-01', 'trend': 15.810629568251665, 'seasonal': 0.02189789187409405, 'resid': -0.5330251976823192, 'observed': 15.29950226244344}, {'date': '2011-05-01', 'trend': 15.991605993913954, 'seasonal': 0.00613243158641516, 'resid': 0.4849387398539594, 'observed': 16.48267716535433}, {'date': '2011-06-01', 'trend': 16.191505172815155, 'seasonal': -0.030307932806885735, 'resid': 0.3496010793194603, 'observed': 16.51079831932773}, {'date': '2011-07-01', 'trend': 16.374712577701146, 'seasonal': 0.005243223388796764, 'resid': 0.10510592730511684, 'observed': 16.48506172839506}, {'date': '2011-08-01', 'trend': 16.541513156944013, 'seasonal': 0.04677017162484018, 'resid': -0.09549347349638734, 'observed': 16.492789855072466}, {'date': '2011-09-01', 'trend': 16.74736510541923, 'seasonal': 0.050974187840284596, 'resid': -0.11779077005276303, 'observed': 16.680548523206753}, {'date': '2011-10-01', 'trend': 16.988299020223373, 'seasonal': 0.08472596878988434, 'resid': 0.18477162115623194, 'observed': 17.25779661016949}, {'date': '2011-11-01', 'trend': 17.1806862665515, 'seasonal': -0.05303848060048236, 'resid': 0.10534071979610485, 'observed': 17.232988505747123}, {'date': '2011-12-01', 'trend': 17.32296555978484, 'seasonal': -0.11016855981499699, 'resid': 0.01054715587431257, 'observed': 17.223344155844156}, {'date': '2012-01-01', 'trend': 17.47914706744071, 'seasonal': -0.09055776109670126, 'resid': -0.30099555634400893, 'observed': 17.08759375}, {'date': '2012-02-01', 'trend': 17.65589571162486, 'seasonal': 0.014301916284665678, 'resid': -0.2501658818777842, 'observed': 17.420031746031743}, {'date': '2012-03-01', 'trend': 17.82761029986977, 'seasonal': 0.05402694293008556, 'resid': 0.37178557599208595, 'observed': 18.253422818791943}, {'date': '2012-04-01', 'trend': 17.968315594668944, 'seasonal': 0.02189789187409405, 'resid': 0.1753543804929724, 'observed': 18.16556786703601}, {'date': '2012-05-01', 'trend': 18.08628632218981, 'seasonal': 0.00613243158641516, 'resid': 0.14148671886059142, 'observed': 18.233905472636817}, {'date': '2012-06-01', 'trend': 18.202423731325155, 'seasonal': -0.030307932806885735, 'resid': 0.0021572511271205352, 'observed': 18.17427304964539}, {'date': '2012-07-01', 'trend': 18.320598459912215, 'seasonal': 0.005243223388796764, 'resid': 0.24410149851717358, 'observed': 18.569943181818186}, {'date': '2012-08-01', 'trend': 18.42990775735452, 'seasonal': 0.04677017162484018, 'resid': 0.17319793308960643, 'observed': 18.649875862068967}, {'date': '2012-09-01', 'trend': 18.494241138814044, 'seasonal': 0.050974187840284596, 'resid': 0.0993973074338729, 'observed': 18.6446126340882}, {'date': '2012-10-01', 'trend': 18.528943565510193, 'seasonal': 0.08472596878988434, 'resid': 0.056990040168004935, 'observed': 18.670659574468083}, {'date': '2012-11-01', 'trend': 18.56571075355779, 'seasonal': -0.05303848060048236, 'resid': 0.13875072899200927, 'observed': 18.65142300194932}, {'date': '2012-12-01', 'trend': 18.60424801768697, 'seasonal': -0.11016855981499699, 'resid': 0.09812802101825394, 'observed': 18.592207478890227}, {'date': '2013-01-01', 'trend': 18.629536628909282, 'seasonal': -0.09055776109670126, 'resid': 0.01594504523090072, 'observed': 18.55492391304348}, {'date': '2013-02-01', 'trend': 18.63721728737489, 'seasonal': 0.014301916284665678, 'resid': -0.0753944820559945, 'observed': 18.576124721603563}, {'date': '2013-03-01', 'trend': 18.645549887500888, 'seasonal': 0.05402694293008556, 'resid': -0.05824583218228971, 'observed': 18.641330998248684}, {'date': '2013-04-01', 'trend': 18.660288795601627, 'seasonal': 0.02189789187409405, 'resid': -0.07166875918886748, 'observed': 18.610517928286853}, {'date': '2013-05-01', 'trend': 18.679619934627297, 'seasonal': 0.00613243158641516, 'resid': -0.01438444168541117, 'observed': 18.6713679245283}, {'date': '2013-06-01', 'trend': 18.68159073892394, 'seasonal': -0.030307932806885735, 'resid': 0.0104221307371343, 'observed': 18.66170493685419}, {'date': '2013-07-01', 'trend': 18.656660551740597, 'seasonal': 0.005243223388796764, 'resid': 0.027534188815457356, 'observed': 18.68943796394485}, {'date': '2013-08-01', 'trend': 18.616553704147748, 'seasonal': 0.04677017162484018, 'resid': 0.051393007344301106, 'observed': 18.71471688311689}, {'date': '2013-09-01', 'trend': 18.563483001311955, 'seasonal': 0.050974187840284596, 'resid': 0.1652968269120169, 'observed': 18.779754016064256}, {'date': '2013-10-01', 'trend': 18.507362836112627, 'seasonal': 0.08472596878988434, 'resid': 0.29716318200725855, 'observed': 18.88925198690977}, {'date': '2013-11-01', 'trend': 18.41784152773823, 'seasonal': -0.05303848060048236, 'resid': 0.5319748789859714, 'observed': 18.89677792612372}, {'date': '2013-12-01', 'trend': 18.275788119034477, 'seasonal': -0.11016855981499699, 'resid': 0.2285322986157389, 'observed': 18.39415185783522}, {'date': '2014-01-01', 'trend': 18.117404473515993, 'seasonal': -0.09055776109670126, 'resid': 0.12780832927896363, 'observed': 18.154655041698256}, {'date': '2014-02-01', 'trend': 17.95915940844471, 'seasonal': 0.014301916284665678, 'resid': 0.04036792599108799, 'observed': 18.013829250720462}, {'date': '2014-03-01', 'trend': 17.798431684504315, 'seasonal': 0.05402694293008556, 'resid': 0.07747097363834579, 'observed': 17.929929601072747}, {'date': '2014-04-01', 'trend': 17.627098506088636, 'seasonal': 0.02189789187409405, 'resid': 0.32603896271619315, 'observed': 17.975035360678923}, {'date': '2014-05-01', 'trend': 17.438528742234112, 'seasonal': 0.00613243158641516, 'resid': -0.28632208266985126, 'observed': 17.158339091150676}, {'date': '2014-06-01', 'trend': 17.258728659263024, 'seasonal': -0.030307932806885735, 'resid': -0.46296876511446683, 'observed': 16.76545196134167}, {'date': '2014-07-01', 'trend': 17.108719410371155, 'seasonal': 0.005243223388796764, 'resid': -0.3294791867461425, 'observed': 16.78448344701381}, {'date': '2014-08-01', 'trend': 16.98323278819809, 'seasonal': 0.04677017162484018, 'resid': -0.2082131214857493, 'observed': 16.821789838337182}, {'date': '2014-09-01', 'trend': 16.877995723697357, 'seasonal': 0.050974187840284596, 'resid': -0.1137542252631317, 'observed': 16.81521568627451}, {'date': '2014-10-01', 'trend': 16.778774884200576, 'seasonal': 0.08472596878988434, 'resid': -0.12170681826720817, 'observed': 16.741794034723252}, {'date': '2014-11-01', 'trend': 16.71401294490989, 'seasonal': -0.05303848060048236, 'resid': -0.14241291850788204, 'observed': 16.518561545801525}, {'date': '2014-12-01', 'trend': 16.69983988983558, 'seasonal': -0.11016855981499699, 'resid': -0.13250508316919682, 'observed': 16.457166246851386}, {'date': '2015-01-01', 'trend': 16.699303658669496, 'seasonal': -0.09055776109670126, 'resid': -0.11732721829564548, 'observed': 16.49141867927715}, {'date': '2015-02-01', 'trend': 16.69508156806944, 'seasonal': 0.014301916284665678, 'resid': -0.04399680336592135, 'observed': 16.665386680988185}, {'date': '2015-03-01', 'trend': 16.69018220555109, 'seasonal': 0.05402694293008556, 'resid': 0.008473474306158219, 'observed': 16.752682622787333}, {'date': '2015-04-01', 'trend': 16.688963166928986, 'seasonal': 0.02189789187409405, 'resid': 0.06012113223847462, 'observed': 16.770982191041554}, {'date': '2015-05-01', 'trend': 16.69606287739243, 'seasonal': 0.00613243158641516, 'resid': 0.10591040883275618, 'observed': 16.8081057178116}, {'date': '2015-06-01', 'trend': 16.713701422181142, 'seasonal': -0.030307932806885735, 'resid': 0.09213852352302712, 'observed': 16.775532012897283}, {'date': '2015-07-01', 'trend': 16.74506767376765, 'seasonal': 0.005243223388796764, 'resid': 0.011222950315704111, 'observed': 16.76153384747215}, {'date': '2015-08-01', 'trend': 16.809868367240238, 'seasonal': 0.04677017162484018, 'resid': -0.1132292753874796, 'observed': 16.7434092634776}, {'date': '2015-09-01', 'trend': 16.898763840746042, 'seasonal': 0.050974187840284596, 'resid': -0.17372646789268384, 'observed': 16.776011560693643}, {'date': '2015-10-01', 'trend': 16.985313395517174, 'seasonal': 0.08472596878988434, 'resid': -0.3182981309334191, 'observed': 16.75174123337364}, {'date': '2015-11-01', 'trend': 17.06891528315394, 'seasonal': -0.05303848060048236, 'resid': -0.33686940427972323, 'observed': 16.679007398273736}, {'date': '2015-12-01', 'trend': 17.17336476486541, 'seasonal': -0.11016855981499699, 'resid': -0.3431507357422, 'observed': 16.720045469308214}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.09055776109670126, 'resid': nan, 'observed': 16.981329494896624}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.014301916284665678, 'resid': nan, 'observed': 17.7306925087108}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.05402694293008556, 'resid': nan, 'observed': 17.82086815920398}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.02189789187409405, 'resid': nan, 'observed': 17.77998596913209}, {'date': '2016-05-01', 'trend': nan, 'seasonal': 0.00613243158641516, 'resid': nan, 'observed': 17.805547243003602}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.030307932806885735, 'resid': nan, 'observed': 18.284878048780485}], 'FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.0908829365079365, 'resid': nan, 'observed': 5.02}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.0898933531746032, 'resid': nan, 'observed': 4.94}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.0028100198412698354, 'resid': nan, 'observed': 4.76}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05135664682539679, 'resid': nan, 'observed': 4.49}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.045783730158730156, 'resid': nan, 'observed': 4.24}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.043670634920634936, 'resid': nan, 'observed': 3.94}, {'date': '2008-02-01', 'trend': 3.311666666666667, 'seasonal': -0.04234623015873019, 'resid': -0.2893204365079368, 'observed': 2.98}, {'date': '2008-03-01', 'trend': 3.0554166666666664, 'seasonal': -0.03817956349206348, 'resid': -0.4072371031746031, 'observed': 2.61}, {'date': '2008-04-01', 'trend': 2.7670833333333333, 'seasonal': -0.029012896825396865, 'resid': -0.45807043650793666, 'observed': 2.28}, {'date': '2008-05-01', 'trend': 2.438333333333333, 'seasonal': -0.019585813492063467, 'resid': -0.4387475198412696, 'observed': 1.98}, {'date': '2008-06-01', 'trend': 2.0975, 'seasonal': 0.027757936507936475, 'resid': -0.12525793650793662, 'observed': 2.0}, {'date': '2008-07-01', 'trend': 1.7695833333333333, 'seasonal': 0.05859126984126982, 'resid': 0.18182539682539667, 'observed': 2.01}, {'date': '2008-08-01', 'trend': 1.4966666666666666, 'seasonal': 0.0908829365079365, 'resid': 0.4124503968253969, 'observed': 2.0}, {'date': '2008-09-01', 'trend': 1.2804166666666665, 'seasonal': 0.0898933531746032, 'resid': 0.4396899801587303, 'observed': 1.81}, {'date': '2008-10-01', 'trend': 1.0904166666666666, 'seasonal': 0.0028100198412698354, 'resid': -0.12322668650793644, 'observed': 0.97}, {'date': '2008-11-01', 'trend': 0.9266666666666664, 'seasonal': -0.05135664682539679, 'resid': -0.4853100198412696, 'observed': 0.39}, {'date': '2008-12-01', 'trend': 0.7770833333333333, 'seasonal': -0.045783730158730156, 'resid': -0.5712996031746032, 'observed': 0.16}, {'date': '2009-01-01', 'trend': 0.6254166666666667, 'seasonal': -0.043670634920634936, 'resid': -0.43174603174603177, 'observed': 0.15}, {'date': '2009-02-01', 'trend': 0.4716666666666666, 'seasonal': -0.04234623015873019, 'resid': -0.2093204365079364, 'observed': 0.22}, {'date': '2009-03-01', 'trend': 0.3258333333333333, 'seasonal': -0.03817956349206348, 'resid': -0.10765376984126984, 'observed': 0.18}, {'date': '2009-04-01', 'trend': 0.22125, 'seasonal': -0.029012896825396865, 'resid': -0.04223710317460314, 'observed': 0.15}, {'date': '2009-05-01', 'trend': 0.1745833333333333, 'seasonal': -0.019585813492063467, 'resid': 0.025002480158730148, 'observed': 0.18}, {'date': '2009-06-01', 'trend': 0.16166666666666665, 'seasonal': 0.027757936507936475, 'resid': 0.020575396825396865, 'observed': 0.21}, {'date': '2009-07-01', 'trend': 0.1583333333333333, 'seasonal': 0.05859126984126982, 'resid': -0.056924603174603114, 'observed': 0.16}, {'date': '2009-08-01', 'trend': 0.15291666666666665, 'seasonal': 0.0908829365079365, 'resid': -0.08379960317460314, 'observed': 0.16}, {'date': '2009-09-01', 'trend': 0.14833333333333332, 'seasonal': 0.0898933531746032, 'resid': -0.08822668650793652, 'observed': 0.15}, {'date': '2009-10-01', 'trend': 0.14958333333333332, 'seasonal': 0.0028100198412698354, 'resid': -0.03239335317460316, 'observed': 0.12}, {'date': '2009-11-01', 'trend': 0.1525, 'seasonal': -0.05135664682539679, 'resid': 0.01885664682539679, 'observed': 0.12}, {'date': '2009-12-01', 'trend': 0.15208333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.01370039682539683, 'observed': 0.12}, {'date': '2010-01-01', 'trend': 0.15166666666666667, 'seasonal': -0.043670634920634936, 'resid': 0.002003968253968265, 'observed': 0.11}, {'date': '2010-02-01', 'trend': 0.15374999999999997, 'seasonal': -0.04234623015873019, 'resid': 0.01859623015873022, 'observed': 0.13}, {'date': '2010-03-01', 'trend': 0.15666666666666665, 'seasonal': -0.03817956349206348, 'resid': 0.041512896825396835, 'observed': 0.16}, {'date': '2010-04-01', 'trend': 0.16124999999999998, 'seasonal': -0.029012896825396865, 'resid': 0.0677628968253969, 'observed': 0.2}, {'date': '2010-05-01', 'trend': 0.1670833333333333, 'seasonal': -0.019585813492063467, 'resid': 0.05250248015873017, 'observed': 0.2}, {'date': '2010-06-01', 'trend': 0.17250000000000001, 'seasonal': 0.027757936507936475, 'resid': -0.020257936507936496, 'observed': 0.18}, {'date': '2010-07-01', 'trend': 0.1775, 'seasonal': 0.05859126984126982, 'resid': -0.05609126984126982, 'observed': 0.18}, {'date': '2010-08-01', 'trend': 0.18124999999999997, 'seasonal': 0.0908829365079365, 'resid': -0.08213293650793646, 'observed': 0.19}, {'date': '2010-09-01', 'trend': 0.18166666666666667, 'seasonal': 0.0898933531746032, 'resid': -0.08156001984126987, 'observed': 0.19}, {'date': '2010-10-01', 'trend': 0.17666666666666667, 'seasonal': 0.0028100198412698354, 'resid': 0.0105233134920635, 'observed': 0.19}, {'date': '2010-11-01', 'trend': 0.16791666666666666, 'seasonal': -0.05135664682539679, 'resid': 0.07343998015873013, 'observed': 0.19}, {'date': '2010-12-01', 'trend': 0.15958333333333333, 'seasonal': -0.045783730158730156, 'resid': 0.06620039682539683, 'observed': 0.18}, {'date': '2011-01-01', 'trend': 0.15125000000000002, 'seasonal': -0.043670634920634936, 'resid': 0.062420634920634925, 'observed': 0.17}, {'date': '2011-02-01', 'trend': 0.14291666666666666, 'seasonal': -0.04234623015873019, 'resid': 0.05942956349206353, 'observed': 0.16}, {'date': '2011-03-01', 'trend': 0.1345833333333333, 'seasonal': -0.03817956349206348, 'resid': 0.04359623015873019, 'observed': 0.14}, {'date': '2011-04-01', 'trend': 0.12499999999999999, 'seasonal': -0.029012896825396865, 'resid': 0.004012896825396885, 'observed': 0.1}, {'date': '2011-05-01', 'trend': 0.11541666666666667, 'seasonal': -0.019585813492063467, 'resid': -0.005830853174603204, 'observed': 0.09}, {'date': '2011-06-01', 'trend': 0.10625, 'seasonal': 0.027757936507936475, 'resid': -0.044007936507936475, 'observed': 0.09}, {'date': '2011-07-01', 'trend': 0.09791666666666668, 'seasonal': 0.05859126984126982, 'resid': -0.08650793650793649, 'observed': 0.07}, {'date': '2011-08-01', 'trend': 0.09166666666666666, 'seasonal': 0.0908829365079365, 'resid': -0.08254960317460315, 'observed': 0.1}, {'date': '2011-09-01', 'trend': 0.08875, 'seasonal': 0.0898933531746032, 'resid': -0.0986433531746032, 'observed': 0.08}, {'date': '2011-10-01', 'trend': 0.09, 'seasonal': 0.0028100198412698354, 'resid': -0.022810019841269825, 'observed': 0.07}, {'date': '2011-11-01', 'trend': 0.09458333333333334, 'seasonal': -0.05135664682539679, 'resid': 0.036773313492063454, 'observed': 0.08}, {'date': '2011-12-01', 'trend': 0.10041666666666667, 'seasonal': -0.045783730158730156, 'resid': 0.015367063492063494, 'observed': 0.07}, {'date': '2012-01-01', 'trend': 0.10708333333333334, 'seasonal': -0.043670634920634936, 'resid': 0.016587301587301602, 'observed': 0.08}, {'date': '2012-02-01', 'trend': 0.11208333333333333, 'seasonal': -0.04234623015873019, 'resid': 0.030262896825396866, 'observed': 0.1}, {'date': '2012-03-01', 'trend': 0.11583333333333333, 'seasonal': -0.03817956349206348, 'resid': 0.052346230158730155, 'observed': 0.13}, {'date': '2012-04-01', 'trend': 0.12208333333333334, 'seasonal': -0.029012896825396865, 'resid': 0.04692956349206354, 'observed': 0.14}, {'date': '2012-05-01', 'trend': 0.12916666666666665, 'seasonal': -0.019585813492063467, 'resid': 0.05041914682539682, 'observed': 0.16}, {'date': '2012-06-01', 'trend': 0.13624999999999998, 'seasonal': 0.027757936507936475, 'resid': -0.0040079365079364535, 'observed': 0.16}, {'date': '2012-07-01', 'trend': 0.1425, 'seasonal': 0.05859126984126982, 'resid': -0.041091269841269804, 'observed': 0.16}, {'date': '2012-08-01', 'trend': 0.14708333333333334, 'seasonal': 0.0908829365079365, 'resid': -0.10796626984126984, 'observed': 0.13}, {'date': '2012-09-01', 'trend': 0.14958333333333332, 'seasonal': 0.0898933531746032, 'resid': -0.09947668650793651, 'observed': 0.14}, {'date': '2012-10-01', 'trend': 0.15041666666666667, 'seasonal': 0.0028100198412698354, 'resid': 0.006773313492063497, 'observed': 0.16}, {'date': '2012-11-01', 'trend': 0.14875, 'seasonal': -0.05135664682539679, 'resid': 0.0626066468253968, 'observed': 0.16}, {'date': '2012-12-01', 'trend': 0.14375000000000002, 'seasonal': -0.045783730158730156, 'resid': 0.06203373015873014, 'observed': 0.16}, {'date': '2013-01-01', 'trend': 0.13791666666666666, 'seasonal': -0.043670634920634936, 'resid': 0.04575396825396829, 'observed': 0.14}, {'date': '2013-02-01', 'trend': 0.13291666666666666, 'seasonal': -0.04234623015873019, 'resid': 0.05942956349206353, 'observed': 0.15}, {'date': '2013-03-01', 'trend': 0.12833333333333333, 'seasonal': -0.03817956349206348, 'resid': 0.049846230158730166, 'observed': 0.14}, {'date': '2013-04-01', 'trend': 0.12291666666666667, 'seasonal': -0.029012896825396865, 'resid': 0.056096230158730186, 'observed': 0.15}, {'date': '2013-05-01', 'trend': 0.11666666666666667, 'seasonal': -0.019585813492063467, 'resid': 0.012919146825396799, 'observed': 0.11}, {'date': '2013-06-01', 'trend': 0.11041666666666666, 'seasonal': 0.027757936507936475, 'resid': -0.04817460317460314, 'observed': 0.09}, {'date': '2013-07-01', 'trend': 0.10458333333333333, 'seasonal': 0.05859126984126982, 'resid': -0.07317460317460316, 'observed': 0.09}, {'date': '2013-08-01', 'trend': 0.09833333333333334, 'seasonal': 0.0908829365079365, 'resid': -0.10921626984126984, 'observed': 0.08}, {'date': '2013-09-01', 'trend': 0.0925, 'seasonal': 0.0898933531746032, 'resid': -0.1023933531746032, 'observed': 0.08}, {'date': '2013-10-01', 'trend': 0.08750000000000001, 'seasonal': 0.0028100198412698354, 'resid': -0.0003100198412698471, 'observed': 0.09}, {'date': '2013-11-01', 'trend': 0.08416666666666667, 'seasonal': -0.05135664682539679, 'resid': 0.047189980158730126, 'observed': 0.08}, {'date': '2013-12-01', 'trend': 0.08374999999999999, 'seasonal': -0.045783730158730156, 'resid': 0.05203373015873016, 'observed': 0.09}, {'date': '2014-01-01', 'trend': 0.08416666666666667, 'seasonal': -0.043670634920634936, 'resid': 0.029503968253968275, 'observed': 0.07}, {'date': '2014-02-01', 'trend': 0.08458333333333333, 'seasonal': -0.04234623015873019, 'resid': 0.027762896825396864, 'observed': 0.07}, {'date': '2014-03-01', 'trend': 0.08541666666666667, 'seasonal': -0.03817956349206348, 'resid': 0.03276289682539681, 'observed': 0.08}, {'date': '2014-04-01', 'trend': 0.08583333333333333, 'seasonal': -0.029012896825396865, 'resid': 0.03317956349206353, 'observed': 0.09}, {'date': '2014-05-01', 'trend': 0.08625, 'seasonal': -0.019585813492063467, 'resid': 0.02333581349206347, 'observed': 0.09}, {'date': '2014-06-01', 'trend': 0.08791666666666667, 'seasonal': 0.027757936507936475, 'resid': -0.01567460317460314, 'observed': 0.1}, {'date': '2014-07-01', 'trend': 0.09083333333333332, 'seasonal': 0.05859126984126982, 'resid': -0.059424603174603144, 'observed': 0.09}, {'date': '2014-08-01', 'trend': 0.09416666666666666, 'seasonal': 0.0908829365079365, 'resid': -0.09504960317460316, 'observed': 0.09}, {'date': '2014-09-01', 'trend': 0.09708333333333333, 'seasonal': 0.0898933531746032, 'resid': -0.09697668650793653, 'observed': 0.09}, {'date': '2014-10-01', 'trend': 0.09958333333333334, 'seasonal': 0.0028100198412698354, 'resid': -0.012393353174603182, 'observed': 0.09}, {'date': '2014-11-01', 'trend': 0.10208333333333333, 'seasonal': -0.05135664682539679, 'resid': 0.03927331349206346, 'observed': 0.09}, {'date': '2014-12-01', 'trend': 0.10458333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.06120039682539683, 'observed': 0.12}, {'date': '2015-01-01', 'trend': 0.10749999999999998, 'seasonal': -0.043670634920634936, 'resid': 0.04617063492063495, 'observed': 0.11}, {'date': '2015-02-01', 'trend': 0.11124999999999999, 'seasonal': -0.04234623015873019, 'resid': 0.0410962301587302, 'observed': 0.11}, {'date': '2015-03-01', 'trend': 0.11541666666666667, 'seasonal': -0.03817956349206348, 'resid': 0.03276289682539681, 'observed': 0.11}, {'date': '2015-04-01', 'trend': 0.11875, 'seasonal': -0.029012896825396865, 'resid': 0.030262896825396866, 'observed': 0.12}, {'date': '2015-05-01', 'trend': 0.12125, 'seasonal': -0.019585813492063467, 'resid': 0.018335813492063466, 'observed': 0.12}, {'date': '2015-06-01', 'trend': 0.1275, 'seasonal': 0.027757936507936475, 'resid': -0.025257936507936472, 'observed': 0.13}, {'date': '2015-07-01', 'trend': 0.1420833333333333, 'seasonal': 0.05859126984126982, 'resid': -0.07067460317460313, 'observed': 0.13}, {'date': '2015-08-01', 'trend': 0.16291666666666668, 'seasonal': 0.0908829365079365, 'resid': -0.11379960317460316, 'observed': 0.14}, {'date': '2015-09-01', 'trend': 0.1845833333333333, 'seasonal': 0.0898933531746032, 'resid': -0.13447668650793648, 'observed': 0.14}, {'date': '2015-10-01', 'trend': 0.20541666666666664, 'seasonal': 0.0028100198412698354, 'resid': -0.08822668650793647, 'observed': 0.12}, {'date': '2015-11-01', 'trend': 0.22624999999999998, 'seasonal': -0.05135664682539679, 'resid': -0.05489335317460319, 'observed': 0.12}, {'date': '2015-12-01', 'trend': 0.24708333333333332, 'seasonal': -0.045783730158730156, 'resid': 0.038700396825396825, 'observed': 0.24}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.043670634920634936, 'resid': nan, 'observed': 0.34}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.04234623015873019, 'resid': nan, 'observed': 0.38}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.03817956349206348, 'resid': nan, 'observed': 0.36}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.029012896825396865, 'resid': nan, 'observed': 0.37}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.019585813492063467, 'resid': nan, 'observed': 0.37}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.027757936507936475, 'resid': nan, 'observed': 0.38}], 'diff1_loan_rate_A': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -0.052743362293617736, 'resid': nan, 'observed': 0.0600000000000013}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.010310036424414365, 'resid': nan, 'observed': 0.0747619047619041}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.07866104363465767, 'resid': nan, 'observed': -0.0114285714285715}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.06447017941083495, 'resid': nan, 'observed': -0.0509090909090916}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.13259497070685478, 'resid': nan, 'observed': -0.0832575757575755}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.031714528696978224, 'resid': nan, 'observed': 0.2655000000000003}, {'date': '2008-02-01', 'trend': 0.04308756038647344, 'seasonal': -0.04724202459970091, 'resid': 0.17315446421322706, 'observed': 0.1689999999999996}, {'date': '2008-03-01', 'trend': 0.031621031746031744, 'seasonal': -0.010757930825500861, 'resid': -0.02491071996814928, 'observed': -0.0040476190476184}, {'date': '2008-04-01', 'trend': 0.04255952380952382, 'seasonal': 0.02255283843870718, 'resid': -0.0993147432006122, 'observed': -0.0342023809523812}, {'date': '2008-05-01', 'trend': 0.06510642135642139, 'seasonal': -0.030587662937704367, 'resid': 0.024826479676521074, 'observed': 0.0593452380952381}, {'date': '2008-06-01', 'trend': 0.09512626262626267, 'seasonal': -0.017830922488024405, 'resid': -0.023223911566808864, 'observed': 0.0540714285714294}, {'date': '2008-07-01', 'trend': 0.10408606557377058, 'seasonal': 0.03184779489655618, 'resid': -0.12099907786163225, 'observed': 0.0149347826086945}, {'date': '2008-08-01', 'trend': 0.07345892882163388, 'seasonal': -0.052743362293617736, 'resid': 0.04584965086328945, 'observed': 0.0665652173913056}, {'date': '2008-09-01', 'trend': 0.06096281033781046, 'seasonal': -0.010310036424414365, 'resid': -0.2576527739133968, 'observed': -0.2070000000000007}, {'date': '2008-10-01', 'trend': 0.053667120029015226, 'seasonal': 0.07866104363465767, 'resid': 0.40052897919346997, 'observed': 0.5328571428571429}, {'date': '2008-11-01', 'trend': 0.057776055562292906, 'seasonal': -0.06447017941083495, 'resid': -0.047375140220722256, 'observed': -0.0540692640692643}, {'date': '2008-12-01', 'trend': 0.06338933770596704, 'seasonal': 0.13259497070685478, 'resid': 0.44439447946596633, 'observed': 0.6403787878787881}, {'date': '2009-01-01', 'trend': 0.06135697966358909, 'seasonal': -0.031714528696978224, 'resid': -0.2727435438627845, 'observed': -0.2431010928961736}, {'date': '2009-02-01', 'trend': 0.038774959850979306, 'seasonal': -0.04724202459970091, 'resid': -0.04898312440638529, 'observed': -0.0574501891551069}, {'date': '2009-03-01', 'trend': 0.022875529287226484, 'seasonal': -0.010757930825500861, 'resid': -0.08962187196600012, 'observed': -0.0775042735042745}, {'date': '2009-04-01', 'trend': 0.007781467220443654, 'seasonal': 0.02255283843870718, 'resid': -0.16617659956596104, 'observed': -0.1358422939068102}, {'date': '2009-05-01', 'trend': -0.015483169794193343, 'seasonal': -0.030587662937704367, 'resid': 0.3056704365802287, 'observed': 0.259599603848331}, {'date': '2009-06-01', 'trend': -0.04104235714622401, 'seasonal': -0.017830922488024405, 'resid': 0.04740911390076451, 'observed': -0.0114641657334839}, {'date': '2009-07-01', 'trend': -0.06674015259826861, 'seasonal': 0.03184779489655618, 'resid': 0.06658614159825002, 'observed': 0.0316937838965376}, {'date': '2009-08-01', 'trend': -0.09139295659941168, 'seasonal': -0.052743362293617736, 'resid': -0.3480259405061433, 'observed': -0.4921622593991728}, {'date': '2009-09-01', 'trend': -0.11437149262309977, 'seasonal': -0.010310036424414365, 'resid': 0.09482267230792384, 'observed': -0.0298588567395903}, {'date': '2009-10-01', 'trend': -0.1062783291583755, 'seasonal': 0.07866104363465767, 'resid': 0.021075795517662227, 'observed': -0.0065414900060556}, {'date': '2009-11-01', 'trend': -0.11133887121885848, 'seasonal': -0.06447017941083495, 'resid': 0.10278713107233983, 'observed': -0.0730219195573536}, {'date': '2009-12-01', 'trend': -0.12049983600396537, 'seasonal': 0.13259497070685478, 'resid': 0.0338158122152519, 'observed': 0.0459109469181413}, {'date': '2010-01-01', 'trend': -0.12134807296956264, 'seasonal': -0.031714528696978224, 'resid': -0.11231774111805665, 'observed': -0.2653803427845975}, {'date': '2010-02-01', 'trend': -0.10525834023147676, 'seasonal': -0.04724202459970091, 'resid': -0.47433787046293924, 'observed': -0.6268382352941169}, {'date': '2010-03-01', 'trend': -0.0819108707400321, 'seasonal': -0.010757930825500861, 'resid': 0.033067709631754365, 'observed': -0.0596010919337786}, {'date': '2010-04-01', 'trend': -0.10373052762973133, 'seasonal': 0.02255283843870718, 'resid': 0.12166813686710085, 'observed': 0.0404904476760767}, {'date': '2010-05-01', 'trend': -0.1475603464738341, 'seasonal': -0.030587662937704367, 'resid': 0.13996186222539117, 'observed': -0.0381861471861473}, {'date': '2010-06-01', 'trend': -0.17029682948139066, 'seasonal': -0.017830922488024405, 'resid': 0.25458618242784387, 'observed': 0.0664584304584288}, {'date': '2010-07-01', 'trend': -0.15216218705932197, 'seasonal': 0.03184779489655618, 'resid': 0.053727892693055786, 'observed': -0.06658649946971}, {'date': '2010-08-01', 'trend': -0.09455752860490758, 'seasonal': -0.052743362293617736, 'resid': 0.13957250057966172, 'observed': -0.0077283903188636}, {'date': '2010-09-01', 'trend': -0.05546174667713392, 'seasonal': -0.010310036424414365, 'resid': 0.11181832507632047, 'observed': 0.0460465419747722}, {'date': '2010-10-01', 'trend': -0.0530929247754636, 'seasonal': 0.07866104363465767, 'resid': -0.6316867729323936, 'observed': -0.6061186540731995}, {'date': '2010-11-01', 'trend': -0.039401117266809255, 'seasonal': -0.06447017941083495, 'resid': -0.42148911107103154, 'observed': -0.5253604077486758}, {'date': '2010-12-01', 'trend': -0.03342727946617814, 'seasonal': 0.13259497070685478, 'resid': -0.14659384831257133, 'observed': -0.0474261570718947}, {'date': '2011-01-01', 'trend': -0.03683063866320891, 'seasonal': -0.031714528696978224, 'resid': 0.3317333466952747, 'observed': 0.2631881793350876}, {'date': '2011-02-01', 'trend': -0.031484501032934334, 'seasonal': -0.04724202459970091, 'resid': 0.30583157112477843, 'observed': 0.2271050454921432}, {'date': '2011-03-01', 'trend': -0.03303165036061288, 'seasonal': -0.010757930825500861, 'resid': 0.06854397473264304, 'observed': 0.0247543935465293}, {'date': '2011-04-01', 'trend': 0.0019066603995984734, 'seasonal': 0.02255283843870718, 'resid': -0.011472811002449254, 'observed': 0.0129866878358564}, {'date': '2011-05-01', 'trend': 0.06281957931593297, 'seasonal': -0.030587662937704367, 'resid': 0.285689076483549, 'observed': 0.3179209928617776}, {'date': '2011-06-01', 'trend': 0.09660137276875817, 'seasonal': -0.017830922488024405, 'resid': -0.2250470526550836, 'observed': -0.1462766023743498}, {'date': '2011-07-01', 'trend': 0.09219874084545351, 'seasonal': 0.03184779489655618, 'resid': -0.0595786231076794, 'observed': 0.0644679126343303}, {'date': '2011-08-01', 'trend': 0.07266339009278908, 'seasonal': -0.052743362293617736, 'resid': -0.03039552709548534, 'observed': -0.010475499296314}, {'date': '2011-09-01', 'trend': 0.06441812681107223, 'seasonal': -0.010310036424414365, 'resid': -0.042446023298720666, 'observed': 0.0116620670879372}, {'date': '2011-10-01', 'trend': 0.06188167776796559, 'seasonal': 0.07866104363465767, 'resid': 0.1262425576560849, 'observed': 0.2667852790587082}, {'date': '2011-11-01', 'trend': 0.04383614949603275, 'seasonal': -0.06447017941083495, 'resid': 0.0842797430262469, 'observed': 0.0636457131114447}, {'date': '2011-12-01', 'trend': 0.03471611807420816, 'seasonal': 0.13259497070685478, 'resid': 0.007019676154726762, 'observed': 0.1743307649357897}, {'date': '2012-01-01', 'trend': 0.04570925645758612, 'seasonal': -0.031714528696978224, 'resid': -0.07822663659251661, 'observed': -0.0642319088319087}, {'date': '2012-02-01', 'trend': 0.044902190815899364, 'seasonal': -0.04724202459970091, 'resid': 0.08801654937899464, 'observed': 0.0856767155951931}, {'date': '2012-03-01', 'trend': 0.042262956633337116, 'seasonal': -0.010757930825500861, 'resid': -0.06320862112556136, 'observed': -0.0317035953177251}, {'date': '2012-04-01', 'trend': 0.03516995902860612, 'seasonal': 0.02255283843870718, 'resid': -0.0491528978017619, 'observed': 0.0085698996655514}, {'date': '2012-05-01', 'trend': 0.02213178752152366, 'seasonal': -0.030587662937704367, 'resid': -0.1022990220781248, 'observed': -0.1107548974943055}, {'date': '2012-06-01', 'trend': 0.018030473650186018, 'seasonal': -0.017830922488024405, 'resid': 0.06331898269578129, 'observed': 0.0635185338579429}, {'date': '2012-07-01', 'trend': 0.019188437545202486, 'seasonal': 0.03184779489655618, 'resid': 0.06747186516135023, 'observed': 0.1185080976031089}, {'date': '2012-08-01', 'trend': 0.0131250513067906, 'seasonal': -0.052743362293617736, 'resid': -0.04426694867874766, 'observed': -0.0838852596655748}, {'date': '2012-09-01', 'trend': 0.009551132283800178, 'seasonal': -0.010310036424414365, 'resid': 0.022489111216318088, 'observed': 0.0217302070757039}, {'date': '2012-10-01', 'trend': 0.020824397347343444, 'seasonal': 0.07866104363465767, 'resid': -0.013000244424603513, 'observed': 0.0864851965573976}, {'date': '2012-11-01', 'trend': 0.01896231043720962, 'seasonal': -0.06447017941083495, 'resid': -0.023462451583598262, 'observed': -0.0689703205572236}, {'date': '2012-12-01', 'trend': 0.0031981506915670166, 'seasonal': 0.13259497070685478, 'resid': 0.07272214429393273, 'observed': 0.2085152656923545}, {'date': '2013-01-01', 'trend': -0.003035419802576943, 'seasonal': -0.031714528696978224, 'resid': -0.035875327608523126, 'observed': -0.0706252761080783}, {'date': '2013-02-01', 'trend': -0.0008950330310897714, 'seasonal': -0.04724202459970091, 'resid': -0.005314129219731817, 'observed': -0.0534511868505225}, {'date': '2013-03-01', 'trend': 0.005412577037834394, 'seasonal': -0.010757930825500861, 'resid': 0.026995604363886767, 'observed': 0.0216502505762203}, {'date': '2013-04-01', 'trend': 0.009242997420316925, 'seasonal': 0.02255283843870718, 'resid': 0.1939785794376204, 'observed': 0.2257744152966445}, {'date': '2013-05-01', 'trend': 0.014444255150244148, 'seasonal': -0.030587662937704367, 'resid': -0.3565060911811502, 'observed': -0.3726494989686104}, {'date': '2013-06-01', 'trend': 0.012049156042391663, 'seasonal': -0.017830922488024405, 'resid': -0.04714493211754186, 'observed': -0.0529266985631746}, {'date': '2013-07-01', 'trend': 0.008257073724790395, 'seasonal': 0.03184779489655618, 'resid': 0.04524276954342472, 'observed': 0.0853476381647713}, {'date': '2013-08-01', 'trend': 0.011644938815555297, 'seasonal': -0.052743362293617736, 'resid': 0.041742905766517235, 'observed': 0.0006444822884548}, {'date': '2013-09-01', 'trend': 0.012858457832634297, 'seasonal': -0.010310036424414365, 'resid': 0.08603468536763466, 'observed': 0.0885831067758546}, {'date': '2013-10-01', 'trend': 0.004237769660182612, 'seasonal': 0.07866104363465767, 'resid': 0.02866357274198722, 'observed': 0.1115623860368275}, {'date': '2013-11-01', 'trend': -0.004335033857545581, 'seasonal': -0.06447017941083495, 'resid': 0.09958788874998034, 'observed': 0.0307826754815998}, {'date': '2013-12-01', 'trend': -0.008880789719724598, 'seasonal': 0.13259497070685478, 'resid': -0.07243428992205868, 'observed': 0.0512798910650715}, {'date': '2014-01-01', 'trend': -0.017619162189213094, 'seasonal': -0.031714528696978224, 'resid': 0.044933813782965515, 'observed': -0.0043998771032258}, {'date': '2014-02-01', 'trend': -0.02311961132920546, 'seasonal': -0.04724202459970091, 'resid': 0.03199381225188907, 'observed': -0.0383678236770173}, {'date': '2014-03-01', 'trend': -0.03397059240086849, 'seasonal': -0.010757930825500861, 'resid': 0.08041986703898045, 'observed': 0.0356913438126111}, {'date': '2014-04-01', 'trend': -0.041985310898930206, 'seasonal': 0.02255283843870718, 'resid': 0.024269278381636228, 'observed': 0.0048368059214132}, {'date': '2014-05-01', 'trend': -0.041198161431693886, 'seasonal': -0.030587662937704367, 'resid': -0.2856733496494574, 'observed': -0.3574591740188557}, {'date': '2014-06-01', 'trend': -0.04789477224075883, 'seasonal': -0.017830922488024405, 'resid': -0.11148946947644248, 'observed': -0.1772151642052257}, {'date': '2014-07-01', 'trend': -0.052218878217136055, 'seasonal': 0.03184779489655618, 'resid': 0.02028624785967833, 'observed': -8.483546090154448e-05}, {'date': '2014-08-01', 'trend': -0.05494124643510578, 'seasonal': -0.052743362293617736, 'resid': 0.06175078528303441, 'observed': -0.0459338234456891}, {'date': '2014-09-01', 'trend': -0.06448933347929107, 'seasonal': -0.010310036424414365, 'resid': -0.05046276330620895, 'observed': -0.1252621332099144}, {'date': '2014-10-01', 'trend': -0.07028956365765798, 'seasonal': 0.07866104363465767, 'resid': 0.12468290209211572, 'observed': 0.1330543820691154}, {'date': '2014-11-01', 'trend': -0.06019034746773441, 'seasonal': -0.06447017941083495, 'resid': 0.15284279354155278, 'observed': 0.0281822666629834}, {'date': '2014-12-01', 'trend': -0.04221359859407286, 'seasonal': 0.13259497070685478, 'resid': -0.1972197316466527, 'observed': -0.1068383595338708}, {'date': '2015-01-01', 'trend': -0.038848053552043765, 'seasonal': -0.031714528696978224, 'resid': 0.12050241231168518, 'observed': 0.0499398300626632}, {'date': '2015-02-01', 'trend': -0.0381625832895537, 'seasonal': -0.04724202459970091, 'resid': -0.0726397601849248, 'observed': -0.1580443680741794}, {'date': '2015-03-01', 'trend': -0.029463840012527352, 'seasonal': -0.010757930825500861, 'resid': -0.033564430012645786, 'observed': -0.073786200850674}, {'date': '2015-04-01', 'trend': -0.03136658171414957, 'seasonal': 0.02255283843870718, 'resid': -0.01607743042066501, 'observed': -0.0248911736961074}, {'date': '2015-05-01', 'trend': -0.04081446354341571, 'seasonal': -0.030587662937704367, 'resid': -0.013947879362049422, 'observed': -0.0853500058431695}, {'date': '2015-06-01', 'trend': -0.03936403641140541, 'seasonal': -0.017830922488024405, 'resid': 0.03931259948639521, 'observed': -0.0178823594130346}, {'date': '2015-07-01', 'trend': -0.03547665094941137, 'seasonal': 0.03184779489655618, 'resid': -0.0750157031915393, 'observed': -0.0786445592443945}, {'date': '2015-08-01', 'trend': -0.02967353755159856, 'seasonal': -0.052743362293617736, 'resid': 0.13149408648278169, 'observed': 0.0490771866375654}, {'date': '2015-09-01', 'trend': -0.034311547465159596, 'seasonal': -0.010310036424414365, 'resid': 0.03311827924503746, 'observed': -0.0115033046445365}, {'date': '2015-10-01', 'trend': -0.04624601382043859, 'seasonal': 0.07866104363465767, 'resid': -0.05878527714941489, 'observed': -0.0263702473351958}, {'date': '2015-11-01', 'trend': -0.02522274760439807, 'seasonal': -0.06447017941083495, 'resid': 0.050550659180140324, 'observed': -0.0391422678350927}, {'date': '2015-12-01', 'trend': 0.006684184979284752, 'seasonal': 0.13259497070685478, 'resid': -0.14398272955368682, 'observed': -0.0047035738675473}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.031714528696978224, 'resid': nan, 'observed': 0.0411022954841966}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.04724202459970091, 'resid': nan, 'observed': -0.0099321119482054}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.010757930825500861, 'resid': nan, 'observed': -0.3332106949021129}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.02255283843870718, 'resid': nan, 'observed': -0.0518938721713642}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.030587662937704367, 'resid': nan, 'observed': 0.4462110818170597}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.017830922488024405, 'resid': nan, 'observed': 0.2163229349351239}], 'diff1_loan_rate_B': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.07046143882383622, 'resid': nan, 'observed': 0.1343589743589746}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.032770515961085024, 'resid': nan, 'observed': -0.2210256410256423}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.0434329852966769, 'resid': nan, 'observed': 0.156666666666668}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.11246094776255657, 'resid': nan, 'observed': 0.0344444444444445}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.018135820501638387, 'resid': nan, 'observed': -0.0802777777777787}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.07022848869063908, 'resid': nan, 'observed': 0.3056250000000009}, {'date': '2008-02-01', 'trend': 0.07253161961495286, 'seasonal': -0.045894739401959414, 'resid': 0.31596845686565844, 'observed': 0.3426053370786519}, {'date': '2008-03-01', 'trend': 0.08760878010878013, 'seasonal': 0.0018055671824235378, 'resid': -0.06804791017630608, 'observed': 0.0213664371148976}, {'date': '2008-04-01', 'trend': 0.10336111111111115, 'seasonal': -0.0004989351068384237, 'resid': 0.0012691199776175741, 'observed': 0.1041312959818903}, {'date': '2008-05-01', 'trend': 0.11530555555555554, 'seasonal': -0.022198231228783477, 'resid': -0.17641872783554527, 'observed': -0.0833114035087732}, {'date': '2008-06-01', 'trend': 0.14536751443001444, 'seasonal': 0.02562000786713773, 'resid': -0.18072237078200068, 'observed': -0.0097348484848485}, {'date': '2008-07-01', 'trend': 0.16546677469571494, 'seasonal': -0.081401970823299, 'resid': 0.005901526093912465, 'observed': 0.0899663299663284}, {'date': '2008-08-01', 'trend': 0.16290479051505097, 'seasonal': 0.07046143882383622, 'resid': 0.05212198614932932, 'observed': 0.2854882154882165}, {'date': '2008-09-01', 'trend': 0.16511287791960133, 'seasonal': 0.032770515961085024, 'resid': -0.20818642418371616, 'observed': -0.0103030303030298}, {'date': '2008-10-01', 'trend': 0.15959744792087738, 'seasonal': 0.0434329852966769, 'resid': 0.12096956678244551, 'observed': 0.3239999999999998}, {'date': '2008-11-01', 'trend': 0.14138640873015867, 'seasonal': -0.11246094776255657, 'resid': 0.1248523168101759, 'observed': 0.153777777777778}, {'date': '2008-12-01', 'trend': 0.13116512161084526, 'seasonal': 0.018135820501638387, 'resid': 0.372574959763418, 'observed': 0.5218759018759016}, {'date': '2009-01-01', 'trend': 0.12665808188724859, 'seasonal': 0.07022848869063908, 'resid': -0.011033003854755474, 'observed': 0.1858535667231322}, {'date': '2009-02-01', 'trend': 0.11628219804886486, 'seasonal': -0.045894739401959414, 'resid': 0.33050169137268, 'observed': 0.4008891500195854}, {'date': '2009-03-01', 'trend': 0.11547369581894905, 'seasonal': 0.0018055671824235378, 'resid': -0.101202541118199, 'observed': 0.0160767218831736}, {'date': '2009-04-01', 'trend': 0.10519924478688923, 'seasonal': -0.0004989351068384237, 'resid': -0.1276496184358118, 'observed': -0.022949308755761}, {'date': '2009-05-01', 'trend': 0.08361755233494368, 'seasonal': -0.022198231228783477, 'resid': -0.45471506045453125, 'observed': -0.3932957393483711}, {'date': '2009-06-01', 'trend': 0.05570939347564919, 'seasonal': 0.02562000786713773, 'resid': -0.02639080485155902, 'observed': 0.0549385964912279}, {'date': '2009-07-01', 'trend': 0.017878874306814792, 'seasonal': -0.081401970823299, 'resid': -0.01935297185958379, 'observed': -0.082876068376068}, {'date': '2009-08-01', 'trend': -0.05598613588513567, 'seasonal': 0.07046143882383622, 'resid': 0.19483409877070224, 'observed': 0.2093094017094028}, {'date': '2009-09-01', 'trend': -0.11263551516313529, 'seasonal': 0.032770515961085024, 'resid': 0.12633672915985508, 'observed': 0.0464717299578048}, {'date': '2009-10-01', 'trend': -0.11396564767314926, 'seasonal': 0.0434329852966769, 'resid': 0.09117107734620196, 'observed': 0.0206384149697296}, {'date': '2009-11-01', 'trend': -0.09415100843104823, 'seasonal': -0.11246094776255657, 'resid': 0.1457907001549597, 'observed': -0.0608212560386451}, {'date': '2009-12-01', 'trend': -0.05602207779516801, 'seasonal': 0.018135820501638387, 'resid': 0.10456538036278631, 'observed': 0.0666791230692567}, {'date': '2010-01-01', 'trend': -0.03414123324984977, 'seasonal': 0.07022848869063908, 'resid': -0.3029693699630377, 'observed': -0.2668821145222484}, {'date': '2010-02-01', 'trend': -0.038466300031433656, 'seasonal': -0.045894739401959414, 'resid': -0.8347743739084519, 'observed': -0.919135413341845}, {'date': '2010-03-01', 'trend': -0.051222045739554606, 'seasonal': 0.0018055671824235378, 'resid': 0.025932661129744267, 'observed': -0.0234838174273868}, {'date': '2010-04-01', 'trend': -0.08204924709401365, 'seasonal': -0.0004989351068384237, 'resid': 0.06723623251531607, 'observed': -0.015311949685536}, {'date': '2010-05-01', 'trend': -0.14012220490245753, 'seasonal': -0.022198231228783477, 'resid': 0.2369386795230695, 'observed': 0.0746182433918285}, {'date': '2010-06-01', 'trend': -0.1759102546194294, 'seasonal': 0.02562000786713773, 'resid': 0.6524091957644453, 'observed': 0.5021189490121536}, {'date': '2010-07-01', 'trend': -0.15017141807608086, 'seasonal': -0.081401970823299, 'resid': 0.22665723709002406, 'observed': -0.0049161518093558}, {'date': '2010-08-01', 'trend': -0.0697244601852828, 'seasonal': 0.07046143882383622, 'resid': 0.02681090374612409, 'observed': 0.0275478823846775}, {'date': '2010-09-01', 'trend': -0.017108422640112805, 'seasonal': 0.032770515961085024, 'resid': -0.093566741033345, 'observed': -0.0779046477123728}, {'date': '2010-10-01', 'trend': -0.01619971963324996, 'seasonal': 0.0434329852966769, 'resid': -0.6220713055305368, 'observed': -0.5948380398671098}, {'date': '2010-11-01', 'trend': 0.007396789260623559, 'seasonal': -0.11246094776255657, 'resid': -0.7340316301025256, 'observed': -0.8390957886044585}, {'date': '2010-12-01', 'trend': 0.010777912766808017, 'seasonal': 0.018135820501638387, 'resid': -0.042873270840701305, 'observed': -0.0139595375722549}, {'date': '2011-01-01', 'trend': -0.011288570651843756, 'seasonal': 0.07022848869063908, 'resid': 0.37254870512083327, 'observed': 0.4314886231596286}, {'date': '2011-02-01', 'trend': -0.013138828366925894, 'seasonal': -0.045894739401959414, 'resid': 0.372254406124316, 'observed': 0.3132208383554307}, {'date': '2011-03-01', 'trend': -0.0032650106639154704, 'seasonal': 0.0018055671824235378, 'resid': 0.008404275440909534, 'observed': 0.0069448319594176}, {'date': '2011-04-01', 'trend': 0.046106888540878604, 'seasonal': -0.0004989351068384237, 'resid': -0.06953968034167218, 'observed': -0.023931726907632}, {'date': '2011-05-01', 'trend': 0.11842691528802644, 'seasonal': -0.022198231228783477, 'resid': 0.5533255500076458, 'observed': 0.6495542340668887}, {'date': '2011-06-01', 'trend': 0.15358747474607576, 'seasonal': 0.02562000786713773, 'resid': -0.17087756012769287, 'observed': 0.0083299224855206}, {'date': '2011-07-01', 'trend': 0.13951867653073943, 'seasonal': -0.081401970823299, 'resid': -0.09883943303780582, 'observed': -0.0407227273303654}, {'date': '2011-08-01', 'trend': 0.10881052011036284, 'seasonal': 0.07046143882383622, 'resid': -0.16032368619048354, 'observed': 0.0189482727437155}, {'date': '2011-09-01', 'trend': 0.10591245912195688, 'seasonal': 0.032770515961085024, 'resid': 0.028983611717797612, 'observed': 0.1676665868008395}, {'date': '2011-10-01', 'trend': 0.12029888775357502, 'seasonal': 0.0434329852966769, 'resid': 0.18078443348448392, 'observed': 0.3445163065347358}, {'date': '2011-11-01', 'trend': 0.09964719123988357, 'seasonal': -0.11246094776255657, 'resid': -0.029955736552082904, 'observed': -0.0427694930747559}, {'date': '2011-12-01', 'trend': 0.08175969247221472, 'seasonal': 0.018135820501638387, 'resid': -0.0663279190826275, 'observed': 0.0335675938912256}, {'date': '2012-01-01', 'trend': 0.09190183274358896, 'seasonal': 0.07022848869063908, 'resid': -0.11581998690615164, 'observed': 0.0463103345280764}, {'date': '2012-02-01', 'trend': 0.1009050265452881, 'seasonal': -0.045894739401959414, 'resid': -0.0936069142453843, 'observed': -0.0385966271020556}, {'date': '2012-03-01', 'trend': 0.09763085440825638, 'seasonal': 0.0018055671824235378, 'resid': 0.18977241210448131, 'observed': 0.2892088336951612}, {'date': '2012-04-01', 'trend': 0.07324583971131912, 'seasonal': -0.0004989351068384237, 'resid': -0.0336683460890211, 'observed': 0.0390785585154596}, {'date': '2012-05-01', 'trend': 0.06075504434068528, 'seasonal': -0.022198231228783477, 'resid': 0.0523464192033007, 'observed': 0.0909032323152025}, {'date': '2012-06-01', 'trend': 0.054106865190672766, 'seasonal': 0.02562000786713773, 'resid': 0.057954080755343716, 'observed': 0.1376809538131542}, {'date': '2012-07-01', 'trend': 0.042441027870710114, 'seasonal': -0.081401970823299, 'resid': 0.11229855080757159, 'observed': 0.0733376078549827}, {'date': '2012-08-01', 'trend': 0.042250133304635284, 'seasonal': 0.07046143882383622, 'resid': 0.00825301667067549, 'observed': 0.120964588799147}, {'date': '2012-09-01', 'trend': 0.028265892129516017, 'seasonal': 0.032770515961085024, 'resid': -0.07396626863395464, 'observed': -0.0129298605433536}, {'date': '2012-10-01', 'trend': 0.012322106610375349, 'seasonal': 0.0434329852966769, 'resid': -0.11588269075461714, 'observed': -0.0601275988475649}, {'date': '2012-11-01', 'trend': 0.0075267811347561175, 'seasonal': -0.11246094776255657, 'resid': 0.16702949004013315, 'observed': 0.0620953234123327}, {'date': '2012-12-01', 'trend': -0.006617476325216561, 'seasonal': 0.018135820501638387, 'resid': -0.24237186637258523, 'observed': -0.2308535221961634}, {'date': '2013-01-01', 'trend': -0.044990516158915986, 'seasonal': 0.07022848869063908, 'resid': 0.0055133824046385005, 'observed': 0.0307513549363616}, {'date': '2013-02-01', 'trend': -0.07684670346080483, 'seasonal': -0.045894739401959414, 'resid': 0.09512232576662744, 'observed': -0.0276191170961368}, {'date': '2013-03-01', 'trend': -0.07540957750188355, 'seasonal': 0.0018055671824235378, 'resid': 0.016213545805839818, 'observed': -0.0573904645136202}, {'date': '2013-04-01', 'trend': -0.05654519169114858, 'seasonal': -0.0004989351068384237, 'resid': 0.0600711310628522, 'observed': 0.0030270042648652}, {'date': '2013-05-01', 'trend': -0.04025695390906634, 'seasonal': -0.022198231228783477, 'resid': 0.07432216028878522, 'observed': 0.0118669751509354}, {'date': '2013-06-01', 'trend': -0.026501686923818615, 'seasonal': 0.02562000786713773, 'resid': -0.12186328900524213, 'observed': -0.122744968061923}, {'date': '2013-07-01', 'trend': -0.017724880517176876, 'seasonal': -0.081401970823299, 'resid': -0.48806257493825056, 'observed': -0.5871894262787265}, {'date': '2013-08-01', 'trend': -0.022383778667604298, 'seasonal': 0.07046143882383622, 'resid': -0.031134532468707926, 'observed': 0.016943127687524}, {'date': '2013-09-01', 'trend': -0.024856579833205748, 'seasonal': 0.032770515961085024, 'resid': 0.11766868745450074, 'observed': 0.12558262358238}, {'date': '2013-10-01', 'trend': -0.024572831959464192, 'seasonal': 0.0434329852966769, 'resid': 0.2352450231471282, 'observed': 0.2541051764843409}, {'date': '2013-11-01', 'trend': -0.04578053795130625, 'seasonal': -0.11246094776255657, 'resid': 0.2970217405642637, 'observed': 0.1387802548504009}, {'date': '2013-12-01', 'trend': -0.07243155863508847, 'seasonal': 0.018135820501638387, 'resid': 0.07688369214516377, 'observed': 0.0225879540117137}, {'date': '2014-01-01', 'trend': -0.05423872138206551, 'seasonal': 0.07022848869063908, 'resid': -0.028036534820687575, 'observed': -0.012046767512114}, {'date': '2014-02-01', 'trend': -0.03397058050497438, 'seasonal': -0.045894739401959414, 'resid': -0.016769230350985306, 'observed': -0.0966345502579191}, {'date': '2014-03-01', 'trend': -0.0424961365791401, 'seasonal': 0.0018055671824235378, 'resid': -0.007031689929556136, 'observed': -0.0477222593262727}, {'date': '2014-04-01', 'trend': -0.05547293814302131, 'seasonal': -0.0004989351068384237, 'resid': 0.056140621297174835, 'observed': 0.0001687480473151}, {'date': '2014-05-01', 'trend': -0.08515159640587747, 'seasonal': -0.022198231228783477, 'resid': -0.38690988480106325, 'observed': -0.4942597124357242}, {'date': '2014-06-01', 'trend': -0.11441681410591502, 'seasonal': 0.02562000786713773, 'resid': -0.16744597064725938, 'observed': -0.2562427768860367}, {'date': '2014-07-01', 'trend': -0.11847281350061632, 'seasonal': -0.081401970823299, 'resid': 0.18281126094185382, 'observed': -0.0170635233820615}, {'date': '2014-08-01', 'trend': -0.12316438274360003, 'seasonal': 0.07046143882383622, 'resid': -0.014044450239189987, 'observed': -0.0667473941589538}, {'date': '2014-09-01', 'trend': -0.135097032058235, 'seasonal': 0.032770515961085024, 'resid': 0.1069863157460306, 'observed': 0.0046597996488806}, {'date': '2014-10-01', 'trend': -0.14024366983363687, 'seasonal': 0.0434329852966769, 'resid': 0.16039544742165096, 'observed': 0.063584762884691}, {'date': '2014-11-01', 'trend': -0.12035307260737171, 'seasonal': -0.11246094776255657, 'resid': -0.15017310948856893, 'observed': -0.3829871298584972}, {'date': '2014-12-01', 'trend': -0.09086346056825195, 'seasonal': 0.018135820501638387, 'resid': -0.08528224601367623, 'observed': -0.1580098860802898}, {'date': '2015-01-01', 'trend': -0.08372327327467737, 'seasonal': 0.07022848869063908, 'resid': 0.084701871691097, 'observed': 0.0712070871070587}, {'date': '2015-02-01', 'trend': -0.0835007527359228, 'seasonal': -0.045894739401959414, 'resid': -0.16309057457081877, 'observed': -0.292486066708701}, {'date': '2015-03-01', 'trend': -0.0816249274058822, 'seasonal': 0.0018055671824235378, 'resid': -0.05843496620327213, 'observed': -0.1382543264267308}, {'date': '2015-04-01', 'trend': -0.08406588342221845, 'seasonal': -0.0004989351068384237, 'resid': 0.051746327067185974, 'observed': -0.0328184914618709}, {'date': '2015-05-01', 'trend': -0.0684165593893712, 'seasonal': -0.022198231228783477, 'resid': 0.10671665112198009, 'observed': 0.0161018605038254}, {'date': '2015-06-01', 'trend': -0.04701617470145597, 'seasonal': 0.02562000786713773, 'resid': -0.03745749405239336, 'observed': -0.0588536608867116}, {'date': '2015-07-01', 'trend': -0.04587836546821761, 'seasonal': -0.081401970823299, 'resid': 0.08419219195591982, 'observed': -0.0430881443355968}, {'date': '2015-08-01', 'trend': -0.034932169714336984, 'seasonal': 0.07046143882383622, 'resid': -0.07091154938480815, 'observed': -0.0353822802753089}, {'date': '2015-09-01', 'trend': -0.01580589910134819, 'seasonal': 0.032770515961085024, 'resid': 0.0013498768264733688, 'observed': 0.0183144936862102}, {'date': '2015-10-01', 'trend': -0.007080095998270134, 'seasonal': 0.0434329852966769, 'resid': -0.04500576484311507, 'observed': -0.0086528755447083}, {'date': '2015-11-01', 'trend': -0.007776782505493978, 'seasonal': -0.11246094776255657, 'resid': 0.18507201562728653, 'observed': 0.064834285359236}, {'date': '2015-12-01', 'trend': 0.0012050536184403146, 'seasonal': 0.018135820501638387, 'resid': -0.1115629429081362, 'observed': -0.0922220687880575}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.07022848869063908, 'resid': nan, 'observed': 0.032726691412547}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.045894739401959414, 'resid': nan, 'observed': 0.0087030270789458}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.0018055671824235378, 'resid': nan, 'observed': 0.0195870744973536}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.0004989351068384237, 'resid': nan, 'observed': 0.0187593820879179}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.022198231228783477, 'resid': nan, 'observed': -0.0521964892193356}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.02562000786713773, 'resid': nan, 'observed': 0.2250087558108724}], 'diff1_loan_rate_C': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.023538262427489662, 'resid': nan, 'observed': 0.2074999999999995}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.05848004070126249, 'resid': nan, 'observed': -0.118333333333334}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.042982962767393634, 'resid': nan, 'observed': -0.0032407407407397}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.13074174163157376, 'resid': nan, 'observed': 0.1411111111111118}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.02963753046734331, 'resid': nan, 'observed': -0.1620370370370381}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.03051548559335375, 'resid': nan, 'observed': 0.4830555555555555}, {'date': '2008-02-01', 'trend': 0.08177216880341873, 'seasonal': -0.009428661869998278, 'resid': 0.16523730114738655, 'observed': 0.237580808080807}, {'date': '2008-03-01', 'trend': 0.07897569444444455, 'seasonal': -0.009181965409746102, 'resid': 0.008248068248478558, 'observed': 0.078041797283177}, {'date': '2008-04-01', 'trend': 0.09493738129154798, 'seasonal': -0.014386009084824822, 'resid': -0.07063578312626287, 'observed': 0.0099155890804603}, {'date': '2008-05-01', 'trend': 0.11193583808167135, 'seasonal': -0.018321804588489938, 'resid': -0.07239296867836602, 'observed': 0.0212210648148154}, {'date': '2008-06-01', 'trend': 0.13923542406561806, 'seasonal': 0.006198504602688547, 'resid': -0.1311310964242989, 'observed': 0.0143028322440077}, {'date': '2008-07-01', 'trend': 0.15975801104653842, 'seasonal': -0.009292603974898495, 'resid': -0.01830100284841311, 'observed': 0.1321644042232268}, {'date': '2008-08-01', 'trend': 0.14655239898989897, 'seasonal': 0.023538262427489662, 'resid': -0.08262271269943802, 'observed': 0.0874679487179506}, {'date': '2008-09-01', 'trend': 0.13420448740276325, 'seasonal': 0.05848004070126249, 'resid': -0.25810119477069143, 'observed': -0.0654166666666657}, {'date': '2008-10-01', 'trend': 0.12909123448091658, 'seasonal': 0.042982962767393634, 'resid': 0.15484887967476466, 'observed': 0.3269230769230749}, {'date': '2008-11-01', 'trend': 0.1266789871705552, 'seasonal': -0.13074174163157376, 'resid': 0.22297301087127616, 'observed': 0.2189102564102576}, {'date': '2008-12-01', 'trend': 0.12436923576842174, 'seasonal': 0.02963753046734331, 'resid': 0.26134711504277275, 'observed': 0.4153538812785378}, {'date': '2009-01-01', 'trend': 0.11464929483655441, 'seasonal': 0.03051548559335375, 'resid': 0.25304194435215915, 'observed': 0.3982067247820673}, {'date': '2009-02-01', 'trend': 0.11559828821777357, 'seasonal': -0.009428661869998278, 'resid': -0.1006746768528264, 'observed': 0.0054949494949489}, {'date': '2009-03-01', 'trend': 0.1351548442760942, 'seasonal': -0.009181965409746102, 'resid': -0.1121951010885707, 'observed': 0.0137777777777774}, {'date': '2009-04-01', 'trend': 0.1295184182180072, 'seasonal': -0.014386009084824822, 'resid': -0.16367087067164196, 'observed': -0.0485384615384596}, {'date': '2009-05-01', 'trend': 0.10425474758308659, 'seasonal': -0.018321804588489938, 'resid': -0.06415176300953446, 'observed': 0.0217811799850622}, {'date': '2009-06-01', 'trend': 0.07938407382962666, 'seasonal': 0.006198504602688547, 'resid': -0.12727389500975692, 'observed': -0.0416913165774417}, {'date': '2009-07-01', 'trend': 0.04480301891962437, 'seasonal': -0.009292603974898495, 'resid': -0.08063044426486578, 'observed': -0.0451200293201399}, {'date': '2009-08-01', 'trend': 0.020944087091242646, 'seasonal': 0.023538262427489662, 'resid': 0.24304587389184482, 'observed': 0.2875282234105771}, {'date': '2009-09-01', 'trend': 0.014079656149836417, 'seasonal': 0.05848004070126249, 'resid': 0.13132070718930378, 'observed': 0.2038804040404027}, {'date': '2009-10-01', 'trend': 0.018584390522217834, 'seasonal': 0.042982962767393634, 'resid': -0.13921557246769328, 'observed': -0.0776482191780818}, {'date': '2009-11-01', 'trend': 0.023467782398942504, 'seasonal': -0.13074174163157376, 'resid': 0.12442741650595125, 'observed': 0.01715345727332}, {'date': '2009-12-01', 'trend': 0.03814044501669851, 'seasonal': 0.02963753046734331, 'resid': -0.04756346515160492, 'observed': 0.0202145103324369}, {'date': '2010-01-01', 'trend': 0.05988987718423532, 'seasonal': 0.03051548559335375, 'resid': -0.12700458488947597, 'observed': -0.0365992221118869}, {'date': '2010-02-01', 'trend': 0.05307828035254506, 'seasonal': -0.009428661869998278, 'resid': -0.1759630859748049, 'observed': -0.1323134674922581}, {'date': '2010-03-01', 'trend': 0.0298807216565112, 'seasonal': -0.009181965409746102, 'resid': -0.0338589040755303, 'observed': -0.0131601478287652}, {'date': '2010-04-01', 'trend': 0.01032291111379784, 'seasonal': -0.014386009084824822, 'resid': 0.09057618697626399, 'observed': 0.086513089005237}, {'date': '2010-05-01', 'trend': -0.022554434520587318, 'seasonal': -0.018321804588489938, 'resid': 0.044807273591834854, 'observed': 0.0039310344827576}, {'date': '2010-06-01', 'trend': -0.04617230149098248, 'seasonal': 0.006198504602688547, 'resid': 0.36827652863930116, 'observed': 0.3283027317510072}, {'date': '2010-07-01', 'trend': -0.040473591202054245, 'seasonal': -0.009292603974898495, 'resid': 0.15663848954924736, 'observed': 0.1068722943722946}, {'date': '2010-08-01', 'trend': -0.019491865253334312, 'seasonal': 0.023538262427489662, 'resid': -0.03198882141657905, 'observed': -0.0279424242424237}, {'date': '2010-09-01', 'trend': -0.0030483304315947, 'seasonal': 0.05848004070126249, 'resid': -0.09282206728107689, 'observed': -0.0373903570114091}, {'date': '2010-10-01', 'trend': -0.008952034962472556, 'seasonal': 0.042982962767393634, 'resid': -0.3397958389563118, 'observed': -0.3057649111513907}, {'date': '2010-11-01', 'trend': 0.008096317190669238, 'seasonal': -0.13074174163157376, 'resid': -0.42114072153771037, 'observed': -0.5437861459786149}, {'date': '2010-12-01', 'trend': 0.021255319357675065, 'seasonal': 0.02963753046734331, 'resid': -0.03656754353013088, 'observed': 0.0143253062948875}, {'date': '2011-01-01', 'trend': 0.005997675966532471, 'seasonal': 0.03051548559335375, 'resid': 0.06954586730005408, 'observed': 0.1060590288599403}, {'date': '2011-02-01', 'trend': 0.000328811447912961, 'seasonal': -0.009428661869998278, 'resid': 0.2376895547272785, 'observed': 0.2285897043051932}, {'date': '2011-03-01', 'trend': 0.014103505047649581, 'seasonal': -0.009181965409746102, 'resid': 0.01565997645763072, 'observed': 0.0205815160955342}, {'date': '2011-04-01', 'trend': 0.05282867463749468, 'seasonal': -0.014386009084824822, 'resid': -0.12736014921280076, 'observed': -0.0889174836601309}, {'date': '2011-05-01', 'trend': 0.09961397300645969, 'seasonal': -0.018321804588489938, 'resid': 0.5072298904055588, 'observed': 0.5885220588235285}, {'date': '2011-06-01', 'trend': 0.1255908836847626, 'seasonal': 0.006198504602688547, 'resid': -0.07226162886907493, 'observed': 0.0595277594183762}, {'date': '2011-07-01', 'trend': 0.12134087167607463, 'seasonal': -0.009292603974898495, 'resid': -0.10258444238367274, 'observed': 0.0094638253175034}, {'date': '2011-08-01', 'trend': 0.1110441126308099, 'seasonal': 0.023538262427489662, 'resid': -0.20116907869280046, 'observed': -0.0665867036345009}, {'date': '2011-09-01', 'trend': 0.11572648891561448, 'seasonal': 0.05848004070126249, 'resid': 0.1576400391574702, 'observed': 0.3318465687743472}, {'date': '2011-10-01', 'trend': 0.12997502514238415, 'seasonal': 0.042982962767393634, 'resid': 0.08144424530935751, 'observed': 0.2544022332191353}, {'date': '2011-11-01', 'trend': 0.10938241985012398, 'seasonal': -0.13074174163157376, 'resid': 0.04025319228746928, 'observed': 0.0188938705060195}, {'date': '2011-12-01', 'trend': 0.08399893170567779, 'seasonal': 0.02963753046734331, 'resid': -0.03854531608349801, 'observed': 0.0750911460895231}, {'date': '2012-01-01', 'trend': 0.10210200836442407, 'seasonal': 0.03051548559335375, 'resid': -0.1893245931009846, 'observed': -0.0567070991432068}, {'date': '2012-02-01', 'trend': 0.12769282473856813, 'seasonal': -0.009428661869998278, 'resid': 0.025969452353417057, 'observed': 0.1442336152219869}, {'date': '2012-03-01', 'trend': 0.12266235492976871, 'seasonal': -0.009181965409746102, 'resid': 0.10383424649402768, 'observed': 0.2173146360140503}, {'date': '2012-04-01', 'trend': 0.0981006307644038, 'seasonal': -0.014386009084824822, 'resid': -0.02740035581575338, 'observed': 0.0563142658638256}, {'date': '2012-05-01', 'trend': 0.08641169864468848, 'seasonal': -0.018321804588489938, 'resid': -0.11902211177087095, 'observed': -0.0509322177146724}, {'date': '2012-06-01', 'trend': 0.09506837094688565, 'seasonal': 0.006198504602688547, 'resid': -0.011488555059705906, 'observed': 0.0897783204898683}, {'date': '2012-07-01', 'trend': 0.1079662325888333, 'seasonal': -0.009292603974898495, 'resid': 0.3150134754419876, 'observed': 0.4136871040559224}, {'date': '2012-08-01', 'trend': 0.10552586473766148, 'seasonal': 0.023538262427489662, 'resid': 0.01430548344138646, 'observed': 0.1433696106065376}, {'date': '2012-09-01', 'trend': 0.09137273738197532, 'seasonal': 0.05848004070126249, 'resid': -0.14869379896111562, 'observed': 0.0011589791221222}, {'date': '2012-10-01', 'trend': 0.0813850371425335, 'seasonal': 0.042982962767393634, 'resid': -0.12875955700732464, 'observed': -0.0043915570973975}, {'date': '2012-11-01', 'trend': 0.08034908737684145, 'seasonal': -0.13074174163157376, 'resid': 0.04754594420411681, 'observed': -0.0028467100506155}, {'date': '2012-12-01', 'trend': 0.07780893990577106, 'seasonal': 0.02963753046734331, 'resid': 0.19714539152577595, 'observed': 0.3045918618988903}, {'date': '2013-01-01', 'trend': 0.03406573649563173, 'seasonal': 0.03051548559335375, 'resid': -0.041240357634815775, 'observed': 0.0233408644541697}, {'date': '2013-02-01', 'trend': -0.014144350512861207, 'seasonal': -0.009428661869998278, 'resid': 0.029189835579346284, 'observed': 0.0056168231964868}, {'date': '2013-03-01', 'trend': -0.014151127597210836, 'seasonal': -0.009181965409746102, 'resid': 0.03958946451003964, 'observed': 0.0162563715030827}, {'date': '2013-04-01', 'trend': 0.005884417671098063, 'seasonal': -0.014386009084824822, 'resid': 0.02616931604191646, 'observed': 0.0176677246281897}, {'date': '2013-05-01', 'trend': 0.01344750950178895, 'seasonal': -0.018321804588489938, 'resid': -0.032274175768945215, 'observed': -0.0371484708556462}, {'date': '2013-06-01', 'trend': -0.021112030362065867, 'seasonal': 0.006198504602688547, 'resid': 0.029944560084530225, 'observed': 0.0150310343251529}, {'date': '2013-07-01', 'trend': -0.057103480777511054, 'seasonal': -0.009292603974898495, 'resid': -0.49500640687029673, 'observed': -0.5614024916227063}, {'date': '2013-08-01', 'trend': -0.07151753908401697, 'seasonal': 0.023538262427489662, 'resid': 0.009396394737863103, 'observed': -0.0385828819186642}, {'date': '2013-09-01', 'trend': -0.0867499712964841, 'seasonal': 0.05848004070126249, 'resid': 0.2112187522181545, 'observed': 0.1829488216229329}, {'date': '2013-10-01', 'trend': -0.09416931841093755, 'seasonal': 0.042982962767393634, 'resid': 0.3458580424847493, 'observed': 0.2946716868412053}, {'date': '2013-11-01', 'trend': -0.11345204937083736, 'seasonal': -0.13074174163157376, 'resid': 0.12379804094977402, 'observed': -0.1203957500526371}, {'date': '2013-12-01', 'trend': -0.1436487743885572, 'seasonal': 0.02963753046734331, 'resid': -0.2932768109103897, 'observed': -0.4072880548316036}, {'date': '2014-01-01', 'trend': -0.13045227407867463, 'seasonal': 0.03051548559335375, 'resid': -0.02863724030070021, 'observed': -0.1285740287860211}, {'date': '2014-02-01', 'trend': -0.10572335453562173, 'seasonal': -0.009428661869998278, 'resid': -0.0732536665138444, 'observed': -0.1884056829194644}, {'date': '2014-03-01', 'trend': -0.11297566582606595, 'seasonal': -0.009181965409746102, 'resid': -0.03314186424436505, 'observed': -0.1552994954801771}, {'date': '2014-04-01', 'trend': -0.1340919811263499, 'seasonal': -0.014386009084824822, 'resid': 0.15963725107574123, 'observed': 0.0111592608645665}, {'date': '2014-05-01', 'trend': -0.15523672623366971, 'seasonal': -0.018321804588489938, 'resid': -0.3198670193074588, 'observed': -0.4934255501296185}, {'date': '2014-06-01', 'trend': -0.14757614909406946, 'seasonal': 0.006198504602688547, 'resid': -0.11203564233476958, 'observed': -0.2534132868261505}, {'date': '2014-07-01', 'trend': -0.12602150861736044, 'seasonal': -0.009292603974898495, 'resid': 0.15907194955803716, 'observed': 0.0237578369657782}, {'date': '2014-08-01', 'trend': -0.11995210145767261, 'seasonal': 0.023538262427489662, 'resid': 0.06616469755630375, 'observed': -0.0302491414738792}, {'date': '2014-09-01', 'trend': -0.11671427012356601, 'seasonal': 0.05848004070126249, 'resid': 0.05879383962979062, 'observed': 0.0005596102074871}, {'date': '2014-10-01', 'trend': -0.11272167572650926, 'seasonal': 0.042982962767393634, 'resid': 0.04000804400895212, 'observed': -0.0297306689501635}, {'date': '2014-11-01', 'trend': -0.0928386868162864, 'seasonal': -0.13074174163157376, 'resid': -0.07988684838908375, 'observed': -0.3034672768369439}, {'date': '2014-12-01', 'trend': -0.06289602463930324, 'seasonal': 0.02963753046734331, 'resid': -0.007104182524930561, 'observed': -0.0403626766968905}, {'date': '2015-01-01', 'trend': -0.05306681380896677, 'seasonal': 0.03051548559335375, 'resid': 0.04436329273589483, 'observed': 0.0218119645202818}, {'date': '2015-02-01', 'trend': -0.05349604629974409, 'seasonal': -0.009428661869998278, 'resid': -0.13020119622351675, 'observed': -0.1931259043932591}, {'date': '2015-03-01', 'trend': -0.053546988518803344, 'seasonal': -0.009181965409746102, 'resid': -0.01014236805927456, 'observed': -0.072871321987824}, {'date': '2015-04-01', 'trend': -0.05173856098857298, 'seasonal': -0.014386009084824822, 'resid': 0.0906779229749733, 'observed': 0.0245533529015755}, {'date': '2015-05-01', 'trend': -0.04497049651300695, 'seasonal': -0.018321804588489938, 'resid': 0.03366439278021779, 'observed': -0.0296279083212791}, {'date': '2015-06-01', 'trend': -0.03874878820579345, 'seasonal': 0.006198504602688547, 'resid': 0.03396324721621081, 'observed': 0.0014129636131059}, {'date': '2015-07-01', 'trend': -0.029666649630916878, 'seasonal': -0.009292603974898495, 'resid': 0.04379190006041227, 'observed': 0.0048326464545969}, {'date': '2015-08-01', 'trend': -0.006025474592698351, 'seasonal': 0.023538262427489662, 'resid': -0.03913831857614461, 'observed': -0.0216255307413533}, {'date': '2015-09-01', 'trend': 0.013596104455676111, 'seasonal': 0.05848004070126249, 'resid': -0.0813627589393993, 'observed': -0.0092866137824607}, {'date': '2015-10-01', 'trend': 0.01692957780197725, 'seasonal': 0.042982962767393634, 'resid': -0.03639472480405788, 'observed': 0.023517815765313}, {'date': '2015-11-01', 'trend': 0.01643604414209548, 'seasonal': -0.13074174163157376, 'resid': -0.07997651664935754, 'observed': -0.1942822141388358}, {'date': '2015-12-01', 'trend': 0.02757739963634078, 'seasonal': 0.02963753046734331, 'resid': -0.05744167012555859, 'observed': -0.0002267400218745}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.03051548559335375, 'resid': nan, 'observed': 0.1996473536423035}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.009428661869998278, 'resid': nan, 'observed': 0.196426907401964}, {'date': '2016-03-01', 'trend': nan, 'seasonal': -0.009181965409746102, 'resid': nan, 'observed': 0.00849376337794}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.014386009084824822, 'resid': nan, 'observed': 0.0231916278470389}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.018321804588489938, 'resid': nan, 'observed': -0.040110991103905}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.006198504602688547, 'resid': nan, 'observed': 0.2792885782576189}], 'diff1_loan_rate_D': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.04796336379262829, 'resid': nan, 'observed': -0.4674444444444443}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.01064043177203127, 'resid': nan, 'observed': 0.1696666666666679}, {'date': '2007-10-01', 'trend': nan, 'seasonal': 0.040188196506185894, 'resid': nan, 'observed': 0.3007017543859636}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.13132803383378105, 'resid': nan, 'observed': -0.1279239766081872}, {'date': '2007-12-01', 'trend': nan, 'seasonal': -0.050693663657927046, 'resid': nan, 'observed': -0.1305555555555546}, {'date': '2008-01-01', 'trend': nan, 'seasonal': -0.022104325250881424, 'resid': nan, 'observed': 0.5293111111111095}, {'date': '2008-02-01', 'trend': 0.07385787037037043, 'seasonal': 0.08221064578468242, 'resid': 0.07165740977087458, 'observed': 0.2277259259259274}, {'date': '2008-03-01', 'trend': 0.09349305555555552, 'seasonal': 0.046161442202006006, 'resid': -0.22960481392738952, 'observed': -0.089950316169828}, {'date': '2008-04-01', 'trend': 0.08309984520123835, 'seasonal': -0.025692635499406227, 'resid': 0.054878050107287774, 'observed': 0.1122852598091199}, {'date': '2008-05-01', 'trend': 0.09042138044616072, 'seasonal': -0.009329044731091838, 'resid': -0.36535320528028664, 'observed': -0.2842608695652178}, {'date': '2008-06-01', 'trend': 0.13210592677931376, 'seasonal': -0.0300039488367143, 'resid': 0.21431468872406756, 'observed': 0.316416666666667}, {'date': '2008-07-01', 'trend': 0.14880971204721205, 'seasonal': 0.04198757175226802, 'resid': -0.16801395046614717, 'observed': 0.0227833333333329}, {'date': '2008-08-01', 'trend': 0.1261103961798854, 'seasonal': 0.04796336379262829, 'resid': -0.02644042663918049, 'observed': 0.1476333333333332}, {'date': '2008-09-01', 'trend': 0.11906648408409015, 'seasonal': 0.01064043177203127, 'resid': -0.10387358252278851, 'observed': 0.0258333333333329}, {'date': '2008-10-01', 'trend': 0.12167145209381695, 'seasonal': 0.040188196506185894, 'resid': 0.03323839061568354, 'observed': 0.1950980392156864}, {'date': '2008-11-01', 'trend': 0.1262629603434582, 'seasonal': -0.13132803383378105, 'resid': 0.15846165793054953, 'observed': 0.1533965844402267}, {'date': '2008-12-01', 'trend': 0.1160520833333332, 'seasonal': -0.050693663657927046, 'resid': 0.5231945757162989, 'observed': 0.588552995391705}, {'date': '2009-01-01', 'trend': 0.1072321969696969, 'seasonal': -0.022104325250881424, 'resid': 0.12596553487459233, 'observed': 0.2110934065934078}, {'date': '2009-02-01', 'trend': 0.12594294576196746, 'seasonal': 0.08221064578468242, 'resid': -0.20699354191885957, 'observed': 0.0011600496277903}, {'date': '2009-03-01', 'trend': 0.13928118530020703, 'seasonal': 0.046161442202006006, 'resid': -0.21788095767298993, 'observed': -0.0324383301707769}, {'date': '2009-04-01', 'trend': 0.13767138626283845, 'seasonal': -0.025692635499406227, 'resid': 0.005313755280079668, 'observed': 0.1172925060435119}, {'date': '2009-05-01', 'trend': 0.1254146494349873, 'seasonal': -0.009329044731091838, 'resid': -0.2951575225121146, 'observed': -0.1790719178082191}, {'date': '2009-06-01', 'trend': 0.09078998616242023, 'seasonal': -0.0300039488367143, 'resid': -0.09461937065903793, 'observed': -0.033833333333332}, {'date': '2009-07-01', 'trend': 0.055669395456693874, 'seasonal': 0.04198757175226802, 'resid': 0.06369909339709831, 'observed': 0.1613560606060602}, {'date': '2009-08-01', 'trend': 0.04727055850255738, 'seasonal': 0.04796336379262829, 'resid': 0.36288465477991383, 'observed': 0.4581185770750995}, {'date': '2009-09-01', 'trend': 0.04948054502565673, 'seasonal': 0.01064043177203127, 'resid': -0.024655138288372395, 'observed': 0.0354658385093156}, {'date': '2009-10-01', 'trend': 0.04255634892211584, 'seasonal': 0.040188196506185894, 'resid': 0.06408581171455706, 'observed': 0.1468303571428588}, {'date': '2009-11-01', 'trend': 0.049111586757990926, 'seasonal': -0.13132803383378105, 'resid': -0.01028097027958308, 'observed': -0.0924974173553732}, {'date': '2009-12-01', 'trend': 0.07381842320261445, 'seasonal': -0.050693663657927046, 'resid': -0.019669680898992403, 'observed': 0.003455078645695}, {'date': '2010-01-01', 'trend': 0.08011500583218881, 'seasonal': -0.022104325250881424, 'resid': -0.10471353417932248, 'observed': -0.0467028535980151}, {'date': '2010-02-01', 'trend': 0.05376202226800264, 'seasonal': 0.08221064578468242, 'resid': -0.07858844513274756, 'observed': 0.0573842229199375}, {'date': '2010-03-01', 'trend': 0.03436319309464945, 'seasonal': 0.046161442202006006, 'resid': -0.11614746220519535, 'observed': -0.0356228269085399}, {'date': '2010-04-01', 'trend': 0.017476626779975968, 'seasonal': -0.025692635499406227, 'resid': -0.037487694984276135, 'observed': -0.0457037037037064}, {'date': '2010-05-01', 'trend': -0.01581361177595589, 'seasonal': -0.009329044731091838, 'resid': 0.16639265650704893, 'observed': 0.1412500000000012}, {'date': '2010-06-01', 'trend': -0.03246178649885558, 'seasonal': -0.0300039488367143, 'resid': 0.30127455886498217, 'observed': 0.2388088235294123}, {'date': '2010-07-01', 'trend': -0.020026136683425844, 'seasonal': 0.04198757175226802, 'resid': 0.017870451784258425, 'observed': 0.0398318868531006}, {'date': '2010-08-01', 'trend': 0.004130948162431072, 'seasonal': 0.04796336379262829, 'resid': -0.10492316666746845, 'observed': -0.0528288547124091}, {'date': '2010-09-01', 'trend': 0.012749065295937015, 'seasonal': 0.01064043177203127, 'resid': 0.05745187306837942, 'observed': 0.0808413701363477}, {'date': '2010-10-01', 'trend': 0.012119709451035108, 'seasonal': 0.040188196506185894, 'resid': -0.3561306719935578, 'observed': -0.3038227660363368}, {'date': '2010-11-01', 'trend': 0.05587205949157374, 'seasonal': -0.13132803383378105, 'resid': -0.3653540451763349, 'observed': -0.4408100195185422}, {'date': '2010-12-01', 'trend': 0.09050694421469359, 'seasonal': -0.050693663657927046, 'resid': -0.08760179309749513, 'observed': -0.0477885125407286}, {'date': '2011-01-01', 'trend': 0.07899627140878604, 'seasonal': -0.022104325250881424, 'resid': 0.2461043870008176, 'observed': 0.3029963331587222}, {'date': '2011-02-01', 'trend': 0.07878745905895457, 'seasonal': 0.08221064578468242, 'resid': 0.12645696762012915, 'observed': 0.2874550724637661}, {'date': '2011-03-01', 'trend': 0.08576555403344432, 'seasonal': 0.046161442202006006, 'resid': -0.19078586148367616, 'observed': -0.0588588652482258}, {'date': '2011-04-01', 'trend': 0.12693172699165314, 'seasonal': -0.025692635499406227, 'resid': -0.13881129713391321, 'observed': -0.0375722056416663}, {'date': '2011-05-01', 'trend': 0.18097642566228855, 'seasonal': -0.009329044731091838, 'resid': 1.0115275219796918, 'observed': 1.1831749029108884}, {'date': '2011-06-01', 'trend': 0.19989917890120262, 'seasonal': -0.0300039488367143, 'resid': -0.14177407609108672, 'observed': 0.0281211539734016}, {'date': '2011-07-01', 'trend': 0.1832074048859894, 'seasonal': 0.04198757175226802, 'resid': -0.25093156757092755, 'observed': -0.0257365909326701}, {'date': '2011-08-01', 'trend': 0.1668005792428685, 'seasonal': 0.04796336379262829, 'resid': -0.20703581635809032, 'observed': 0.0077281266774065}, {'date': '2011-09-01', 'trend': 0.20585194847521862, 'seasonal': 0.01064043177203127, 'resid': -0.02873371211296368, 'observed': 0.1877586681342862}, {'date': '2011-10-01', 'trend': 0.24093391480414197, 'seasonal': 0.040188196506185894, 'resid': 0.2961259756524087, 'observed': 0.5772480869627366}, {'date': '2011-11-01', 'trend': 0.19238724632812745, 'seasonal': -0.13132803383378105, 'resid': -0.0858673169167124, 'observed': -0.024808104422366}, {'date': '2011-12-01', 'trend': 0.14227929323333952, 'seasonal': -0.050693663657927046, 'resid': -0.10122997947837928, 'observed': -0.0096443499029668}, {'date': '2012-01-01', 'trend': 0.15618150765586591, 'seasonal': -0.022104325250881424, 'resid': -0.2698275882491408, 'observed': -0.1357504058441563}, {'date': '2012-02-01', 'trend': 0.17674864418415093, 'seasonal': 0.08221064578468242, 'resid': 0.07347870606290956, 'observed': 0.3324379960317429}, {'date': '2012-03-01', 'trend': 0.17171458824491456, 'seasonal': 0.046161442202006006, 'resid': 0.6155150423132797, 'observed': 0.8333910727602003}, {'date': '2012-04-01', 'trend': 0.1407052947991684, 'seasonal': -0.025692635499406227, 'resid': -0.20286761105569498, 'observed': -0.0878549517559328}, {'date': '2012-05-01', 'trend': 0.11797072752086618, 'seasonal': -0.009329044731091838, 'resid': -0.040304077188967946, 'observed': 0.0683376056008064}, {'date': '2012-06-01', 'trend': 0.11613740913534441, 'seasonal': -0.0300039488367143, 'resid': -0.1457658832900574, 'observed': -0.0596324229914273}, {'date': '2012-07-01', 'trend': 0.11817472858706453, 'seasonal': 0.04198757175226802, 'resid': 0.23550783183346014, 'observed': 0.3956701321727927}, {'date': '2012-08-01', 'trend': 0.10930929744230405, 'seasonal': 0.04796336379262829, 'resid': -0.07733998098414745, 'observed': 0.0799326802507849}, {'date': '2012-09-01', 'trend': 0.06433338145952333, 'seasonal': 0.01064043177203127, 'resid': -0.0802370412123206, 'observed': -0.005263227980766}, {'date': '2012-10-01', 'trend': 0.034702426696149306, 'seasonal': 0.040188196506185894, 'resid': -0.0488436828224537, 'observed': 0.0260469403798815}, {'date': '2012-11-01', 'trend': 0.03676718804759696, 'seasonal': -0.13132803383378105, 'resid': 0.07532427326741989, 'observed': -0.0192365725187642}, {'date': '2012-12-01', 'trend': 0.03853726412917851, 'seasonal': -0.050693663657927046, 'resid': -0.047059123530342556, 'observed': -0.0592155230590911}, {'date': '2013-01-01', 'trend': 0.025288611222311335, 'seasonal': -0.022104325250881424, 'resid': -0.040467851818179315, 'observed': -0.0372835658467494}, {'date': '2013-02-01', 'trend': 0.007680658465607946, 'seasonal': 0.08221064578468242, 'resid': -0.06869049569020547, 'observed': 0.0212008085600849}, {'date': '2013-03-01', 'trend': 0.008332600125998891, 'seasonal': 0.046161442202006006, 'resid': 0.010712234317116001, 'observed': 0.0652062766451209}, {'date': '2013-04-01', 'trend': 0.014738908100739245, 'seasonal': -0.025692635499406227, 'resid': -0.01985934256316332, 'observed': -0.0308130699618303}, {'date': '2013-05-01', 'trend': 0.019331139025670276, 'seasonal': -0.009329044731091838, 'resid': 0.050847901946869264, 'observed': 0.0608499962414477}, {'date': '2013-06-01', 'trend': 0.001970804296641291, 'seasonal': -0.0300039488367143, 'resid': 0.01837015686596121, 'observed': -0.0096629876741118}, {'date': '2013-07-01', 'trend': -0.02493018718334297, 'seasonal': 0.04198757175226802, 'resid': 0.01067564252174015, 'observed': 0.0277330270906652}, {'date': '2013-08-01', 'trend': -0.04010684759284679, 'seasonal': 0.04796336379262829, 'resid': 0.017422402972249493, 'observed': 0.025278919172031}, {'date': '2013-09-01', 'trend': -0.053070702835793224, 'seasonal': 0.01064043177203127, 'resid': 0.10746740401113256, 'observed': 0.0650371329473706}, {'date': '2013-10-01', 'trend': -0.0561201651993278, 'seasonal': 0.040188196506185894, 'resid': 0.12542993953865528, 'observed': 0.1094979708455134}, {'date': '2013-11-01', 'trend': -0.08952130837439813, 'seasonal': -0.13132803383378105, 'resid': 0.2283752814221278, 'observed': 0.0075259392139486}, {'date': '2013-12-01', 'trend': -0.14205340870375593, 'seasonal': -0.050693663657927046, 'resid': -0.30987899592681656, 'observed': -0.5026260682884995}, {'date': '2014-01-01', 'trend': -0.1583836455184818, 'seasonal': -0.022104325250881424, 'resid': -0.05900884536759997, 'observed': -0.2394968161369632}, {'date': '2014-02-01', 'trend': -0.15824506507128125, 'seasonal': 0.08221064578468242, 'resid': -0.06479137169119416, 'observed': -0.140825790977793}, {'date': '2014-03-01', 'trend': -0.16072772394039378, 'seasonal': 0.046161442202006006, 'resid': 0.03066663209067208, 'observed': -0.0838996496477157}, {'date': '2014-04-01', 'trend': -0.1713331784156777, 'seasonal': -0.025692635499406227, 'resid': 0.2421315735212604, 'observed': 0.0451057596061765}, {'date': '2014-05-01', 'trend': -0.18856976385452964, 'seasonal': -0.009329044731091838, 'resid': -0.6187974609426257, 'observed': -0.8166962695282471}, {'date': '2014-06-01', 'trend': -0.1798000829710844, 'seasonal': -0.0300039488367143, 'resid': -0.1830830980012057, 'observed': -0.3928871298090044}, {'date': '2014-07-01', 'trend': -0.15000924889187242, 'seasonal': 0.04198757175226802, 'resid': 0.1270531628117415, 'observed': 0.0190314856721371}, {'date': '2014-08-01', 'trend': -0.12548662217305767, 'seasonal': 0.04796336379262829, 'resid': 0.11482964970380218, 'observed': 0.0373063913233728}, {'date': '2014-09-01', 'trend': -0.10523706450073718, 'seasonal': 0.01064043177203127, 'resid': 0.08802248066603362, 'observed': -0.0065741520626723}, {'date': '2014-10-01', 'trend': -0.09922083949678266, 'seasonal': 0.040188196506185894, 'resid': -0.014389008560660332, 'observed': -0.0734216515512571}, {'date': '2014-11-01', 'trend': -0.06476193929068524, 'seasonal': -0.13132803383378105, 'resid': -0.027142515797261096, 'observed': -0.2232324889217274}, {'date': '2014-12-01', 'trend': -0.014173055074311044, 'seasonal': -0.050693663657927046, 'resid': 0.003471419782099597, 'observed': -0.0613952989501385}, {'date': '2015-01-01', 'trend': -0.0005362311660852649, 'seasonal': -0.022104325250881424, 'resid': 0.05689298884272978, 'observed': 0.0342524324257631}, {'date': '2015-02-01', 'trend': -0.0042220906000517075, 'seasonal': 0.08221064578468242, 'resid': 0.09597944652640449, 'observed': 0.1739680017110352}, {'date': '2015-03-01', 'trend': -0.004899362518352068, 'seasonal': 0.046161442202006006, 'resid': 0.046033862115494165, 'observed': 0.0872959417991481}, {'date': '2015-04-01', 'trend': -0.001219038622103299, 'seasonal': -0.025692635499406227, 'resid': 0.04521124237573082, 'observed': 0.0182995682542213}, {'date': '2015-05-01', 'trend': 0.007099710463441603, 'seasonal': -0.009329044731091838, 'resid': 0.039352861037695835, 'observed': 0.0371235267700456}, {'date': '2015-06-01', 'trend': 0.017638544788709968, 'seasonal': -0.0300039488367143, 'resid': -0.020208300866312073, 'observed': -0.0325737049143164}, {'date': '2015-07-01', 'trend': 0.03136625158651259, 'seasonal': 0.04198757175226802, 'resid': -0.08735198876391281, 'observed': -0.0139981654251322}, {'date': '2015-08-01', 'trend': 0.06480069347258703, 'seasonal': 0.04796336379262829, 'resid': -0.13088864125976782, 'observed': -0.0181245839945525}, {'date': '2015-09-01', 'trend': 0.08889547350580251, 'seasonal': 0.01064043177203127, 'resid': -0.06693360806178938, 'observed': 0.0326022972160444}, {'date': '2015-10-01', 'trend': 0.08654955477113249, 'seasonal': 0.040188196506185894, 'resid': -0.1510080785973219, 'observed': -0.0242703273200035}, {'date': '2015-11-01', 'trend': 0.08360188763677234, 'seasonal': -0.13132803383378105, 'resid': -0.025007688902894593, 'observed': -0.0727338350999033}, {'date': '2015-12-01', 'trend': 0.1044494817114668, 'seasonal': -0.050693663657927046, 'resid': -0.01271774701906156, 'observed': 0.0410380710344782}, {'date': '2016-01-01', 'trend': nan, 'seasonal': -0.022104325250881424, 'resid': nan, 'observed': 0.2612840255884094}, {'date': '2016-02-01', 'trend': nan, 'seasonal': 0.08221064578468242, 'resid': nan, 'observed': 0.7493630138141754}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.046161442202006006, 'resid': nan, 'observed': 0.0901756504931796}, {'date': '2016-04-01', 'trend': nan, 'seasonal': -0.025692635499406227, 'resid': nan, 'observed': -0.0408821900718905}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.009329044731091838, 'resid': nan, 'observed': 0.0255612738715136}, {'date': '2016-06-01', 'trend': nan, 'seasonal': -0.0300039488367143, 'resid': nan, 'observed': 0.4793308057768826}], 'diff1_FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': 0.03606088789682545, 'resid': nan, 'observed': -0.2400000000000002}, {'date': '2007-09-01', 'trend': nan, 'seasonal': 0.00277963789682541, 'resid': nan, 'observed': -0.0799999999999991}, {'date': '2007-10-01', 'trend': nan, 'seasonal': -0.08331411210317463, 'resid': nan, 'observed': -0.1800000000000006}, {'date': '2007-11-01', 'trend': nan, 'seasonal': -0.05039744543650794, 'resid': nan, 'observed': -0.2699999999999996}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.009342137896825394, 'resid': nan, 'observed': -0.25}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.01609064980158735, 'resid': nan, 'observed': -0.3000000000000002}, {'date': '2008-02-01', 'trend': -0.2612499999999999, 'seasonal': -0.05034536210317461, 'resid': -0.6484046378968253, 'observed': -0.96}, {'date': '2008-03-01', 'trend': -0.2562499999999999, 'seasonal': 0.007935887896825382, 'resid': -0.12168588789682556, 'observed': -0.3700000000000001}, {'date': '2008-04-01', 'trend': -0.2883333333333333, 'seasonal': 0.012935887896825347, 'resid': -0.054602554563492084, 'observed': -0.33}, {'date': '2008-05-01', 'trend': -0.32875, 'seasonal': 0.013196304563492082, 'resid': 0.015553695436508082, 'observed': -0.2999999999999998}, {'date': '2008-06-01', 'trend': -0.3408333333333333, 'seasonal': 0.05111297123015873, 'resid': 0.3097203621031746, 'observed': 0.02}, {'date': '2008-07-01', 'trend': -0.32791666666666663, 'seasonal': 0.03460255456349204, 'resid': 0.30331411210317427, 'observed': 0.0099999999999997}, {'date': '2008-08-01', 'trend': -0.27291666666666664, 'seasonal': 0.03606088789682545, 'resid': 0.22685577876984153, 'observed': -0.0099999999999997}, {'date': '2008-09-01', 'trend': -0.21625, 'seasonal': 0.00277963789682541, 'resid': 0.023470362103174696, 'observed': -0.1899999999999999}, {'date': '2008-10-01', 'trend': -0.18999999999999997, 'seasonal': -0.08331411210317463, 'resid': -0.5666858878968255, 'observed': -0.8400000000000001}, {'date': '2008-11-01', 'trend': -0.16374999999999998, 'seasonal': -0.05039744543650794, 'resid': -0.3658525545634921, 'observed': -0.58}, {'date': '2008-12-01', 'trend': -0.14958333333333332, 'seasonal': 0.009342137896825394, 'resid': -0.08975880456349208, 'observed': -0.23}, {'date': '2009-01-01', 'trend': -0.15166666666666664, 'seasonal': 0.01609064980158735, 'resid': 0.1255760168650793, 'observed': -0.01}, {'date': '2009-02-01', 'trend': -0.15374999999999994, 'seasonal': -0.05034536210317461, 'resid': 0.27409536210317453, 'observed': 0.07}, {'date': '2009-03-01', 'trend': -0.14583333333333331, 'seasonal': 0.007935887896825382, 'resid': 0.09789744543650793, 'observed': -0.04}, {'date': '2009-04-01', 'trend': -0.1045833333333333, 'seasonal': 0.012935887896825347, 'resid': 0.061647445436507964, 'observed': -0.03}, {'date': '2009-05-01', 'trend': -0.04666666666666665, 'seasonal': 0.013196304563492082, 'resid': 0.06347036210317457, 'observed': 0.03}, {'date': '2009-06-01', 'trend': -0.012916666666666656, 'seasonal': 0.05111297123015873, 'resid': -0.008196304563492074, 'observed': 0.03}, {'date': '2009-07-01', 'trend': -0.00333333333333332, 'seasonal': 0.03460255456349204, 'resid': -0.08126922123015862, 'observed': -0.0499999999999999}, {'date': '2009-08-01', 'trend': -0.0054166666666666495, 'seasonal': 0.03606088789682545, 'resid': -0.030644221230158798, 'observed': 0.0}, {'date': '2009-09-01', 'trend': -0.004583333333333315, 'seasonal': 0.00277963789682541, 'resid': -0.008196304563492095, 'observed': -0.01}, {'date': '2009-10-01', 'trend': 0.0012500000000000163, 'seasonal': -0.08331411210317463, 'resid': 0.05206411210317462, 'observed': -0.03}, {'date': '2009-11-01', 'trend': 0.002916666666666683, 'seasonal': -0.05039744543650794, 'resid': 0.047480778769841255, 'observed': 0.0}, {'date': '2009-12-01', 'trend': -0.00041666666666665005, 'seasonal': 0.009342137896825394, 'resid': -0.008925471230158744, 'observed': 0.0}, {'date': '2010-01-01', 'trend': -0.00041666666666665374, 'seasonal': 0.01609064980158735, 'resid': -0.025673983134920596, 'observed': -0.0099999999999999}, {'date': '2010-02-01', 'trend': 0.0020833333333333415, 'seasonal': -0.05034536210317461, 'resid': 0.06826202876984128, 'observed': 0.02}, {'date': '2010-03-01', 'trend': 0.002916666666666675, 'seasonal': 0.007935887896825382, 'resid': 0.01914744543650794, 'observed': 0.03}, {'date': '2010-04-01', 'trend': 0.004583333333333341, 'seasonal': 0.012935887896825347, 'resid': 0.022480778769841312, 'observed': 0.04}, {'date': '2010-05-01', 'trend': 0.0058333333333333405, 'seasonal': 0.013196304563492082, 'resid': -0.01902963789682542, 'observed': 0.0}, {'date': '2010-06-01', 'trend': 0.005416666666666674, 'seasonal': 0.05111297123015873, 'resid': -0.0765296378968254, 'observed': -0.02}, {'date': '2010-07-01', 'trend': 0.005000000000000007, 'seasonal': 0.03460255456349204, 'resid': -0.03960255456349204, 'observed': 0.0}, {'date': '2010-08-01', 'trend': 0.0037500000000000077, 'seasonal': 0.03606088789682545, 'resid': -0.029810887896825455, 'observed': 0.01}, {'date': '2010-09-01', 'trend': 0.00041666666666667894, 'seasonal': 0.00277963789682541, 'resid': -0.003196304563492089, 'observed': 0.0}, {'date': '2010-10-01', 'trend': -0.004999999999999984, 'seasonal': -0.08331411210317463, 'resid': 0.08831411210317461, 'observed': 0.0}, {'date': '2010-11-01', 'trend': -0.008749999999999982, 'seasonal': -0.05039744543650794, 'resid': 0.05914744543650792, 'observed': 0.0}, {'date': '2010-12-01', 'trend': -0.008333333333333316, 'seasonal': 0.009342137896825394, 'resid': -0.011008804563492078, 'observed': -0.01}, {'date': '2011-01-01', 'trend': -0.008333333333333312, 'seasonal': 0.01609064980158735, 'resid': -0.017757316468253938, 'observed': -0.0099999999999999}, {'date': '2011-02-01', 'trend': -0.008333333333333309, 'seasonal': -0.05034536210317461, 'resid': 0.04867869543650792, 'observed': -0.01}, {'date': '2011-03-01', 'trend': -0.008333333333333307, 'seasonal': 0.007935887896825382, 'resid': -0.019602554563491977, 'observed': -0.0199999999999999}, {'date': '2011-04-01', 'trend': -0.009583333333333305, 'seasonal': 0.012935887896825347, 'resid': -0.043352554563492046, 'observed': -0.04}, {'date': '2011-05-01', 'trend': -0.009583333333333305, 'seasonal': 0.013196304563492082, 'resid': -0.013612971230158777, 'observed': -0.01}, {'date': '2011-06-01', 'trend': -0.009166666666666639, 'seasonal': 0.05111297123015873, 'resid': -0.04194630456349209, 'observed': 0.0}, {'date': '2011-07-01', 'trend': -0.008333333333333309, 'seasonal': 0.03460255456349204, 'resid': -0.04626922123015863, 'observed': -0.0199999999999999}, {'date': '2011-08-01', 'trend': -0.006249999999999984, 'seasonal': 0.03606088789682545, 'resid': 0.00018911210317453658, 'observed': 0.03}, {'date': '2011-09-01', 'trend': -0.002916666666666654, 'seasonal': 0.00277963789682541, 'resid': -0.019862971230158757, 'observed': -0.02}, {'date': '2011-10-01', 'trend': 0.001250000000000007, 'seasonal': -0.08331411210317463, 'resid': 0.07206411210317473, 'observed': -0.0099999999999999}, {'date': '2011-11-01', 'trend': 0.004583333333333336, 'seasonal': -0.05039744543650794, 'resid': 0.0558141121031745, 'observed': 0.0099999999999999}, {'date': '2011-12-01', 'trend': 0.005833333333333333, 'seasonal': 0.009342137896825394, 'resid': -0.025175471230158626, 'observed': -0.0099999999999999}, {'date': '2012-01-01', 'trend': 0.006666666666666661, 'seasonal': 0.01609064980158735, 'resid': -0.012757316468254112, 'observed': 0.0099999999999999}, {'date': '2012-02-01', 'trend': 0.004999999999999991, 'seasonal': -0.05034536210317461, 'resid': 0.06534536210317463, 'observed': 0.02}, {'date': '2012-03-01', 'trend': 0.003749999999999992, 'seasonal': 0.007935887896825382, 'resid': 0.018314112103174622, 'observed': 0.03}, {'date': '2012-04-01', 'trend': 0.006249999999999983, 'seasonal': 0.012935887896825347, 'resid': -0.00918588789682533, 'observed': 0.01}, {'date': '2012-05-01', 'trend': 0.007083333333333312, 'seasonal': 0.013196304563492082, 'resid': -0.00027963789682549506, 'observed': 0.0199999999999999}, {'date': '2012-06-01', 'trend': 0.007083333333333312, 'seasonal': 0.05111297123015873, 'resid': -0.05819630456349204, 'observed': 0.0}, {'date': '2012-07-01', 'trend': 0.006249999999999983, 'seasonal': 0.03460255456349204, 'resid': -0.04085255456349202, 'observed': 0.0}, {'date': '2012-08-01', 'trend': 0.004583333333333321, 'seasonal': 0.03606088789682545, 'resid': -0.07064422123015876, 'observed': -0.03}, {'date': '2012-09-01', 'trend': 0.0024999999999999875, 'seasonal': 0.00277963789682541, 'resid': 0.004720362103174603, 'observed': 0.01}, {'date': '2012-10-01', 'trend': 0.0008333333333333214, 'seasonal': -0.08331411210317463, 'resid': 0.10248077876984121, 'observed': 0.0199999999999999}, {'date': '2012-11-01', 'trend': -0.0016666666666666746, 'seasonal': -0.05039744543650794, 'resid': 0.05206411210317462, 'observed': 0.0}, {'date': '2012-12-01', 'trend': -0.005, 'seasonal': 0.009342137896825394, 'resid': -0.004342137896825394, 'observed': 0.0}, {'date': '2013-01-01', 'trend': -0.005833333333333333, 'seasonal': 0.01609064980158735, 'resid': -0.030257316468253918, 'observed': -0.0199999999999999}, {'date': '2013-02-01', 'trend': -0.004999999999999996, 'seasonal': -0.05034536210317461, 'resid': 0.06534536210317451, 'observed': 0.0099999999999999}, {'date': '2013-03-01', 'trend': -0.004583333333333325, 'seasonal': 0.007935887896825382, 'resid': -0.013352554563491957, 'observed': -0.0099999999999999}, {'date': '2013-04-01', 'trend': -0.005416666666666658, 'seasonal': 0.012935887896825347, 'resid': 0.002480778769841211, 'observed': 0.0099999999999999}, {'date': '2013-05-01', 'trend': -0.0062499999999999865, 'seasonal': 0.013196304563492082, 'resid': -0.046946304563492, 'observed': -0.0399999999999999}, {'date': '2013-06-01', 'trend': -0.0062499999999999865, 'seasonal': 0.05111297123015873, 'resid': -0.06486297123015874, 'observed': -0.02}, {'date': '2013-07-01', 'trend': -0.005833333333333324, 'seasonal': 0.03460255456349204, 'resid': -0.028769221230158716, 'observed': 0.0}, {'date': '2013-08-01', 'trend': -0.0062499999999999865, 'seasonal': 0.03606088789682545, 'resid': -0.03981088789682536, 'observed': -0.0099999999999999}, {'date': '2013-09-01', 'trend': -0.005833333333333324, 'seasonal': 0.00277963789682541, 'resid': 0.003053695436507914, 'observed': 0.0}, {'date': '2013-10-01', 'trend': -0.004999999999999999, 'seasonal': -0.08331411210317463, 'resid': 0.09831411210317453, 'observed': 0.0099999999999999}, {'date': '2013-11-01', 'trend': -0.0033333333333333366, 'seasonal': -0.05039744543650794, 'resid': 0.043730778769841376, 'observed': -0.0099999999999999}, {'date': '2013-12-01', 'trend': -0.0004166666666666752, 'seasonal': 0.009342137896825394, 'resid': 0.0010745287698411803, 'observed': 0.0099999999999999}, {'date': '2014-01-01', 'trend': 0.0004166666666666582, 'seasonal': 0.01609064980158735, 'resid': -0.036507316468253906, 'observed': -0.0199999999999999}, {'date': '2014-02-01', 'trend': 0.00041666666666665374, 'seasonal': -0.05034536210317461, 'resid': 0.04992869543650796, 'observed': 0.0}, {'date': '2014-03-01', 'trend': 0.0008333333333333161, 'seasonal': 0.007935887896825382, 'resid': 0.0012307787698412013, 'observed': 0.0099999999999999}, {'date': '2014-04-01', 'trend': 0.00041666666666665374, 'seasonal': 0.012935887896825347, 'resid': -0.003352554563492101, 'observed': 0.0099999999999999}, {'date': '2014-05-01', 'trend': 0.00041666666666665374, 'seasonal': 0.013196304563492082, 'resid': -0.013612971230158736, 'observed': 0.0}, {'date': '2014-06-01', 'trend': 0.001666666666666654, 'seasonal': 0.05111297123015873, 'resid': -0.04277963789682539, 'observed': 0.01}, {'date': '2014-07-01', 'trend': 0.002916666666666658, 'seasonal': 0.03460255456349204, 'resid': -0.0475192212301587, 'observed': -0.01}, {'date': '2014-08-01', 'trend': 0.0033333333333333244, 'seasonal': 0.03606088789682545, 'resid': -0.039394221230158774, 'observed': 0.0}, {'date': '2014-09-01', 'trend': 0.002916666666666662, 'seasonal': 0.00277963789682541, 'resid': -0.005696304563492072, 'observed': 0.0}, {'date': '2014-10-01', 'trend': 0.0024999999999999996, 'seasonal': -0.08331411210317463, 'resid': 0.08081411210317463, 'observed': 0.0}, {'date': '2014-11-01', 'trend': 0.0024999999999999996, 'seasonal': -0.05039744543650794, 'resid': 0.04789744543650794, 'observed': 0.0}, {'date': '2014-12-01', 'trend': 0.0024999999999999996, 'seasonal': 0.009342137896825394, 'resid': 0.018157862103174605, 'observed': 0.03}, {'date': '2015-01-01', 'trend': 0.0029166666666666664, 'seasonal': 0.01609064980158735, 'resid': -0.029007316468253917, 'observed': -0.0099999999999999}, {'date': '2015-02-01', 'trend': 0.00375, 'seasonal': -0.05034536210317461, 'resid': 0.04659536210317461, 'observed': 0.0}, {'date': '2015-03-01', 'trend': 0.004166666666666667, 'seasonal': 0.007935887896825382, 'resid': -0.01210255456349205, 'observed': 0.0}, {'date': '2015-04-01', 'trend': 0.003333333333333333, 'seasonal': 0.012935887896825347, 'resid': -0.00626922123015878, 'observed': 0.0099999999999999}, {'date': '2015-05-01', 'trend': 0.0025, 'seasonal': 0.013196304563492082, 'resid': -0.01569630456349208, 'observed': 0.0}, {'date': '2015-06-01', 'trend': 0.0062499999999999995, 'seasonal': 0.05111297123015873, 'resid': -0.04736297123015873, 'observed': 0.01}, {'date': '2015-07-01', 'trend': 0.014583333333333328, 'seasonal': 0.03460255456349204, 'resid': -0.04918588789682537, 'observed': 0.0}, {'date': '2015-08-01', 'trend': 0.020833333333333322, 'seasonal': 0.03606088789682545, 'resid': -0.04689422123015877, 'observed': 0.01}, {'date': '2015-09-01', 'trend': 0.021666666666666647, 'seasonal': 0.00277963789682541, 'resid': -0.024446304563492057, 'observed': 0.0}, {'date': '2015-10-01', 'trend': 0.02083333333333332, 'seasonal': -0.08331411210317463, 'resid': 0.04248077876984131, 'observed': -0.02}, {'date': '2015-11-01', 'trend': 0.020833333333333322, 'seasonal': -0.05039744543650794, 'resid': 0.029564112103174618, 'observed': 0.0}, {'date': '2015-12-01', 'trend': 0.020833333333333322, 'seasonal': 0.009342137896825394, 'resid': 0.08982452876984129, 'observed': 0.12}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.01609064980158735, 'resid': nan, 'observed': 0.1}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.05034536210317461, 'resid': nan, 'observed': 0.0399999999999999}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.007935887896825382, 'resid': nan, 'observed': -0.02}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.012935887896825347, 'resid': nan, 'observed': 0.01}, {'date': '2016-05-01', 'trend': nan, 'seasonal': 0.013196304563492082, 'resid': nan, 'observed': 0.0}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.05111297123015873, 'resid': nan, 'observed': 0.01}], 'diff2_FEDFUNDS': [{'date': '2007-08-01', 'trend': nan, 'seasonal': -8.8045634920585e-05, 'resid': nan, 'observed': -0.25}, {'date': '2007-09-01', 'trend': nan, 'seasonal': -0.034827628968254, 'resid': nan, 'observed': 0.160000000000001}, {'date': '2007-10-01', 'trend': nan, 'seasonal': -0.087640128968254, 'resid': nan, 'observed': -0.1000000000000014}, {'date': '2007-11-01', 'trend': nan, 'seasonal': 0.03137028769841272, 'resid': nan, 'observed': -0.0899999999999989}, {'date': '2007-12-01', 'trend': nan, 'seasonal': 0.05819320436507936, 'resid': nan, 'observed': 0.0199999999999995}, {'date': '2008-01-01', 'trend': nan, 'seasonal': 0.018572668650793697, 'resid': nan, 'observed': -0.0500000000000002}, {'date': '2008-02-01', 'trend': 0.009583333333333338, 'seasonal': -0.06279637896825398, 'resid': -0.6067869543650791, 'observed': -0.6599999999999997}, {'date': '2008-03-01', 'trend': 0.004999999999999993, 'seasonal': 0.056734871031745994, 'resid': 0.5282651289682538, 'observed': 0.5899999999999999}, {'date': '2008-04-01', 'trend': -0.03208333333333333, 'seasonal': 0.0034536210317460195, 'resid': 0.06862971230158732, 'observed': 0.04}, {'date': '2008-05-01', 'trend': -0.040416666666666656, 'seasonal': -0.001285962301587255, 'resid': 0.0717026289682541, 'observed': 0.0300000000000002}, {'date': '2008-06-01', 'trend': -0.012083333333333354, 'seasonal': 0.03637028769841267, 'resid': 0.29571304563492046, 'observed': 0.3199999999999998}, {'date': '2008-07-01', 'trend': 0.012916666666666684, 'seasonal': -0.018056795634920637, 'resid': -0.004859871031746246, 'observed': -0.0100000000000002}, {'date': '2008-08-01', 'trend': 0.055, 'seasonal': -8.8045634920585e-05, 'resid': -0.07491195436507891, 'observed': -0.0199999999999995}, {'date': '2008-09-01', 'trend': 0.056666666666666664, 'seasonal': -0.034827628968254, 'resid': -0.20183903769841277, 'observed': -0.1800000000000001}, {'date': '2008-10-01', 'trend': 0.026250000000000006, 'seasonal': -0.087640128968254, 'resid': -0.5886098710317461, 'observed': -0.6500000000000001}, {'date': '2008-11-01', 'trend': 0.02625, 'seasonal': 0.03137028769841272, 'resid': 0.2023797123015874, 'observed': 0.2600000000000001}, {'date': '2008-12-01', 'trend': 0.014166666666666671, 'seasonal': 0.05819320436507936, 'resid': 0.27764012896825396, 'observed': 0.35}, {'date': '2009-01-01', 'trend': -0.0020833333333333073, 'seasonal': 0.018572668650793697, 'resid': 0.2035106646825396, 'observed': 0.22}, {'date': '2009-02-01', 'trend': -0.0020833333333333194, 'seasonal': -0.06279637896825398, 'resid': 0.1448797123015873, 'observed': 0.08}, {'date': '2009-03-01', 'trend': 0.007916666666666655, 'seasonal': 0.056734871031745994, 'resid': -0.17465153769841266, 'observed': -0.11}, {'date': '2009-04-01', 'trend': 0.04125, 'seasonal': 0.0034536210317460195, 'resid': -0.03470362103174602, 'observed': 0.01}, {'date': '2009-05-01', 'trend': 0.05791666666666667, 'seasonal': -0.001285962301587255, 'resid': 0.003369295634920581, 'observed': 0.06}, {'date': '2009-06-01', 'trend': 0.03375, 'seasonal': 0.03637028769841267, 'resid': -0.07012028769841266, 'observed': 0.0}, {'date': '2009-07-01', 'trend': 0.009583333333333345, 'seasonal': -0.018056795634920637, 'resid': -0.0715265376984126, 'observed': -0.0799999999999999}, {'date': '2009-08-01', 'trend': -0.0020833333333333186, 'seasonal': -8.8045634920585e-05, 'resid': 0.0521713789682538, 'observed': 0.0499999999999999}, {'date': '2009-09-01', 'trend': 0.0008333333333333439, 'seasonal': -0.034827628968254, 'resid': 0.023994295634920653, 'observed': -0.01}, {'date': '2009-10-01', 'trend': 0.00583333333333334, 'seasonal': -0.087640128968254, 'resid': 0.061806795634920766, 'observed': -0.0199999999999999}, {'date': '2009-11-01', 'trend': 0.0016666666666666735, 'seasonal': 0.03137028769841272, 'resid': -0.0030369543650793952, 'observed': 0.03}, {'date': '2009-12-01', 'trend': -0.0033333333333333253, 'seasonal': 0.05819320436507936, 'resid': -0.054859871031746034, 'observed': 0.0}, {'date': '2010-01-01', 'trend': 3.2786273695961652e-18, 'seasonal': 0.018572668650793697, 'resid': -0.028572668650793602, 'observed': -0.0099999999999999}, {'date': '2010-02-01', 'trend': 0.002500000000000003, 'seasonal': -0.06279637896825398, 'resid': 0.09029637896825397, 'observed': 0.03}, {'date': '2010-03-01', 'trend': 0.000833333333333341, 'seasonal': 0.056734871031745994, 'resid': -0.047568204365079435, 'observed': 0.0099999999999999}, {'date': '2010-04-01', 'trend': 0.0016666666666666705, 'seasonal': 0.0034536210317460195, 'resid': 0.00487971230158731, 'observed': 0.01}, {'date': '2010-05-01', 'trend': 0.0012499999999999996, 'seasonal': -0.001285962301587255, 'resid': -0.039964037698412745, 'observed': -0.04}, {'date': '2010-06-01', 'trend': -0.00041666666666666664, 'seasonal': 0.03637028769841267, 'resid': -0.055953621031746004, 'observed': -0.02}, {'date': '2010-07-01', 'trend': -0.00041666666666666973, 'seasonal': -0.018056795634920637, 'resid': 0.03847346230158731, 'observed': 0.02}, {'date': '2010-08-01', 'trend': -0.0012500000000000074, 'seasonal': -8.8045634920585e-05, 'resid': 0.011338045634920593, 'observed': 0.01}, {'date': '2010-09-01', 'trend': -0.003333333333333333, 'seasonal': -0.034827628968254, 'resid': 0.02816096230158733, 'observed': -0.01}, {'date': '2010-10-01', 'trend': -0.005416666666666658, 'seasonal': -0.087640128968254, 'resid': 0.09305679563492066, 'observed': 0.0}, {'date': '2010-11-01', 'trend': -0.0037499999999999916, 'seasonal': 0.03137028769841272, 'resid': -0.027620287698412727, 'observed': 0.0}, {'date': '2010-12-01', 'trend': 0.00041666666666667445, 'seasonal': 0.05819320436507936, 'resid': -0.06860987103174604, 'observed': -0.01}, {'date': '2011-01-01', 'trend': 1.2189323624530364e-17, 'seasonal': 0.018572668650793697, 'resid': -0.018572668650793683, 'observed': 2.775557561562892e-17}, {'date': '2011-02-01', 'trend': 1.2171976389770596e-17, 'seasonal': -0.06279637896825398, 'resid': 0.06279637896825394, 'observed': -2.775557561562892e-17}, {'date': '2011-03-01', 'trend': 8.211024452956887e-18, 'seasonal': 0.056734871031745994, 'resid': -0.0667348710317459, 'observed': -0.0099999999999999}, {'date': '2011-04-01', 'trend': -0.0012499999999999924, 'seasonal': 0.0034536210317460195, 'resid': -0.02220362103174603, 'observed': -0.02}, {'date': '2011-05-01', 'trend': 3.4231876592608994e-18, 'seasonal': -0.001285962301587255, 'resid': 0.03128596230158725, 'observed': 0.03}, {'date': '2011-06-01', 'trend': 0.00041666666666667027, 'seasonal': 0.03637028769841267, 'resid': -0.02678695436507934, 'observed': 0.01}, {'date': '2011-07-01', 'trend': 0.0008333333333333352, 'seasonal': -0.018056795634920637, 'resid': -0.0027765376984125976, 'observed': -0.0199999999999999}, {'date': '2011-08-01', 'trend': 0.002083333333333332, 'seasonal': -8.8045634920585e-05, 'resid': 0.04800471230158715, 'observed': 0.0499999999999999}, {'date': '2011-09-01', 'trend': 0.0033333333333333244, 'seasonal': -0.034827628968254, 'resid': -0.018505704365079333, 'observed': -0.05}, {'date': '2011-10-01', 'trend': 0.004166666666666654, 'seasonal': -0.087640128968254, 'resid': 0.09347346230158735, 'observed': 0.01}, {'date': '2011-11-01', 'trend': 0.003333333333333321, 'seasonal': 0.03137028769841272, 'resid': -0.01470362103174614, 'observed': 0.0199999999999999}, {'date': '2011-12-01', 'trend': 0.0012499999999999872, 'seasonal': 0.05819320436507936, 'resid': -0.07944320436507925, 'observed': -0.0199999999999999}, {'date': '2012-01-01', 'trend': 0.0008333333333333207, 'seasonal': 0.018572668650793697, 'resid': 0.0005939980158728812, 'observed': 0.0199999999999999}, {'date': '2012-02-01', 'trend': -0.0016666666666666794, 'seasonal': -0.06279637896825398, 'resid': 0.07446304563492065, 'observed': 0.01}, {'date': '2012-03-01', 'trend': -0.001250000000000009, 'seasonal': 0.056734871031745994, 'resid': -0.04548487103174609, 'observed': 0.0099999999999999}, {'date': '2012-04-01', 'trend': 0.0024999999999999883, 'seasonal': 0.0034536210317460195, 'resid': -0.025953621031745908, 'observed': -0.0199999999999999}, {'date': '2012-05-01', 'trend': 0.0008333333333333248, 'seasonal': -0.001285962301587255, 'resid': 0.01045262896825383, 'observed': 0.0099999999999999}, {'date': '2012-06-01', 'trend': -4.336808689942018e-18, 'seasonal': 0.03637028769841267, 'resid': -0.05637028769841257, 'observed': -0.0199999999999999}, {'date': '2012-07-01', 'trend': -0.0008333333333333335, 'seasonal': -0.018056795634920637, 'resid': 0.01889012896825397, 'observed': 0.0}, {'date': '2012-08-01', 'trend': -0.0016666666666666629, 'seasonal': -8.8045634920585e-05, 'resid': -0.028245287698412752, 'observed': -0.03}, {'date': '2012-09-01', 'trend': -0.0020833333333333255, 'seasonal': -0.034827628968254, 'resid': 0.07691096230158732, 'observed': 0.04}, {'date': '2012-10-01', 'trend': -0.0016666666666666588, 'seasonal': -0.087640128968254, 'resid': 0.09930679563492056, 'observed': 0.0099999999999999}, {'date': '2012-11-01', 'trend': -0.002499999999999992, 'seasonal': 0.03137028769841272, 'resid': -0.04887028769841263, 'observed': -0.0199999999999999}, {'date': '2012-12-01', 'trend': -0.0033333333333333244, 'seasonal': 0.05819320436507936, 'resid': -0.054859871031746034, 'observed': 0.0}, {'date': '2013-01-01', 'trend': -0.0008333333333333337, 'seasonal': 0.018572668650793697, 'resid': -0.037739335317460265, 'observed': -0.0199999999999999}, {'date': '2013-02-01', 'trend': 0.0008333333333333378, 'seasonal': -0.06279637896825398, 'resid': 0.09196304563492054, 'observed': 0.0299999999999999}, {'date': '2013-03-01', 'trend': 0.00041666666666667114, 'seasonal': 0.056734871031745994, 'resid': -0.07715153769841257, 'observed': -0.0199999999999999}, {'date': '2013-04-01', 'trend': -0.0008333333333333318, 'seasonal': 0.0034536210317460195, 'resid': 0.017379712301587212, 'observed': 0.0199999999999999}, {'date': '2013-05-01', 'trend': -0.0008333333333333326, 'seasonal': -0.001285962301587255, 'resid': -0.04788070436507931, 'observed': -0.0499999999999999}, {'date': '2013-06-01', 'trend': -4.383067982634732e-18, 'seasonal': 0.03637028769841267, 'resid': -0.016370287698412766, 'observed': 0.0199999999999999}, {'date': '2013-07-01', 'trend': 0.00041666666666665916, 'seasonal': -0.018056795634920637, 'resid': 0.03764012896825398, 'observed': 0.02}, {'date': '2013-08-01', 'trend': -0.0004166666666666755, 'seasonal': -8.8045634920585e-05, 'resid': -0.009495287698412638, 'observed': -0.0099999999999999}, {'date': '2013-09-01', 'trend': 0.0004166666666666503, 'seasonal': -0.034827628968254, 'resid': 0.044410962301587247, 'observed': 0.0099999999999999}, {'date': '2013-10-01', 'trend': 0.0008333333333333127, 'seasonal': -0.087640128968254, 'resid': 0.09680679563492059, 'observed': 0.0099999999999999}, {'date': '2013-11-01', 'trend': 0.00166666666666665, 'seasonal': 0.03137028769841272, 'resid': -0.05303695436507927, 'observed': -0.0199999999999999}, {'date': '2013-12-01', 'trend': 0.0029166666666666542, 'seasonal': 0.05819320436507936, 'resid': -0.04110987103174611, 'observed': 0.0199999999999999}, {'date': '2014-01-01', 'trend': 0.0008333333333333255, 'seasonal': 0.018572668650793697, 'resid': -0.049406001984126924, 'observed': -0.0299999999999999}, {'date': '2014-02-01', 'trend': -1.2238474123016374e-17, 'seasonal': -0.06279637896825398, 'resid': 0.0827963789682539, 'observed': 0.0199999999999999}, {'date': '2014-03-01', 'trend': 0.0004166666666666546, 'seasonal': 0.056734871031745994, 'resid': -0.04715153769841275, 'observed': 0.0099999999999999}, {'date': '2014-04-01', 'trend': -0.0004166666666666711, 'seasonal': 0.0034536210317460195, 'resid': -0.0030369543650793484, 'observed': 0.0}, {'date': '2014-05-01', 'trend': -3.469446951953614e-18, 'seasonal': -0.001285962301587255, 'resid': -0.008714037698412641, 'observed': -0.0099999999999999}, {'date': '2014-06-01', 'trend': 0.0012499999999999963, 'seasonal': 0.03637028769841267, 'resid': -0.027620287698412665, 'observed': 0.01}, {'date': '2014-07-01', 'trend': 0.0012500000000000007, 'seasonal': -0.018056795634920637, 'resid': -0.003193204365079364, 'observed': -0.02}, {'date': '2014-08-01', 'trend': 0.00041666666666666767, 'seasonal': -8.8045634920585e-05, 'resid': 0.009671378968253918, 'observed': 0.01}, {'date': '2014-09-01', 'trend': -0.0004166666666666624, 'seasonal': -0.034827628968254, 'resid': 0.03524429563492066, 'observed': 0.0}, {'date': '2014-10-01', 'trend': -0.00041666666666666236, 'seasonal': -0.087640128968254, 'resid': 0.08805679563492066, 'observed': 0.0}, {'date': '2014-11-01', 'trend': -4.9150498486009767e-20, 'seasonal': 0.03137028769841272, 'resid': -0.03137028769841272, 'observed': 0.0}, {'date': '2014-12-01', 'trend': 7.719519468096792e-19, 'seasonal': 0.05819320436507936, 'resid': -0.028193204365079362, 'observed': 0.03}, {'date': '2015-01-01', 'trend': 0.00041666666666666686, 'seasonal': 0.018572668650793697, 'resid': -0.05898933531746026, 'observed': -0.0399999999999999}, {'date': '2015-02-01', 'trend': 0.0008333333333333334, 'seasonal': -0.06279637896825398, 'resid': 0.07196304563492054, 'observed': 0.0099999999999999}, {'date': '2015-03-01', 'trend': 0.00041666666666666686, 'seasonal': 0.056734871031745994, 'resid': -0.05715153769841266, 'observed': 0.0}, {'date': '2015-04-01', 'trend': -0.0008333333333333333, 'seasonal': 0.0034536210317460195, 'resid': 0.007379712301587214, 'observed': 0.0099999999999999}, {'date': '2015-05-01', 'trend': -0.0008333333333333328, 'seasonal': -0.001285962301587255, 'resid': -0.007880704365079311, 'observed': -0.0099999999999999}, {'date': '2015-06-01', 'trend': 0.003749999999999999, 'seasonal': 0.03637028769841267, 'resid': -0.030120287698412667, 'observed': 0.01}, {'date': '2015-07-01', 'trend': 0.008333333333333333, 'seasonal': -0.018056795634920637, 'resid': -0.00027653769841269604, 'observed': -0.01}, {'date': '2015-08-01', 'trend': 0.006250000000000003, 'seasonal': -8.8045634920585e-05, 'resid': 0.0038380456349205823, 'observed': 0.01}, {'date': '2015-09-01', 'trend': 0.0008333333333333408, 'seasonal': -0.034827628968254, 'resid': 0.023994295634920657, 'observed': -0.01}, {'date': '2015-10-01', 'trend': -0.0008333333333333208, 'seasonal': -0.087640128968254, 'resid': 0.06847346230158732, 'observed': -0.02}, {'date': '2015-11-01', 'trend': 1.2238474123016374e-17, 'seasonal': 0.03137028769841272, 'resid': -0.011370287698412734, 'observed': 0.02}, {'date': '2015-12-01', 'trend': 8.578207588705312e-18, 'seasonal': 0.05819320436507936, 'resid': 0.06180679563492062, 'observed': 0.12}, {'date': '2016-01-01', 'trend': nan, 'seasonal': 0.018572668650793697, 'resid': nan, 'observed': -0.0199999999999999}, {'date': '2016-02-01', 'trend': nan, 'seasonal': -0.06279637896825398, 'resid': nan, 'observed': -0.06}, {'date': '2016-03-01', 'trend': nan, 'seasonal': 0.056734871031745994, 'resid': nan, 'observed': -0.06}, {'date': '2016-04-01', 'trend': nan, 'seasonal': 0.0034536210317460195, 'resid': nan, 'observed': 0.03}, {'date': '2016-05-01', 'trend': nan, 'seasonal': -0.001285962301587255, 'resid': nan, 'observed': -0.01}, {'date': '2016-06-01', 'trend': nan, 'seasonal': 0.03637028769841267, 'resid': nan, 'observed': 0.01}]}, value_formatter=None))\n\n\n\n\n\nOff ValidMind\n\n# Seasonal decomposition\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nsd = seasonal_decompose(df['loan_rate_A'], model=\"additive\") \nresiduals = sd.resid\n\n\n # Create subplots\nfrom statsmodels.graphics.tsaplots import plot_acf\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\n# Plot 1: Residuals histogram\nsns.histplot(residuals, kde=True, ax=axes[0, 0])\naxes[0, 0].set_xlabel(\"Residuals\")\naxes[0, 0].set_title(\"Residuals Histogram\")\n\n# Plot 2: Residuals Q-Q plot\nstats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title(\"Residuals Q-Q Plot\")\n\n# Plot 3: Residuals autocorrelation plot\nplot_acf(residuals, ax=axes[1, 0], lags=100)\naxes[1, 0].set_title(\"Residuals Autocorrelation\")\n\n# Plot 4: Residuals box plot\nsns.boxplot(y=residuals, ax=axes[1, 1])\naxes[1, 1].set_ylabel(\"Residuals\")\naxes[1, 1].set_title(\"Residuals Box Plot\")\n\n# Adjust the layout\nplt.tight_layout()\n\nWarning: converting a masked element to nan.\n\n\n\n\n\nIn ValidMind\n\nfrom validmind.model_validation.statsmodels.metrics import ResidualsVisualInspection\ntest_context = TestContext(train_ds=vm_train_ds)\nrvi_test = ResidualsVisualInspection(test_context=test_context)\nrvi_test.run()\n\nTestPlanMetricResult(figures=[Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None), Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)], metric=None)\n\n\n\nrvi_test.result.figures\n\n[Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None),\n Figure(key='residuals_visual_inspection', metadata={}, figure=<Figure size 2000x1000 with 4 Axes>, extras=None)]\n\n\n\nrvi_test.result.figures[0].figure\n\n\n\n\n\nrvi_test.result.figures[1].figure"
  },
  {
    "objectID": "notebooks/time_series/time_series_data_quality_test_plan.html",
    "href": "notebooks/time_series/time_series_data_quality_test_plan.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhzo21s006wl9rl0swhv40h\")\n\nConnected to ValidMind. Project: Stock Price Prediction Model - Initial Validation (clhhzo21s006wl9rl0swhv40h)\n\n\n\n\n\n\nimport glob\n# ML libraries\nimport pandas as pd\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = './../../notebooks/datasets/time_series/raw/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\ndf.info()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"MORTGAGE30US\"\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\nvm.test_suites.list_suites()\n\n\n\n\nID                                Name                           Description                                      Test Plans                                                                                                                                   \n\n\nbinary_classifier_full_suite      BinaryClassifierFullSuite      Full test suite for binary classification models.tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\nbinary_classifier_model_validationBinaryClassifierModelValidationTest suite for binary classification models.     binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis                                                   \ntabular_dataset                   TabularDataset                 Test suite for tabular datasets.                 tabular_dataset_description, tabular_data_quality                                                                                            \ntime_series_dataset               TimeSeriesDataset              Test suite for time series datasets.             time_series_data_quality, time_series_univariate, time_series_multivariate                                                                   \ntime_series_model_validation      TimeSeriesModelValidation      Test suite for time series model validation.     regression_model_performance, regression_models_comparison, time_series_forecast                                                             \n\n\n\n\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3.5,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\ntest_suite = vm.run_test_suite(\"time_series_dataset\", dataset=vm_dataset, config=config)\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nNo frequency could be inferred for variable 'MORTGAGE30US'. Skipping seasonal decomposition and plots for this variable.\n\n\nFrequency of UNRATE: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of GS10: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of FEDFUNDS: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\n\n\n\n\n\n\n\n\n\ndf = df.resample('MS').last()\ndf = df.dropna()\n\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"MORTGAGE30US\"\n)\ntest_suite = vm.run_test_suite(\"time_series_dataset\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of UNRATE: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of GS10: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of FEDFUNDS: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters."
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#import-libraries",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#import-libraries",
    "title": "ValidMind",
    "section": "Import Libraries",
    "text": "Import Libraries\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\n\n# ValidMind libraries \nimport validmind as vm\n\n# Plotting libraries"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#data-collection",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#data-collection",
    "title": "ValidMind",
    "section": "Data Collection",
    "text": "Data Collection\n\nLoad FRED Data\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile = '../datasets/time_series/fred_loan_rates.csv'\nfred_df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\ndisplay(fred_df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\n\n\nPreselection of Variables\n\ntarget_column = ['MORTGAGE30US']\nfeature_columns = ['UNRATE', 'GS10', 'FEDFUNDS']\nfred_df = fred_df[target_column + feature_columns]\ndisplay(fred_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      UNRATE\n      GS10\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n    \n    \n      2023-04-06\n      6.28\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      6.27\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      6.39\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      6.43\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 4 columns"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#validmind-setup",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#validmind-setup",
    "title": "ValidMind",
    "section": "ValidMind Setup",
    "text": "ValidMind Setup\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n  project = \"clgo0g0rt0000fjy6ozl9pb69\"\n)\n  \n\nConnected to ValidMind\n\n\n\ndf = fred_df\nvm_dataset = vm.init_dataset(dataset=df)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#data-description",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#data-description",
    "title": "ValidMind",
    "section": "Data Description",
    "text": "Data Description"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#data-quality",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#data-quality",
    "title": "ValidMind",
    "section": "Data Quality",
    "text": "Data Quality\n\nFrequency of the Series\nHandling Frequencies\n\ndf = df.resample('MS').last()\nvm_dataset = vm.init_dataset(dataset=df)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#univariate-analysis",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#univariate-analysis",
    "title": "ValidMind",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\n\n\nAttribute       Value                                                                                                                                                                                                                           \n\n\nID              time_series_univariate                                                                                                                                                                                                          \nName            TimeSeriesUnivariate                                                                                                                                                                                                            \nDescription     Test plan to perform time series univariate analysis.                                                                                                                                                                           \nRequired Context['dataset']                                                                                                                                                                                                                     \nTests           TimeSeriesLinePlot (Metric), TimeSeriesHistogram (Metric), ACFandPACFPlot (Metric), SeasonalDecompose (Metric), AutoSeasonality (Metric), AutoStationarity (Metric), RollingStatsPlot (Metric), AutoAR (Metric), AutoMA (Metric)\nTest Plans      []                                                                                                                                                                                                                              \n\n\n\n\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\n\nvm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nRunning Metric: acf_pacf_plot:  22%|██▏       | 2/9 [00:00<00:02,  3.15it/s]        The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nRunning Metric: seasonal_decompose:  33%|███▎      | 3/9 [00:01<00:02,  2.35it/s]The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nRunning Metric: auto_ma:  89%|████████▉ | 8/9 [00:04<00:00,  1.92it/s]           Non-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nFailed to log result: TestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures) for test plan result 'TestPlanMetricResult(result_id=\"seasonal_decompose\", metric, figures)':  33%|███▎      | 3/9 [00:12<00:24,  4.16s/it]\n\n\nCould not log metrics to ValidMind API\n{\"code\":\"IntenalError\",\"message\":\"Internal server error\"}\n\n\n\nException: {\"code\":\"IntenalError\",\"message\":\"Internal server error\"}"
  },
  {
    "objectID": "notebooks/time_series/loan_rates_forecast_demo.html#multivariate-analysis",
    "href": "notebooks/time_series/loan_rates_forecast_demo.html#multivariate-analysis",
    "title": "ValidMind",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\n\nvm.test_plans.describe_plan(\"time_series_multivariate\")\n\n\n\n\nAttribute       Value                                                                                                   \n\n\nID              time_series_multivariate                                                                                \nName            TimeSeriesMultivariate                                                                                  \nDescription     Test plan to perform time series multivariate analysis.                                                 \nRequired Context['dataset']                                                                                             \nTests           ScatterPlot (Metric), LaggedCorrelationHeatmap (Metric), EngleGrangerCoint (Metric), SpreadPlot (Metric)\nTest Plans      []                                                                                                      \n\n\n\n\n\ntest_plan_config = {\n    \"scatter_plot\": {\n        \"columns\": target_column + feature_columns\n    },\n    \"lagged_correlation_heatmap\": {\n        \"target_col\": target_column,\n        \"independent_vars\": feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\n\nvm.run_test_plan(\"time_series_multivariate\", config=test_plan_config, dataset=vm_dataset)\n\n                                                                                                                                     \n\n\nResults for Time Series Multivariate Test Plan:\n        This test plan provides a preliminary understanding of the features\n        and relationship in multivariate dataset. It presents various\n        multivariate visualizations that can help identify patterns, trends,\n        and relationships between pairs of variables. The visualizations are\n        designed to explore the relationships between multiple features\n        simultaneously. They allow you to quickly identify any patterns or\n        trends in the data, as well as any potential outliers or anomalies.\n        The individual feature distribution can also be explored to provide\n        insight into the range and frequency of values observed in the data.\n        This multivariate analysis test plan aims to provide an overview of\n        the data structure and guide further exploration and modeling.\n        \n            Logged the following plot\n            to the ValidMind platform:\n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            Logged the following plot\n            to the ValidMind platform:\n            \n        \n            Metric Plots\n            \n                \n        \n            \n        \n        \n            \n        \n        \n        \n        \n        \n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        engle_granger_coint\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        \n                    \n                \n                \n                    Metric Value\n                    \n                        [{'Variable 1': 'MORTGAGE30US', 'Variable 2': 'UNRATE', 'Test': 'Engle-Granger', 'p-value': 0.6278763587562293, 'Threshold': 0.05, 'Pass/Fail': 'Fail', 'Decision': 'Not cointegrated'}, {'Variable 1': 'MORTGAGE30US', 'Variable 2': 'GS10', 'Test': 'Engle-Granger', 'p-value': 0.00868759943010125, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'MORTGAGE30US', 'Variable 2': 'FEDFUNDS', 'Test': 'Engle-Granger', 'p-value': 0.02041650659325254, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'UNRATE', 'Variable 2': 'GS10', 'Test': 'Engle-Granger', 'p-value': 0.013241818497465465, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'UNRATE', 'Variable 2': 'FEDFUNDS', 'Test': 'Engle-Granger', 'p-value': 0.027578724502124778, 'Threshold': 0.05, 'Pass/Fail': 'Pass', 'Decision': 'Cointegrated'}, {'Variable 1': 'GS10', 'Variable 2': 'FEDFUNDS', 'Test': 'Engle-Granger', 'p-value': 0.005832335278420137, 'Threshold': 0.05,...\n                    \n                \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n        \n            \n                Metric Plots\n            \n            \n                \n                Show All Plots"
  },
  {
    "objectID": "notebooks/time_series/time_series_data_validation_demo.html",
    "href": "notebooks/time_series/time_series_data_validation_demo.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Demo notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework. Ensuring the quality and an a robust exploratory data analysis of time series data is essential for accurate model predictions and robust decision-making processes.\nIn this demo, we will walk through different data validation suites of tests tailored for time series data, showcasing how these tools can assist you in identifying potential issues and inconsistencies in the data.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\nfrom validmind.datasets.regression import (\n    identify_frequencies, \n    resample_to_common_frequency\n)\n\nThe dotenv extension is already loaded. To reload it, use:\n  %reload_ext dotenv\n\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\nWe can find all the test plans available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\n\nConigure your use case.\n\n# from validmind.datasets.classification import lending_club as demo_dataset\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\n\n# Split the dataset into test and training \ndf = demo_dataset.load_data()\n\n\n\n\n\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n      GS10\n      UNRATE\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n    \n    \n      2023-04-06\n      6.28\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      6.27\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      6.39\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      6.43\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 4 columns\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   FEDFUNDS      825 non-null    float64\n 2   GS10          841 non-null    float64\n 3   UNRATE        903 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\n\nWe will now run the default data quality test plan that will execute a suite of data quality tests such as:\n\nFrequency missmatches\nMissing values\nOutliers\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\n\n\nAttribute       Value                                                                                                           \n\n\nID              time_series_data_quality                                                                                        \nName            TimeSeriesDataQuality                                                                                           \nDescription     Test plan for data quality on time series datasets                                                              \nRequired Context['dataset']                                                                                                     \nTests           TimeSeriesOutliers (ThresholdTest), TimeSeriesMissingValues (ThresholdTest), TimeSeriesFrequency (ThresholdTest)\nTest Plans      []                                                                                                              \n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df\n)\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\nvm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\nTimeSeriesDataQuality(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1947-01-01           NaN       NaN   NaN     NaN\n1947-02-01           NaN       NaN   NaN     NaN\n1947-03-01           NaN       NaN   NaN     NaN\n1947-04-01           NaN       NaN   NaN     NaN\n1947-05-01           NaN       NaN   NaN     NaN\n...                  ...       ...   ...     ...\n2023-04-01           NaN       NaN  3.46     NaN\n2023-04-06          6.28       NaN   NaN     NaN\n2023-04-13          6.27       NaN   NaN     NaN\n2023-04-20          6.39       NaN   NaN     NaN\n2023-04-27          6.43       NaN   NaN     NaN\n\n[3551 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': 3.46, 'UNRATE': nan}, {'MORTGAGE30US': 6.28, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': 6.27, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': 6.39, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': 6.43, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}]}], shape={'rows': 3551, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _feature_lookup={}, _transformed_df=None), model=None, models=None, context_data=None), config={...})\n\n\nIdentify the frequencies of each variable in the raw dataset.\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      FEDFUNDS\n      MS\n    \n    \n      2\n      GS10\n      MS\n    \n    \n      3\n      UNRATE\n      MS\n    \n  \n\n\n\n\nHandle frequencies by resampling all variables to a common frequency.\n\npreprocessed_df = resample_to_common_frequency(df, common_frequency=demo_dataset.frequency)\nfrequencies = identify_frequencies(preprocessed_df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      MS\n    \n    \n      1\n      FEDFUNDS\n      MS\n    \n    \n      2\n      GS10\n      MS\n    \n    \n      3\n      UNRATE\n      MS\n    \n  \n\n\n\n\nRun Data Quality Test Plan again this time with the resampled dataset preprocessed_df and same configuration.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df\n)\nvm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\nTimeSeriesDataQuality(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1947-01-01           NaN       NaN   NaN     NaN\n1947-02-01           NaN       NaN   NaN     NaN\n1947-03-01           NaN       NaN   NaN     NaN\n1947-04-01           NaN       NaN   NaN     NaN\n1947-05-01           NaN       NaN   NaN     NaN\n...                  ...       ...   ...     ...\n2022-12-01          6.49      4.10  3.62     3.5\n2023-01-01          6.49      4.33  3.53     3.4\n2023-02-01          6.49      4.57  3.75     3.6\n2023-03-01          6.49      4.65  3.66     3.5\n2023-04-01          6.49       NaN  3.46     NaN\n\n[916 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}, {'MORTGAGE30US': nan, 'FEDFUNDS': nan, 'GS10': nan, 'UNRATE': nan}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.1, 'GS10': 3.62, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.33, 'GS10': 3.53, 'UNRATE': 3.4}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.57, 'GS10': 3.75, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.65, 'GS10': 3.66, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': nan, 'GS10': 3.46, 'UNRATE': nan}]}], shape={'rows': 916, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _feature_lookup={}, _transformed_df=None), model=None, models=None, context_data=None), config={...})\n\n\nHandle the missing values by droping all the nan values.\n\npreprocessed_df = preprocessed_df.dropna()\n\nRun Data Quality Test Plan again to check there are no missin values and frequencies of all variables are the same.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df\n)\nvm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\nTimeSeriesDataQuality(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1971-10-01      7.670000      5.20  5.93     5.8\n1971-11-01      7.647273      4.91  5.81     6.0\n1971-12-01      7.624545      4.14  5.93     6.0\n1972-01-01      7.601818      3.51  5.95     5.8\n1972-02-01      7.579091      3.30  6.08     5.7\n...                  ...       ...   ...     ...\n2022-11-01      6.213333      3.78  3.89     3.6\n2022-12-01      6.490000      4.10  3.62     3.5\n2023-01-01      6.490000      4.33  3.53     3.4\n2023-02-01      6.490000      4.57  3.75     3.6\n2023-03-01      6.490000      4.65  3.66     3.5\n\n[618 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 7.67, 'FEDFUNDS': 5.2, 'GS10': 5.93, 'UNRATE': 5.8}, {'MORTGAGE30US': 7.647272727272727, 'FEDFUNDS': 4.91, 'GS10': 5.81, 'UNRATE': 6.0}, {'MORTGAGE30US': 7.624545454545454, 'FEDFUNDS': 4.14, 'GS10': 5.93, 'UNRATE': 6.0}, {'MORTGAGE30US': 7.601818181818182, 'FEDFUNDS': 3.51, 'GS10': 5.95, 'UNRATE': 5.8}, {'MORTGAGE30US': 7.579090909090909, 'FEDFUNDS': 3.3, 'GS10': 6.08, 'UNRATE': 5.7}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': 6.213333333333334, 'FEDFUNDS': 3.78, 'GS10': 3.89, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.1, 'GS10': 3.62, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.33, 'GS10': 3.53, 'UNRATE': 3.4}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.57, 'GS10': 3.75, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.65, 'GS10': 3.66, 'UNRATE': 3.5}]}], shape={'rows': 618, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _feature_lookup={}, _transformed_df=None), model=None, models=None, context_data=None), config={...})\n\n\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\n\n\nAttribute       Value                                                                                                                                                                                                                           \n\n\nID              time_series_univariate                                                                                                                                                                                                          \nName            TimeSeriesUnivariate                                                                                                                                                                                                            \nDescription     Test plan to perform time series univariate analysis.                                                                                                                                                                           \nRequired Context['dataset']                                                                                                                                                                                                                     \nTests           TimeSeriesLinePlot (Metric), TimeSeriesHistogram (Metric), ACFandPACFPlot (Metric), SeasonalDecompose (Metric), AutoSeasonality (Metric), AutoStationarity (Metric), RollingStatsPlot (Metric), AutoAR (Metric), AutoMA (Metric)\nTest Plans      []                                                                                                                                                                                                                              \n\n\n\n\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": df.columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": df.columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": df.columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df\n)\nvm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of FEDFUNDS: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of GS10: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of UNRATE: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\n\n\n\nTimeSeriesUnivariate(test_context=TestContext(dataset=Dataset(raw_dataset=            MORTGAGE30US  FEDFUNDS  GS10  UNRATE\nDATE                                            \n1971-10-01      7.670000      5.20  5.93     5.8\n1971-11-01      7.647273      4.91  5.81     6.0\n1971-12-01      7.624545      4.14  5.93     6.0\n1972-01-01      7.601818      3.51  5.95     5.8\n1972-02-01      7.579091      3.30  6.08     5.7\n...                  ...       ...   ...     ...\n2022-11-01      6.213333      3.78  3.89     3.6\n2022-12-01      6.490000      4.10  3.62     3.5\n2023-01-01      6.490000      4.33  3.53     3.4\n2023-02-01      6.490000      4.57  3.75     3.6\n2023-03-01      6.490000      4.65  3.66     3.5\n\n[618 rows x 4 columns], fields=[{'id': 'MORTGAGE30US', 'type': 'Numeric'}, {'id': 'FEDFUNDS', 'type': 'Numeric'}, {'id': 'GS10', 'type': 'Numeric'}, {'id': 'UNRATE', 'type': 'Numeric'}], sample=[{'id': 'head', 'data': [{'MORTGAGE30US': 7.67, 'FEDFUNDS': 5.2, 'GS10': 5.93, 'UNRATE': 5.8}, {'MORTGAGE30US': 7.647272727272727, 'FEDFUNDS': 4.91, 'GS10': 5.81, 'UNRATE': 6.0}, {'MORTGAGE30US': 7.624545454545454, 'FEDFUNDS': 4.14, 'GS10': 5.93, 'UNRATE': 6.0}, {'MORTGAGE30US': 7.601818181818182, 'FEDFUNDS': 3.51, 'GS10': 5.95, 'UNRATE': 5.8}, {'MORTGAGE30US': 7.579090909090909, 'FEDFUNDS': 3.3, 'GS10': 6.08, 'UNRATE': 5.7}]}, {'id': 'tail', 'data': [{'MORTGAGE30US': 6.213333333333334, 'FEDFUNDS': 3.78, 'GS10': 3.89, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.1, 'GS10': 3.62, 'UNRATE': 3.5}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.33, 'GS10': 3.53, 'UNRATE': 3.4}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.57, 'GS10': 3.75, 'UNRATE': 3.6}, {'MORTGAGE30US': 6.49, 'FEDFUNDS': 4.65, 'GS10': 3.66, 'UNRATE': 3.5}]}], shape={'rows': 618, 'columns': 4}, correlation_matrix=None, correlations=None, type='training', options=None, statistics=None, targets=None, target_column=None, class_labels=None, _feature_lookup={}, _transformed_df=None), model=None, models=None, context_data={'seasonal_decompose': {'MORTGAGE30US': <statsmodels.tsa.seasonal.DecomposeResult object at 0x290ceba30>, 'FEDFUNDS': <statsmodels.tsa.seasonal.DecomposeResult object at 0x291d70eb0>, 'GS10': <statsmodels.tsa.seasonal.DecomposeResult object at 0x29223abc0>, 'UNRATE': <statsmodels.tsa.seasonal.DecomposeResult object at 0x29262bca0>}}), config={...})\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_multivariate\")\n\n\n\n\nAttribute       Value                                                                                                   \n\n\nID              time_series_multivariate                                                                                \nName            TimeSeriesMultivariate                                                                                  \nDescription     Test plan to perform time series multivariate analysis.                                                 \nRequired Context['dataset']                                                                                             \nTests           ScatterPlot (Metric), LaggedCorrelationHeatmap (Metric), EngleGrangerCoint (Metric), SpreadPlot (Metric)\nTest Plans      []                                                                                                      \n\n\n\n\n\ntest_plan_config = {\n    \"scatter_plot\": {\n        \"columns\": df.columns\n    },\n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nvm.run_test_plan(\"time_series_multivariate\", config=test_plan_config, dataset=vm_dataset)\n\n\n\n\nValueError: The 'target_col' must be a single string or a list containing a single string"
  },
  {
    "objectID": "notebooks/time_series/infrastructure_change_model.html",
    "href": "notebooks/time_series/infrastructure_change_model.html",
    "title": "ValidMind",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv .env\n\nimport os\nos.chdir(os.path.join(os.getcwd(), \"../..\"))\n\n\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhzo21s006wl9rl0swhv40h\")\n\nConnected to ValidMind. Project: Stock Price Prediction Model - Initial Validation (clhhzo21s006wl9rl0swhv40h)\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = './notebooks/datasets/time_series/raw/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\ndf = df.resample('MS').last()\ndf = df.dropna()\ndf_diff = df.diff().dropna()\n\n\n\n\n\n\n\n\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()\n\n\n\n\n\n\n\nStep 1: Fit Model\n\nm1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\n\n# Add the independent variables with no intercept\nX_1 = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny_1 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.OLS(y_1, X_1).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Sun, 14 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        15:44:10   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nStep 1: Fit Model\n\nm2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_training_dataset['const'] = 1.0\nm2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_test_dataset['const'] = 1.0\n\n\n\n# Add a constant to the independent variables for the linear regression model\nX_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny_2 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.OLS(y_2, X_2).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Sun, 14 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        15:44:11   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nStep 2: Reasoning\n\n\n\n\nm3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\nm3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\n\n\n# Add a constant to the independent variables for the linear regression model\nX_3 = df_train_diff['GS10']\n\n# Define the dependent variable \ny_3 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.OLS(y_3, X_3).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Sun, 14 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        15:44:11   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nvm.test_plans.list_plans()\n# vm.test_suites.list_suites()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\n\n\nvm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\n\nmodel_performance_test_plan = vm.run_test_plan(\"regression_model_performance\", \n                                             model=vm_model_1\n                                            )\n\nvm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\n\n\nvm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\n\n\nmodel_comparison_test_plan = vm.run_test_plan(\"regression_models_comparison\", \n                                             model = vm_model_1,\n                                             models= [vm_model_2, vm_model_3],\n                                            )\n\n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types..."
  },
  {
    "objectID": "notebooks/time_series/time_series_forecast_gls_algo.html",
    "href": "notebooks/time_series/time_series_forecast_gls_algo.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhzo21s006wl9rl0swhv40h\"\n)\n\nConnected to ValidMind. Project: Stock Price Prediction Model - Initial Validation (clhhzo21s006wl9rl0swhv40h)\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import coint\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = '../datasets/time_series/raw/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\nHandling frequencies.\n\ndf = df.resample('MS').last()\n\n\n\nDrop missing values.\n\ndf = df.dropna()\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df\n)\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\ndata_quality_testplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": df.columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": df.columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": df.columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\nvm_dataset = vm.init_dataset(\n    dataset=df, target_column=\"MORTGAGE30US\"\n)\nunivariate_testplan = vm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of UNRATE: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of GS10: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of FEDFUNDS: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_multivariate\")\ntest_plan_config = {\n    \"scatter_plot\": {\n        \"columns\": df.columns\n    },\n    \"lagged_correlation_heatmap\": {\n        \"target_col\": 'MORTGAGE30US',\n        \"independent_vars\": [\"GS10\", \"FEDFUNDS\", \"UNRATE\"]\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nmultivariate_plan = vm.run_test_plan(\"time_series_multivariate\", config=test_plan_config, dataset=vm_dataset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()\n\n\n\n\n\n\n\nStep 1: Fit Model\n\nm1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\n\n# Add the independent variables with no intercept\nX_1 = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny_1 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.GLS(y_1, X_1).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 GLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            GLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Sun, 14 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        15:51:54   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\nm2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_training_dataset['const'] = 1.0\nm2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_test_dataset['const'] = 1.0\n\n# Add a constant to the independent variables for the linear regression model\nX_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny_2 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.GLS(y_2, X_2).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            GLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            GLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Sun, 14 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        15:51:54   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nStep 2: Reasoning\n\n\n\n\nm3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\nm3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\n\n# Add a constant to the independent variables for the linear regression model\nX_3 = df_train_diff['GS10']\n\n# Define the dependent variable \ny_3 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.OLS(y_3, X_3).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Sun, 14 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        15:51:54   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_4 = sm.GLS(y, X).fit()\n\n# Display the model summary\nprint(model_4.summary())\n\n                                 GLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            GLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Sun, 14 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        15:51:54   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\n# Add a constant to the independent variables for the linear regression model\nX = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n\n# Define the dependent variable \ny = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_5 = sm.GLS(y, X).fit()\n\n# Display the model summary\nprint(model_5.summary())\n\n                                 GLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            GLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Sun, 14 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        15:51:54   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\n\n\nvm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\n\nmodel_performance_test_plan = vm.run_test_plan(\"regression_model_performance\", \n                                             model=vm_model_1\n                                            )\n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\nvm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\n\n\nvm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\n\n\nmodel_comparison_test_plan = vm.run_test_plan(\"regression_models_comparison\", \n                                             model = vm_model_1,\n                                             models= [vm_model_2, vm_model_3],\n                                            )\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\nconfig= {\n    \"regression_forecast_plot\": {\n        \"start_date\": '2010-01-01',\n        \"end_date\": '2022-01-01'\n    }\n}\n\nforcasting_testplan = vm.run_test_plan(\"time_series_forecast\",\n                                        models=[vm_model_1],\n                                        config=config)"
  },
  {
    "objectID": "notebooks/time_series/time_series_models_comparison_test_plan.html",
    "href": "notebooks/time_series/time_series_models_comparison_test_plan.html",
    "title": "ValidMind",
    "section": "",
    "text": "%load_ext dotenv\n%dotenv .env\n\nimport os\nos.chdir(os.path.join(os.getcwd(), \"../..\"))\n\n\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhzo21s006wl9rl0swhv40h\")\n\nConnected to ValidMind. Project: Stock Price Prediction Model - Initial Validation (clhhzo21s006wl9rl0swhv40h)\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = './notebooks/datasets/time_series/raw/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\ndf = df.resample('MS').last()\ndf = df.dropna()\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\n\n\n\n\n\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()\n\n\n\n\n\n\n\nStep 1: Fit Model\n\nm1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\n\n# Add the independent variables with no intercept\nX_1 = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny_1 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.OLS(y_1, X_1).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Thu, 11 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        20:24:35   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Fit Model\n\nm1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\n\n# Add the independent variables with no intercept\nX_1 = df_train_diff['FEDFUNDS']\n\n# Define the dependent variable \ny_1 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = sm.OLS(y_1, X_1).fit()\n\n# Display the model summary\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Thu, 11 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        20:24:35   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nStep 1: Fit Model\n\nm2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_training_dataset['const'] = 1.0\nm2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_test_dataset['const'] = 1.0\n\n\n\n# Add a constant to the independent variables for the linear regression model\nX_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\n\n# Define the dependent variable \ny_2 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = sm.OLS(y_2, X_2).fit()\n\n# Display the model summary\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Thu, 11 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        20:24:35   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nStep 2: Reasoning\n\n\n\n\nm3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\nm3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\n\n\n# Add a constant to the independent variables for the linear regression model\nX_3 = df_train_diff['GS10']\n\n# Define the dependent variable \ny_3 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = sm.OLS(y_3, X_3).fit()\n\n# Display the model summary\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Thu, 11 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        20:24:35   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \nbinary_classifier                BinaryClassifier           Test plan for sklearn classifier models that includes\n    both metrics and validation tests                                                                            \ntabular_dataset                  TabularDataset             Test plan for generic tabular datasets                                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \nnormality_test_plan              NormalityTestPlan          Test plan to perform normality tests.                                      \nautocorrelation_test_plan        AutocorrelationTestPlan    Test plan to perform autocorrelation tests.                                \nseasonality_test_plan            SesonalityTestPlan         Test plan to perform seasonality tests.                                    \nunit_root                        UnitRoot                   Test plan to perform unit root tests.                                      \nstationarity_test_plan           StationarityTestPlan       Test plan to perform stationarity tests.                                   \ntimeseries                       TimeSeries                 Test plan for time series statsmodels that includes\n    both metrics and validation tests                                                                            \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_dataset              TimeSeriesDataset          Test plan for time series  datasets                                        \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\n\nvm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\n\nmodel_performance_test_plan = vm.run_test_plan(\"regression_model_performance\", \n                                             model=vm_model_1\n                                            )\n\nvm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\n\n\nvm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\n\n\nmodel_comparison_test_plan = vm.run_test_plan(\"regression_models_comparison\", \n                                             model = vm_model_1,\n                                             models= [vm_model_2, vm_model_3],\n                                            )\n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                          \n\n\nTest plan for performance metric of regression model of statsmodels library\n            \n            \n            \n                \n                    \n                        Metric Name\n                        \n                    \n                    \n                        Metric Type\n                        \n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        {'Independent Variables': ['FEDFUNDS'], 'R-Squared': 0.2857335514089734, 'Adjusted R-Squared': 0.2842963955767983, 'MSE': 0.0738243956495036, 'RMSE': 0.27170645124748805}\n                    \n                \n            \n        \n        \n        \n        \n        \n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                                  \n\n\nTest plan for metrics comparison of regression model of statsmodels library\n            \n            \n            \n                \n                    \n                        Metric Name\n                        \n                    \n                    \n                        Metric Type\n                        \n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        [{'Model': 'Model_1', 'Independent Variables': ['FEDFUNDS'], 'R-Squared': 0.2857335514089734, 'Adjusted R-Squared': 0.2842963955767983, 'MSE': 0.0738243956495036, 'RMSE': 0.27170645124748805}, {'Model': 'Model_2', 'Independent Variables': ['const', 'FEDFUNDS'], 'R-Squared': 0.28560235448239246, 'Adjusted R-Squared': 0.28416203664868767, 'MSE': 0.07394328220568015, 'RMSE': 0.2719251408120995}, {'Model': 'Model_3', 'Independent Variables': ['GS10'], 'R-Squared': 0.5289543117383433, 'Adjusted R-Squared': 0.5280065336935513, 'MSE': 0.048685841716096745, 'RMSE': 0.22064868392106204}]\n                    \n                \n            \n        \n        \n        \n            \n            \n            \n                \n                    \n                        Metric Name\n                        \n                    \n                    \n                        Metric Type\n                        \n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n                \n                    Metric Value\n                    \n                        [{'Model': '(FEDFUNDS)', 'MSE': 0.05180070381223498, 'RMSE': 0.22759767971628134}, {'Model': '(const, FEDFUNDS)', 'MSE': 0.12208298758088999, 'RMSE': 0.3494037601126954}, {'Model': '(GS10)', 'MSE': 0.024309938653901447, 'RMSE': 0.15591644766958182}]"
  },
  {
    "objectID": "notebooks/time_series/time_series_model_validation_full_suite.html",
    "href": "notebooks/time_series/time_series_model_validation_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Full Suite notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework.\nIn this demo, we will use the time_series_model_validation test suite to run multiple model validation metrics on several pre-trained time series models.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\nWe can find all the test suites and test plans available in the developer framework by calling the following functions:\n\nAll test suites: vm.test_suites.list_suites()\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_suites.list_suites()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_full_suite\n      BinaryClassifierFullSuite\n      Full test suite for binary classification models.\n      tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      binary_classifier_model_validation\n      BinaryClassifierModelValidation\n      Test suite for binary classification models.\n      binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      tabular_dataset\n      TabularDataset\n      Test suite for tabular datasets.\n      tabular_dataset_description, tabular_data_quality\n    \n    \n      time_series_dataset\n      TimeSeriesDataset\n      Test suite for time series datasets.\n      time_series_data_quality, time_series_univariate, time_series_multivariate\n    \n    \n      time_series_model_validation\n      TimeSeriesModelValidation\n      Test suite for time series model validation.\n      regression_model_performance, regression_models_comparison, time_series_forecast\n    \n  \n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\n\n\n\n\n\n\n# Currently only fred pre-trained models are available\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\n\n\n\n\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n\nlist_of_models = [vm_model_A, vm_model_B]\n\n\n\n\n\n\n\nUsers can input the configuration to a test suite using config, allowing fine-tuning the suite according to their specific data requirements.\nTime Series Forecast params - transformation specify the desired plotting settings for regression forecast evaluation. In this particular configuration, the chosen transformation is “integrate.”\n\nconfig= {\n    \"regression_forecast_plot_levels\": {\n        \"transformation\": \"integrate\",\n    }\n}\n\n\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model = vm_model_B,\n    models = list_of_models,\n    config = config,\n)\n\n\n\n\n{'transformation': 'integrate'}"
  },
  {
    "objectID": "notebooks/time_series/time_series_data_validation_full_suite.html",
    "href": "notebooks/time_series/time_series_data_validation_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Demo notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework. Ensuring the quality and an a robust exploratory data analysis of time series data is essential for accurate model predictions and robust decision-making processes.\nIn this demo, we will walk through different data validation suites of tests tailored for time series data, showcasing how these tools can assist you in identifying potential issues and inconsistencies in the data.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\nfrom validmind.datasets.regression import (\n    identify_frequencies, \n    resample_to_common_frequency\n)\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\n\n\n\nWe can find all the test suites and test plans available in the developer framework by calling the following functions:\n\nAll test suites: vm.test_suites.list_suites()\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_suites.list_suites()\n\n\nvm.test_plans.list_plans()\n\n\n\n\n\nConigure your use case.\n\n# from validmind.datasets.classification import lending_club as demo_dataset\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\n# Split the dataset into test and training \ndf = demo_dataset.load_data()\n\n\n\n\n\ndf.info()\n\n\n\n\n\n\nUsers can input the configuration to a test suite using config, allowing fine-tuning the suite according to their specific data requirements.\nTime Series Data Quality params - time_series_outliers is set to identify outliers using a specific Z-score threshold - time_series_missing_values defines a minimum threshold to identify missing data points.\nTime Series Univariate params - Visualization: time_series_line_plot and time_series_histogram are designed to generate line and histogram plots respectively for each column in a DataFrame.\n\nSeasonality: seasonal_decompose and auto_seasonality are dedicated to analyzing the seasonal component of the time series. seasonal_decompose performs a seasonal decomposition of the data, while auto_seasonality aids in the automatic detection of seasonality.\nStationarity: window_size determines the number of consecutive data points used for calculating the rolling mean and standard deviation.\nARIMA: acf_pacf_plot, auto_ar, and auto_ma are part of the ARIMA (Autoregressive Integrated Moving Average) model analysis. acf_pacf_plot generates autocorrelation and partial autocorrelation plots, auto_ar determines the order of the autoregressive part of the model, and auto_ma does the same for the moving average part.\n\nTime Series Multivariate params - Visualization: scatter_plot is used to create scatter plots for each column in the DataFrame, offering a visual tool to understand the relationship between different variables in the dataset.\n\nCorrelation: lagged_correlation_heatmap facilitates the creation of a heatmap, which visually represents the lagged correlation between the target column and the feature columns of a demo dataset. This provides a convenient way to examine the time-delayed correlation between different series.\nCointegration: engle_granger_coint sets a threshold for conducting the Engle-Granger cointegration test, which is a statistical method used to identify the long-term correlation between two or more time series.\n\n\nconfig={\n    \n    # TIME SERIES DATA QUALITY PARAMS\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    },\n    \n    # TIME SERIES UNIVARIATE PARAMS \n    \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n     \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive'\n    },\n     \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 4\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS \n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\n\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)\n\n\n\n\n\nShow the frequencies of each variable in the raw dataset.\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\nHandle frequencies by resampling all variables to a common frequency.\n\npreprocessed_df = resample_to_common_frequency(df, common_frequency=demo_dataset.frequency)\nfrequencies = identify_frequencies(preprocessed_df)\ndisplay(frequencies)\n\n\n\nRun the same suite again after handling frequencies.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\n\n\n\n\nHandle the missing values by droping all the nan values.\n\npreprocessed_df = preprocessed_df.dropna()\n\n\n\nRun the same test suite to check there are no missing values and frequencies of all variables are the same.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\n\n\n\n\nHandle stationarity by taking the first difference.\n\npreprocessed_df = preprocessed_df.diff().fillna(method='bfill')\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)"
  },
  {
    "objectID": "notebooks/time_series/time_series_forecast_sklearn_linear_regression_algo.html",
    "href": "notebooks/time_series/time_series_forecast_sklearn_linear_regression_algo.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\nimport validmind as vm\nvm.init(  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhzo21s006wl9rl0swhv40h\"\n)\n\nConnected to ValidMind. Project: Stock Price Prediction Model - Initial Validation (clhhzo21s006wl9rl0swhv40h)\n\n\n\n# System libraries\nimport glob\n\n# ML libraries\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom numpy import argmax\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\ndef merge_fred_csv_files(file_pattern):\n    # Use glob to find all files matching the specified pattern\n    file_list = glob.glob(file_pattern)\n\n    # Initialize an empty list to store individual DataFrames\n    dataframes = []\n\n    # Iterate through each file in the file list\n    for file in file_list:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\n\n        # Add the DataFrame to the list of DataFrames\n        dataframes.append(df)\n\n    # Merge all the DataFrames in the list into a single DataFrame\n    merged_df = pd.concat(dataframes, axis=1)\n\n    return merged_df\n\n\nfile_pattern = '../datasets/time_series/raw/fred/*.csv'\ndf = merge_fred_csv_files(file_pattern)\ndisplay(df)\n\n\n\n\n\n  \n    \n      \n      GDPC1\n      GS5\n      GS10\n      GS3\n      MORTGAGE30US\n      UNRATE\n      CPIAUCSL\n      FEDFUNDS\n      GDP\n    \n    \n      DATE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1947-01-01\n      2034.450\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.48\n      NaN\n      243.164\n    \n    \n      1947-02-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.62\n      NaN\n      NaN\n    \n    \n      1947-03-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      NaN\n    \n    \n      1947-04-01\n      2029.024\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      22.00\n      NaN\n      245.968\n    \n    \n      1947-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      21.95\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2023-04-01\n      NaN\n      NaN\n      3.46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-06\n      NaN\n      NaN\n      NaN\n      NaN\n      6.28\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-13\n      NaN\n      NaN\n      NaN\n      NaN\n      6.27\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-20\n      NaN\n      NaN\n      NaN\n      NaN\n      6.39\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2023-04-27\n      NaN\n      NaN\n      NaN\n      NaN\n      6.43\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n3551 rows × 9 columns\n\n\n\nPreselection of variables.\n\nselected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS'] \ndf = df[selected_cols]\n\nPlot time series.\n\ndef plot_time_series(df, cols_to_plot=None, title=''):\n    \"\"\"\n    Plot multiple time-series in the same axes using seaborn.\n\n    :param df: DataFrame with time-series data\n    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n    :param title: Title of the plot, default is ''\n    \"\"\"\n    if cols_to_plot is None:\n        cols_to_plot = df.columns.tolist()\n\n    # Create a new DataFrame with the columns to plot\n    plot_df = df[cols_to_plot]\n\n    # Set seaborn plot style\n    sns.set(style=\"darkgrid\")\n\n    # Plot the time-series data\n    plt.figure(figsize=(12, 6))\n    for col in plot_df.columns:\n        sns.lineplot(data=plot_df[col], label=col)\n\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_time_series(df, title='All Variables')\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   UNRATE        903 non-null    float64\n 2   GS10          841 non-null    float64\n 3   FEDFUNDS      825 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\nHandling frequencies.\n\ndf = df.resample('MS').last()\n\n\n\nDrop missing values.\n\ndf = df.dropna()\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df, type=\"generic\", target_column=\"MORTGAGE30US\"\n)\n\nconfig={\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    }\n}\n\ndata_quality_testplan = vm.run_test_plan(\"time_series_data_quality\", dataset=vm_dataset, config=config)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_diff = df.diff().dropna()\n\ntest_plan_config = {\n    \"time_series_line_plot\": {\n        \"columns\": df.columns\n    },\n    \"time_series_histogram\": {\n        \"columns\": df.columns\n    },\n    \"acf_pacf_plot\": {\n        \"columns\": df.columns\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 3\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n    \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive',\n         \"fig_size\": (40,30)\n    },\n    \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n      \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n}\n\nvm_dataset = vm.init_dataset(\n    dataset=df, type=\"generic\", target_column=\"MORTGAGE30US\"\n)\nunivariate_testplan = vm.run_test_plan(\"time_series_univariate\", config=test_plan_config, dataset=vm_dataset)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of UNRATE: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of GS10: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of FEDFUNDS: MS\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\n\n\n\n\n\n\n\nvm.test_plans.describe_plan(\"time_series_multivariate\")\ntest_plan_config = {\n    \"scatter_plot\": {\n        \"columns\": df.columns\n    },\n    \"lagged_correlation_heatmap\": {\n        \"target_col\": 'MORTGAGE30US',\n        \"independent_vars\": [\"GS10\", \"FEDFUNDS\", \"UNRATE\"]\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nmultivariate_plan = vm.run_test_plan(\"time_series_multivariate\", config=test_plan_config, dataset=vm_dataset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Split dataset into Training and Test\n\nsplit_index = int(len(df) * 0.8)   # use 80% of the data for training\ndf_train, df_test = df[:split_index], df[split_index:]\n\nStep 2: Create a Stationary Train and Test Dataset\n\n# Apply first difference to both training and test df\ndf_train_diff = df_train.diff().dropna()\ndf_test_diff = df_test.diff().dropna()\n\n\n\n\n\n\n\nStep 1: Fit Model\n\nm1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\n\n# Add the independent variables with no intercept\nX_1 = np.array(df_train_diff['FEDFUNDS']).reshape(-1,1)\n\n# Define the dependent variable \ny_1 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_1 = LinearRegression().fit(X_1, y_1)\n\n# Display the model summary\nprint(repr(model_1))\n\nLinearRegression()\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\nm2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_training_dataset['const'] = 1.0\nm2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\nm2_test_dataset['const'] = 1.0\n\ntemp_df = df_train_diff.copy(deep=True)\ntemp_df['const'] = 1.0\n# Add a constant to the independent variables for the linear regression model\nX_2 = temp_df[['const', 'FEDFUNDS']]\n\n# Define the dependent variable \ny_2 = temp_df['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_2 = LinearRegression().fit(X=X_2, y=y_2)\n\n# Display the model summary\nprint(model_2)\n\nLinearRegression()\n\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nStep 2: Reasoning\n\n\n\n\nm3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\nm3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\n\n# Add a constant to the independent variables for the linear regression model\nX_3 = np.array(df_train_diff['GS10']).reshape(-1,1)\n\n# Define the dependent variable \ny_3 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_3 = LinearRegression().fit(X_3, y_3)\n\n# Display the model summary\nprint(model_3)\n\nLinearRegression()\n\n\n\n\n\nStep 1: Fit Model\n\nm4_training_dataset = df_train_diff[['GS10', 'FEDFUNDS', 'MORTGAGE30US']]\nm4_test_dataset = df_test_diff[['GS10', 'FEDFUNDS', 'MORTGAGE30US']]\n\n# Add a constant to the independent variables for the linear regression model\nX_4 = df_train_diff[['GS10', 'FEDFUNDS']]\n\n# Define the dependent variable \ny_4 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_4 = LinearRegression().fit(X_4, y_4)\n\n# Display the model summary\nprint(model_4)\n\nLinearRegression()\n\n\nStep 2: Reasoning\n\n\n\nStep 1: Fit Model\n\nm5_training_dataset = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE', 'MORTGAGE30US']]\nm5_test_dataset = df_test_diff[['GS10', 'FEDFUNDS', 'UNRATE', 'MORTGAGE30US']]\n\n# Add a constant to the independent variables for the linear regression model\nX_5 = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n\n# Define the dependent variable \ny_5 = df_train_diff['MORTGAGE30US']\n\n# Fit the linear regression model\nmodel_5 = LinearRegression().fit(X=X_5, y=y_5)\n\n# Display the model summary\nprint(model_5)\n\nLinearRegression()\n\n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\nID                               Name                       Description                                                                \n\n\nbinary_classifier_metrics        BinaryClassifierMetrics    Test plan for sklearn classifier metrics                                   \nbinary_classifier_validation     BinaryClassifierPerformanceTest plan for sklearn classifier models                                    \nbinary_classifier_model_diagnosisBinaryClassifierDiagnosis  Test plan for sklearn classifier model diagnosis tests                     \ntabular_dataset_description      TabularDatasetDescription  Test plan to extract metadata and descriptive\n    statistics from a tabular dataset                                                                            \ntabular_data_quality             TabularDataQuality         Test plan for data quality on tabular datasets                             \ntime_series_data_quality         TimeSeriesDataQuality      Test plan for data quality on time series datasets                         \ntime_series_univariate           TimeSeriesUnivariate       Test plan to perform time series univariate analysis.                      \ntime_series_multivariate         TimeSeriesMultivariate     Test plan to perform time series multivariate analysis.                    \ntime_series_forecast             TimeSeriesForecast         Test plan to perform time series forecast tests.                           \nregression_model_performance     RegressionModelPerformance Test plan for performance metric of regression model of statsmodels library\nregression_models_comparison     RegressionModelsComparison Test plan for metrics comparison of regression model of statsmodels library\n\n\n\n\n\n\n\n\n\nvm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\n\nmodel_performance_test_plan = vm.run_test_plan(\"regression_model_performance\", \n                                             model=vm_model_1\n                                            )\n\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\n\n\n\n\n\nX has feature names, but LinearRegression was fitted without feature names\n\n\n\n\n\n\n\n\n\nvm_train_ds_4 = vm.init_dataset(dataset=m4_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_4 = vm.init_dataset(dataset=m4_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_4 = vm.init_model(model_4, train_ds=vm_train_ds_4, test_ds=vm_test_ds_4, validation_ds=vm_test_ds_4)\n\n\nvm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\"generic\", target_column=\"MORTGAGE30US\")\nvm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\n\n\nmodel_comparison_test_plan = vm.run_test_plan(\"regression_models_comparison\", \n                                             model = vm_model_1,\n                                             models= [vm_model_3, vm_model_4],\n                                            )\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\n\n\n\n\n\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names\n\n\n\n\n\n\n\n\n\nconfig= {\n    \"regression_forecast_plot\": {\n        \"start_date\": '2010-01-01',\n        \"end_date\": '2022-01-01'\n    }\n}\n\nforcasting_testplan = vm.run_test_plan(\"time_series_forecast\",\n                                        models=[vm_model_4, vm_model_3],\n                                        config=config)\n\n\n\n\nX has feature names, but LinearRegression was fitted without feature names\nX has feature names, but LinearRegression was fitted without feature names"
  },
  {
    "objectID": "notebooks/time_series/time_series_model_validation_poc.html",
    "href": "notebooks/time_series/time_series_model_validation_poc.html",
    "title": "ValidMind",
    "section": "",
    "text": "# System libraries\nimport glob\n\n# ML libraries\nimport pickle \nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import coint\nfrom arch.unitroot import PhillipsPerron, DFGLS\nimport xgboost as xgb\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\n\n# Plotting libraries \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_1.pkl', 'rb') as f:\n    model_1 = pickle.load(f)\nprint(model_1.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.286\nModel:                            OLS   Adj. R-squared (uncentered):              0.284\nMethod:                 Least Squares   F-statistic:                              198.8\nDate:                Tue, 09 May 2023   Prob (F-statistic):                    3.22e-38\nTime:                        13:36:27   Log-Likelihood:                         -57.220\nNo. Observations:                 498   AIC:                                      116.4\nDf Residuals:                     497   BIC:                                      120.7\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nFEDFUNDS       0.2901      0.021     14.100      0.000       0.250       0.330\n==============================================================================\nOmnibus:                      139.024   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.633\nSkew:                           1.080   Prob(JB):                    6.95e-176\nKurtosis:                       8.849   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_2.pkl', 'rb') as f:\n    model_2 = pickle.load(f)\nprint(model_2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           MORTGAGE30US   R-squared:                       0.286\nModel:                            OLS   Adj. R-squared:                  0.284\nMethod:                 Least Squares   F-statistic:                     198.3\nDate:                Tue, 09 May 2023   Prob (F-statistic):           3.99e-38\nTime:                        13:36:27   Log-Likelihood:                -57.120\nNo. Observations:                 498   AIC:                             118.2\nDf Residuals:                     496   BIC:                             126.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0055      0.012     -0.448      0.654      -0.029       0.018\nFEDFUNDS       0.2899      0.021     14.082      0.000       0.249       0.330\n==============================================================================\nOmnibus:                      138.997   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              806.946\nSkew:                           1.080   Prob(JB):                    5.94e-176\nKurtosis:                       8.850   Cond. No.                         1.69\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_3.pkl', 'rb') as f:\n    model_3 = pickle.load(f)\nprint(model_3.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.529\nModel:                            OLS   Adj. R-squared (uncentered):              0.528\nMethod:                 Least Squares   F-statistic:                              558.1\nDate:                Tue, 09 May 2023   Prob (F-statistic):                    2.80e-83\nTime:                        13:36:27   Log-Likelihood:                          46.439\nNo. Observations:                 498   AIC:                                     -90.88\nDf Residuals:                     497   BIC:                                     -86.67\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.7428      0.031     23.624      0.000       0.681       0.805\n==============================================================================\nOmnibus:                      216.020   Durbin-Watson:                   1.949\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27264.604\nSkew:                           0.803   Prob(JB):                         0.00\nKurtosis:                      39.213   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_4.pkl', 'rb') as f:\n    model_4 = pickle.load(f)\nprint(model_4.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.621\nModel:                            OLS   Adj. R-squared (uncentered):              0.620\nMethod:                 Least Squares   F-statistic:                              407.0\nDate:                Tue, 09 May 2023   Prob (F-statistic):                   2.45e-105\nTime:                        13:36:27   Log-Likelihood:                          100.84\nNo. Observations:                 498   AIC:                                     -197.7\nDf Residuals:                     496   BIC:                                     -189.3\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6298      0.030     20.970      0.000       0.571       0.689\nFEDFUNDS       0.1756      0.016     11.005      0.000       0.144       0.207\n==============================================================================\nOmnibus:                      252.158   Durbin-Watson:                   1.918\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4954.128\nSkew:                           1.729   Prob(JB):                         0.00\nKurtosis:                      18.060   Cond. No.                         2.09\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nwith open('../models/time_series/fred_loan_rates_model_5.pkl', 'rb') as f:\n    model_5 = pickle.load(f)\nprint(model_5.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:           MORTGAGE30US   R-squared (uncentered):                   0.622\nModel:                            OLS   Adj. R-squared (uncentered):              0.619\nMethod:                 Least Squares   F-statistic:                              271.0\nDate:                Tue, 09 May 2023   Prob (F-statistic):                   4.91e-104\nTime:                        13:36:27   Log-Likelihood:                          100.97\nNo. Observations:                 498   AIC:                                     -195.9\nDf Residuals:                     495   BIC:                                     -183.3\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGS10           0.6283      0.030     20.805      0.000       0.569       0.688\nFEDFUNDS       0.1741      0.016     10.718      0.000       0.142       0.206\nUNRATE        -0.0258      0.051     -0.508      0.612      -0.126       0.074\n==============================================================================\nOmnibus:                      255.442   Durbin-Watson:                   1.911\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5061.282\nSkew:                           1.758   Prob(JB):                         0.00\nKurtosis:                      18.217   Cond. No.                         3.48\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n# Extract the endogenous (target) variable from the model fit\ntrain_df = pd.Series(model_1.model.endog, index=model_1.model.data.row_labels)\ntrain_df = train_df.to_frame()\ntarget_var_name = model_1.model.endog_names\ntrain_df.columns = [target_var_name]\n\n# Extract the exogenous (explanatory) variables from the model fit\nexog_df = pd.DataFrame(model_1.model.exog, index=model_1.model.data.row_labels, columns=model_1.model.exog_names)\n\n# Concatenate the endogenous (target) and exogenous (explanatory) variables\ntrain_df = pd.concat([train_df, exog_df], axis=1)\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      1971-05-01\n      0.17\n      0.47\n    \n    \n      1971-06-01\n      0.08\n      0.28\n    \n    \n      1971-07-01\n      0.15\n      0.40\n    \n    \n      1971-08-01\n      0.00\n      0.26\n    \n    \n      1971-09-01\n      -0.02\n      -0.02\n    \n  \n\n\n\n\n\ntrain_df.tail()\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n    \n  \n  \n    \n      2012-06-01\n      -0.09\n      0.00\n    \n    \n      2012-07-01\n      -0.17\n      0.00\n    \n    \n      2012-08-01\n      0.10\n      -0.03\n    \n    \n      2012-09-01\n      -0.19\n      0.01\n    \n    \n      2012-10-01\n      0.01\n      0.02\n    \n  \n\n\n\n\n\n\n\nLoad raw test dataset.\n\nfile = '../datasets/time_series/fred_loan_rates_test_1.csv'\nraw_test_df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\ndisplay(raw_test_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US\n      UNRATE\n      GS10\n      FEDFUNDS\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      2012-11-01\n      3.32\n      7.7\n      1.65\n      0.16\n    \n    \n      2012-12-01\n      3.35\n      7.9\n      1.72\n      0.16\n    \n    \n      2013-01-01\n      3.53\n      8.0\n      1.91\n      0.14\n    \n    \n      2013-02-01\n      3.51\n      7.7\n      1.98\n      0.15\n    \n    \n      2013-03-01\n      3.57\n      7.5\n      1.96\n      0.14\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-11-01\n      6.58\n      3.6\n      3.89\n      3.78\n    \n    \n      2022-12-01\n      6.42\n      3.5\n      3.62\n      4.10\n    \n    \n      2023-01-01\n      6.13\n      3.4\n      3.53\n      4.33\n    \n    \n      2023-02-01\n      6.50\n      3.6\n      3.75\n      4.57\n    \n    \n      2023-03-01\n      6.32\n      3.5\n      3.66\n      4.65\n    \n  \n\n125 rows × 4 columns\n\n\n\nTransform raw test dataset using same transformation used in the train dataset.\n\ntransform_func = 'diff'\nif transform_func == 'diff':\n    test_df = raw_test_df.diff().dropna()\n\n\n\n\n\ndef get_model_prediction(model_fits_dict, df_test):\n    # Extract the training data from the first model fit\n    first_model_fit = list(model_fits_dict.values())[0]\n    train_data = pd.Series(first_model_fit.model.endog, index=first_model_fit.model.data.row_labels)\n    train_data = train_data.to_frame()\n    target_var_name = first_model_fit.model.endog_names\n    train_data.columns = [f'{target_var_name}_train']\n\n    # Initialize an empty DataFrame to store the predictions\n    prediction_df = pd.DataFrame(index=df_test.index)\n    prediction_df[f'{target_var_name}_test'] = np.nan\n\n    # Concatenate the train_data and prediction_df\n    combined_df = pd.concat([train_data, prediction_df], axis=0)\n\n    # Loop through each model fit\n    for model_name, model_fit in model_fits_dict.items():\n        # Prepare the test dataset\n        exog_names = model_fit.model.exog_names\n        X_test = df_test.copy()\n\n        # Add the constant if it's missing\n        if 'const' in exog_names and 'const' not in X_test.columns:\n            X_test['const'] = 1.0\n\n        # Select the necessary columns\n        X_test = X_test[exog_names]\n\n        # Generate the predictions\n        predictions = model_fit.predict(X_test)\n\n        # Add the predictions to the DataFrame\n        combined_df[model_name] = np.nan\n        combined_df[model_name].iloc[len(train_data):] = predictions\n\n    # Add the test data to the '<target_variable>_test' column\n    combined_df[f'{target_var_name}_test'].iloc[len(train_data):] = df_test[target_var_name]\n\n    return combined_df\n\n\n\n# Replace with your list of model fits\nmodel_fits = {\n    'model_1': model_1,\n    'model_3': model_3\n}  \nprediction_df = get_model_prediction(model_fits, test_df)\ndisplay(prediction_df)\n\n\n\n\n\n  \n    \n      \n      MORTGAGE30US_train\n      MORTGAGE30US_test\n      model_1\n      model_3\n    \n    \n      DATE\n      \n      \n      \n      \n    \n  \n  \n    \n      1971-05-01\n      0.17\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-06-01\n      0.08\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-07-01\n      0.15\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-08-01\n      0.00\n      NaN\n      NaN\n      NaN\n    \n    \n      1971-09-01\n      -0.02\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-11-01\n      NaN\n      -0.50\n      0.203037\n      -0.066856\n    \n    \n      2022-12-01\n      NaN\n      -0.16\n      0.092817\n      -0.200567\n    \n    \n      2023-01-01\n      NaN\n      -0.29\n      0.066712\n      -0.066856\n    \n    \n      2023-02-01\n      NaN\n      0.37\n      0.069613\n      0.163425\n    \n    \n      2023-03-01\n      NaN\n      -0.18\n      0.023204\n      -0.066856\n    \n  \n\n622 rows × 4 columns\n\n\n\n\ndef plot_predictions(prediction_df, subplot=True):\n    n_models = prediction_df.shape[1] - 2\n    \n    if subplot:\n        fig, axes = plt.subplots(n_models, 1, figsize=(12, 6 * n_models), sharex=True)\n        \n        for i in range(n_models):\n            axes[i].plot(prediction_df.index, prediction_df.iloc[:, 0], label=prediction_df.columns[0], color='grey')\n            axes[i].plot(prediction_df.index, prediction_df.iloc[:, 1], label=prediction_df.columns[1], color='lightgrey')\n            axes[i].plot(prediction_df.index, prediction_df.iloc[:, i + 2], label=prediction_df.columns[i + 2], linestyle='-')\n            axes[i].set_ylabel('Target Variable')\n            axes[i].set_title(f'Test Data vs. {prediction_df.columns[i + 2]}')\n            axes[i].legend()\n            axes[i].grid(True)\n        plt.xlabel('Date')\n        plt.tight_layout()\n        plt.show()\n        \n    else:\n        plt.figure(figsize=(12, 6))\n        plt.plot(prediction_df.index, prediction_df.iloc[:, 0], label=prediction_df.columns[0], color='grey')\n        plt.plot(prediction_df.index, prediction_df.iloc[:, 1], label=prediction_df.columns[1], color='lightgrey')\n        \n        for i in range(2, prediction_df.shape[1]):\n            plt.plot(prediction_df.index, prediction_df.iloc[:, i], label=prediction_df.columns[i], linestyle='-')\n        \n        plt.xlabel('Date')\n        plt.ylabel('Target Variable')\n        plt.title('Test Data vs. Model Forecasts')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n\nplot_predictions(prediction_df, subplot=True)"
  },
  {
    "objectID": "notebooks/time_series_model_validation_full_suite.html",
    "href": "notebooks/time_series_model_validation_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Full Suite notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework.\nIn this demo, we will use the time_series_model_validation test suite to run multiple model validation metrics on several pre-trained time series models.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\nWe can find all the test suites and test plans available in the developer framework by calling the following functions:\n\nAll test suites: vm.test_suites.list_suites()\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_suites.list_suites()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_full_suite\n      BinaryClassifierFullSuite\n      Full test suite for binary classification models.\n      tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      binary_classifier_model_validation\n      BinaryClassifierModelValidation\n      Test suite for binary classification models.\n      binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      tabular_dataset\n      TabularDataset\n      Test suite for tabular datasets.\n      tabular_dataset_description, tabular_data_quality\n    \n    \n      time_series_dataset\n      TimeSeriesDataset\n      Test suite for time series datasets.\n      time_series_data_quality, time_series_univariate, time_series_multivariate\n    \n    \n      time_series_model_validation\n      TimeSeriesModelValidation\n      Test suite for time series model validation.\n      regression_model_performance, regression_models_comparison, time_series_forecast\n    \n  \n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\n\n\n\n\n\n\n# Currently only fred pre-trained models are available\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_1')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_C, train_df_C, test_df_C = demo_dataset.load_model('fred_loan_rates_model_4')\nmodel_D, train_df_D, test_df_D = demo_dataset.load_model('fred_loan_rates_model_5')\n\n\n\n\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model C\nvm_train_ds_C = vm.init_dataset(dataset=train_df_C, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_C = vm.init_dataset(dataset=test_df_C, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model D\nvm_train_ds_D = vm.init_dataset(dataset=train_df_D, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_D = vm.init_dataset(dataset=test_df_D, type=\"generic\", target_column=demo_dataset.target_column)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n# Initialize model C\nvm_model_C = vm.init_model(\n    model=model_C, \n    train_ds=vm_train_ds_C, \n    test_ds=vm_test_ds_C\n)\n\n# Initialize model D\nvm_model_D = vm.init_model(\n    model=model_D,\n    train_ds=vm_train_ds_D,\n    test_ds=vm_test_ds_D\n)\n\n\nlist_of_models = [vm_model_A, vm_model_B, vm_model_C, vm_model_D]\n\n\n\n\n\n\n\nUsers can input the configuration to a test suite using config, allowing fine-tuning the suite according to their specific data requirements.\nTime Series Data Quality params - time_series_outliers is set to identify outliers using a specific Z-score threshold - time_series_missing_values defines a minimum threshold to identify missing data points.\nTime Series Univariate params - Visualization: The keys time_series_line_plot and time_series_histogram are designed to generate line and histogram plots respectively for each column in a DataFrame.\n\nSeasonality: The keys seasonal_decompose and auto_seasonality are dedicated to analyzing the seasonal component of the time series. seasonal_decompose performs a seasonal decomposition of the data, while auto_seasonality aids in the automatic detection of seasonality.\nARIMA: The keys acf_pacf_plot, auto_ar, and auto_ma are part of the ARIMA (Autoregressive Integrated Moving Average) model analysis. acf_pacf_plot generates autocorrelation and partial autocorrelation plots, auto_ar determines the order of the autoregressive part of the model, and auto_ma does the same for the moving average part.\n\nTime Series Multivariate params - Visualization: The scatter_plot key is used to create scatter plots for each column in the DataFrame, offering a visual tool to understand the relationship between different variables in the dataset.\n\nCorrelation: The lagged_correlation_heatmap key facilitates the creation of a heatmap, which visually represents the lagged correlation between the target column and the feature columns of a demo dataset. This provides a convenient way to examine the time-delayed correlation between different series.\nCointegration: The engle_granger_coint key sets a threshold for conducting the Engle-Granger cointegration test, which is a statistical method used to identify the long-term correlation between two or more time series.\n\n\nconfig= {\n    \"regression_forecast_plot\": {\n        \"start_date\": '2010-01-01',\n        \"end_date\": '2022-01-01'\n    }\n}\n\n\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model = vm_model_B,\n    models = list_of_models,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/intro-r.html",
    "href": "notebooks/intro-r.html",
    "title": "ValidMind",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "notebooks/intro-r.html#initializing-the-validmind-library",
    "href": "notebooks/intro-r.html#initializing-the-validmind-library",
    "title": "ValidMind",
    "section": "Initializing the ValidMind Library",
    "text": "Initializing the ValidMind Library\nAfter creating an account with ValidMind, we can find the project’s API key and secret in the settings page of the ValidMind dashboard:\n\nThe library credentials can be configured in two ways:\n\nBy setting the VM_API_KEY and VM_API_SECRET environment variables or\nBy passing api_key and api_secret arguments to the init function like this:\n\nvm.init(\n    api_key='<your-api-key>',\n    api_secret='<your-api-secret>',\n    project=\"cl2r3k1ri000009jweny7ba1g\"\n)\nThe project argument is mandatory since it allows the library to associate all data collected with a specific account project.\n\nimport validmind as vm\n\nvm.init(\n      api_host = \"http://localhost:3000/api/v1/tracking\",\n      api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n      api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n      project = \"clfmztk8t0000qvoo2wh30ex7\"\n)\n\nTrue\n\n\n\nUsing a demo dataset\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following library functionalities:\n\nLogging information about a dataset\nRunning data quality tests on a dataset\nLogging information about a model\nLogging training metrics for a model\nRunning model evaluation tests\n\n\n\nRunning a dataset evaluation test plan\nWe will now run a basic tabular dataset test plan that will compute and log the description, metrics and tests for the dataset. For now, ValidMind supports metrics and tests for a single dataset, so we first will combine the test and train sets into one to get complete datapoints.\n\n## load the train and test dataframes\n# assume that the train and test datasets have been saved to disk as csv files\n# see the r-customer-churn-model.ipynb notebook for details on how this can be done\ndf_train = pd.read_csv(\"r_demo/r_churn_train.csv\")\ndf_test = pd.read_csv(\"r_demo/r_churn_test.csv\")\n\nvm_train_ds = vm.init_dataset(dataset=df_train, type=\"generic\", target_column=\"Exited\")\nvm_test_ds = vm.init_dataset(dataset=df_test, type=\"generic\", target_column=\"Exited\")\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n# run dataset tests on df_train and df_test combined\nvm_dataset_combined = vm.init_dataset(\n    dataset=pd.concat([df_train, df_test]),\n    type=\"generic\",\n    target_column=\"Exited\",\n)\nvm.run_test_plan(\"tabular_dataset\", dataset=vm_dataset_combined)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n                                                                                                                                                                                                                 \n\n\n\nResults for tabular_dataset_description Test Plan:Logged the following dataset to the ValidMind platform:\n  \n    \n      \n      GeographyFrance\n      GeographyGermany\n      Gender\n      Age\n      Tenure\n      Balance\n      NumOfProducts\n      HasCrCard\n      IsActiveMember\n      EstimatedSalary\n      Exited\n    \n  \n  \n    \n      count\n      8000.00000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n      8000.000000\n    \n    \n      mean\n      0.50125\n      0.251125\n      0.549500\n      38.948875\n      5.033875\n      76434.096511\n      1.532500\n      0.702625\n      0.519875\n      99790.187959\n      0.202000\n    \n    \n      std\n      0.50003\n      0.433687\n      0.497575\n      10.458952\n      2.885267\n      62612.251258\n      0.580505\n      0.457132\n      0.499636\n      57520.508892\n      0.401517\n    \n    \n      min\n      0.00000\n      0.000000\n      0.000000\n      18.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      11.580000\n      0.000000\n    \n    \n      25%\n      0.00000\n      0.000000\n      0.000000\n      32.000000\n      3.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      50857.102500\n      0.000000\n    \n    \n      50%\n      1.00000\n      0.000000\n      1.000000\n      37.000000\n      5.000000\n      97263.675000\n      1.000000\n      1.000000\n      1.000000\n      99504.890000\n      0.000000\n    \n    \n      75%\n      1.00000\n      1.000000\n      1.000000\n      44.000000\n      8.000000\n      128044.507500\n      2.000000\n      1.000000\n      1.000000\n      149216.320000\n      0.000000\n    \n    \n      max\n      1.00000\n      1.000000\n      1.000000\n      92.000000\n      10.000000\n      250898.090000\n      4.000000\n      1.000000\n      1.000000\n      199992.480000\n      1.000000\n    \n  \n\n            Logged the following dataset metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        dataset_correlations\n                    \n                    \n                        Metric Type\n                        dataset\n                    \n                    \n                        Metric Scope\n                        generic\n                    \n                \n            \n                \n                    \n                        Metric Plots\n                        \n                            Show All Plots\n                        \n                    \n                    \n                        \n                \n                    \n                \n                \n                        \n                            \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                \n                    \n                \n                \n                        \n                    \n                \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for tabular_data_quality Test Plan:\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Class Imbalance\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    class_imbalance\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_percent_threshold': 0.2}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Duplicates\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    duplicates\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Cardinality\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    cardinality\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Pearson Correlation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    pearson_correlation\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 0.3}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=False, values={'correlations': [{'column': 'GeographyGermany', 'correlation': -0.5805318438630581}]}), TestResult(test_name=None, column='GeographyGermany', passed=False, values={'correlations': [{'column': 'Balance', 'correlation': 0.4062612295669473}]}), TestResult(test_name=None, column='Balance', passed=False, values={'correlations': [{'column': 'NumOfProducts', 'correlation': -0.3044645622389454}]})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Missing\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    missing\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Skewness\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    skewness\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.024522142979951}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.0076920437747027195}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111807}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077725})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Unique\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    unique\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_percent_threshold': 1}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='GeographyFrance', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='GeographyGermany', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Gender', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='Age', passed=True, values={'n_unique': 69, 'p_unique': 0.008625}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_unique': 11, 'p_unique': 0.001375}), TestResult(test_name=None, column='Balance', passed=True, values={'n_unique': 5088, 'p_unique': 0.636}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_unique': 4, 'p_unique': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_unique': 2, 'p_unique': 0.00025}), TestResult(test_name=None, column='EstimatedSalary', passed=False, values={'n_unique': 8000, 'p_unique': 1.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_unique': 2, 'p_unique': 0.00025})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Zeros\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    zeros\n                \n                \n                    Category\n                    data_quality\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'max_percent_threshold': 0.03}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})]\n            \n        \n        \n        \n        \n        \n        \n\n\n\n\nRunning a model evaluation test plan for our LogisticRegression R model\nWe will now run a basic model evaluation test plan that is compatible with the R model we will be loading.\n\n## load the model\n# assume that the model has been saved to disk as a serialized R object (.rds)\n# see the r-customer-churn-model.ipynb notebook for details on how this can be done\n# model_type must be passed for R models:\n# Currently, LogisticRegression, LinearRegression (glm and lm in R) XGBClassifier and XGBRegressor are supported\nvm_model = vm.init_r_model(\"r_demo/r_log_reg_churn_model.rds\", model_type=\"LogisticRegression\")\n\n\nvm.run_test_plan(\"sklearn_classifier\", model=vm_model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n                                                                                                                                                                                                                 \n\n\nResults for sklearn_classifier_metrics Test Plan:\n        Logged the following model to the ValidMind platform:\n        \n            \n                \n                    \n                        XGBClassifier (main)\n                    \n                    📦\n                \n            \n            \n                \n                    Framework\n                    \n                        XGBoost\n                        (v1.7.4)\n                    \n                \n                \n                    Architecture\n                    Extreme Gradient Boosting\n                \n                \n                    Task\n                    classification\n                \n                \n                    Subtask\n                    binary\n                \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        accuracy\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.8604166666666667\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        confusion_matrix\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        f1_score\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.5743329097839899\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pfi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training_dataset\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        pr_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        precision\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7687074829931972\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        recall\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.45841784989858014\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_auc\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n            \n                Metric Value\n                \n                    0.7113798740840568\n                \n            \n            \n        \n        \n        \n            Logged the following evaluation metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        roc_curve\n                    \n                    \n                        Metric Type\n                        evaluation\n                    \n                    \n                        Metric Scope\n                        test\n                    \n                \n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        csi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                    {'GeographyFrance': 2e-06, 'GeographyGermany': 5e-05, 'Gender': 7e-06, 'Age': 0.000446, 'Tenure': 0.000397, 'Balance': 0.001007, 'NumOfProducts': 0.000273, 'HasCrCard': 0.000234, 'IsActiveMember': 4.2e-05, 'EstimatedSalary': 0.000501}\n                \n            \n            \n        \n        \n        \n            Logged the following training metric to the ValidMind platform:\n            \n            \n                \n                    \n                        Metric Name\n                        psi\n                    \n                    \n                        Metric Type\n                        training\n                    \n                    \n                        Metric Scope\n                        training:validation\n                    \n                \n            \n            \n                Metric Value\n                \n                         initial  percent_initial   new  percent_new       psi\nbin                                                       \n1       2840          0.50714  1218     0.507500  0.000000\n2       1155          0.20625   474     0.197500  0.000379\n3        464          0.08286   218     0.090833  0.000733\n4        301          0.05375   126     0.052500  0.000029\n5        172          0.03071    72     0.030000  0.000017\n6        174          0.03107    73     0.030417  0.000014\n7        116          0.02071    57     0.023750  0.000415\n8        121          0.02161    54     0.022500  0.000036\n9        152          0.02714    67     0.027917  0.000022\n10       105          0.01875    41     0.017083  0.000155\n                \n            \n            \n        \n        \n        \n            Logged the following plots\n            to the ValidMind platform:\n            \n                \n                    Metric Plots\n                    \n                        \n                \n                    \n                \n                \n                \n                    \n                \n                \n                    \n                \n                \n        \n        \n        \n        \n        \n\n\nResults for sklearn_classifier_validation Test Plan:\n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Accuracy Score\n                    \n                    \n                        ✅\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    accuracy_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    True\n                \n                \n                    Params\n                    {'min_threshold': 0.7}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=True, values={'score': 0.72125, 'threshold': 0.7})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        F1 Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    f1_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.14993646759847523, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Roc Auc Score\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    roc_auc_score\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'min_threshold': 0.5}\n                \n            \n            \n                Results\n                [TestResult(test_name=None, column=None, passed=False, values={'score': 0.49822262593987565, 'threshold': 0.5})]\n            \n        \n        \n        \n        \n        Logged the following test result to the ValidMind platform:\n        \n            \n                \n                    \n                        Training Test Degradation\n                    \n                    \n                        ❌\n                    \n                \n                \n                    See Result Details\n                \n            \n            \n                \n                    Test Name\n                    training_test_degradation\n                \n                \n                    Category\n                    model_performance\n                \n                \n                    Passed\n                    False\n                \n                \n                    Params\n                    {'metrics': ['accuracy', 'precision', 'recall', 'f1']}\n                \n            \n            \n                Results\n                [TestResult(test_name='accuracy', column=None, passed=True, values={'test_score': 0.72125, 'train_score': 0.7255357142857143, 'degradation': 0.005906965296578953}), TestResult(test_name='precision', column=None, passed=False, values={'test_score': 0.20068027210884354, 'train_score': 0.19011976047904192, 'degradation': -0.05554662810005893}), TestResult(test_name='recall', column=None, passed=False, values={'test_score': 0.11967545638945233, 'train_score': 0.11308993766696349, 'degradation': -0.05823257893980294}), TestResult(test_name='f1', column=None, passed=False, values={'test_score': 0.14993646759847523, 'train_score': 0.14182021217197097, 'degradation': -0.05722918688531157})]"
  },
  {
    "objectID": "notebooks/train_toy_models.html",
    "href": "notebooks/train_toy_models.html",
    "title": "ValidMind",
    "section": "",
    "text": "The following code snippets can be used to train a toy model for a tabular dataset. As long as training and test datasets are defined in a generic way (using Pandas), we should be able to train any of these models and use it with the developer framework interchangeably.\n\n\nWe assume the a tabular dataset was preprocessed and we have the following dataframes available:\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\n\n\nstatsmodels uses a different interface than SKLearn, so we access the trained model instance differently, by using the return value of model.fit():\nimport statsmodels.api as sm\n\nmodel = sm.Logit(y_train, x_train)\nresult = model.fit()\nThe result object should be passed to the vm.init_model() function:\nvm_model = vm.init_model(result)\n\n\n\nfrom catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(iterations=10,\n                           depth=2,\n                           learning_rate=1,\n                           loss_function='Logloss')\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\n\n\n\n\nThere are multiple ways of training a model with PyTorch. We will use the torch.nn module to define a simple neural network:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n\n        return x\n    \nnet = Net(11, 5, 1)\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nepochs = 10000\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(torch.tensor(x_train.values, dtype=torch.float32))\n    loss = criterion(outputs, torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n    loss.backward()\n    optimizer.step()\n    if epoch % 1000 == 0:\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\nWhen calling vm.init_model(), we need to instance of the nn.Module, i.e. the net object instance in our example:\nvm_model = vm.init_model(net)"
  },
  {
    "objectID": "notebooks/time_series_data_validation_full_suite.html",
    "href": "notebooks/time_series_data_validation_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "The Time Series Data Validation Demo notebook aims to demonstrate the application of various data validation tests using the ValidMind MRM Platform and Developer Framework. Ensuring the quality and an a robust exploratory data analysis of time series data is essential for accurate model predictions and robust decision-making processes.\nIn this demo, we will walk through different data validation suites of tests tailored for time series data, showcasing how these tools can assist you in identifying potential issues and inconsistencies in the data.\n\n\n\nPrepare the environment for our analysis. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\n\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\n# ValidMind libraries \nimport validmind as vm\nfrom validmind.datasets.regression import (\n    identify_frequencies, \n    resample_to_common_frequency\n)\n\n\n\n\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhhz04x40000wcy6shay2oco\"\n)\n\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (clhhz04x40000wcy6shay2oco)\n\n\n\n\n\nWe can find all the test suites and test plans available in the developer framework by calling the following functions:\n\nAll test suites: vm.test_suites.list_suites()\nAll test plans: vm.test_plans.list_plans()\nDescribe a test plan: vm.test_plans.describe_plan(\"time_series_data_quality\")\nList all available tests: vm.test_plans.list_tests()\n\n\nvm.test_suites.list_suites()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n      Test Plans\n    \n  \n  \n    \n      binary_classifier_full_suite\n      BinaryClassifierFullSuite\n      Full test suite for binary classification models.\n      tabular_dataset_description, tabular_data_quality, binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      binary_classifier_model_validation\n      BinaryClassifierModelValidation\n      Test suite for binary classification models.\n      binary_classifier_metrics, binary_classifier_validation, binary_classifier_model_diagnosis\n    \n    \n      tabular_dataset\n      TabularDataset\n      Test suite for tabular datasets.\n      tabular_dataset_description, tabular_data_quality\n    \n    \n      time_series_dataset\n      TimeSeriesDataset\n      Test suite for time series datasets.\n      time_series_data_quality, time_series_univariate, time_series_multivariate\n    \n    \n      time_series_model_validation\n      TimeSeriesModelValidation\n      Test suite for time series model validation.\n      regression_model_performance, regression_models_comparison, time_series_forecast\n    \n  \n\n\n\n\nvm.test_plans.list_plans()\n\n\n\n\n  \n    \n      ID\n      Name\n      Description\n    \n  \n  \n    \n      binary_classifier_metrics\n      BinaryClassifierMetrics\n      Test plan for sklearn classifier metrics\n    \n    \n      binary_classifier_validation\n      BinaryClassifierPerformance\n      Test plan for sklearn classifier models\n    \n    \n      binary_classifier_model_diagnosis\n      BinaryClassifierDiagnosis\n      Test plan for sklearn classifier model diagnosis tests\n    \n    \n      tabular_dataset_description\n      TabularDatasetDescription\n      Test plan to extract metadata and descriptive\n    statistics from a tabular dataset\n    \n    \n      tabular_data_quality\n      TabularDataQuality\n      Test plan for data quality on tabular datasets\n    \n    \n      time_series_data_quality\n      TimeSeriesDataQuality\n      Test plan for data quality on time series datasets\n    \n    \n      time_series_univariate\n      TimeSeriesUnivariate\n      Test plan to perform time series univariate analysis.\n    \n    \n      time_series_multivariate\n      TimeSeriesMultivariate\n      Test plan to perform time series multivariate analysis.\n    \n    \n      time_series_forecast\n      TimeSeriesForecast\n      Test plan to perform time series forecast tests.\n    \n    \n      regression_model_performance\n      RegressionModelPerformance\n      Test plan for performance metric of regression model of statsmodels library\n    \n    \n      regression_models_comparison\n      RegressionModelsComparison\n      Test plan for metrics comparison of regression model of statsmodels library\n    \n  \n\n\n\n\n\n\n\nConigure your use case.\n\n# from validmind.datasets.classification import lending_club as demo_dataset\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\n# Split the dataset into test and training \ndf = demo_dataset.load_data()\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 3551 entries, 1947-01-01 to 2023-04-27\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   MORTGAGE30US  2718 non-null   float64\n 1   FEDFUNDS      825 non-null    float64\n 2   GS10          841 non-null    float64\n 3   UNRATE        903 non-null    float64\ndtypes: float64(4)\nmemory usage: 138.7 KB\n\n\n\n\n\n\n\nUsers can input the configuration to a test suite using config, allowing fine-tuning the suite according to their specific data requirements.\nTime Series Data Quality params - time_series_outliers is set to identify outliers using a specific Z-score threshold - time_series_missing_values defines a minimum threshold to identify missing data points.\nTime Series Univariate params - Visualization: time_series_line_plot and time_series_histogram are designed to generate line and histogram plots respectively for each column in a DataFrame.\n\nSeasonality: seasonal_decompose and auto_seasonality are dedicated to analyzing the seasonal component of the time series. seasonal_decompose performs a seasonal decomposition of the data, while auto_seasonality aids in the automatic detection of seasonality.\nStationarity: window_size determines the number of consecutive data points used for calculating the rolling mean and standard deviation.\nARIMA: acf_pacf_plot, auto_ar, and auto_ma are part of the ARIMA (Autoregressive Integrated Moving Average) model analysis. acf_pacf_plot generates autocorrelation and partial autocorrelation plots, auto_ar determines the order of the autoregressive part of the model, and auto_ma does the same for the moving average part.\n\nTime Series Multivariate params - Visualization: scatter_plot is used to create scatter plots for each column in the DataFrame, offering a visual tool to understand the relationship between different variables in the dataset.\n\nCorrelation: lagged_correlation_heatmap facilitates the creation of a heatmap, which visually represents the lagged correlation between the target column and the feature columns of a demo dataset. This provides a convenient way to examine the time-delayed correlation between different series.\nCointegration: engle_granger_coint sets a threshold for conducting the Engle-Granger cointegration test, which is a statistical method used to identify the long-term correlation between two or more time series.\n\n\nconfig={\n    \n    # TIME SERIES DATA QUALITY PARAMS\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    },\n    \n    # TIME SERIES UNIVARIATE PARAMS \n    \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n     \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive'\n    },\n     \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 4\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 3\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS \n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\n\n\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\nNo frequency could be inferred for variable 'MORTGAGE30US'. Skipping seasonal decomposition and plots for this variable.\n\n\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNo frequency information was provided, so inferred frequency MS will be used.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nShow the frequencies of each variable in the raw dataset.\n\nfrequencies = identify_frequencies(df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      None\n    \n    \n      1\n      FEDFUNDS\n      MS\n    \n    \n      2\n      GS10\n      MS\n    \n    \n      3\n      UNRATE\n      MS\n    \n  \n\n\n\n\nHandle frequencies by resampling all variables to a common frequency.\n\npreprocessed_df = resample_to_common_frequency(df, common_frequency=demo_dataset.frequency)\nfrequencies = identify_frequencies(preprocessed_df)\ndisplay(frequencies)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Frequency\n    \n  \n  \n    \n      0\n      MORTGAGE30US\n      MS\n    \n    \n      1\n      FEDFUNDS\n      MS\n    \n    \n      2\n      GS10\n      MS\n    \n    \n      3\n      UNRATE\n      MS\n    \n  \n\n\n\n\n\n\nRun the same suite again after handling frequencies.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nHandle the missing values by droping all the nan values.\n\npreprocessed_df = preprocessed_df.dropna()\n\n\n\nRun the same test suite to check there are no missing values and frequencies of all variables are the same.\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\nWarning: GS10 is not stationary. Results may be inaccurate.\nWarning: MORTGAGE30US is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: FEDFUNDS is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\n\n\nWarning: GS10 is not stationary. Results may be inaccurate.\n\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nNon-invertible starting MA parameters found. Using zeros as starting parameters.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\nHandle stationarity by taking the first difference.\n\npreprocessed_df = preprocessed_df.diff().fillna(method='bfill')\n\n\n\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nThe default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n\n\nFrequency of MORTGAGE30US: MS\nFrequency of FEDFUNDS: MS\nFrequency of GS10: MS\nFrequency of UNRATE: MS\n\n\niteritems is deprecated and will be removed in a future version. Use .items instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead."
  },
  {
    "objectID": "notebooks/log_image.html",
    "href": "notebooks/log_image.html",
    "title": "ValidMind",
    "section": "",
    "text": "# Quick hack to load local SDK code\nimport os\nos.chdir(os.path.join(os.getcwd(), \"..\"))\n\n# Load API key and secret from environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialize ValidMind SDK\nimport validmind as vm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nprint(os.getcwd())\n\n/Users/panchicore/www/validmind/validmind-sdk\n\n\n\n\n\nvm.init(project=\"cl2r3k1ri000009jweny7ba1g\")\nrun_cuid = vm.start_run()\nprint(run_cuid)\n\ncl5ciojr70000c1sr0usfmiq0\n\n\n\n\n\n\npath_to_img = \"notebooks/images/jupiter_png.png\"\n\nimg = mpimg.imread(path_to_img)\nimgplot = plt.imshow(img)\n\nmetadata = {\"caption\": \"Y Planet\", \"vars\": [\"a\", \"b\", \"c\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(path_to_img, key=\"jupiter\", metadata=metadata, run_cuid=run_cuid)\n\n{'created_at': 1657288332.311301,\n 'cuid': 'cl5ciolcv0002c1sric4lpngt',\n 'filename': 'jupiter.png',\n 'key': 'jupiter',\n 'metadata': {'caption': 'Y Planet',\n  'config': {'x': 1, 'y': 2},\n  'vars': ['a', 'b', 'c']},\n 'test_run_cuid': 'cl5ciojr70000c1sr0usfmiq0',\n 'type': 'file_path',\n 'updated_at': 1657288332.324928,\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/cl5ciojr70000c1sr0usfmiq0/jupiter.png'}\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\n\nfig, ax = plt.subplots()\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\nn, bins, patches = ax.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n\n\nax.set_xlabel('Smarts')\nax.set_ylabel('Probability')\nax.set_title('Histogram of IQ')\nax.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nax.axis([40, 160, 0, 0.03])\nax.grid(True)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(fig, key=\"matplot\", metadata=metadata, run_cuid=run_cuid)\n\n\n{'key': 'matplot',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/matplot.png'}\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\", color_codes=True)\ntips = sns.load_dataset(\"tips\")\ncatplot = sns.catplot(x=\"day\", y=\"total_bill\", data=tips)\n\nmetadata = {\"caption\": \"The Caption\", \"vars\": [\"a\", \"b\"], \"config\": {\"x\": 1, \"y\": 2}}\nvm.log_figure(catplot.fig, key=\"seaborn\", metadata=metadata, run_cuid=run_cuid)\n\n{'key': 'seaborn',\n 'url': 'https://vm-dev-api-project-assets.s3.amazonaws.com/cl2r3k1ri000009jweny7ba1g/figures/seaborn.png'}"
  },
  {
    "objectID": "notebooks/quickstart_classification_full_suite.html",
    "href": "notebooks/quickstart_classification_full_suite.html",
    "title": "ValidMind",
    "section": "",
    "text": "This notebooks provides a quick introduction to documenting a model using the ValidMind developer framework. We will use sample datasets provided by the library and train a simple classification model.\n\n\n\n%load_ext dotenv\n%dotenv .env\n\nimport validmind as vm\nimport xgboost as xgb\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  project = \"clhqxa93500vh0i8hxz2gfzzo\"\n)\n\nConnected to ValidMind. Project: Test No Dataset (clhqxa93500vh0i8hxz2gfzzo)\n\n\n\n\n\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\n\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=['error', 'logloss', 'auc'],\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\nPandas dataset detected. Initializing VM Dataset instance...\nInferring dataset types...\n\n\n\n\n\n\nsuite_config = {\n    \"robustness\":{\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        \"accuracy_decay_threshold\": 4,\n    }\n}\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model,\n    config=suite_config,\n)"
  }
]