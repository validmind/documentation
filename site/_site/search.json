[
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This tutorial shows Model Developers on how to use and configure the Developer Framework and the MRM ValidMind Platform. The following steps will guide you to automatically document and test Time Series Forecasting models:\n\nStep 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Run Data Validation Test Suite on Raw Data\nStep 4: Preprocess Data\nStep 5: Run Data Validation Test Suite on Processed Data\nStep 6: Load Pre-Trained Models\nStep 7: Run Model Validation Test Suite on Models"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#introduction",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#introduction",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "",
    "text": "This tutorial shows Model Developers on how to use and configure the Developer Framework and the MRM ValidMind Platform. The following steps will guide you to automatically document and test Time Series Forecasting models:\n\nStep 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Run Data Validation Test Suite on Raw Data\nStep 4: Preprocess Data\nStep 5: Run Data Validation Test Suite on Processed Data\nStep 6: Load Pre-Trained Models\nStep 7: Run Model Validation Test Suite on Models"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#before-you-begin",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#before-you-begin",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#install-the-client-library",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#install-the-client-library",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#initialize-the-client-library",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#initialize-the-client-library",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    api_secret=\"API_SECRET\",\n    project=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-1-connect-notebook-to-validmind-project",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-1-connect-notebook-to-validmind-project",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 1: Connect Notebook to ValidMind Project",
    "text": "Step 1: Connect Notebook to ValidMind Project\nPrepare the environment for our analysis by following the Before you begin section. First, import all necessary libraries and modules required for our analysis. Next, connect to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\nFinally, define and configure the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis.\n\nExplore Test Suites, Test Plans and Tests\n\nvm.test_suites.list_suites()\n\n\nvm.test_plans.list_plans()\n\n\nvm.tests.list_tests()"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-2-import-raw-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-2-import-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 2: Import Raw Data",
    "text": "Step 2: Import Raw Data\n\nImport FRED Dataset\n\nfrom validmind.datasets.regression import fred as demo_dataset\n\ntarget_column = demo_dataset.target_column\nfeature_columns = demo_dataset.feature_columns\n\ndf = demo_dataset.load_data()\ndf.tail(10)"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-3-run-data-validation-test-suite-on-raw-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-3-run-data-validation-test-suite-on-raw-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 3: Run Data Validation Test Suite on Raw Data",
    "text": "Step 3: Run Data Validation Test Suite on Raw Data\n\nExplore the Time Series Dataset Test Suite\n\nvm.test_suites.describe_suite(\"time_series_dataset\")\n\n\n\nExplore Test Plans\n\nvm.test_plans.describe_plan(\"time_series_data_quality\")\n\n\nvm.test_plans.describe_plan(\"time_series_univariate\")\n\n\nConnect Raw Dataset to ValidMind Platform\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n)\n\n\n\nRun Time Series Dataset Test Suite on Raw Dataset\n\nconfig={\n    \n    # TIME SERIES DATA QUALITY PARAMS\n    \"time_series_outliers\": {\n        \"zscore_threshold\": 3,\n    },\n    \"time_series_missing_values\":{\n        \"min_threshold\": 2,\n    },\n    \n    # TIME SERIES UNIVARIATE PARAMS \n    \"rolling_stats_plot\": {\n        \"window_size\": 12    \n    },\n     \"seasonal_decompose\": {\n        \"seasonal_model\": 'additive'\n    },\n     \"auto_seasonality\": {\n        \"min_period\": 1,\n        \"max_period\": 3\n    },\n      \"auto_stationarity\": {\n        \"max_order\": 3,\n        \"threshold\": 0.05\n    },\n    \"auto_ar\": {\n        \"max_ar_order\": 2\n    },\n    \"auto_ma\": {\n        \"max_ma_order\": 2\n    },\n\n    # TIME SERIES MULTIVARIATE PARAMS \n    \"lagged_correlation_heatmap\": {\n        \"target_col\": demo_dataset.target_column,\n        \"independent_vars\": demo_dataset.feature_columns\n    },\n    \"engle_granger_coint\": {\n        \"threshold\": 0.05\n    },\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-4-preprocess-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-4-preprocess-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 4: Preprocess Data",
    "text": "Step 4: Preprocess Data\n\nHandle Frequencies, Missing Values and Stationairty\n\n# Sample frequencies to Monthly\nresampled_df = df.resample(\"MS\").last()\n\n# Remove all missing values\nnona_df = resampled_df.dropna()\n\n# Take the first different across all variables\npreprocessed_df = nona_df.diff().dropna()"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-5-run-data-validation-test-suite-on-processed-data",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-5-run-data-validation-test-suite-on-processed-data",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 5: Run Data Validation Test Suite on Processed Data",
    "text": "Step 5: Run Data Validation Test Suite on Processed Data\n\nvm_dataset = vm.init_dataset(\n    dataset=preprocessed_df,\n    target_column=demo_dataset.target_column,\n)\n\nfull_suite = vm.run_test_suite(\n    \"time_series_dataset\",\n    dataset=vm_dataset,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-6-load-pre-trained-models",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-6-load-pre-trained-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 6: Load Pre-Trained Models",
    "text": "Step 6: Load Pre-Trained Models\n\nLoad Pre-Trained Models\n\nfrom validmind.datasets.regression import fred as demo_dataset\nmodel_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\nmodel_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\n\n\nConnect a List of Models To the ValidMind Platform\n\n# Initialize training and testing datasets for model A\nvm_train_ds_A = vm.init_dataset(dataset=train_df_A, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_A = vm.init_dataset(dataset=test_df_A, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize training and testing datasets for model B\nvm_train_ds_B = vm.init_dataset(dataset=train_df_B, type=\"generic\", target_column=demo_dataset.target_column)\nvm_test_ds_B = vm.init_dataset(dataset=test_df_B, type=\"generic\", target_column=demo_dataset.target_column)\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = model_A, \n    train_ds=vm_train_ds_A, \n    test_ds=vm_test_ds_A)\n\n# Initialize model B\nvm_model_B = vm.init_model(\n    model = model_B,\n    train_ds=vm_train_ds_B,\n    test_ds=vm_test_ds_B)\n\n\nlist_of_models = [vm_model_A, vm_model_B]"
  },
  {
    "objectID": "notebooks/time_series/tutorial_time_series_forecasting.html#step-7-run-model-validation-test-suite-on-models",
    "href": "notebooks/time_series/tutorial_time_series_forecasting.html#step-7-run-model-validation-test-suite-on-models",
    "title": "Time Series Forecasting Model Tutorial",
    "section": "Step 7: Run Model Validation Test Suite on Models",
    "text": "Step 7: Run Model Validation Test Suite on Models\n\nExplore the Time Series Model Validation Test Suite\n\nvm.test_suites.describe_test_suite(\"time_series_model_validation\")\n\n\n\nExplore Test Plans\n\nvm.test_plans.describe_plan(\"regression_model_description\")\n\n\nvm.test_plans.describe_plan(\"regression_models_evaluation\")\n\n\nvm.test_plans.describe_plan(\"time_series_forecast\")\n\n\nvm.test_plans.describe_plan(\"time_series_sensitivity\")\n\n\n\nRun Model Validation Test Suite on a List of Models\n\nconfig= {\n    \"regression_forecast_plot_levels\": {\n        \"transformation\": \"integrate\",\n    },\n    \"regression_sensitivity_plot\": {\n        \"transformation\": \"integrate\",\n        \"shocks\": [0.3],\n    }\n}\n\nfull_suite = vm.run_test_suite(\n    \"time_series_model_validation\",\n    model = vm_model_B,\n    models = list_of_models,\n    config = config,\n)"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html",
    "href": "notebooks/how_to/implementing_custom_tests.html",
    "title": "Implementing Custom Tests",
    "section": "",
    "text": "Custom metrics allow you to extend the default set of metrics provided by ValidMind and provide full flexibility for documenting any type of model or use case. Metrics and threshold tests are similar in that they both provide a way to evaluate a model. The difference is that metrics capture any arbitrary set of values that measure a behavior in a dataset(s) or model(s), while threshold tests are Boolean tests that evaluate whether a behavior passes or fails a set of criteria.\n\n\nA metric is composed of the following documentation elements:\n\nTitle\nDescription\nResults Table(s)\nPlot(s)\n\nA threshold test is composed of the following documentation elements:\n\nTitle\nDescription\nTest Parameters\nResults Table(s)\nPlot(s)"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#implementing-custom-metrics-and-threshold-tests",
    "href": "notebooks/how_to/implementing_custom_tests.html#implementing-custom-metrics-and-threshold-tests",
    "title": "Implementing Custom Tests",
    "section": "",
    "text": "Custom metrics allow you to extend the default set of metrics provided by ValidMind and provide full flexibility for documenting any type of model or use case. Metrics and threshold tests are similar in that they both provide a way to evaluate a model. The difference is that metrics capture any arbitrary set of values that measure a behavior in a dataset(s) or model(s), while threshold tests are Boolean tests that evaluate whether a behavior passes or fails a set of criteria.\n\n\nA metric is composed of the following documentation elements:\n\nTitle\nDescription\nResults Table(s)\nPlot(s)\n\nA threshold test is composed of the following documentation elements:\n\nTitle\nDescription\nTest Parameters\nResults Table(s)\nPlot(s)"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#before-you-begin",
    "href": "notebooks/how_to/implementing_custom_tests.html#before-you-begin",
    "title": "Implementing Custom Tests",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#install-the-client-library",
    "href": "notebooks/how_to/implementing_custom_tests.html#install-the-client-library",
    "title": "Implementing Custom Tests",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/how_to/implementing_custom_tests.html#initialize-the-client-library",
    "href": "notebooks/how_to/implementing_custom_tests.html#initialize-the-client-library",
    "title": "Implementing Custom Tests",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    api_secret=\"API_SECRET\",\n    project=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n\nMetric class signature\nIn order to implement a custom metric or threshold test, you must create a class that inherits from the Metric or ThresholdTest class. The class signatures below show the different methods that need to be implemented in order to provide the required documentation elements:\n@dataclass\nclass ExampleMetric(Metric):\n    name = \"mean_of_values\"\n\n    # Markdown compatible description of the metric\n    def description(self):\n\n    # Code to compute the metric and cache its results and Figures\n    def run(self):\n\n    # Code to build a list of ResultSummaries that form the results tables\n    def summary(self, metric_values):\nWe’ll now implement a sample metric to illustrate their different documentation components.\n\n\nImplementing a custom metric\nThe following example shows how to implement a custom metric that calculates the mean of a list of numbers.\n\nBasic metric implementation\nAt its most basic, a metric implementation requires a run() method that computes the metric and caches its results and Figures. The run() method is called by the ValidMind client when the metric is executed. The run() should return any value that can be serialized to JSON.\nIn the example below we also provide a simple description for the metric:\n\nfrom dataclasses import dataclass\nfrom validmind.vm_models import Metric\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        \n        return self.cache_results(mean)\n\n\n\nTesting the custom metric\nWe should run a metric first without running an entire test plan and test its behavior.\nThe only requirement to run a metric is build a TestContext object and pass it to the metric initializer. Test context objects allow metrics and tests to access data inside their class methods in a predictable way. By default, ValidMind provides support for the following special keys in a test context objects:\n\ndataset\nmodel\nmodels\n\nWhen a test context object is build with one of these keys, the corresponding value is automatically added to the object as an attribute. For example, if you build a test context object with the dataset key, you can access the dataset inside the metric’s run() method as self.dataset. We’ll illustrate this in detail in the next section.\nIn our simple example, we don’t need to pass any arguments to the TestContext initializer.\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\nYou can also inspect the results of the metric by accessing the result variable:\n\nmean_metric.result.show()\n\n\n\n\nAdd a summary() method to the custom metric\nThe summary() method is used to build a ResultSummary object that can display the results of our test as a list of one or more summray tables. The ResultSummary class takes a results argument that is a list of ResultTable objects.\nEach ResultTable object is composed of a data and metadata attribute. The data attribute is any valid Pandas tabular DataFrame and metadata is a ResultTableMetadata instance that takes title as the table description.\n\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def summary(self, metric_value):\n        # Create a dataframe structure that can be rendered as a table\n        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n\n        return ResultSummary(\n            results=[\n                ResultTable(\n                    data=simple_df,\n                    metadata=ResultTableMetadata(title=\"Example Table\"),\n                ),                \n            ]\n        )        \n        \n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n        return self.cache_results(mean)\n\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\n\nmean_metric.result.show()\n\n\n\nAdd figures to a metric\nYou can also add figures to a metric by passing a figures list to cache_results(). Each figure is a Figure object that takes the following arguments:\n\nfor_object: The name of the object that the figure is for. Usually defaults to self\nfigure: A Matplotlib or Plotly figure object\nkey: A unique key for the figure\n\nThe developer framework uses for_object and key to link figures to the corresponding metric or test.\n\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom validmind.vm_models import Figure, Metric, ResultSummary, ResultTable, ResultTableMetadata\n\n@dataclass\nclass MeanMetric(Metric):\n    name = \"mean_of_values\"\n\n    def description(self):\n        return \"Calculates the mean of the provided values\"\n\n    def summary(self, metric_value):\n        # Create a dataframe structure that can be rendered as a table\n        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n\n        return ResultSummary(\n            results=[\n                ResultTable(\n                    data=simple_df,\n                    metadata=ResultTableMetadata(title=\"Example Table\"),\n                ),\n            ]\n        )        \n\n        \n    def run(self):\n        if \"values\" not in self.params:\n            raise ValueError(\"values must be provided in params\")\n\n        if not isinstance(self.params[\"values\"], list):\n            raise ValueError(\"values must be a list\")\n        \n        values = self.params[\"values\"]\n        mean = sum(values) / len(values)\n\n        # Create a random histogram with matplotlib\n        fig, ax = plt.subplots()\n        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n        ax.set_title(\"Histogram of random numbers\")\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Frequency\")\n\n        # Do this if you want to prevent the figure from being displayed\n        plt.close(\"all\")\n        \n        figure = Figure(\n            for_object=self,\n            key=self.key,\n            figure=fig\n        )\n\n        return self.cache_results(mean, figures=[figure])\n\n\nfrom validmind.vm_models.test_context import TestContext\n\ntest_context = TestContext()\nmean_metric = MeanMetric(test_context=test_context, params={\n    \"values\": [1, 2, 3, 4, 5]\n})\nmean_metric.run()\n\n\nmean_metric.result.show()\n\n\nfrom validmind.vm_models import TestPlan\n\nclass MyCustomTestPlan(TestPlan):\n    \"\"\"\n    Custom test plan\n    \"\"\"\n\n    name = \"my_custom_test_plan\"\n    required_context = []\n    tests = [MeanMetric]\n\nmy_custom_test_plan = MyCustomTestPlan(config={\n    \"mean_of_values\": {\n        \"values\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n})\nresults = my_custom_test_plan.run()"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "",
    "text": "This notebook aim to demostrate the list of interfaces available to get details of test suites, test plans and tests."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#before-you-begin",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#before-you-begin",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#install-the-client-library",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#install-the-client-library",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#initialize-the-client-library",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#initialize-the-client-library",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"API_SECRET\",\n  project = \"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Available test suites",
    "text": "Available test suites\nThe interface will provide the list of test suites available in the ValidMind framework\n\nvm.test_suites.list_suites()"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#test-plans",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#test-plans",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Test plans",
    "text": "Test plans\nThe list of test plans available in a given test suite\n\nvm.test_suites.describe_suite(\"binary_classifier_full_suite\")\n\n\nTest plan description and list of tests\nThe list of tests avaiable in a specific test plan\n\nvm.test_plans.describe_plan(\"tabular_dataset_description\")\n\n\n\nTest detail\n\nvm.tests.describe_test('DescriptiveStatistics')\n\n\n\nDetails of test suites, test plans and tests\nThis interface provide comprehensive details of test suites, test plans and tests\n\nvm.test_suites.describe_suite(\"binary_classifier_full_suite\", verbose=True)"
  },
  {
    "objectID": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites-1",
    "href": "notebooks/how_to/explore_test_suites_test_plans_and_tests.html#available-test-suites-1",
    "title": "Viewing all available Test suites, Test plans and tests",
    "section": "Available test suites",
    "text": "Available test suites\n\nvm.tests.list_tests()"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html",
    "href": "notebooks/how_to/run_a_test_plan.html",
    "title": "Running an Individual Test Plan",
    "section": "",
    "text": "This notebook shows how to run an individual test plan and pass custom config parameters for the tests."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#before-you-begin",
    "href": "notebooks/how_to/run_a_test_plan.html#before-you-begin",
    "title": "Running an Individual Test Plan",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#install-the-client-library",
    "href": "notebooks/how_to/run_a_test_plan.html#install-the-client-library",
    "title": "Running an Individual Test Plan",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_a_test_plan.html#initialize-the-client-library",
    "title": "Running an Individual Test Plan",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    api_secret=\"API_SECRET\",\n    project=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_plan.html#load-the-demo-dataset",
    "title": "Running an Individual Test Plan",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the Raw Dataset\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test_plan.html#train-a-model-for-testing",
    "title": "Running an Individual Test Plan",
    "section": "Train a Model for Testing",
    "text": "Train a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_plan.html#import-and-run-the-individual-test-plan",
    "href": "notebooks/how_to/run_a_test_plan.html#import-and-run-the-individual-test-plan",
    "title": "Running an Individual Test Plan",
    "section": "Import and Run the Individual Test Plan",
    "text": "Import and Run the Individual Test Plan\n\nInitialize ValidMind objects\nWe initize the objects required to run test plans using the ValidMind framework\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nList of available Test Plans\nThe interface to show list of test plans available in the ValidMind development framework\n\nvm.test_plans.list_plans()\n\n\n\nDetail of an individual Test Plan\nThe interface will get detail of a specific test plan\n\nvm.test_plans.describe_plan(\"binary_classifier_model_diagnosis\")\n\n\n\nDefine the required config parameters\nThe config can be apply to specific test to override the default configuration parameters.\nThe format of a config is:\nconfig = {\n    \"&lt;test1_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n     \"&lt;test2_id&gt;\": {\n        \"&lt;default_param_1&gt;\": value,\n        \"&lt;default_param_2&gt;\": value,\n    },\n}\nUsers can input the configuration to test plan using config, allowing fine-tuning the suite according to their specific data requirements.\n\nconfig={\n    \"overfit_regions\": {\n        \"cut_off_percentage\": 3,\n        \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"]\n    },\n    \"weak_spots\":{\n        \"features_columns\": [\"Age\", \"Balance\"],\n        \"accuracy_gap_threshold\": 85,\n    },\n    \"robustness\":{\n        \"features_columns\": [ \"Balance\", \"Tenure\"],\n        \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n        \"accuracy_decay_threshold\": 4,\n    }\n}\n\n\n\nRun the test plan and display results\n\nmodel_diagnosis_test_plan = vm.run_test_plan(\"binary_classifier_model_diagnosis\", \n                                             model=vm_model,\n                                             config=config)\n\n\n\nAccessing the test plan results\nWe can now access all the results of the test plan, including subtest plans using test_plan.get_results().\n\ntest_plan.get_results(): With no arguments, this returns a list of all results\ntest_plan.get_results(test_id): If provided with a test id, this returns the all results that match the given test id\n\nBy default, get_results() returns a list, in case there are multiple tests with the same id.\n\nmodel_diagnosis_test_plan.get_results()\n\n\nmodel_robustness = model_diagnosis_test_plan.get_results(\"robustness\")[0]\nmodel_robustness.show()"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html",
    "href": "notebooks/Introduction_Customer_Churn.html",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "",
    "text": "This interactive notebook will guide you through using the ValidMind Developer Framework to document a model built in Python.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#before-you-begin",
    "href": "notebooks/Introduction_Customer_Churn.html#before-you-begin",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Before you begin",
    "text": "Before you begin\nClick File &gt; Save a copy in Drive &gt; to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#install-the-client-library",
    "href": "notebooks/Introduction_Customer_Churn.html#install-the-client-library",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue on to the next cell."
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#initialize-the-client-library",
    "href": "notebooks/Introduction_Customer_Churn.html#initialize-the-client-library",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n&lt;p&gt;This step requires a documentation project. &lt;a href=\"https://docs.validmind.ai/guide/create-your-first-documentation-project.html\"&gt;Learn how you can create one&lt;/a&gt;.&lt;/p&gt;\n\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"API_SECRET\",\n  project = \"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n\nInitializing Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Introduction_Customer_Churn.html#training-an-example-model",
    "href": "notebooks/Introduction_Customer_Churn.html#training-an-example-model",
    "title": "Introduction - Customer Churn Model Documentation (Data and Model)",
    "section": "Training an Example Model",
    "text": "Training an Example Model\nWe will now train an example model that will be used to demonstrate the ValidMind Developer Framework functions. The following demo datasets are available to use, and on this notebook we’ll train a model for the Bank Customer Churn dataset.\n\nLoading demo dataset\n\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\n\nPreparing the training dataset\nBefore we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n\nDropping irrelevant variables\nEncoding categorical variables\n\n\nDropping irrelevant variables\nThe following variables will be dropped from the dataset:\n\nRowNumber: it’s a unique identifier to the record\nCustomerId: it’s a unique identifier to the customer\nSurname: no predictive power for this variable\nCreditScore: we didn’t observer any correlation between CreditScore and our target column Exited\n\n\ndf.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n\n\n\nEncoding categorical variables\nWe will apply one-hot or dummy encoding to the following variables:\n\nGeography: only 3 unique values found in the dataset\nGender: convert from string to integer\n\n\ngenders = {\"Male\": 0, \"Female\": 1}\ndf.replace({\"Gender\": genders}, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\ndf.drop(\"Geography\", axis=1, inplace=True)\n\nWe are now ready to train our model with the preprocessed dataset:\n\ndf.head()\n\n\n\nDataset preparation\nFor training our model, we will randomly split the dataset in 3 parts:\n\ntraining split with 60% of the rows\nvalidation split with 20% of the rows\ntest split with 20% of the rows\n\nThe test dataset will be our held out dataset for model evaluation.\n\ntrain_df, test_df = train_test_split(df, test_size=0.20)\n\n# This guarantees a 60/20/20 split\ntrain_ds, val_ds = train_test_split(train_df, test_size=0.25)\n\n# For training\nx_train = train_ds.drop(\"Exited\", axis=1)\ny_train = train_ds.loc[:, \"Exited\"].astype(int)\nx_val = val_ds.drop(\"Exited\", axis=1)\ny_val = val_ds.loc[:, \"Exited\"].astype(int)\n\n# For testing\nx_test = test_df.drop(\"Exited\", axis=1)\ny_test = test_df.loc[:, \"Exited\"].astype(int)\n\n\n\n\nModel training\nWe will train a simple XGBoost model and set its eval_set to [(x_train, y_train), (x_val, y_val)] in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of “in training” metrics so model developers can provide additional context to model validators if necessary.\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_train, y_train), (x_val, y_val)],\n    verbose=False,\n)\n\n\ny_pred = model.predict_proba(x_val)[:, -1]\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_val, predictions)\n\nprint(f\"Accuracy: {accuracy}\")\n\nNow that we are satisfied with our model, we can begin using the ValidMind Library to generate test and document it.\n\n\nViewing all test plans available in the developer framework\nWe can find all the test plans and tests available in the developer framework by calling the following functions:\n\nAll test plans: vm.test_plans.list_plans()\nList all available tests: vm.test_plans.list_tests()\nDescribe a test plan: vm.test_plans.describe_plan(\"tabular_data_quality\")\n\nHere is an example:\n\nvm.test_plans.list_plans()\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\n\nRunning a data quality test plan\nWe will now run the default data quality test plan that will collect the following metadata from a dataset:\n\nField types and descriptions\nDescriptive statistics\nData distribution histograms\nFeature correlations\n\nand will run a collection of data quality tests such as:\n\nClass imbalance\nDuplicates\nHigh cardinality\nMissing values\nSkewness\n\nValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=\"Exited\",\n    class_labels={\n        \"0\": \"Did not exit\",\n        \"1\": \"Exited\",\n    }\n)\n\n\n\nInitialize and run the TabularDataset test plan\nWe can now initialize the TabularDataset test suite. The primary method of doing this is with the run_test_suite function from the vm module. This function takes in a test suite name (in this case tabular_dataset) and a dataset keyword argument (the vm_dataset object we created earlier):\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Data Preparation” section of the model documentation.\n\n\n\nRunning a model evaluation test plan\nWe will now run a basic model evaluation test plan that is compatible with the model we have trained. Since we have trained an XGBoost model with a sklearn-like API, we will use the SKLearnClassifier test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\nThe following model metadata is collected:\n\nModel framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\nModel task details (e.g. binary classification, regression, etc.)\nModel hyperparameters (e.g. number of trees, max depth, etc.)\n\nThe model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n\nAUC\nError rate\nLogloss\nFeature importance\n\nSimilarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n\nSimple training/test overfit test\nTraining/test performance degradation\nBaseline test dataset performance test\n\n\nInitialize VM model object and train/test datasets\nIn order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=\"Exited\"\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nWe can now run the BinaryClassifierModelValidation test plan:\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html",
    "title": "Probability of Default Model using ValidMind",
    "section": "",
    "text": "Step 1: Connect Notebook to ValidMind Project\nStep 2: Import Raw Data\nStep 3: Data Description\nStep 4: Data Preparation\nStep 5: Data Description on Preprocessed Data\nStep 6: Univariate Analysis\nStep 7: Multivariate Analysis\nStep 8: Model Training"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-1-connect-to-validmind-project",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-1-connect-to-validmind-project",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 1: Connect to ValidMind Project",
    "text": "Step 1: Connect to ValidMind Project\n\nImport Libraries\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy.stats import chi2_contingency\n%matplotlib inline\n\n\n\nConnect Notebook to ValidMind Project\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n  project = \"cliwzqjgv00001fy6869rlav9\"\n)\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-2-import-raw-data",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-2-import-raw-data",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 2: Import Raw Data",
    "text": "Step 2: Import Raw Data\n\nImport Lending Club Dataset\n\n# Specify the path to the zip file\nfilepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\ndf = pd.read_csv(filepath)\ndf.info()\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-3-data-description",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-3-data-description",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 3: Data Description",
    "text": "Step 3: Data Description\n\nDescribe Raw Dataset\nCreate VM Dataset and Run Test\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n\nvm_df = vm.init_dataset(dataset=df)\ntest_context = TestContext(dataset=vm_df)\n\nmetric = TabularDescriptionTables(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nIdentify Missing Values\n\nfrom validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n\nvm_df = vm.init_dataset(dataset=df)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"threshold\": 80,\n          \"xticks_fontsize\": 8}\n\nmetric = MissingValuesBarPlot(test_context, params)\nmetric.run()\nmetric.result.show()\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-4-data-preparation",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-4-data-preparation",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 4: Data Preparation",
    "text": "Step 4: Data Preparation\n\nIdentify Target Variable\nDefinition of Default\nWe categorizing Fully Paid loans as “default = 0” and Charged Off loans as “default = 1”. This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk).\nLoans with Current status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis.\nAdd default Variable\n\ndef add_target_column(df, target_column):\n    # Assuming the column name is 'loan_status'\n    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n    # Remove rows where the target column is NaN\n    df = df.dropna(subset=[target_column])\n    # Convert target column to integer\n    df[target_column] = df[target_column].astype(int)\n    return df\n\n: \n\n\n\ntarget_column = 'default'\ndf = add_target_column(df, target_column)\n\n# Drop 'loan_status' variable \ndf.drop(columns='loan_status', axis=1, inplace=True)\n\n: \n\n\n\n\nRemove Unused Variables\nRemove all the Demographic and Customer Behavioural features which is of no use for default analysis for credit approval.\n\nunused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n                    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n                    'earliest_cr_line', 'issue_d']\n\ndf = df.drop(columns=unused_variables)\n\n: \n\n\n\n\nRemove Variables with Large Number of Missing Values\n\ndef variables_with_min_missing(df, min_missing_percentage):\n    # Calculate the percentage of missing values in each column\n    missing_percentages = df.isnull().mean() * 100\n\n    # Get the variables where the percentage of missing values is greater than the specified minimum\n    variables_to_drop = missing_percentages[missing_percentages &gt; min_missing_percentage].index.tolist()\n\n    # Also add any columns where all values are missing\n    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n\n    # Remove duplicates (if any)\n    variables_to_drop = list(set(variables_to_drop))\n\n    return variables_to_drop\n\nmin_missing_count = 80\nvariables_to_drop = variables_with_min_missing(df, min_missing_count)\ndf.drop(columns=variables_to_drop, axis=1, inplace=True)\n\ndf.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\ndf.dropna(axis=0, subset=[\"revol_util\"], inplace=True)\n\n: \n\n\n\n\nData Cleaning\n\nfrom typing import List\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport re\n\ndef clean_term_column(df, column):\n    \"\"\"\n    Function to remove 'months' string from the 'term' column and convert it to categorical\n    \"\"\"\n    # Ensure the column exists in the dataframe\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n    \n    df[column] = df[column].str.replace(' months', '')\n    \n    # Convert to categorical\n    df[column] = df[column].astype('object')\n\n    return df\n\ndef clean_rate_columns(df, column):\n    \"\"\"\n    Clean interest rate column. Remove the '%' sign and convert to numeric.\n\n    Parameters:\n    df (pandas.DataFrame): DataFrame to be processed.\n    column (str): Name of the interest rate column to be cleaned.\n    \"\"\"\n    df[column] = df[column].str.replace('%', '')\n    df[column] = pd.to_numeric(df[column])\n\ndef clean_emp_length_column(df, column):\n    \"\"\"\n    Function to clean 'emp_length' column and convert it to categorical.\n    \"\"\"\n    # Ensure the column exists in the dataframe\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n    \n    df[column] = df[column].replace('n/a', np.nan)\n    df[column] = df[column].str.replace('&lt; 1 year', str(0))\n    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n    df[column].fillna(value = 0, inplace=True)\n\n    # Convert to categorical\n    df[column] = df[column].astype('object')\n\n    return df\n\ndef clean_inq_last_6mths(df, column):\n    \"\"\"\n    Function to convert 'inq_last_6mths' column into categorical.\n    \"\"\"\n    # Ensure the column exists in the dataframe\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n\n    # Convert to categorical\n    df[column] = df[column].astype('category')\n\n    return df\n\nclean_emp_length_column(df, 'emp_length')\nclean_term_column(df, 'term')\nclean_inq_last_6mths(df, 'inq_last_6mths')\n\n: \n\n\n\n\nOutliers\n\ndef get_numerical_columns(df):\n        numerical_columns = df.select_dtypes(\n            include=[\"int\", \"float\", \"uint\"]\n        ).columns.tolist()\n        return numerical_columns\n\ndef get_categorical_columns(df):\n        categorical_columns = df.select_dtypes(\n            include=[\"object\", \"category\"]\n        ).columns.tolist()\n        return categorical_columns\n\ndef compute_outliers(series, threshold=1.5):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - threshold * IQR\n    upper_bound = Q3 + threshold * IQR\n    return series[(series &lt; lower_bound) | (series &gt; upper_bound)]\n\ndef remove_iqr_outliers(df, target_column, threshold=1.5):\n    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n    for col in num_cols:\n        outliers = compute_outliers(df[col], threshold)\n        df = df[~df[col].isin(outliers)]\n    return df\n\ndf = remove_iqr_outliers(df, target_column, threshold=1.5)\n\n: \n\n\n\nfrom validmind.tests.data_validation.IQROutliersPlots import IQROutliersPlots\n\nvm_df = vm.init_dataset(dataset=df,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nnum_features = get_numerical_columns(df)\nparams = {\"num_features\": num_features,\n          \"threshold\": 1.5}\n\nmetric = IQROutliersPlots(test_context, params)\nmetric.run()\nmetric.result.show()\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-5-training-data",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-5-training-data",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 5: Training Data",
    "text": "Step 5: Training Data\n\nSampling\nWe employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the stratify = y parameter is set, it ensures that the distribution of the target variable (y) in the test set is the same as that in the original dataset.\nThis is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards.\n\n# Split data into train and test \nX = df.drop(target_column, axis = 1)\ny = df[target_column]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n                                                    random_state = 42, stratify = y)\n\n# Concatenate X_train with y_train to form df_train\ndf_train = pd.concat([X_train, y_train], axis=1)\n\n# Concatenate X_test with y_test to form df_test\ndf_test = pd.concat([X_test, y_test], axis=1)\n\n: \n\n\n\n\nClass Imbalance\nClass imbalance is a common issue in credit risk scorecards and datasets like the Lending Club’s. This imbalance arises when the number of defaulting loans (negative class) is significantly smaller than the number of loans that are paid off (positive class). Such imbalance can lead to biased models that favor the majority class, thus affecting predictive performance.\nSpecial techniques like oversampling, undersampling, or cost-sensitive learning are often needed to ensure that the minority class is appropriately represented during model training.\nUpdate VM Dataset and Run Test\n\nfrom validmind.tests.data_validation.ClassImbalance import ClassImbalance\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nmetric = ClassImbalance(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nFeature Selection\n\nChi-Squared Test on Categorical Features\nRun Test\n\nfrom validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\ncat_features = get_categorical_columns(df_train)\nparams = {\"cat_features\": cat_features,\n          \"p_threshold\": 0.05}\n\nmetric = ChiSquaredFeaturesTable(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nANOVA Test on Numerical Features\nRun Test\n\nfrom validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nnum_features = get_numerical_columns(df_train)\nparams = {\"num_features\": num_features,\n          \"p_threshold\": 0.05}\n\nmetric = ANOVAOneWayTable(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nHeatmap Correlation of Numerical Features\nRun Test\n\nfrom validmind.tests.data_validation.HeatmapFeatureCorrelations import HeatmapFeatureCorrelations\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"declutter\": False,\n          \"features\": None,\n          \"fontsize\": 13}\n\nmetric = HeatmapFeatureCorrelations(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nCorrelations of Numerical Features with Target Variable\nRun Test\n\nfrom validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"declutter\": False,\n          \"features\": None,\n          \"fontsize\": 13}\n\nmetric = FeatureTargetCorrelationPlot(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nSelection of Features\n\ndrop_categorical_features = ['addr_state']\ndrop_numerical_features = ['total_rec_int', 'loan_amnt',\n                           'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n                           'total_pymnt_inv', 'last_pymnt_amnt',]\n\ndf_train.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)\n\n# Update df_test \ndf_test.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-8-feature-engineering",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-8-feature-engineering",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 8: Feature Engineering",
    "text": "Step 8: Feature Engineering\n\nEncoding of Numerical Features\n\nimport pandas as pd\nimport numpy as np\n\ndef encode_numerical_features(df):\n    \n    # term\n    df['term'] = df['term'].replace({' 36': '36M', ' 60': '60M'})\n\n    # emp_length_int\n    df['emp_length'] = df['emp_length'].replace('10+', '10')  # Replace '10+' with '10'\n    df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')  # Convert to numeric\n    df['emp_length'].fillna(-1, inplace=True)\n    bins = [0,1,2,3,5,8,10,999]\n    df['emp_length_bucket'] = pd.cut(df['emp_length'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='emp_length', inplace=True)\n\n    # inq_last_6mths\n    df['inq_last_6mths'].fillna(-1, inplace=True)\n    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n    df['inq_last_6mths_bucket'] = pd.cut(df['inq_last_6mths'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='inq_last_6mths', inplace=True)\n    \n    # total_acc\n    df['total_acc'].fillna(-1, inplace=True)\n    bins = [-1, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 999]\n    df['total_acc_bucket'] = pd.cut(df['total_acc'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='total_acc', inplace=True)\n\n    # annual_inc\n    df['annual_inc'].fillna(-1, inplace=True)\n    df['annual_inc_1000'] = df['annual_inc']/1000\n    bins = [-1, 0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 1000, 10000]\n    df['annual_inc_bucket'] = pd.cut(df['annual_inc_1000'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='annual_inc', inplace=True)\n    df.drop(columns='annual_inc_1000', inplace=True)\n    \n    # int_rate\n    df['int_rate'].fillna(-1, inplace=True)\n    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n    df['int_rate_bucket'] = pd.cut(df['int_rate'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='int_rate', inplace=True)\n\n    # installment\n    df['installment'].fillna(-1, inplace=True)\n    bins = [-1, 0, 100, 200, 300, 400, 500, 750, 1000, 1500]\n    df['installment_bucket'] = pd.cut(df['installment'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='installment', inplace=True)\n\n    # open_acc\n    df['open_acc'].replace(\"N/A\", 1, inplace=True)\n    df['open_acc'].fillna(-1, inplace=True)\n    bins = [-1, 0, 1, 2, 3, 4, 5, 8, 10, 100]\n    df['open_acc_bucket'] = pd.cut(df['open_acc'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='open_acc', inplace=True)\n\n    return df\n\n\ndf_train = encode_numerical_features(df_train)\n\n# Update df_test\ndf_test = encode_numerical_features(df_test)\n\n: \n\n\n\ndef find_categorical_features(df):\n    # Get the column names of features with the data type \"category\"\n    categorical_features = df.select_dtypes(include='category').columns.tolist()\n\n    return categorical_features\n\n\ndef convert_categorical_to_object(df):\n    # Find the categorical features\n    categorical_features = find_categorical_features(df)\n\n    # Convert the categorical features to object type\n    df[categorical_features] = df[categorical_features].astype(str)\n\n    return df\n\ndf_train = convert_categorical_to_object(df_train)\n\n# Update df_test\ndf_test = convert_categorical_to_object(df_test)\n\n: \n\n\n\nvm_df_train = vm.init_dataset(dataset=df_train)\ntest_context = TestContext(dataset=vm_df_train)\n\nmetric = TabularDescriptionTables(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nWoE and IV\n\nfrom validmind.tests.data_validation.WOEIVTable import WOEIVTable\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\n# Configure test parameters\n\nparams = {\n    \"features\": None,\n    \"order_by\": [\"Feature\", \"WoE\"]\n}\n\n# Run test\nmetric = WOEIVTable(test_context, params=params)\nmetric.run()\nwoe_iv_df = metric.result.metric.value['woe_iv']\nmetric.result.show()\n\n: \n\n\n\n\nGroup Buckets\n\nimport pandas as pd\n\ndef coarse_classing(df, mappings):\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_new = df.copy()\n\n    # Loop through each feature and merge set\n    for feature, merge_sets in mappings.items():\n        for merge_set in merge_sets:\n            # Merge the specified categories into a new category\n            df_new[feature] = df_new[feature].apply(lambda x: f\"[{','.join(merge_set)}]\" if x in merge_set else x)\n\n    return df_new\n\n# Create a dictionary of features and the sets to merge\nmappings = {\n    'sub_grade': [['B2','B3','B4','B5','C3','D1'], ['C1','C2','C4','C5'], ['D3','D4','D5','E3','G4'], ['E1','E2','E4','E5','F1','F2','F3','F4','G1','G2','G3','G5','F5']],\n    'grade': [['F','G']],\n    'purpose': [['wedding','major_purchase'], ['credit_card','car'], ['debt_consolidation','other','vacation'], ['medical','moving','house','educational'], ['renewable_energy','small_business']],\n    'home_ownership': [['MORTGAGE','OWN','RENT']],\n    'annual_inc_bucket': [['[250, 1000)','[100, 150)','[150, 250)','[1000, 10000)'], ['[50, 75)','[40, 50)'], ['[10, 20)','[0, 10)']],\n    'emp_length_bucket': [['[2, 3)','[40, 50)','[3, 5)','[1, 2)','[0, 1)','[5, 8)','[8, 10)']],\n    'inq_last_6mths_bucket': [['[4, 5)','[1, 2)'], ['[5, 10)','[3, 4)']],\n    'installment_bucket': [['[300, 400)','[200, 300)','[0, 100)'], ['[400, 500)', '[500, 750)']],\n    'total_acc_bucket': [['[20, 25)','[30, 35)','[15, 20)','[45, 50)','[40, 45)','[35, 40)','[10, 15)','[5, 10)']],\n    'open_acc_bucket': [['[5, 8)','[8, 10)','[10, 100)','[4, 5)'], ['[1, 2)','[2, 3)']]\n}\n\ndf_train = coarse_classing(df_train, mappings)\ndf_train = df_train[~df_train['home_ownership'].isin(['OTHER', 'NONE'])]\ndf_train.drop(columns=\"home_ownership\", inplace=True)\n\n# Update df_test\ndf_test = coarse_classing(df_test, mappings)\ndf_test = df_test[~df_test['home_ownership'].isin(['OTHER', 'NONE'])]\ndf_test.drop(columns=\"home_ownership\", inplace=True)\n\n: \n\n\n\ndef shorten_category_names(df, max_length=20, suffix=\"...\"):\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_new = df.copy()\n    \n    # Iterate over each column in the DataFrame\n    for feature in df_new.columns:\n        # Check if the column has the \"object\" data type\n        if df_new[feature].dtype.name == 'object':\n            # Shorten long category names\n            df_new[feature] = df_new[feature].apply(lambda x: x[:max_length] + suffix if len(x) &gt; max_length else x)\n    \n    return df_new\n\ndf_train = shorten_category_names(df_train, max_length=15, suffix=\"...\")\n\n# Update df_test\ndf_test = shorten_category_names(df_test, max_length=15, suffix=\"...\")\n\n: \n\n\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\n# Configure test parameters\nparams = {\n    \"features\": None,\n    \"order_by\": [\"Feature\", \"WoE\"]\n}\n\n# Run test\nmetric = WOEIVTable(test_context, params=params)\nmetric.run()\nwoe_iv_df = metric.result.metric.value['woe_iv']\nmetric.result.show()\n\n: \n\n\n\nfrom validmind.tests.data_validation.WOEIVPlots import WOEIVPlots\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\nparams = {\n    \"features\": None,\n    \"label_rotation\": 90\n}\n\n# Run test\nmetric = WOEIVPlots(test_context, params=params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nAdd WoE as Features\n\ndef woe_encoder(woe_df, original_df, target):\n    # Initiate an empty DataFrame\n    woe_encoded_df = pd.DataFrame()\n\n    # Loop through each feature-category and get the corresponding WoE value\n    for feature in woe_df['Feature'].unique():\n        for category in woe_df[woe_df['Feature'] == feature]['Category'].unique():\n            woe_value = woe_df[(woe_df['Feature'] == feature) & (woe_df['Category'] == category)]['WoE'].values[0]\n            original_df.loc[original_df[feature] == category, feature] = woe_value\n\n        # Convert the feature to float type\n        original_df[feature] = original_df[feature].astype(float)\n\n    # Creating a new dataframe with WoE values\n    for feature in woe_df['Feature'].unique():\n        woe_encoded_df = pd.concat([woe_encoded_df, original_df[feature]], axis=1)\n\n    # Add the target column to the new DataFrame\n    woe_encoded_df[target] = original_df[target]\n\n    return woe_encoded_df\n\n\ndf_train = woe_encoder(woe_iv_df, df_train, target='default')\n\n# Update df_test\ndf_test = woe_encoder(woe_iv_df, df_test, target='default')\n\n: \n\n\n\nvm_df_train = vm.init_dataset(dataset=df_train)\ntest_context = TestContext(dataset=vm_df_train)\n\nmetric = TabularDescriptionTables(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nfrom validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"declutter\": False,\n          \"features\": None,\n          \"fontsize\": 13}\n\nmetric = FeatureTargetCorrelationPlot(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"declutter\": False,\n          \"features\": None,\n          \"fontsize\": 13}\n\nmetric = HeatmapFeatureCorrelations(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nimport statsmodels.api as sm\n\ny_train = df_train[target_column]\nX_train = df_train.drop(target_column, axis=1)\n\n# Add constant to X_train for intercept term\nX_train = sm.add_constant(X_train)\ndf_train = pd.concat([X_train, y_train], axis=1)\n\n# Update df_test\ny_test = df_test[target_column]\nX_test = df_test.drop(target_column, axis=1)\nX_test = sm.add_constant(X_test)\ndf_test = pd.concat([X_test, y_test], axis=1)\ndf_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n\n# Define the model\nmodel = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n\n# Fit the model\nmodel_fit_glm = model.fit()\n\n# Print out the statistics\nprint(model_fit_glm.summary())\n\n: \n\n\n\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\n\ndef compute_auc(y_true, y_scores):\n    \"\"\"Computes the Area Under the Curve (AUC).\"\"\"\n    auc = roc_auc_score(y_true, y_scores)\n    return auc\n\ndef compute_gini(y_true, y_scores):\n    \"\"\"Computes the Gini coefficient.\"\"\"\n    auc = compute_auc(y_true, y_scores)\n    gini = 2*auc - 1\n    return gini\n\ndef compute_metrics(model, X_train, y_train, X_test, y_test):\n    \"\"\"Computes and prints AUC and GINI for train and test sets.\"\"\"\n\n    metrics_dict = {\"Dataset\": [\"Train\", \"Test\"],\n                    \"AUC\": [],\n                    \"GINI\": []}\n\n    for dataset, X, y in zip([\"Train\", \"Test\"], [X_train, X_test], [y_train, y_test]):\n        # Get predicted probabilities\n        y_scores = model.predict(X)\n\n        # Compute AUC and GINI\n        auc = compute_auc(y, y_scores)\n        gini = compute_gini(y, y_scores)\n\n        # Add the metrics to the dictionary\n        metrics_dict[\"AUC\"].append(auc)\n        metrics_dict[\"GINI\"].append(gini)\n\n    # Convert dictionary to DataFrame for nicer display\n    metrics_df = pd.DataFrame(metrics_dict)\n    return metrics_df\n\nmetrics_df = compute_metrics(model_fit_glm, X_train, y_train, X_test, y_test)\ndisplay(metrics_df)\n\n: \n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curve(y_true, y_scores):\n    fpr, tpr, _ = roc_curve(y_true, y_scores)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC)')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n# Call the function using y_test and y_scores\nplot_roc_curve(y_test, y_scores)\n\n: \n\n\n\n# Cerate VM dataset\nvm_train_ds = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\nvm_test_ds = vm.init_dataset(dataset=df_test,\n                        target_column=target_column)\n\n# Create VM model\nvm_model_glm = vm.init_model(\n    model = model_fit_glm, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\n\n: \n\n\n\nfrom validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\n\ntest_context = TestContext(model= vm_model_glm)\nmetric = ConfusionMatrix(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nfrom validmind.tests.model_validation.sklearn.ROCCurve import ROCCurve\n\ntest_context = TestContext(model= vm_model_glm)\n\nmetric = ROCCurve(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\nMore plots on performance in-sample out of sample.\n\n\nScorecard Development\n\nDistribution of Probability of Default\n\ndef compute_pd(model_fit, X_train):\n\n    # Predict probabilities\n    probabilities = model_fit.predict(X_train)\n\n    # The probabilities are a 2D array with probabilities for the two classes.\n    # We are interested in the probability of default, which is the second column.\n    pd = probabilities\n\n    # Add PD as a new column in X_train\n    X_train['PD'] = pd\n\n    return X_train\n\nX_train_pd = compute_pd(model_fit_glm, X_train)\ndf_train_pd = pd.concat([X_train_pd, y_train], axis=1)\n\n# Update df_test\nX_test_pd = compute_pd(model_fit_glm, X_test)\ndf_test_pd = pd.concat([X_test_pd, y_test], axis=1)\n\n: \n\n\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef plot_pd_histogram(df_train, df_test, pd_col, target_col):\n    # Separate PD based on target column for training data\n    pd_train_0 = df_train[df_train[target_col] == 0][pd_col]\n    pd_train_1 = df_train[df_train[target_col] == 1][pd_col]\n\n    # Separate PD based on target column for testing data\n    pd_test_0 = df_test[df_test[target_col] == 0][pd_col]\n    pd_test_1 = df_test[df_test[target_col] == 1][pd_col]\n\n    # Create subplot\n    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n\n    # Create histograms for training data\n    trace_train_0 = go.Histogram(x=pd_train_0, opacity=0.75, name=f'Train {target_col} = 0')\n    trace_train_1 = go.Histogram(x=pd_train_1, opacity=0.75, name=f'Train {target_col} = 1')\n\n    # Create histograms for testing data\n    trace_test_0 = go.Histogram(x=pd_test_0, opacity=0.75, name=f'Test {target_col} = 0')\n    trace_test_1 = go.Histogram(x=pd_test_1, opacity=0.75, name=f'Test {target_col} = 1')\n\n    # Add traces to the subplots\n    fig.add_trace(trace_train_0, row=1, col=1)\n    fig.add_trace(trace_train_1, row=1, col=1)\n    fig.add_trace(trace_test_0, row=1, col=2)\n    fig.add_trace(trace_test_1, row=1, col=2)\n\n    # Update layout to overlay the histograms in each subplot\n    fig.update_layout(barmode='overlay', title_text='Histogram of Probability of Default')\n\n    # Show the figure\n    fig.show()\n\nplot_pd_histogram(df_train_pd,\n                  df_test_pd, \n                  pd_col='PD', \n                  target_col=target_column)\n\n: \n\n\n\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef plot_cumulative_pd(df_train, df_test, pd_col, target_col):\n    # Separate PD based on target column for training data\n    pd_train_0 = np.sort(df_train[df_train[target_col] == 0][pd_col])\n    pd_train_1 = np.sort(df_train[df_train[target_col] == 1][pd_col])\n\n    # Separate PD based on target column for testing data\n    pd_test_0 = np.sort(df_test[df_test[target_col] == 0][pd_col])\n    pd_test_1 = np.sort(df_test[df_test[target_col] == 1][pd_col])\n\n    # Calculate cumulative distributions\n    cumulative_pd_train_0 = np.cumsum(pd_train_0) / np.sum(pd_train_0)\n    cumulative_pd_train_1 = np.cumsum(pd_train_1) / np.sum(pd_train_1)\n    cumulative_pd_test_0 = np.cumsum(pd_test_0) / np.sum(pd_test_0)\n    cumulative_pd_test_1 = np.cumsum(pd_test_1) / np.sum(pd_test_1)\n\n    # Create subplot\n    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n\n    # Create line plots for training data\n    trace_train_0 = go.Scatter(x=pd_train_0, y=cumulative_pd_train_0, mode='lines', name=f'Train {target_col} = 0')\n    trace_train_1 = go.Scatter(x=pd_train_1, y=cumulative_pd_train_1, mode='lines', name=f'Train {target_col} = 1')\n\n    # Create line plots for testing data\n    trace_test_0 = go.Scatter(x=pd_test_0, y=cumulative_pd_test_0, mode='lines', name=f'Test {target_col} = 0')\n    trace_test_1 = go.Scatter(x=pd_test_1, y=cumulative_pd_test_1, mode='lines', name=f'Test {target_col} = 1')\n\n    # Add traces to the subplots\n    fig.add_trace(trace_train_0, row=1, col=1)\n    fig.add_trace(trace_train_1, row=1, col=1)\n    fig.add_trace(trace_test_0, row=1, col=2)\n    fig.add_trace(trace_test_1, row=1, col=2)\n\n    # Update layout\n    fig.update_layout(title_text='Cumulative Probability of Default')\n\n    # Show the figure\n    fig.show()\n\nplot_cumulative_pd(df_train_pd,\n                  df_test_pd, \n                  pd_col='PD', \n                  target_col=target_column)\n\n: \n\n\n\n\nDistribution of Credit Scores\n\ndef compute_credit_score(model_fit, X_train, target_score, target_odds, pdo):\n    # Get logistic regression coefficients\n    beta = model_fit.params.values\n\n    # Get intercept (alpha)\n    alpha = model_fit.params[0]  # Intercept is the first parameter in statsmodels\n\n    # Calculate factor\n    factor = pdo / np.log(2)\n\n    # Calculate offset\n    offset = target_score - (factor * np.log(target_odds))\n\n    # Initialize an empty list to store scores\n    scores = []\n\n    # Loop over each row in the training data\n    for _, row in X_train.iterrows():\n        # Initialize score for current row\n        score_i = 0\n\n        # Add contribution of each feature to the score\n        for i in range(len(beta)):\n            WoE_i = row[i + 1]  # WoE for feature i, assuming intercept is in the first column\n            score_i += (beta[i] * WoE_i + alpha / len(beta)) * factor + offset / len(beta)\n\n        # Add score to the list of scores\n        scores.append(score_i)\n\n    # Add scores as a new column in X_train\n    X_train['score'] = scores\n\n    return X_train\n\n\n# Set target_score, target_odds, and pdo\ntarget_score = 600\ntarget_odds = 50\npdo = 20\n\n# Compute credit scores and add to df_train\nX_train_scores = compute_credit_score(model_fit_glm, X_train_pd, target_score, target_odds, pdo)\ndf_train_scores = pd.concat([X_train_scores, y_train], axis=1)\n\n# Update df_test \nX_test_scores = compute_credit_score(model_fit_glm, X_test_pd, target_score, target_odds, pdo)\ndf_test_scores = pd.concat([X_test_scores, y_test], axis=1)\n\n: \n\n\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef plot_score_histogram(df_train, df_test, score_col, target_col):\n    # Separate scores based on target column for training data\n    scores_train_0 = df_train[df_train[target_col] == 0][score_col]\n    scores_train_1 = df_train[df_train[target_col] == 1][score_col]\n\n    # Separate scores based on target column for testing data\n    scores_test_0 = df_test[df_test[target_col] == 0][score_col]\n    scores_test_1 = df_test[df_test[target_col] == 1][score_col]\n\n    # Create subplot\n    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n\n    # Create histograms for training data\n    trace_train_0 = go.Histogram(x=scores_train_0, opacity=0.75, name=f'Train {target_col} = 0')\n    trace_train_1 = go.Histogram(x=scores_train_1, opacity=0.75, name=f'Train {target_col} = 1')\n\n    # Create histograms for testing data\n    trace_test_0 = go.Histogram(x=scores_test_0, opacity=0.75, name=f'Test {target_col} = 0')\n    trace_test_1 = go.Histogram(x=scores_test_1, opacity=0.75, name=f'Test {target_col} = 1')\n\n    # Add traces to the subplots\n    fig.add_trace(trace_train_0, row=1, col=1)\n    fig.add_trace(trace_train_1, row=1, col=1)\n    fig.add_trace(trace_test_0, row=1, col=2)\n    fig.add_trace(trace_test_1, row=1, col=2)\n\n    # Update layout to overlay the histograms in each subplot\n    fig.update_layout(barmode='overlay', title_text='Histogram of Scores')\n\n    # Show the figure\n    fig.show()\n\nplot_score_histogram(df_train_scores, \n                     df_test_scores, \n                     score_col='score', \n                     target_col=target_column)\n\n: \n\n\nMethod B.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndef calculate_credit_scores(model, scaling_factor=None, base_points=None):\n    # Set default values if not provided\n    if scaling_factor is None:\n        scaling_factor = 20 / np.log(2)\n    if base_points is None:\n        base_points = 500\n\n    # Get the coefficients from the model\n    coefficients = model.params.values\n    \n    # Get the feature names from the model\n    selected_features = model.params.index\n\n    # Calculate odds ratios\n    odds_ratios = np.exp(coefficients).reshape(-1)\n    \n    # Calculate the scores for each coefficient\n    scores = scaling_factor * np.log(odds_ratios)\n    scores = base_points - scores\n\n    # Create a DataFrame to store feature names and their corresponding scores\n    feature_scores = pd.DataFrame({'Feature': selected_features, 'Score': scores})\n\n    # Sort the DataFrame in descending order of scores\n    feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n\n    return feature_scores\n\n\nscores = calculate_credit_scores(model_fit_glm)\ndisplay(scores)\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#model-training",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#model-training",
    "title": "Probability of Default Model using ValidMind",
    "section": "Model Training",
    "text": "Model Training\nSeparating Features and Target Variables for Training and Test Sets.\n\nX_train = df_train.drop(target_column, axis=1)  \ny_train = df_train[target_column]  \n\nX_test = df_test.drop(target_column, axis=1)  \ny_test = df_test[target_column]\n\n: \n\n\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"features\": None, \n          \"declutter\": False,\n          \"fontsize\": 13}\n\nmetric = HeatmapFeatureCorrelations(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nfeatures = get_numerical_columns(df_train)\nparams = {\"declutter\": True,\n          \"features\": None,\n          \"fontsize\": 13}\n\nmetric = FeatureTargetCorrelationPlot(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nvm_df = vm.init_dataset(dataset=X_train)\ntest_context = TestContext(dataset=vm_df)\n\nmetric = TabularDescriptionTables(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nFeature Statistical Significance\nTrain a GLM Logistic Regression Model.\n\nimport statsmodels.api as sm\n\n# Add constant to X_train for intercept term\n#X_train = sm.add_constant(X_train)\n\n# Define the model\nmodel = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n\n# Fit the model\nmodel_fit = model.fit()\n\n# Print out the statistics\nprint(model_fit.summary())\n\n: \n\n\nRun VM Test\n\nfrom validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n\n# Create VM test and train datasets\nvm_train_ds = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\nvm_test_ds = vm.init_dataset(dataset=df_test,\n                        target_column=target_column)\n\n# Create VM model\nvm_model_reg = vm.init_model(\n    model = model_fit, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\nlist_of_models = [vm_model_reg]\ntest_context = TestContext(models=list_of_models)\n\n# Run test\nmetric = RegressionModelsCoeffs(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\nStatistical Significance of Features.\nRun VM Test\n\nfrom validmind.tests.model_validation.statsmodels.RegressionFeatureSignificance import RegressionFeatureSignificance\n\nparams = {\"p_threshold\": 0.1,\n          \"fontsize\": 12}\n\nmetric = RegressionFeatureSignificance(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nFeature Importance\nBuild a Decision Tree model to calculate feature importance on all preliminary features.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Initialize the model\ntree_model = DecisionTreeClassifier(random_state=0)\n\n# Fit the model\ntree_model_fit = tree_model.fit(X_train, y_train)\n\n: \n\n\n\nfrom validmind.tests.model_validation.sklearn.PermutationFeatureImportance import PermutationFeatureImportance\n\n# Create VM model\nvm_model_pfi = vm.init_model(\n    model = tree_model_fit, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\n\ntest_context = TestContext(model=vm_model_pfi)\n\nparams = {\"fontsize\": None,\n          \"figure_height\": 1000}\n\nmetric = PermutationFeatureImportance(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nFeature Importance vs Significance\n\nfrom validmind.tests.model_validation.statsmodels.FeatureImportanceAndSignificance import FeatureImportanceAndSignificance\n\ntest_context = TestContext(models=[vm_model_reg, vm_model_pfi])\n\nparams = {\"fontsize\": 12,\n          \"p_threshold\": 0.05,\n          \"significant_only\": False,\n          \"figure_height\": 1000,\n          \"bar_width\": 0.4}\n\nmetric = FeatureImportanceAndSignificance(test_context, params)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nDrop Features\n\ndrop_features = ['total_acc', 'purpose__house', 'purpose__medical', 'home_ownership__OTHER', \n                 'purpose__vacation', 'purpose__renewable_energy', 'grade__F', \n                 'purpose__major_purchase', 'purpose__wedding', 'purpose__home_improvement', 'grade__G',\n                 'purpose__moving', 'purpose__other', 'verification_status__Source Verified']\n\nX_train.drop(drop_features, axis=1, inplace=True)\nX_test.drop(drop_features, axis=1, inplace=True)\n\n# If y_train and y_test are Series objects, convert them to DataFrame\nif isinstance(y_train, pd.Series):\n    y_train = y_train.to_frame()\nif isinstance(y_test, pd.Series):\n    y_test = y_test.to_frame()\n\n# Concatenate X_train with y_train and X_test with y_test\ndf_train = pd.concat([X_train, y_train], axis=1)\ndf_test = pd.concat([X_test, y_test], axis=1)\n\n: \n\n\n\n\nFit GLM Model\n\n# Update VM dataset\nvm_train_ds = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\nvm_test_ds = vm.init_dataset(dataset=df_test,\n                        target_column=target_column)\n\n# Fit model\n# X_train = sm.add_constant(X_train) #BUG: need to fix model.py to support models with intercept\nmodel = sm.GLM(y_train, X_train, family=sm.families.Binomial())\nmodel_fit_glm = model.fit()\nprint(model_fit_glm.summary())\n\n\n# Add constant to the input data if necessary\n# X_test = sm.add_constant(X_test) #BUG: fix model.py to support intercepts in regression models \ny_pred = model_fit_glm.predict(X_test)\n\n: \n\n\nCompute Metric Risk Scores\n\n\n\n: \n\n\nDefine Metric Risk Thresholds\n\n\n\n: \n\n\nCompute Metric Risk Scoring\n\n\n\n: \n\n\n\n# Create VM model\nvm_model_glm = vm.init_model(\n    model = model_fit_glm, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\n\n: \n\n\n\n\nFit Decision Tree Model\n\n# Fit Decision Tree model\nmodel_fit_tree = tree_model.fit(X_train, y_train)\n\n# Create VM model\nvm_model_tree = vm.init_model(\n    model = model_fit_tree, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#model-evaluation",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#model-evaluation",
    "title": "Probability of Default Model using ValidMind",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nModel Performance Metrics\nDefine Model Risk Scoring Thresholds\n\nimport pandas as pd\n\ndef risk_scoring_thresholds(metric_ranges):\n    # Create an empty list to store rows\n    rows = []\n\n    # Iterate through each metric in the dictionary\n    for metric, (min_value, max_value) in metric_ranges.items():\n        # Calculate the color ranges and round the values\n        red_range = [round(min_value, 2), round(min_value + (max_value - min_value) / 3, 2)]\n        amber_range = [round(min_value + (max_value - min_value) / 3, 2), round(min_value + 2 * (max_value - min_value) / 3, 2)]\n        green_range = [round(min_value + 2 * (max_value - min_value) / 3, 2), round(max_value, 2)]\n\n        # Append metric and its corresponding ranges to the rows list\n        rows.append([metric, red_range, amber_range, green_range])\n\n    # Create a dataframe from the rows list\n    table = pd.DataFrame(rows, columns=[\"Metric\", \"RED\", \"AMBER\", \"GREEN\"])\n\n    return table\n\nmetric_ranges = {\n    \"Accuracy\": [0.1, 0.9],\n    \"ROC-AUC\": [0.1, 0.9],\n    \"Precision\": [0.1, 0.9],\n    \"Recall\": [0.1, 0.9],\n    \"F1\": [0.1, 0.9]\n}\n\nrisk_thresholds = risk_scoring_thresholds(metric_ranges)\ndisplay(risk_thresholds)\n\n: \n\n\n\n\n\n: \n\n\n\n\nConfusion Matrix\n\nfrom validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\n\ntest_context = TestContext(model= vm_model_glm)\n\nmetric = ConfusionMatrix(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nROC-AUC Curve\n\nfrom validmind.tests.model_validation.sklearn.ROCCurve import ROCCurve\n\ntest_context = TestContext(model= vm_model_glm)\n\nmetric = ROCCurve(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\nfrom validmind.tests.model_validation.sklearn.MinimumROCAUCScore import MinimumROCAUCScore\n\ntest_context = TestContext(model= vm_model_glm)\n\nmetric = MinimumROCAUCScore(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nGINI Coefficients\n\nfrom sklearn.metrics import roc_auc_score\n\ndef gini(true, pred):\n    \"\"\"Calculate Gini coefficient given true and predicted labels\"\"\"\n    gini_score = 2 * roc_auc_score(true, pred) - 1\n    return gini_score\n\ngini_coefficient = gini(y_test, y_pred)\n(f\"Gini Coefficient: {gini_coefficient}\")\n\n: \n\n\n\n\n\n: \n\n\n\n\nScorecard Development\n\n\nMap Model Fit Coefficients to Scores\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\ndef calculate_scores(model, scaling_factor=None, base_points=None):\n    # Set default values if not provided\n    if scaling_factor is None:\n        scaling_factor = 20 / np.log(2)\n    if base_points is None:\n        base_points = 500\n\n    # Get the coefficients from the model\n    coefficients = model.params.values\n    \n    # Get the feature names from the model\n    selected_features = model.params.index\n\n    # Calculate odds ratios\n    odds_ratios = np.exp(coefficients).reshape(-1)\n    \n    # Calculate the scores for each coefficient\n    scores = scaling_factor * np.log(odds_ratios)\n    scores = base_points - scores\n\n    # Create a DataFrame to store feature names and their corresponding scores\n    feature_scores = pd.DataFrame({'Feature': selected_features, 'Score': scores})\n\n    # Sort the DataFrame in descending order of scores\n    feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n\n    return feature_scores\n\n\nscores = calculate_scores(model_fit)\ndisplay(scores)\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#model-risk-assessment",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#model-risk-assessment",
    "title": "Probability of Default Model using ValidMind",
    "section": "Model Risk Assessment",
    "text": "Model Risk Assessment\n\nModel Fit Metric Risk Scores\nDefine Metric Risk Thresholds\n\nmodel_fit_risk_thresholds = {\n    \"D-Squared\": {\n        \"red\": [0, 0.4],\n        \"amber\": [0.4, 0.7],\n        \"green\": [0.7, 1.0]\n    },\n    \"Ratio of Significant Features\": {\n        \"red\": [0, 40],\n        \"amber\": [40, 70],\n        \"green\": [70, 100]\n    }\n}\n\n: \n\n\nCompute Metric Risk Scores\n\ndef regression_model_fit_risk_scores(model_fit_glm):\n    # Risk Measure 1: D Squared\n    d_squared = 1 - (model_fit_glm.deviance / model_fit_glm.null_deviance)\n\n    # Risk Measure 2: Percentage of features with p-value less than 0.05\n    pvalues = model_fit_glm.pvalues\n    significant_features = np.sum(pvalues &lt; 0.05)\n    total_features = pvalues.shape[0]\n    percent_significant_features = (significant_features / total_features)\n\n    # Create DataFrame\n    data = {\n        \"Metric Risk Measure\": [\"D-Squared\", \"Ratio of Significant Features\"],\n        \"Description\": [\n            \"D-Squared: Proportion of the variability in the response variable explained by the model.\",\n            \"Ratio of Significant Features: Percentage of features with a p-value less than 0.05.\"\n        ],\n        \"Metric Risk Score\": [d_squared, percent_significant_features],\n    }\n\n    risk_scores = pd.DataFrame(data)\n\n    # Round to 1 decimal place\n    risk_scores[\"Metric Risk Score\"] = risk_scores[\"Metric Risk Score\"].round(1)\n\n    return risk_scores\n\nmodel_fit_risk_scores = regression_model_fit_risk_scores(model_fit_glm)\ndisplay(model_fit_risk_scores)\n\n: \n\n\n\n\nModel Performance Metric Risk Scores\nDefine Metric Risk Thresholds\n\nmodel_performance_risk_thresholds = {\n    \"Accuracy\": {\n        \"red\": [0, 0.5],\n        \"amber\": [0.5, 0.75],\n        \"green\": [0.75, 1.0]\n    },\n    \"ROC-AUC\": {\n        \"red\": [0, 0.6],\n        \"amber\": [0.6, 0.85],\n        \"green\": [0.85, 1.0]\n    },\n    \"Precision\": {\n        \"red\": [0, 0.4],\n        \"amber\": [0.4, 0.6],\n        \"green\": [0.6, 1.0]\n    },\n    \"Recall\": {\n        \"red\": [0, 0.4],\n        \"amber\": [0.4, 0.6],\n        \"green\": [0.6, 1.0]\n    },\n    \"F1\": {\n        \"red\": [0, 0.4],\n        \"amber\": [0.4, 0.6],\n        \"green\": [0.6, 1.0]\n    }\n}\n\n: \n\n\nCompute Metric Risk Scores\n\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\ndef regression_performance_risk_scores(y_true, y_pred_probs, threshold=0.5):\n    # Threshold the probabilities to get the binary predictions\n    y_pred = (y_pred_probs &gt; threshold).astype(int)\n\n    # Compute the metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    roc_auc = roc_auc_score(y_true, y_pred_probs)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    # Create a DataFrame\n    metrics_df = pd.DataFrame({\n        \"Metric Risk Measure\": [\"Accuracy\", \"ROC-AUC\", \"Precision\", \"Recall\", \"F1\"],\n         \"Description\": [\n            \"Proportion of the total number of predictions that were correct.\",\n            \"Aggregate measure of performance across all possible classification thresholds.\",\n            \"Proportion of positive identifications that were actually correct.\",\n            \"Proportion of actual positives that were identified correctly.\",\n            \"Harmonic mean of precision and recall, it tries to find the balance between precision and recall.\"\n        ],\n        \"Metric Risk Score\": [accuracy, roc_auc, precision, recall, f1],\n    })\n\n    return metrics_df\n\ny_pred = model_fit_glm.predict(X_test)\nmodel_performance_risk_scores = regression_performance_risk_scores(y_test, y_pred)\ndisplay(model_performance_risk_scores)\n\n: \n\n\n\n\nModel Risk Assessment\n\ndef metric_risk_assessment(risk_thresholds, test_results):\n    # Prepare thresholds and test_results data\n    thresholds_df = pd.DataFrame(risk_thresholds).T.reset_index()\n    thresholds_df.columns = ['Metric Risk Measure', 'RED', 'AMBER', 'GREEN']\n    \n    test_results_df = test_results.rename(columns={\"Risk Measure\": \"Metric Risk Measure\", \"Risk Score\": \"Metric Risk Score\"})\n\n    # Add a \"GREY\" column to the thresholds DataFrame and initialize it with \"Fail\"\n    thresholds_df[\"GREY\"] = \"Fail\"\n\n    # Replace the range values in the thresholds DataFrame with \"Pass\" or \"Fail\"\n    for row in thresholds_df.index:\n        metric = thresholds_df.loc[row, \"Metric Risk Measure\"]\n        test_result = test_results_df[test_results_df[\"Metric Risk Measure\"] == metric][\"Metric Risk Score\"].values[0]\n        grey_pass = True\n\n        for col in ['RED', 'AMBER', 'GREEN']:\n            range_values = thresholds_df.loc[row, col]\n            range_start, range_end = extract_range(range_values)\n            if range_start is not None and range_end is not None:\n                if range_start &lt;= test_result &lt;= range_end:\n                    thresholds_df.loc[row, col] = \"Pass\"\n                    grey_pass = False\n                else:\n                    thresholds_df.loc[row, col] = \"Fail\"\n\n        if grey_pass:\n            thresholds_df.loc[row, \"GREY\"] = \"Pass\"\n\n    # Consolidate the risk levels into a single column\n    risk_levels = ['GREY', 'RED', 'AMBER', 'GREEN']\n    risk_scoring_table = pd.concat([test_results_df, thresholds_df[risk_levels]], axis=1)\n    risk_scoring_table['Metric Risk Assessment'] = risk_scoring_table[risk_levels].apply(\n        lambda x: next((level for level in risk_levels if x[level] == \"Pass\"), None),\n        axis=1\n    )\n    risk_scoring_table.drop(columns=risk_levels, inplace=True)\n\n    # Reorder the columns to desired order\n    risk_scoring_table = risk_scoring_table[['Metric Risk Measure', 'Description', 'Metric Risk Score', 'Metric Risk Assessment']]\n\n    return risk_scoring_table\n\ndef extract_range(value):\n    if isinstance(value, (list, tuple)) and len(value) == 2:\n        return value[0], value[1]\n    else:\n        return None, None\n\ndef color_cells(val):\n    colors = {\"GREEN\": \"green\", \"AMBER\": \"yellow\", \"RED\": \"red\", \"GREY\": \"grey\"}\n    return 'background-color: %s' % colors[val]\n\n: \n\n\n\n# Compute risk assessments for all metrics\nmodel_fit_risk_assessment = metric_risk_assessment(model_fit_risk_thresholds, model_fit_risk_scores)\nmodel_performance_risk_assessment = metric_risk_assessment(model_performance_risk_thresholds, model_performance_risk_scores)\n\nmodel_risk_assessment = pd.concat([model_performance_risk_assessment, model_fit_risk_assessment]).reset_index(drop=True)\nmodel_risk_assessment.style.applymap(color_cells, subset=['Metric Risk Assessment'])\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-8-univariate-analysis",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-8-univariate-analysis",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 8: Univariate Analysis",
    "text": "Step 8: Univariate Analysis\n\nHistograms of Numerical Features\nUpdate VM Dataset and Run Test\n\nfrom validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n\nvm_df_train = vm.init_dataset(dataset=df_train)\ntest_context = TestContext(dataset=vm_df_train)\n\nmetric = TabularNumericalHistograms(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n# If 'df' is your DataFrame and 'column_name' is the name of the column\nunique_values = df['inq_last_6mths'].unique()\nprint(unique_values)\n\n: \n\n\n\n\nHigh Cardinality of Categorical Features\nRun Test\n\nfrom validmind.tests.data_validation.HighCardinality import HighCardinality\nmetric = HighCardinality(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nBar Plots of Categorical Features\nRun Test\n\nfrom validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\nmetric = TabularCategoricalBarPlots(test_context)\nmetric.run()\nmetric.result.show()\n\n: \n\n\n\n\nDefault Ratios by Categorical Feature\nRun Test\n\nfrom validmind.tests.data_validation.DefaultRateBarPlots import DefaultRatioBarPlots\n\n# Configure the metric\nparams = {\n    \"default_column\": target_column,\n    \"columns\": None\n}\n\nmetric = DefaultRatioBarPlots(test_context, params=params)\nmetric.run()\nmetric.result.show()\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-9-multivariate-analysis",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-9-multivariate-analysis",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 9: Multivariate Analysis",
    "text": "Step 9: Multivariate Analysis\n\nBivariate Bar Plots of Default Ratios\nUpdate VM Dataset and Run Test\n\nfrom validmind.tests.data_validation.BivariateFeaturesBarPlots import BivariateFeaturesBarPlots\n\n# Pass target column to validmind dataset\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\n# Configure the metric\nfeatures_pairs = {'home_ownership': 'grade', \n                  'purpose': 'grade',\n                  'grade': 'verification_status'}\n\nparams = {\n    \"features_pairs\": features_pairs,\n}\n\nmetric = BivariateFeaturesBarPlots(test_context, params=params)\nmetric.run()\n\n: \n\n\n\n\nScatter Plots by Default Status\nRun Test\n\nfrom validmind.tests.data_validation.BivariateScatterPlots import BivariateScatterPlots\n\nfeatures_pairs = {'int_rate': 'annual_inc', \n                  'funded_amnt_inv': 'dti', \n                  'annual_inc': 'funded_amnt_inv',\n                  'loan_amnt': 'int_rate',\n                  'int_rate': 'annual_inc',\n                  'earliest_cr_line': 'int_rate'}\n\nparams = {\n    \"features_pairs\": features_pairs,\n    \"target_filter\": None\n}\n\nmetric = BivariateScatterPlots(test_context, params=params)\nmetric.run()\n\n: \n\n\n\n\nBivariate Histograms\nRun Test\n\nfrom validmind.tests.data_validation.BivariateHistograms import BivariateHistograms\n\nfeatures_pairs = {'int_rate': 'annual_inc', \n                  'funded_amnt_inv': 'dti', \n                  'annual_inc': 'funded_amnt_inv',\n                  'loan_amnt': 'int_rate',\n                  'int_rate': 'annual_inc',\n                  'earliest_cr_line': 'int_rate'}\n\nparams = {\n    \"features_pairs\": features_pairs,\n    \"target_filter\": None\n}\n\nmetric = BivariateHistograms(test_context, params=params)\nmetric.run()\n\n: \n\n\nRun Test\n\nfrom validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n\nmetric = PearsonCorrelationMatrix(test_context)\nmetric.run()\nmetric.result.show()\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-10-feature-engineering",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-10-feature-engineering",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 10: Feature Engineering",
    "text": "Step 10: Feature Engineering\n\nAdd Dummy Catergorical Variables\n\ndef add_dummy_variables(df, columns_list):\n    \"\"\"\n    Generate dummy variables for specified columns in the DataFrame,\n    concatenate them with the original DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): DataFrame to be processed.\n    columns_list (list): List of column names to be processed.\n    \"\"\"\n    for column in columns_list:\n        dummies = pd.get_dummies(df[column], prefix=column + \":\", drop_first=False)\n        df = pd.concat([df, dummies], axis=1)\n    return df\n\n: \n\n\n\n# df_train = add_dummy_variables(df_train, ['grade', 'home_ownership', 'verification_status', 'purpose'])\n# df_test = add_dummy_variables(df_test, ['grade', 'home_ownership', 'verification_status', 'purpose'])\n\n: \n\n\n\n# Adjust the X_test DataFrame to match the column structure of the X_train DataFrame\n# df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n\n: \n\n\n\n\nWeight of Evidence (WoE) Binning\nFrom a modelling perspective, the WoE allows us to transform raw variables into a format which provides a more robust base for statistical analysis. Specifically, the WoE measures the predictive power of an individual class of a categorical variable, distinguishing between ‘good’ (non-defaulters) and ‘bad’ (defaulters) risks. This is accomplished by comparing the distribution of ‘good’ and ‘bad’ risks within a specific category to the overall ‘good’/‘bad’ distribution. If the ‘good’/‘bad’ ratio of a particular category is significantly divergent from the overall ratio, it suggests that category is a strong predictor of credit risk.\nInformation Value (IV), on the other hand, is a fundamental metric we use to quantify the predictive power of each input variable in our scorecards. The IV is calculated by taking the sum of the differences between the WoE of each category and the overall WoE, multiplied by the WoE of that category. In other words, IV measures the total amount of ‘information’ or predictive power a variable brings to the model. For example, variables with an IV between 0.1 and 0.3 provide a weak predictive power, those between 0.3 and 0.5 a medium predictive power, and those with an IV greater than 0.5 have strong predictive power. Therefore, we utilize the IV to prioritize variables for inclusion in the model and to ensure the model’s stability and accuracy.\n\nWoE and IV for Categorical Variables\n\n\n\n: \n\n\n\n#categorical_woe_iv_df = calculate_woe_iv(df_train, target_column, categorical_features)\n#display(categorical_woe_iv_df)\n\n: \n\n\nUpdate VM Dataset and Run Test\n\nfrom validmind.tests.data_validation.WOEIVPlots import WoEandIVPlots\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\n# Configure test parameters\nparams = {\n    \"features\": categorical_features,\n    \"label_rotation\": 90\n}\n\n# Run test\nmetric = WoEandIVPlots(test_context, params=params)\nmetric.run()\nmetric.result.show()\n\n:"
  },
  {
    "objectID": "notebooks/probability_of_default/probability_of_default_validmind.html#step-11-model-training",
    "href": "notebooks/probability_of_default/probability_of_default_validmind.html#step-11-model-training",
    "title": "Probability of Default Model using ValidMind",
    "section": "Step 11: Model Training",
    "text": "Step 11: Model Training\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# First, we define the preprocessing steps\nnumeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\ncategorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range']\n\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000))])\n\n\n\n# Train the model\nclf.fit(X_train, y_train)\n\n# We can now evaluate on the test set\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\n\n: \n\n\n\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# First, we define the preprocessing steps\nnumeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\ncategorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range', 'installment']  # Added 'installment'\n\n# Handle categorical features\ndf_encoded = pd.get_dummies(df_multivariate, columns=categorical_features)\n\n# Split the data\nX = df_encoded.drop('loan_status', axis=1)\ny = df_encoded['loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Add a constant to the independent values\nX_train = sm.add_constant(X_train)\n\n# Define the model\nglm_model_fit = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n\n# Fit the model\nresults = glm_model_fit.fit()\n\n# Print the summary\nprint(results.summary())\n\n# Evaluate on the test set\nX_test = sm.add_constant(X_test)  # Adding a constant to the test data\ny_pred = results.predict(X_test)\n\n# You can then further analyze y_pred to measure model performance on the test set.\n\n: \n\n\nScale variable X.\n\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import scale\n\n# Scale your variables\nX_scaled = scale(X)\n\n# Add a constant to the independent values\nX_scaled = sm.add_constant(X_scaled)\n\n# Define the model\nmodel = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n\n# Fit the model\nresults = model.fit()\n\n# Print the summary\nprint(results.summary())\n\n: \n\n\n#### ValidMind Models\n\n# Initialize training and testing datasets for model A\nvm_train_ds = vm.init_dataset(dataset=X_train, type=\"generic\", target_column='loan_status')\nvm_test_ds = vm.init_dataset(dataset=X_test, type=\"generic\", target_column='loan_status')\n\n# Initialize model A\nvm_model_A = vm.init_model(\n    model = glm_model_fit, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\n\n:"
  },
  {
    "objectID": "notebooks/external_test_providers_demo.html",
    "href": "notebooks/external_test_providers_demo.html",
    "title": "Integrate an External Test Provider",
    "section": "",
    "text": "This notebook demonstrates how to use a custom test provider to be able to use custom tests with the Validmind Developer Framework. In the notebook, we load a couple different demo test providers and register them with the Validmind framework to be able to run a template that utilizes those tests."
  },
  {
    "objectID": "notebooks/external_test_providers_demo.html#before-you-begin",
    "href": "notebooks/external_test_providers_demo.html#before-you-begin",
    "title": "Integrate an External Test Provider",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/external_test_providers_demo.html#install-the-client-library",
    "href": "notebooks/external_test_providers_demo.html#install-the-client-library",
    "title": "Integrate an External Test Provider",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/external_test_providers_demo.html#initialize-the-client-library",
    "href": "notebooks/external_test_providers_demo.html#initialize-the-client-library",
    "title": "Integrate an External Test Provider",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    api_secret=\"API_SECRET\",\n    project=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n\nUpdate your template\nWithin the project you would like to use, create a new Content Block in the documentation template and add the following code:\n      - content_type: metric\n        content_id: my_local_provider.tests.MyCustomTest\n      - content_type: metric\n        content_id: my_inline_provider.tests.MyCustomTest\nSee the section below on setting up and registering test providers for more info on how these content_id’s get mapped to the actual test providers and tests.\n\n\nPreview the template for the current project to validate that it looks correct\n\n# we should see two custom content blocks in the template whose IDs are under the namespaces registered below (`my_inline_provider` and `my_local_provider`)\n# the ID should match the path to the `tests` directory in this repo\nvm.preview_template()\n\n\n\nRegister external test providers\nWe will now instantiate and register the Test Provider classes that we imported above.\nFor the Github provider, we will need to specify the org and repo to pull from as well as optionally pass a token if the repo is private.\nFor the Local Filesystem provider, we will just need to specify the root folder under which the provider class will look for tests. In this case, it is the current directory for this demo so you may have to adjust the path for your machine.\nImport the Local File System Test Provider from the validmind.tests module\n\nfrom validmind.tests import LocalTestProvider\n\nCreate an inline TestProvider Class that just returns a single test\n\nimport pandas as pd\n\nclass MySecondCustomTest(vm.vm_models.Metric):\n    # The metric name should match the content ID on the template\n    name = \"my_inline_provider.tests.MyCustomTest\"\n\n    def description(self):\n        return \"This is a custom test from an external test provider.\"\n\n    def run(self):\n        return self.cache_results([{\"foo\": \"bar\"}])\n\n    def summary(self, results):\n        return vm.vm_models.ResultSummary(\n            results=[\n                vm.vm_models.ResultTable(\n                    data=pd.DataFrame(results),\n                    metadata=vm.vm_models.ResultTableMetadata(\n                        title=\"Results from Test Provider Inside Notebook\"\n                    ),\n                )\n            ]\n        )\n\n\nclass TestProviderInline:\n    def load_test(self, test_id):\n        # ignore the test_id and just return the single test above\n        return MySecondCustomTest\n\n\n# instantiate the test provider\ninline_test_provider = TestProviderInline()\nlocal_test_provider = LocalTestProvider(root_folder=\".\")\n\n# register the test providers\nvm.tests.register_test_provider(\n    namespace=\"my_inline_provider\",\n    test_provider=inline_test_provider,\n) # validmind will now call the `TestProviderInline.load_test` method whenever it encounters a test ID that starts with `my_inline_provider`\n\nvm.tests.register_test_provider(\n    namespace=\"my_local_provider\",\n    test_provider=local_test_provider,\n) # validmind will now call the `LocalTestProvider.load_test` method whenever it encounters a test ID that starts with `my_local_provider`\n\n\n\nRunning the template\nNow we can run the template as usual and it will use the external test providers to load the appropriate tests. Note that we’re not passing any context such as dataset and model to run_template(). This is because the template we’re using for this demo only has a single section and does not require any additional context to run.\n\nvm.run_template()"
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html",
    "href": "releases/2023-aug-15/highlights.html",
    "title": "August 15, 2023",
    "section": "",
    "text": "This release includes a number of improvements for the developer experience when using the ValidMind Developer Framework, along with a visual redesign of the Platform UI.\n\n\n\n\n\nImproved developer experience for the Metric test class. We made a number of enhancements to the ValidMind Developer Framework to improve the experience for developers:\n\nBetter test organization: Added a metadata and tags attribute to the Test base class for better categorization and filtering of tests.\nBetter filtering: Added a new task_type argument to the list_tests() function to allow simple filtering by task_type set in a test.\nClearer naming: Renamed required_context to required_inputs across the whole ValidMind SDK.\n\n\n\n\nEnhancements to test configuration discovery: To make the discovery of required context and default configuration easier, this update includes a number of changes to how required_context and default_parameters are presented:\n\nPreview template expansion: With vm.preview_template(), you can now view detailed, expandable information for each content block driven by a test. This includes the test’s name, description, required context, and default parameters.\nSimplified test listing: The vm.tests.list_tests() function has been refined to show only the ID, Name, and Description for each test, making it more user-friendly.\nDetailed test descriptions: You can now obtain complete test details, including required_context and default_params, using the vm.tests.describe_test() function.\nRetrieve test suite: The new function vm.get_test_suite() allows you to access a specific test suite instance via the high-level Validmind API.\nAccess to default configuration: A new method, get_default_config(), is now part of both the TestPlan and TestSuite classes, letting you obtain a dictionary containing all the configuration parameters used by the tests.\nEasier required context: A complementary new method, get_required_context(), now returns a list of all the context variables needed for tests to run, making it easier for you to ensure that everything is set up correctly.\n\n\n\n\n\n\nRedesigned application and navigation bar layout. The new application design offers a cleaner, more intuitive interface that enhances user experience. The updated navigation bar layout specifically contributes to easier navigation, allowing users to find what they need with greater efficiency."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html#release-highlights",
    "href": "releases/2023-aug-15/highlights.html#release-highlights",
    "title": "August 15, 2023",
    "section": "",
    "text": "This release includes a number of improvements for the developer experience when using the ValidMind Developer Framework, along with a visual redesign of the Platform UI.\n\n\n\n\n\nImproved developer experience for the Metric test class. We made a number of enhancements to the ValidMind Developer Framework to improve the experience for developers:\n\nBetter test organization: Added a metadata and tags attribute to the Test base class for better categorization and filtering of tests.\nBetter filtering: Added a new task_type argument to the list_tests() function to allow simple filtering by task_type set in a test.\nClearer naming: Renamed required_context to required_inputs across the whole ValidMind SDK.\n\n\n\n\nEnhancements to test configuration discovery: To make the discovery of required context and default configuration easier, this update includes a number of changes to how required_context and default_parameters are presented:\n\nPreview template expansion: With vm.preview_template(), you can now view detailed, expandable information for each content block driven by a test. This includes the test’s name, description, required context, and default parameters.\nSimplified test listing: The vm.tests.list_tests() function has been refined to show only the ID, Name, and Description for each test, making it more user-friendly.\nDetailed test descriptions: You can now obtain complete test details, including required_context and default_params, using the vm.tests.describe_test() function.\nRetrieve test suite: The new function vm.get_test_suite() allows you to access a specific test suite instance via the high-level Validmind API.\nAccess to default configuration: A new method, get_default_config(), is now part of both the TestPlan and TestSuite classes, letting you obtain a dictionary containing all the configuration parameters used by the tests.\nEasier required context: A complementary new method, get_required_context(), now returns a list of all the context variables needed for tests to run, making it easier for you to ensure that everything is set up correctly.\n\n\n\n\n\n\nRedesigned application and navigation bar layout. The new application design offers a cleaner, more intuitive interface that enhances user experience. The updated navigation bar layout specifically contributes to easier navigation, allowing users to find what they need with greater efficiency."
  },
  {
    "objectID": "releases/2023-aug-15/highlights.html#how-to-upgrade",
    "href": "releases/2023-aug-15/highlights.html#how-to-upgrade",
    "title": "August 15, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html",
    "href": "guide/edit-model-inventory-fields.html",
    "title": "Edit model inventory fields",
    "section": "",
    "text": "Learn how to edit individual model detailed fields in the model inventory. This topic is relevant for model owners who want to make model details are accurate and up to date in the Inventory."
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#prerequisites",
    "href": "guide/edit-model-inventory-fields.html#prerequisites",
    "title": "Edit model inventory fields",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nYou are the Model Owner for the specific model you would like edit the details of, or an administrator\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#steps",
    "href": "guide/edit-model-inventory-fields.html#steps",
    "title": "Edit model inventory fields",
    "section": "Steps",
    "text": "Steps\n\n\nNavigate to the relevant model details in the model inventory:\n\nFrom the ValidMind Home page, click Model Inventory on the left.\nClick the relevant model entry to view the model details.\n\nUse the Edit buttons to edit specific fields on the model details page:\n\nMake the Edit button appear by hovering over a data field you would like to edit.\nClick the Edit button to modify entries in each field.\n\nClick Done to save your edits.\n\n\n\n\n\n\n\n\n\n\nThe following fields cannot be edited:\n\nID\nModel Validators"
  },
  {
    "objectID": "guide/edit-model-inventory-fields.html#related-topics",
    "href": "guide/edit-model-inventory-fields.html#related-topics",
    "title": "Edit model inventory fields",
    "section": "Related topics",
    "text": "Related topics\n\nCreate a new documentation project"
  },
  {
    "objectID": "guide/faq-inventory.html",
    "href": "guide/faq-inventory.html",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "href": "guide/faq-inventory.html#can-permissions-for-the-model-inventory-be-configured",
    "title": "Model Inventory",
    "section": "",
    "text": "ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles."
  },
  {
    "objectID": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "href": "guide/faq-inventory.html#is-it-possible-to-track-or-view-a-summary-of-questions-asked-by-validators",
    "title": "Model Inventory",
    "section": "Is it possible to track or view a summary of questions asked by validators?",
    "text": "Is it possible to track or view a summary of questions asked by validators?\nQuestions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind UI."
  },
  {
    "objectID": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "href": "guide/faq-inventory.html#can-the-model-inventory-track-revalidation-periodic-validation-dates-and-more",
    "title": "Model Inventory",
    "section": "Can the model inventory track revalidation, periodic validation dates, and more?",
    "text": "Can the model inventory track revalidation, periodic validation dates, and more?\nIn addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "href": "guide/faq-inventory.html#do-you-support-executive-reporting-for-senior-leaders-in-our-bus",
    "title": "Model Inventory",
    "section": "Do you support executive reporting for senior leaders in our BUs?",
    "text": "Do you support executive reporting for senior leaders in our BUs?\nValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.\nThese metrics can be exported into a customizable report for the customer."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html",
    "href": "guide/swap-documentation-project-templates.html",
    "title": "Swap Documentation Project Templates",
    "section": "",
    "text": "Learn how to swap documentation templates for projects in the ValidMind Platform UI. By swapping templates, you can keep your documentation projects up-to-date without starting a new project from scratch. This topic is useful for:\nSwapping templates allows you to switch to a completely different template, upgrade to a more recent version of your current template, or make changes to both the template and its version at the same time."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#prerequisites",
    "href": "guide/swap-documentation-project-templates.html#prerequisites",
    "title": "Swap Documentation Project Templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you are not sure which template or which version of a template a documentation project is using, check the Project Overview page of your documentation project. The Documentation Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#steps",
    "href": "guide/swap-documentation-project-templates.html#steps",
    "title": "Swap Documentation Project Templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Documentation Projects.\nOpen the documentation project that you want to update the template for. The right sidebar lists the templates currently in use.\nSelect the template currently in use under one of the following:\n\nDocumentation Template\nValidation Report Template\n\nThe window pane that opens shows the JSON for the current template along with other information, such as the name and the current version.\nClick Swap Template.\nThe window pane now shows the JON for two templates side-by-side:\n\nOn the left, your current template is shown.\nOn the right, you can select a different template and version.\n\nInitially, both templates are the same.\nOn the right, select a different template or version:\n\nTemplate: Change to a different template entirely\nVersion: Change to a different version of the template you selected\n\nFor example: Select a previous version of the template currently in use to revert to that version.\nAfter you select a different template or version, the JSON differences between the templates are highlighted.\nClick Prepare Swap.\nEnter a note to enable completing the swap and click Swap Template.\n\nAfter your model documentation template has been swapped successfully, you can now continue to work on your documentation project."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#about-swapping-templates",
    "href": "guide/swap-documentation-project-templates.html#about-swapping-templates",
    "title": "Swap Documentation Project Templates",
    "section": "About swapping templates",
    "text": "About swapping templates\n\nWhen swapping templates, only the document structure is changed. Any modifications that you might have made to content will be preserved inside each content block or section.  If you added a simple text block to your old template and want to reuse the content, you can temporarily switch back to the old template, copy the content, swap back to the new template, and then paste in the content."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#whats-next",
    "href": "guide/swap-documentation-project-templates.html#whats-next",
    "title": "Swap Documentation Project Templates",
    "section": "What’s next",
    "text": "What’s next\nYou can see which version of a template is used in the Project Overview page of each documentation project."
  },
  {
    "objectID": "guide/swap-documentation-project-templates.html#related-topics",
    "href": "guide/swap-documentation-project-templates.html#related-topics",
    "title": "Swap Documentation Project Templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html",
    "href": "guide/view-documentation-guidelines.html",
    "title": "View documentation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for model documentation associated with a template. This topic is relevant for model developers who need to ensure that they are following the guidelines for a template."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#prerequisites",
    "href": "guide/view-documentation-guidelines.html#prerequisites",
    "title": "View documentation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online editor in the ValidMind Platform UI\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#steps",
    "href": "guide/view-documentation-guidelines.html#steps",
    "title": "View documentation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Documentation page.\nIn any section of the documentation for a model, click the ValidMind Insights button on the top right to expand the ValidMind Insights right sidebar: \n\nThe Documentation Guidelines tab shows the documentation guidelines associated with this documentation template that have been configured by your model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation.\n\n\n\n\n\n\n\n\n\n\n\nThe documentation guidelines for each template can be configured by an administrator."
  },
  {
    "objectID": "guide/view-documentation-guidelines.html#related-topics",
    "href": "guide/view-documentation-guidelines.html#related-topics",
    "title": "View documentation guidelines",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/faq-workflows.html",
    "href": "guide/faq-workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "How are parallel editing and version control handled?\nValidMind currently allows multiple users to simultaneously edit documentation in the ValidMind UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail.\nValidMind is implementing more sophisticated version control features:\n\nValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).\nThe platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).\nAdministrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).\n\n\n\nCan we work with disconnected workflows?\nValidMind supports disconnected workflows natively at the data-collection level since the Developer Framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.\nVisualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.\n\n\nCan the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?\nWith ValidMind, administrators can create custom workflows for the review and approval of documentation.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage.\nValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).\n\n\n\nHow flexible is ValidMind to accommodate our own model development and review workflows?\nValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.\nThese workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.\nYou can also leverage ValidMind’s Developer Framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html",
    "href": "guide/install-and-initialize-developer-framework.html",
    "title": "Install and initialize the Developer Framework",
    "section": "",
    "text": "These steps show how a model developer can integrate the Developer Framework in our own developer environment by installing and initializing it.\nFor example, you can use these steps to initialize the Developer Framework as part of a Jupyter notebook or use it in other parts of your customer infrastructure, such as MLOps."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "href": "guide/install-and-initialize-developer-framework.html#prerequisites",
    "title": "Install and initialize the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to integrate the Developer Framework and to be able to upload to the ValidMind Platform, you must provide the following information:\n\n\n\nArgument\nDescription\n\n\n\n\napi_host\nThe location of the ValidMind API\n\n\napi_key\nThe account API key\n\n\napi_secret\nThe account secret key\n\n\nproject\nThe project identifier"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#locate-the-framework-integration-instructions",
    "href": "guide/install-and-initialize-developer-framework.html#locate-the-framework-integration-instructions",
    "title": "Install and initialize the Developer Framework",
    "section": "Locate the framework integration instructions",
    "text": "Locate the framework integration instructions\nFor existing projects, this information can be found in the ValidMind UI:\n\nGo to the Documentation Projects page and select the project.\nClick Client integration and scroll down to Initializing the client library.\nLocate the code snippet and click Copy to clipboard.\n\nIf you do not have an existing project, you can create one.\nThe Developer Framework also requires access to the data sources where data sets used for training, testing, and trained model files are stored. This access is needed to run model documentation and validation tests, and to upload to the ValidMind Platform to populate the model documentation and validation reports."
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#install-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Install the client library",
    "text": "Install the client library\nTo install the client library:\npip install validmind"
  },
  {
    "objectID": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "href": "guide/install-and-initialize-developer-framework.html#initialize-the-client-library",
    "title": "Install and initialize the Developer Framework",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nTo initialize the client library, paste the code snippet with the client integration details directly into your development source code, replacing this example with your own:\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.dev.vm.validmind.ai/api/v1/tracking/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  project = \"&lt;project-identifier&gt;\"\n)\n\n\n\n\n\n\n\n\nDon’t forget\n\n\n\nReplace the API key and secret shown in these steps with your own.\n\n\nAfter you have pasted the code snippet into your development source code and executed the code, the Python client library will register with ValidMind. You can now use the Developer Framework to document and test your models, and to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/register-models.html",
    "href": "guide/register-models.html",
    "title": "Register models in the inventory",
    "section": "",
    "text": "Learn how to register a model you are documenting in the model inventory. This topic is relevant for model owners who want to enable their model development teams to use ValidMind’s model documentation and validation features."
  },
  {
    "objectID": "guide/register-models.html#prerequisites",
    "href": "guide/register-models.html#prerequisites",
    "title": "Register models in the inventory",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/register-models.html#steps",
    "href": "guide/register-models.html#steps",
    "title": "Register models in the inventory",
    "section": "Steps",
    "text": "Steps\n\nFrom the Home page, navigate to the Model Inventory page on the left. \nIn the Model Inventory page, click Register new model.\nFill in the required information on the registration form:\n\nProvide a model name\nSelect the relevant business unit\nSelect the relevant model methodology being used\nSelect the relevant model use case\nProvide a purpose statement to explain what the model will be used for\nselect the preliminary risk tier for the model\n\n\n\n\nClick Register new model to create a new entry in the model inventory.\nYou can now access the model details from the Model Inventory page."
  },
  {
    "objectID": "guide/register-models.html#related-topics",
    "href": "guide/register-models.html#related-topics",
    "title": "Register models in the inventory",
    "section": "Related topics",
    "text": "Related topics\n\nEdit model inventory fields\nCreate a new documentation project"
  },
  {
    "objectID": "guide/faq-privacy.html",
    "href": "guide/faq-privacy.html",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "href": "guide/faq-privacy.html#how-are-users-added-to-validmind",
    "title": "Data handling and privacy",
    "section": "",
    "text": "ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-end-user-computing-and-spreadsheet-models",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle end-user computing and spreadsheet models?",
    "text": "How does ValidMind handle end-user computing and spreadsheet models?\nCustomers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models."
  },
  {
    "objectID": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "href": "guide/faq-privacy.html#what-model-artifacts-are-automatically-imported-into-documentation-and-how-are-they-retained",
    "title": "Data handling and privacy",
    "section": "What model artifacts are automatically imported into documentation and how are they retained?",
    "text": "What model artifacts are automatically imported into documentation and how are they retained?\nValidMind stores the following artifacts in the documentation via our API:\n\nDataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for “common logistic regression limitations” when a logistic regression model has been passed to the ValidMind test plan execution)\nQuality and performance metrics collected from the dataset and model\nOutputs from executed test plans\nImages, plots, and visuals generated as part of extracting metrics and running tests\n\nValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a “Virtual Private ValidMind” option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind’s data retention policy complies with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "href": "guide/faq-privacy.html#how-does-validmind-handle-large-datasets-what-about-the-confidentiality-of-data-sent-to-validmind",
    "title": "Data handling and privacy",
    "section": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?",
    "text": "How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?\nValidMind does not send datasets outside the client’s environment. The Developer Framework executes test plans and functions locally in your environment and is not limited by dataset size.\nAdditionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard."
  },
  {
    "objectID": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "href": "guide/faq-privacy.html#what-solutions-do-you-offer-and-how-do-you-handle-privacy",
    "title": "Data handling and privacy",
    "section": "What solutions do you offer and how do you handle privacy?",
    "text": "What solutions do you offer and how do you handle privacy?\nValidMind is a Developer Framework and cloud platform available in multiple editions catering to different organizational needs:\n\nStandard Edition: Our introductory offering, providing essential features and services.\nEnterprise Edition: Builds upon the Standard Edition by adding features tailored for large-scale organizations.\nVirtual Private ValidMind (VPV): Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.\n\nAccess to any edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check Configure AWS PrivateLink. ValidMind does not send any personally identifiable information (PII) through our API."
  },
  {
    "objectID": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "href": "guide/faq-privacy.html#can-the-tool-automatically-document-other-non-standard-etl-steps-or-performance-metrics-from-notebooks",
    "title": "Data handling and privacy",
    "section": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?",
    "text": "Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?\nSupport for more complex data processing pipelines is on our roadmap, currently scheduled for Q4’2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "href": "guide/faq-privacy.html#how-does-the-tool-manage-model-changes",
    "title": "Data handling and privacy",
    "section": "How does the tool manage model changes?",
    "text": "How does the tool manage model changes?\nValidMind allows model developers to re-run documentation functions with the Developer Framework to capture changes in the model, such as changes in the number of features or hyperparameters.\nAfter a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.\nValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation."
  },
  {
    "objectID": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "href": "guide/faq-privacy.html#can-you-accommodate-spark-dataframes",
    "title": "Data handling and privacy",
    "section": "Can you accommodate Spark DataFrames?",
    "text": "Can you accommodate Spark DataFrames?\nOur Developer Framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more. Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs)."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html",
    "href": "guide/create-your-first-documentation-project.html",
    "title": "Create Your First Documentation Project",
    "section": "",
    "text": "Let’s learn how to create your own documentation project. You can use this project to upload tests and documentation and then add that to the Quickstart notebook you looked at earlier."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html#steps",
    "href": "guide/create-your-first-documentation-project.html#steps",
    "title": "Create Your First Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the landing page by clicking on the ValidMind logo or Log in to the ValidMind UI.\nFrom the left sidebar, select Documentation Projects and on the page that opens, click the Create new Project button at top right of the screen.\nSelect the right options in the form:\n\n\nModel: [Quickstart] Customer Churn Model\nType: Initial Validation (selected automatically) \nProject name: Enter your preferred name\n\nClick Create Project.\nValidMind will create an empty documentation project associated with the customer churn model.\nYou can now access this project from the UI on the Documentation Projects page or by navigating to the relevant model - [Quickstart] Customer Churn Model - in the Model Inventory page.\nFrom the left sidebar, select Getting started.\nThe page that opens provides you with the credentials for the newly created project to use with the ValidMind Developer Framework.\nLocate the project identifier, API key, and secret:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform.\nTo follow best practices, you can also store the credentials in a .env file and pass them in via environment variables.\n\n\n\n\n\n\n\n\n\nMissing the API_SECRET?\n\n\n\nTry this: Use the  icon to copy the API_SECRET to your clipboard."
  },
  {
    "objectID": "guide/create-your-first-documentation-project.html#whats-next",
    "href": "guide/create-your-first-documentation-project.html#whats-next",
    "title": "Create Your First Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nContinue with Upload to your documentation project to learn about how you can use the ValidMind Platform for your projects."
  },
  {
    "objectID": "guide/supported-models.html",
    "href": "guide/supported-models.html",
    "title": "Supported models",
    "section": "",
    "text": "Our Supported Models provide an array of templates for testing and documentation, aimed at specific use-cases. This topic is relevant for model developers who want to know what functionality the Developer Framework provides and how they can extend it."
  },
  {
    "objectID": "guide/supported-models.html#what-is-a-supported-model",
    "href": "guide/supported-models.html#what-is-a-supported-model",
    "title": "Supported models",
    "section": "What is a supported model?",
    "text": "What is a supported model?\nA supported model refers to a model for which predefined testing or documentation functions exist in the ValidMind Developer Framework, provided that the model you are developing is documented using a supported version of our client library.\n\nPlease note the inherent dependency on data types, such as tabular, time series, and text data types. This distinction isn’t always immediately evident. For example: while binary classification is possible on text datasets, only the tabular_dataset suite is explicitly mentioned in our list of supported models.\nAdditionally, the framework might accommodate additional models not explicitly listed. The absence of pre-configured tests for these models doesn’t necessarily indicate a lack of support. You always have the flexibility to integrate your own tests or connect to an external test provider, if needed."
  },
  {
    "objectID": "guide/supported-models.html#currently-supported-models",
    "href": "guide/supported-models.html#currently-supported-models",
    "title": "Supported models",
    "section": "Currently supported models",
    "text": "Currently supported models\nAs of release (v1.16.0), the Developer Framework supports the following model types:\nThe following table presents an overview of libraries supported by each test plan, as well as the tests which comprise each test plan as of the current Developer Framework release.\n\n\n\n\n\n\n\n\n\n\n\nProblem type\nGoal\nTest Suite\nTest plans\nSupported Libraries\nTests\n\n\n\n\nBinary classification\nData validation\ntabular_dataset\ntabular_dataset_description\nnumpy, pandas\nDatasetMetadata, DatasetDescription, DescriptiveStatistics, DatasetCorrelations\n\n\nBinary classification\nData validation\ntabular_dataset\ntabular_data_quality\nnumpy, pandas\nClassImbalance, Duplicates, HighCardinality, HighPearsonCorrelation, MissingValues, Skewness, UniqueRows, TooManyZeroValues\n\n\nBinary classification\nModel validation\nbinary_classifier_model_validation\nbinary_classifier_metrics\nsklearn.RandomForestClassifier, sklearn.LogisticRegression, xgboost.XGBClassifier, CatBoostClassifier\nModelMetadata, DatasetSplit, ConfusionMatrix, ClassifierInSamplePerformance, ClassifierOutOfSamplePerformance, PermutationFeatureImportance, PrecisionRecallCurve, ROCCurve, CharacteristicStabilityIndex, PopulationStabilityIndex, SHAPGlobalImportance\n\n\nBinary classification\nModel validation\nbinary_classifier_model_validation\nbinary_classifier_validation\nsklearn.RandomForestClassifier, sklearn.LogisticRegression, xgboost.XGBClassifier, CatBoostClassifier\nMinimumAccuracy, MinimumF1Score, MinimumROCAUCScore, TrainingTestDegradation\n\n\nBinary classification\nModel validation\nbinary_classifier_model_validation\nbinary_classifier_model_diagnosis\nsklearn.RandomForestClassifier, sklearn.LogisticRegression, xgboost.XGBClassifier, CatBoostClassifier\nOverfitDiagnosis, WeakspotsDiagnosis, RobustnessDiagnosis\n\n\nTime series\nData validation\ntime_series_dataset\ntime_series_data_quality\nnumpy, pandas\nTimeSeriesOutliers, TimeSeriesMissingValues, TimeSeriesFrequency\n\n\nTime series\nData validation\ntime_series_dataset\ntime_series_univariate\nnumpy, pandas\nTimeSeriesLinePlot, TimeSeriesHistogram, ACFandPACFPlot, SeasonalDecompose, AutoSeasonality, AutoStationarity, RollingStatsPlot, AutoAR, AutoMA\n\n\nTime series\nData validation\ntime_series_dataset\ntime_series_multivariate\nnumpy, pandas\nScatterPlot, LaggedCorrelationHeatmap, EngleGrangerCoint, SpreadPlot\n\n\nTime series\nModel validation\ntime_series_model_validation\nregression_model_performance\nstatsmodels.regression.linear_model.OLS, statsmodels.regression.linear_model.GLS, sklearn.linear_model.LinearRegression, xgboost.XGBRegressor\nRegressionModelSummary\n\n\nTime series\nModel validation\ntime_series_model_validation\nregression_models_comparison\nstatsmodels.regression.linear_model.OLS, statsmodels.regression.linear_model.GLS, sklearn.linear_model.LinearRegression, xgboost.XGBRegressor\nRegressionModelInsampleComparison, RegressionModelOutsampleComparison\n\n\nTime series\nModel validation\ntime_series_model_validation\ntime_series_forecast\nstatsmodels.regression.linear_model.OLS, statsmodels.regression.linear_model.GLS, sklearn.linear_model.LinearRegression, xgboost.XGBRegressor\nRegressionModelForecastPlot"
  },
  {
    "objectID": "guide/supported-models.html#related-topics",
    "href": "guide/supported-models.html#related-topics",
    "title": "Supported models",
    "section": "Related Topics",
    "text": "Related Topics\n\nCheck out our Developer Framework documentation for more details on how to use our documentation and testing functions with supported models.\nFor additional tests that are not supported, refer to our documentation on Implementing Custom Tests and Integrating External Test Providers."
  },
  {
    "objectID": "guide/document-models-with-ui.html",
    "href": "guide/document-models-with-ui.html",
    "title": "Document models with the ValidMind UI",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to edit the content of a documentation project. This topic is relevant for model developers who want to view make qualitative edits to their model documentation."
  },
  {
    "objectID": "guide/document-models-with-ui.html#prerequisites",
    "href": "guide/document-models-with-ui.html#prerequisites",
    "title": "Document models with the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API keuy and secret\nYou have already initialized the Developer Framework for your model\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-ui.html#steps",
    "href": "guide/document-models-with-ui.html#steps",
    "title": "Document models with the ValidMind UI",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Project Overview page, select Documentation on the left-hand side.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view.\n\nIn any section of the documentation, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nSave your edits when done by clicking on the  save icon to the right of the textbox to save your changes.\n\n\n\n\n\n\n\n\n\n\nThe section activity at the bottom of the page records a new event every time edits are made to the contents of the page."
  },
  {
    "objectID": "guide/document-models-with-ui.html#related-topics",
    "href": "guide/document-models-with-ui.html#related-topics",
    "title": "Document models with the ValidMind UI",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html",
    "href": "guide/release-notes-2023-jun-22.html",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the Platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our Developer Framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with Validmind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test plans. Learn more …\n\n\n\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. Learn more …\n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the Platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#release-highlights",
    "href": "guide/release-notes-2023-jun-22.html#release-highlights",
    "title": "June 22, 2023",
    "section": "",
    "text": "This release includes a number of major enhancements to the ValidMind Developer Framework that will make it easier for users to edit templates and add custom tests that can be reused across templates. In addition, this release also includes a new notebook to illustrate support for NLP models and PyTorch, as well as the ability to edit templates directly in the Platform UI.\n\n\n\nImplement your own custom tests. With support for custom tests, you can now go beyond the default set of documentation and testing components provided by ValidMind, and use our Developer Framework to document any type of model or use case. Learn more …\nIntegrate external test providers. With test providers, you can now integrate external test libraries to expand the set of tests available through the ValidMind Developer Framework, or create your own test libraries. This enables registering custom tests under a namespace to make them available globally. We provide a couple of demo scenarios with such test providers and show how you can register them with Validmind so that they can run a template utilizing your tests. Learn more …\nSimplified documentation and testing using templates. Templates now function as dynamic test suites, allowing you to identify all the documentation components & test results mapped to a specific template. This makes it easier to fill a pre-configured template (including boilerplates and spaces designated for documentation and test results) with a single command, instead of running multiple test plans. Learn more …\n\n\n\nNew notebook to illustrate natural language processing (NLP) data analysis and text classification use case. Learn how to use the ValidMind Framework to document and test an NLP data analysis and text classification model. The emphasis of this notebook is on documenting and testing the in-depth analysis and preprocessing of the text data, in this case, COVID-19 related tweets. Learn more …\n\n\n\n\n\nYou can now edit the documentation templates that get used for model documentation or for validation reports. This feature is relevant for administrators who need to configure templates for specific use cases, or where the existing templates supplied by ValidMind need to be customized. Editing allows to configure sections, sub-sections, and content blocks inside a template. Learn more …\nWe improved the search feature in the Platform UI to make search results more compact."
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#enhancements",
    "href": "guide/release-notes-2023-jun-22.html#enhancements",
    "title": "June 22, 2023",
    "section": "Enhancements",
    "text": "Enhancements\nWe revised our Quickstart guide to be more modular and to highlight that our suggested starting point with the ValidMind Developer Framework is now Jupyter Hub. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jun-22.html#how-to-upgrade",
    "title": "June 22, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/license-agreement.html",
    "href": "guide/license-agreement.html",
    "title": "SOFTWARE LICENSE AGREEMENT",
    "section": "",
    "text": "IMPORTANT: READ THIS SOFTWARE LICENSE AGREEMENT (THIS “AGREEMENT”) CAREFULLY BEFORE USING THE SOFTWARE. BY USING THE SOFTWARE, YOU ARE AGREEING TO BE BOUND BY THE TERMS OF THIS LICENSE AGREEMENT:\nAs between you and Licensor, this software and associated media, printed materials, and “online” or electronic documentation files (collectively, the “Software”), is the proprietary information of ValidMind Inc. (“Licensor”), and is protected under copyright and other intellectual property laws.\nNo part of this Software may be copied, reproduced, distributed, republished, displayed, posted or transmitted in any form or by any means, including, but not limited to, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of Licensor.\nBy installing, copying, or otherwise using the Software, you as a user of the Software (“you”) agree to be bound by the terms of this Agreement. If you do not agree to the terms of this Agreement, do not install or use the Software.\n\nLICENSE GRANT. Subject to the terms and conditions of this Agreement, Licensor grants you a non-exclusive, non-transferable, limited license (without the right to sublicense) during the term of this Agreement to install and use the Software only in object code or byte code form for the sole purpose of testing its functionality.\nOWNERSHIP. The Software is owned by Licensor and its licensors and is protected by copyright laws and international copyright treaties, as well as other intellectual property laws and treaties. As between you and Licensor, Licensor retains all right, title, and interest in and to the Software, including all intellectual property rights. This Agreement does not transfer any ownership of the Software to you.\nRESTRICTIONS. You will not use the Software for any commercial, production, or operational purposes. You will not (a) modify, adapt, translate, reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code of the Software including (without limitation) for the purpose of obtaining unauthorized access to the Software, (b) modify, translate, copy, or create derivative works based on the Software, (c) use the Software to create or develop a competitive product or service, (d) circumvent, remove, alter, or thwart any technological measure or content protections of the Software, (e) distribute, sublicense, rent, lease, or lend the Software to any third party, (f) remove or alter any copyright, trademark, or proprietary rights notice contained in the Software or (g) use the Software for any purpose except as expressly permitted under this Agreement.\nCONFIDENTIALITY. You agree to safeguard the Software and its related materials with the same care and precaution as you use to protect your own proprietary information and trade secrets, but in no event less than a reasonable degree of care so that no unauthorized use is made of them and no disclosure of any part of their contents is made to anyone other than your employees, agents or consultants whose duties reasonably require such disclosure, or as necessary in the ordinary course of business. You shall make all such persons fully aware of their responsibility to fulfill your obligations under this Agreement and agree to be responsible for any breach of this Agreement by such persons. You agree to promptly notify Licensor if you obtain information as to any unauthorized possession, use or disclosure of the Software by any person or entity, and further agrees to cooperate with Licensor in protecting Licensor’s proprietary rights. Upon the request of Licensor, copies and embodiments of the Software and its related materials shall be promptly returned to Licensor by you or destroyed by you, and you agree to certify such destruction in writing.\nTERM & TERMINATION. This Agreement is effective until terminated. The Licensee may terminate this Agreement at any time by destroying all copies of the Software and its related materials. The Licensor may terminate this License if you fail to comply with any term or condition of this Agreement. Upon termination, you must destroy all copies of the Software and its related materials in your possession.\nDISCLAIMER OF WARRANTY. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. LICENSOR DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE.\nLIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT SHALL LICENSOR BE LIABLE FOR ANY (A) INDIRECT, PUNITIVE, EXEMPLARY, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES (INCLUDING LOST PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR INABILITY TO USE THE SOFTWARE, EVEN IF LICENSEE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR (B) ANY AMOUNTS IN EXCESS OF THE AMOUNT PAID BY YOU FOR THE SOFTWARE.\nINDEMNIFICATION. You agree to indemnify and hold Licensor harmless from and against all loss, cost, expense or liability (including reasonable attorney’s fees) (i) arising out of a claim by a third party against Licensor based upon your use of the Software, or (ii) related to, or associated with your customizations, updates and/or corrections to the Software. Licensor shall have no liability to you for any damage sustained by you as a result of your use of the Software, whether such damages would arise as a result of breach of contract, tort or otherwise.\nGOVERNING LAW. This Agreement and any dispute arising hereunder shall be governed by and construed in accordance with the laws of the State of California and the federal laws of the United States of America, without giving effect to any principles of conflicts of law.\nENTIRE AGREEMENT. This Agreement constitutes the entire agreement between you and Licensor with respect to the Software and supersedes all prior or contemporaneous communications and proposals, whether oral or written, between you and Licensor.\nASSIGNMENT. You will not assign your rights and obligations under this Agreement without prior written consent of the Licensor. Licensor may freely assign its rights and obligations without your consent.\nMISCELLANEOUS. If any provision of this Agreement is found to be unenforceable or invalid, that provision will be limited or eliminated to the minimum extent necessary so that this Agreement will otherwise remain in full force and effect and enforceable. Without limiting anything herein, and except for your payment obligations, neither party shall have any liability for any failure or delay resulting from any condition beyond the reasonable control of such party, including but not limited to governmental action or acts of terrorism, earthquake or other acts of God, labor conditions, epidemics, pandemics, and power failures. For all purposes under this Agreement each party shall be and act as an independent contractor and shall not bind nor attempt to bind the other to any contract. Any notices in connection with this Agreement will be in writing.\n\nBy installing or using the Software, you acknowledge that you have read this Agreement, understand it, and agree to be bound by its terms and conditions.\nCopyright © 2023 ValidMind Inc. All rights reserved."
  },
  {
    "objectID": "guide/faq.html",
    "href": "guide/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Find answers to frequently asked questions (FAQs), grouped by topic:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html",
    "href": "guide/release-notes-2023-jul-24.html",
    "title": "July 24, 2023",
    "section": "",
    "text": "This release improves the developer experience within the ValidMind Developer Framework and introduces an updated notebook to demonstrate support for NLP models, now using CatBoost for greater performance. Moreover, we’ve upgraded our Platform UI with several major new features, including an all-new text editor, the capability to insert new content blocks into documentation projects, template swapping for keeping your documentation projects current, and much more.\n\n\n\n\n\nUpdated notebook for NLP use case. Now with enhanced performance by using CatBoost compared to the original PyTorch-based notebook, this updated notebook showcases natural language processing (NLP) data analysis and text classification using the ValidMind Developer Framework with emphasis on the in-depth analysis and preprocessing of COVID-19 tweets. Try it …\nImproved developer experience when you encounter error conditions. Unhandled errors now return an error code with a more descriptive message that can help you with looking them up in the troubleshooting section of our documentation. We also improved error handling when tests fail to run and the test results summary widget now display error information for each test that fails to run.\n\n\n\n\n\nNew full-featured text editor. When collaborating on documentation projects in the Platform UI with others, you now have a new text editor at your disposal. The interface should be intuitive and familiar, allowing you to manage model documentation and validation reports with greater ease. In addition to the new editing features, you can use the editor to participate in comment threads, track changes, see the revision history, enter math formulas, and more. Try it …\n\n\n\nAdd new content blocks to documentation projects. For model developers and model validators who want to add new sections to a documentation project, you can now do just that. The new content blocks feature provide you with additional sections that you fill in with text, metrics, and test results. Supported content block types:\nSimple text block: Can be added anywhere on model documentation pages and edited to include additional documentation in text format.\nTest-driven block: Can be added to display one of the supported metrics or threshold test results collected by the Developer Framework.\n\nYou can add new content block to an existing documentation project simply by hovering over the dashed line between sections. Try it …\n\n\nSwap templates for documentation projects. This feature is designed to enhance the flexibility of documenting models. With it, you can seamlessly update an existing documentation project by either switching to an entirely different template or by simply upgrading to a more recent version of the template currently in use. This new feature ensures that your documentation projects can remain up-to-date and align with the latest preferred template without the need to start from scratch. Try it …\nPlatform UI enhancements: \n\nRenamed Client Integration page to Getting Started. To better reflect when you use the credentials for a newly created documentation project with the ValidMind Developer Framework, we have renamed the Client Integration page to Getting Started.\nImproved activity widget: We enhanced the user interface with new tabs, enabling you to filter events based on activity type.\nImproved Findings section and Findings Details page: We made UI enhancements for both project findings and the findings details sections for a better look and feel.\nRefined Models and Projects lists : We refined the user interface, featuring more intuitive default sorting and more user-friendly filtering."
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#release-highlights",
    "href": "guide/release-notes-2023-jul-24.html#release-highlights",
    "title": "July 24, 2023",
    "section": "",
    "text": "This release improves the developer experience within the ValidMind Developer Framework and introduces an updated notebook to demonstrate support for NLP models, now using CatBoost for greater performance. Moreover, we’ve upgraded our Platform UI with several major new features, including an all-new text editor, the capability to insert new content blocks into documentation projects, template swapping for keeping your documentation projects current, and much more.\n\n\n\n\n\nUpdated notebook for NLP use case. Now with enhanced performance by using CatBoost compared to the original PyTorch-based notebook, this updated notebook showcases natural language processing (NLP) data analysis and text classification using the ValidMind Developer Framework with emphasis on the in-depth analysis and preprocessing of COVID-19 tweets. Try it …\nImproved developer experience when you encounter error conditions. Unhandled errors now return an error code with a more descriptive message that can help you with looking them up in the troubleshooting section of our documentation. We also improved error handling when tests fail to run and the test results summary widget now display error information for each test that fails to run.\n\n\n\n\n\nNew full-featured text editor. When collaborating on documentation projects in the Platform UI with others, you now have a new text editor at your disposal. The interface should be intuitive and familiar, allowing you to manage model documentation and validation reports with greater ease. In addition to the new editing features, you can use the editor to participate in comment threads, track changes, see the revision history, enter math formulas, and more. Try it …\n\n\n\nAdd new content blocks to documentation projects. For model developers and model validators who want to add new sections to a documentation project, you can now do just that. The new content blocks feature provide you with additional sections that you fill in with text, metrics, and test results. Supported content block types:\nSimple text block: Can be added anywhere on model documentation pages and edited to include additional documentation in text format.\nTest-driven block: Can be added to display one of the supported metrics or threshold test results collected by the Developer Framework.\n\nYou can add new content block to an existing documentation project simply by hovering over the dashed line between sections. Try it …\n\n\nSwap templates for documentation projects. This feature is designed to enhance the flexibility of documenting models. With it, you can seamlessly update an existing documentation project by either switching to an entirely different template or by simply upgrading to a more recent version of the template currently in use. This new feature ensures that your documentation projects can remain up-to-date and align with the latest preferred template without the need to start from scratch. Try it …\nPlatform UI enhancements: \n\nRenamed Client Integration page to Getting Started. To better reflect when you use the credentials for a newly created documentation project with the ValidMind Developer Framework, we have renamed the Client Integration page to Getting Started.\nImproved activity widget: We enhanced the user interface with new tabs, enabling you to filter events based on activity type.\nImproved Findings section and Findings Details page: We made UI enhancements for both project findings and the findings details sections for a better look and feel.\nRefined Models and Projects lists : We refined the user interface, featuring more intuitive default sorting and more user-friendly filtering."
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#documentation",
    "href": "guide/release-notes-2023-jul-24.html#documentation",
    "title": "July 24, 2023",
    "section": "Documentation",
    "text": "Documentation\nTo make it easier to try out our Jupyter notebooks, we now provide a download button for all notebooks used in our documentation:\n\n\nDownload Notebooks\n\n\nThis download includes:\n\nQuickstart notebooks\nUse case notebooks\nTesting notebooks"
  },
  {
    "objectID": "guide/release-notes-2023-jul-24.html#how-to-upgrade",
    "href": "guide/release-notes-2023-jul-24.html#how-to-upgrade",
    "title": "July 24, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, reload your browser tab.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Jupyter Hub: reload your browser tab and re-run the !pip install --upgrade validmind cell.\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/work-with-validation-reports.html",
    "href": "guide/work-with-validation-reports.html",
    "title": "Work with validation reports",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to create, edit, and publish a validation report for a given model. This topic is relevant for model validators who want to capture their observations and conclusions on the model documentation prepared by a model developer."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#prerequisites",
    "href": "guide/work-with-validation-reports.html#prerequisites",
    "title": "Work with validation reports",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA model developer has marked their model documentation project as Ready for Validation\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-a-validation-report",
    "href": "guide/work-with-validation-reports.html#view-a-validation-report",
    "title": "Work with validation reports",
    "section": "View a validation report",
    "text": "View a validation report\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Validation Report on the left.\nYou can now jump to any section of the Validation Report by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "href": "guide/work-with-validation-reports.html#add-content-to-or-edit-a-validation-report",
    "title": "Work with validation reports",
    "section": "Add content to or edit a validation report",
    "text": "Add content to or edit a validation report\n\nIn any section of the validation report, hover over text content and click the  edit icon that appears on the right of the textbox. \nYou can now use the text editor functions to edit the content of the section.\nWhen done, click the  save icon."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "href": "guide/work-with-validation-reports.html#post-a-comment-on-a-validation-report",
    "title": "Work with validation reports",
    "section": "Post a comment on a validation report",
    "text": "Post a comment on a validation report\n\nIn any section of the validation report, select a portion of text that you would like to comment on and click the Add comment button that appears. \nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "href": "guide/work-with-validation-reports.html#view-validation-guidelines-and-comments",
    "title": "Work with validation reports",
    "section": "View validation guidelines and comments",
    "text": "View validation guidelines and comments\n\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar. \n\nThe Validation Guidelines tab shows the validation report guidelines associated with this template that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the model documentation."
  },
  {
    "objectID": "guide/work-with-validation-reports.html#related-topics",
    "href": "guide/work-with-validation-reports.html#related-topics",
    "title": "Work with validation reports",
    "section": "Related topics",
    "text": "Related topics\n\nSubmit for approval"
  },
  {
    "objectID": "guide/document-models-with-framework.html",
    "href": "guide/document-models-with-framework.html",
    "title": "Document models with the Developer Framework",
    "section": "",
    "text": "Learn how to generate model documentation by using the ValidMind Developer Framework. This topic is relevant for model developers who want to document information about their data and model in accordance to template requirements configured by model validators."
  },
  {
    "objectID": "guide/document-models-with-framework.html#prerequisites",
    "href": "guide/document-models-with-framework.html#prerequisites",
    "title": "Document models with the Developer Framework",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model developer has provided some content on the documentation, either using the Developer Framework or via the online UI editor\nYou have already located the project identifier, API key and secret\nYou have already installed the ValidMind client library in your developer environment\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "href": "guide/document-models-with-framework.html#document-dataset-and-data-quality-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document dataset and data quality metrics",
    "text": "Document dataset and data quality metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Getting started page.\nRun the {…} test plan.\nView results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "href": "guide/document-models-with-framework.html#document-model-description-and-model-performance-metrics",
    "title": "Document models with the Developer Framework",
    "section": "Document model description and model performance metrics",
    "text": "Document model description and model performance metrics\n\nInitialize the ValidMind library in your developer source:\nUse the project identifier from the associated model documentation project, accessible through the Getting started page.\nRun the {…} test plan.\nView the results in the UI."
  },
  {
    "objectID": "guide/document-models-with-framework.html#related-topics",
    "href": "guide/document-models-with-framework.html#related-topics",
    "title": "Document models with the Developer Framework",
    "section": "Related topics",
    "text": "Related topics\n\nReview and comment on documentation projects\nSubmit project for approval\nExport documentation"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html",
    "href": "guide/review-data-streams-and-audit-trails.html",
    "title": "Review Audit Trail",
    "section": "",
    "text": "Learn how to access and use the audit trail functionality in the ValidMind Platform. This topic matters for for model developers, model validators, and auditors who are looking to track or audit all the information events associated with a specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "href": "guide/review-data-streams-and-audit-trails.html#prerequisites",
    "title": "Review Audit Trail",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#steps",
    "href": "guide/review-data-streams-and-audit-trails.html#steps",
    "title": "Review Audit Trail",
    "section": "Steps",
    "text": "Steps\n\nIn the ValidMind platform, navigate to the relevant model documentation project.\nFrom the Overview page, select Audit Trail on the left.\n\nThe table in this page shows a record of all activities generated from the Developer Framework and actions performed by users in the organization related to this specific project."
  },
  {
    "objectID": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "href": "guide/review-data-streams-and-audit-trails.html#whats-next",
    "title": "Review Audit Trail",
    "section": "What’s next",
    "text": "What’s next\n\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html",
    "href": "guide/try-developer-framework-with-colab.html",
    "title": "Try it with Google Colaboratory",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Google Colaboratory."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#prerequisites",
    "href": "guide/try-developer-framework-with-colab.html#prerequisites",
    "title": "Try it with Google Colaboratory",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to Google Colaboratory (Colab).\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. You can work with, run, and download our sample Jupyter notebooks from there."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#steps",
    "href": "guide/try-developer-framework-with-colab.html#steps",
    "title": "Try it with Google Colaboratory",
    "section": "Steps",
    "text": "Steps\n\n\n\n\n\n\n\n\nAbout our Jupyter notebooks\n\n\n\nNotebooks from ValidMind are safe to run — If you get a warning that this notebook was not authored by Google, we welcome you to inspect the notebook source.  Runtime errors — We recommend that you not use the Run all option. Run each cell individually to see what is happening in the notebook. If you do see errors, re-run the notebook cells.\n\n\n\nOpen the Quickstart notebook in Google Colaboratory: \n\nClick File &gt; Save a copy in Drive to make a copy of the Quickstart notebook so that you can modify it later.\n\nAlternatively, you can download the notebook source and work with it in your own developer environment."
  },
  {
    "objectID": "guide/try-developer-framework-with-colab.html#whats-next",
    "href": "guide/try-developer-framework-with-colab.html#whats-next",
    "title": "Try it with Google Colaboratory",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/edit-templates.html",
    "href": "guide/edit-templates.html",
    "title": "Edit templates",
    "section": "",
    "text": "Learn how to edit templates that get used for model documentation or for validation reports. This topic is relevant for administrators who need to configure templates for specific use cases or where the existing templates supplied by ValidMind need to be customized.\nDocumentation templates are stored as YAML files that you edit directly in the online editor. These templates are versioned and saving a documentation template after making changes or reverting to a previous version state always creates a new version."
  },
  {
    "objectID": "guide/edit-templates.html#prerequisites",
    "href": "guide/edit-templates.html#prerequisites",
    "title": "Edit templates",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe template you want to edit must have been added to the ValidMind Platform already.\nIf you are not sure which template or which version of a template a documentation project is using, check the Project Overview page of your documentation project. The Documentation Template section in the right sidebar lists the information."
  },
  {
    "objectID": "guide/edit-templates.html#template-schema",
    "href": "guide/edit-templates.html#template-schema",
    "title": "Edit templates",
    "section": "Template schema",
    "text": "Template schema\n Schema Docs\n\n\n\n\n\n\n\n\n  Type: object     template_id Required     root    template_idType: string Unique identifier for the template.          template_name Required     root    template_nameType: string Name of the template.          version Required     root    versionType: string Version of the template.          description     root    descriptionType: string Description of the template.          sections Required     root    sectionsType: array Documentation sections of the template.  Each item of this array must be:   root    sections    sectionType: object     id Required     root    sections    sections items    idType: string Unique identifier for the section.          title Required     root    sections    sections items    titleType: string Title of the section.          description     root    sections    sections items    descriptionType: string Description of the section.          parent_section     root    sections    sections items    parent_sectionType: string ID of the parent section.          order     root    sections    sections items    orderType: integer Order of the section in the navigation menu. By default sections are ordered alphabetically. If order is specified, sections will be ordered by the order value, and then alphabetically.          default_text     root    sections    sections items    default_textType: string Default text for the section. If set, a metadata content row will be created with this text when installing the template on a given project          index_only     root    sections    sections items    index_onlyType: boolean If true, the section will be displayed in the navigation menu, but it will not be accessible via direct link.          condensed     root    sections    sections items    condensedType: boolean If true, the section will condense all of its subsections into a single section.          guidelines     root    sections    sections items    guidelinesType: array of string Documentation or validation guidelines for the section.  Each item of this array must be:   root    sections    sections items    guidelines    guidelines itemsType: string           contents     root    sections    sections items    contentsType: array Contents to be displayed on the section.  Each item of this array must be:   root    sections    sections items    contents    section_contentsType: object Single content block of the module.      content_type Required     root    sections    sections items    contents    contents items    content_typeType: enum (of string) Default: \"metadata_text\"  Must be one of: \"metadata_text\"\"dynamic\"\"metric\"\"test\"   Examples: \"metadata_text\"\n \"test\"\n          content_id     root    sections    sections items    contents    contents items    content_idType: string ID of the content to be displayed for the given content type (text, metric, testm, etc.).   Examples: \"sample_text\"\n \"section_intro\"\n          options     root    sections    sections items    contents    contents items    optionsType: object Options for the content block.   Examples: {\n    \"default_text\": \"This is a sample text block.\"\n}\n {\n    \"metric_id\": \"metric_1\",\n    \"title\": \"Custom Title for Metric 1\"\n}\n {\n    \"test_id\": \"adf_test\"\n}\n      default_text     root    sections    sections items    contents    contents items    options    default_textType: string Default text for the content block. Only applicable for metadata_text content blocks.          title     root    sections    sections items    contents    contents items    options    titleType: string Title of the content block. Only applicable for metric and test content blocks.                       Generated using json-schema-for-humans on 2023-06-08 at 14:44:40 -0700"
  },
  {
    "objectID": "guide/edit-templates.html#steps",
    "href": "guide/edit-templates.html#steps",
    "title": "Edit templates",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Templates.\nSelect one of the tabs for the type of template you want to edit:\n\nDocumentation Templates\nValidation Report Templates\n\nLocate the template to edit and, at the bottom of the template card, click Edit Template.\nIn the YAML editor that opens, make your changes.\n\nUse See changes to view a side-by-side comparison of your changes with the latest version of the template.\nUse Reset changes to delete your changes and return to the latest version of the template.\n\nClick Prepare new version to save your changes.\n\nAdd a description in Version notes to track the changes that were made once the version is saved.\n\n\nAfter you have saved a new version, it becomes available for use with model documentation or validation reports."
  },
  {
    "objectID": "guide/edit-templates.html#troubleshooting",
    "href": "guide/edit-templates.html#troubleshooting",
    "title": "Edit templates",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nThe documentation template editor validates the YAML changes you make and flags any errors that it finds. If you make a change that the editor cannot parse correctly, the editor will not let you save the changes until you correct the YAML.\nCommon issues with YAML include incorrect indenting, imbalanced quotes, or missing colons between keys and values. If you run into issues with incorrect YAML, check the error message provided by the template editor, as it might provide a line and column number where the error occurs."
  },
  {
    "objectID": "guide/edit-templates.html#whats-next",
    "href": "guide/edit-templates.html#whats-next",
    "title": "Edit templates",
    "section": "What’s next",
    "text": "What’s next\nYou can see which version of a template is used in the Project Overview page of each documentation project."
  },
  {
    "objectID": "guide/support.html",
    "href": "guide/support.html",
    "title": "Support",
    "section": "",
    "text": "Our support team can provide you with quick and easy access to the resources you need to troubleshoot technical issues and help you get the most out of the ValidMind Platform."
  },
  {
    "objectID": "guide/support.html#check-the-faqs",
    "href": "guide/support.html#check-the-faqs",
    "title": "Support",
    "section": "Check the FAQs",
    "text": "Check the FAQs\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy"
  },
  {
    "objectID": "guide/support.html#get-help",
    "href": "guide/support.html#get-help",
    "title": "Support",
    "section": "Get help",
    "text": "Get help\nDon’t see what you are looking for? Email support@validmind.com to get help from a human."
  },
  {
    "objectID": "guide/faq-documentation.html",
    "href": "guide/faq-documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "href": "guide/faq-documentation.html#can-documentation-templates-be-configured-per-model-use-case-or-to-match-our-existing-templates",
    "title": "Documentation",
    "section": "",
    "text": "ValidMind’s platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.\nDocumentation templates can be modified by configuring a YAML file in the backend.\nValidMind is working on a UI feature that will enable user role administrators (such as the model validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "href": "guide/faq-documentation.html#can-the-documentation-be-exported",
    "title": "Documentation",
    "section": "Can the documentation be exported?",
    "text": "Can the documentation be exported?\nValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "href": "guide/faq-documentation.html#can-we-attach-files-to-the-documentation-on-the-ui-what-file-formats-are-supported",
    "title": "Documentation",
    "section": "Can we attach files to the documentation on the UI? What file formats are supported?",
    "text": "Can we attach files to the documentation on the UI? What file formats are supported?\nYou can attach image files to documentation cells and comments on the UI. The following file formats are supported:\n\nJPEG\nPNG\nGIF\nTIFF\nBMP\nSVG\nRAW\nWebP\nHEIF\nPSD\n\nAdditionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023)."
  },
  {
    "objectID": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "href": "guide/faq-documentation.html#can-the-documentation-be-initialized-from-the-ui-instead-of-the-developer-framework",
    "title": "Documentation",
    "section": "Can the documentation be initialized from the UI instead of the Developer Framework?",
    "text": "Can the documentation be initialized from the UI instead of the Developer Framework?\nValidMind allows you to writr documentation directly in the online UI editor, without having to use the Developer Framework.\nFrom the online UI editor, you can edit text and tables and upload your test results, including images. Using the Developer Framework, you can execute test plans and generate the corresponding documentation."
  },
  {
    "objectID": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "href": "guide/faq-documentation.html#can-we-export-the-documentation-produced-by-validmind-to-the-storageworkflow-system-used-by-the-model-validation-team",
    "title": "Documentation",
    "section": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?",
    "text": "Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?\nDocumentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems."
  },
  {
    "objectID": "guide/reference.html",
    "href": "guide/reference.html",
    "title": "Reference",
    "section": "",
    "text": "Find reference information for our Developer Framework."
  },
  {
    "objectID": "guide/add-content-blocks.html",
    "href": "guide/add-content-blocks.html",
    "title": "Add content blocks to documentation",
    "section": "",
    "text": "Learn how to add new content blocks to your documentation project, to write and update your model’s documentation. This topic is relevant for model developers and model validators who want to add new sections to a documentation project."
  },
  {
    "objectID": "guide/add-content-blocks.html#what-are-content-blocks",
    "href": "guide/add-content-blocks.html#what-are-content-blocks",
    "title": "Add content blocks to documentation",
    "section": "What are content blocks?",
    "text": "What are content blocks?\nContent blocks provide you with sections that are part of a template. You can think of these sections as an empty canvas that you fill in with text, metrics, and test results. Multiple sections are joined to create a longer document with a table of contents that has different heading and subheading levels, such as 1., 1.1., and so on.\nTypes of content blocks:\n\nSimple text block\n\nCan be added anywhere on model documentation pages and edited to include additional documentation in text format.\n\nTest-driven block\n\nCan be added to display one of the supported metrics or threshold test results collected by the Developer Framework."
  },
  {
    "objectID": "guide/add-content-blocks.html#prerequisites",
    "href": "guide/add-content-blocks.html#prerequisites",
    "title": "Add content blocks to documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nYou are logged into the ValidMind Platform as a model developer"
  },
  {
    "objectID": "guide/add-content-blocks.html#steps",
    "href": "guide/add-content-blocks.html#steps",
    "title": "Add content blocks to documentation",
    "section": "Steps",
    "text": "Steps\n\nNavigate to the relevant model documentation project:\n\nIn the Documentation Projects page, select the project you want to edit.\nInside your documentation project, navigate to the Documentation page.\nSelect one of the numbered sections, such as 1.1 Model Overview.\n\nIn your documentation, hover your mouse over the space where you want your new block to go until a horizontal dashed line with a  sign appears that indicates you can insert a new block:\n\nClick  and then select one of the available options:\n\nSimple text block: Adds a new section with a blank content block. After the new content block has been added, click  to edit the contents of the section like any other.\nTest-driven block: Select one of the options:\n\nMetric: Select one of the available metrics, such as Confusion Matrix.\nThreshold test: Select one of the available threshold tests, such as Data Quality: Skewness or Model Diagnosis: Overfit Regions.\n\n\nFor test-driven blocks, a preview of the available metrics or threshold test gets shown. Click Insert module when you are ready.\n\nAfter you have completed these steps, the new content block becomes a part of your model documentation."
  },
  {
    "objectID": "guide/add-content-blocks.html#related-topics",
    "href": "guide/add-content-blocks.html#related-topics",
    "title": "Add content blocks to documentation",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Validmind UI"
  },
  {
    "objectID": "guide/editions-and-features.html",
    "href": "guide/editions-and-features.html",
    "title": "Editions and features",
    "section": "",
    "text": "ValidMind offers its solution in multiple editions to choose from. Each edition is priced on an annual subscription basis, depending on the number of models registered on the platform and your support requirements."
  },
  {
    "objectID": "guide/editions-and-features.html#editions",
    "href": "guide/editions-and-features.html#editions",
    "title": "Editions and features",
    "section": "Editions",
    "text": "Editions\n\nDeveloper Edition\nThe Developer Edition is the ideal training ground for developers to play around with ValidMind’s automated model documentation and to test the robustness of our developer framework, documentation, and testing features. The Developer Edition is free, allowing developers who are new to model documentation and model risk management to build, implement, test, and maintain higher quality models and model documentation.\nThe Developer Edition is only for personal testing purposes and cannot be used as a commercial model documentation or model risk management solution.\n\n\nEssential Edition\nWith the Essential Edition, you get an advanced model risk management (MRM) solution. It offers your organization all the features and services of the Developer Edition, plus additional features tailored to the needs of larger-scale organizations.\n\n\nBusiness Critical\nProvides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. This edition encompasses all features and services of the Essential Edition but within a separate ValidMind environment, isolated from other ValidMind accounts via Virtual Private ValidMind (VPV). VPV accounts do not share resources with non-VPV accounts."
  },
  {
    "objectID": "guide/editions-and-features.html#features",
    "href": "guide/editions-and-features.html#features",
    "title": "Editions and features",
    "section": "Features",
    "text": "Features\n\n\n\n\nModel development & documentation\nDeveloper\nEssential\nBusiness Critical\n\n\n\n\nAutomated model documentation\n\n\n\n\n\nPlatform-independent developer framework\n\n\n\n\n\nOnline documentation editing\n\n\n\n\n\nAdvanced editing & readability assistance\n\n\n\n\n\nDocumentation quality measurement\n\n\n\n\n\nOffline document ingestion\n\n\n\n\n\nFeedback capture on online document\n\n\n\n\n\nDocumentation version history management\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nStandard tests & validation libraries\n\n\n\n\n\nConfigure / customize tests & validation libraries\n\n\n\n\n\nSupport for customer-provided tests\n\n\n\n\n\nDeveloper workflow management\n\n\n\n\n\nPre-configured documentation templates & boilerplates\n\n\n\n\n\nConfigurable documentation templates & boilerplates\n\n\n\n\n\nModel validation & audit\n\n\n\n\n\nModel validation report automation\n\n\n\n\n\nFindings / issues & remediation actions tracking\n\n\n\n\n\nConfigurable approval workflows\n\n\n\n\n\nMRM workflows & validation lifecycle tracking\n\n\n\n\n\nMRM resource & workflow management\n\n\n\n\n\nCentral model inventory\n\n\n\n\n\nHistorical documentation repository /documentation CMS\n\n\n\n\n\nGrammarly integration\n\n\n\n\n\nExecutive reporting\n\n\n\n\n\nPlatform integration & support\n\n\n\n\n\nData lake integration, such as Evidence Storeand monitoring data\n\n\n\n\n\nSSO integration\n\n\n\n\n\nCustomer managed encryption\n\n\n\n\n\nSupport 8/5 (one timezone)\n\n\n\n\n\nSupport 24/7 (global)\n\n\n\n\n\nPlatform deployment\n\n\n\n\n\nMulti-tenant SaaS\n\n\n\n\n\nVirtual private ValidMind (VPV)\n\n\n\n\n\nSelf-managed VPV\n\n\n\n\n\n\nContact Us\nContact Us\nContact Us"
  },
  {
    "objectID": "guide/get-started.html",
    "href": "guide/get-started.html",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in machine learning (ML), natural language processing (NLP), and large language models (LLMs). The platform offers tools that cater to both model developers and validators, simplifying key aspects of model risk management."
  },
  {
    "objectID": "guide/get-started.html#welcome-to-validmind",
    "href": "guide/get-started.html#welcome-to-validmind",
    "title": "Get started",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in machine learning (ML), natural language processing (NLP), and large language models (LLMs). The platform offers tools that cater to both model developers and validators, simplifying key aspects of model risk management."
  },
  {
    "objectID": "guide/get-started.html#what-do-i-use-the-validmind-platform-for",
    "href": "guide/get-started.html#what-do-i-use-the-validmind-platform-for",
    "title": "Get started",
    "section": "What do I use the ValidMind platform for?",
    "text": "What do I use the ValidMind platform for?\nModel developers and validators play important roles in managing model risk, including risk that stems from generative AI and machine learning models. From complying with regulations to ensuring that institutional standards are followed, your team members are tasked with the careful documentation, testing, and independent validation of models.\nThe purpose of these efforts is to ensure that good risk management principles are followed throughout the model lifecycle. To assist you with these processes of documenting and validating models, ValidMind provides a number of tools that you can employ regardless of the technology used to build your models.\n\n\n\n\n\nThe ValidMind AI risk platform provides two main products components:\n\nThe Developer Framework is a library of tools and methods designed to automate generating model documentation and running validation tests. The framework is designed to be platform agnostic and integrates with your existing development environment.\nFor Python developers, a single installation command provides access to all the functions:\npip install validmind\nThe ValidMind AI Risk Platform is an easy-to-use web-based UI that enables you to track the model lifecycle:\n\n\nCustomize workflows to manage the model documentation and validation process.\nReview and edit the documentation and test metrics generated by the Developer Framework.\nCollaborate with and capture feedback from model developers and model validators.\nGenerate validation reports and approvals\n\nFor more information about the benefits that ValidMind can offer, check out the ValidMind overview.\n\n\n\n\n\n\n Expand to learn about key ValidMind concepts\n\n\n\n\n\n\n\nModel documentation\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model. Provides a comprehensive record and description of a quantitative model. Should encompass all relevant information about the model in accordance with:\n\n\n\nIntended use\nRegulatory requirements set by regulatory bodies\nModel risk policies set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\n\n\nDocumentation project\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s lifecycle constitutes a new project and can be configured with its own templates and workflows.\n\nTemplate\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results.\n\n\nModel documentation is populated when the tests contained in a template run successfully, completing the test suite. This process ensures that the model meets all the specified requirements. At the same time, template placeholders get populated with content that documents the model. This content is generated by the ValidMind Developer Framework, providing a coherent structure for model information.\nEssentially, our platform scans these templates, identifies all tests, and systematically organizes them into a well-structured test suite. This automation enhances the efficiency and consistency of the validation process.\nThe criteria for these templates are typically provided by your model risk management team. We provide some templates out of the box that you can use and programmatically customize to suit the requirements of each model use case. This task of customization is usually performed by an administrator, who ensures that the templates align with the organizational standards and specific needs of each model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates."
  },
  {
    "objectID": "guide/get-started.html#how-do-i-get-started",
    "href": "guide/get-started.html#how-do-i-get-started",
    "title": "Get started",
    "section": "How do I get started?",
    "text": "How do I get started?\nOn the ValidMind platform, everything starts with the model inventory: you first register a new model and then manage the model lifecycle through the different activities that are part of your existing model risk management processes.\n\nApproval workflow\nA typical high-level model approval workflow looks like this:\n\n\n\n\ngraph LR\n    A[Model&lt;br&gt;registration] --&gt; B[Initial&lt;br&gt;validation]\n    B --&gt; C[Validation&lt;br&gt;approval]\n    C --&gt; D[In production]\n    D --&gt; E[Periodic review&lt;br&gt;and revalidation]\n    E --&gt; B\n\n\n\n\n\n\n\nNew model registration\n\nSelect a documentation template when registering a new inventory model to start a documentation project. You then use the model inventory to manage the metadata associated with the model, including all compliance and regulatory attributes.\n\nInitial validation\n\nTriggers a new documentation workflow to yield a model that will be ready for production deployment after its documentation and validation reports have been approved.\n\nValidation approval\n\nPerform validation of the model to ensure that it meets the needs for which it was designed. You can also connect to third-party systems to send events when a model has been approved for production.\n\nIn production\n\nUse the model in production while ensuring its ongoing reliability, accuracy, and compliance with regulations by monitoring the model’s performance.\n\nPeriodic review and revalidation\n\nAs part of regular performance monitoring or change management, you follow a process similar to that seen in the Initial validation step.\n\n\n\n\nDocumentation workflow\nOut of the box, the documentation project workflow is configured like this:\n\n\n\n\ngraph LR\n    A[In documentation] --&gt; B[In validation]\n    B --&gt; C[In review /&lt;br&gt;under approval]\n    C --&gt; D[Approved]\n\n\n\n\n\n\n\nIn documentation\n\nYour model developers use the ValidMind Developer Framework to run validation tests and generate automated model documentation. They then use the ValidMind UI to populate qualitative documentation sections. This phase is the most critical and involved for model developers.\n\n\nTo learn more about documenting models, see Get started with the Developer Framework.\n\nIn validation\n\nYour model validators review the model documentation and challenge the model:\n\n\n\nCollaborate in the ValidMind UI to facilitate question and answer threads to collect more information from model developers.\nChallenge the model with the ValidMind Developer Framework to replicate test results and conduct your own testing.\n\nValidators then use the ValidMind UI to generate a validation report and manage the findings discovered during the model review process.\n\nIn review / under approval\n\nSenior risk managers and executives use the ValidMind UI to review validation report and findings to make a final decision.\n\n\nThe generated validation report contains detailed information about the models being evaluated, including their underlying assumptions, methodologies, and performance metrics. It also highlights any potential weaknesses or vulnerabilities that may be present in the models.\n\nApproved\n\nApproval signifies the final endorsement and authorization for a model to be used in production. Approval occurs after careful examination and validation of the model, ensuring that it meets institutional standards and guidelines, and regulatory requirements."
  },
  {
    "objectID": "guide/get-started.html#next-steps",
    "href": "guide/get-started.html#next-steps",
    "title": "Get started",
    "section": "Next steps",
    "text": "Next steps\n\n\n\n\n\n\nTry it\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nThe fastest way to explore what ValidMind can offer is with our Quickstart, where you can:\n\nTry out our Developer Framework with a code sample\nExplore the ValidMind Platform UI\n\nIf you have already tried the Quickstart, more how-to instructions and links to our FAQs can be found under Next steps."
  },
  {
    "objectID": "guide/faq-testing.html",
    "href": "guide/faq-testing.html",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "href": "guide/faq-testing.html#how-did-validmind-develop-the-tests-that-are-currently-in-the-library",
    "title": "Testing",
    "section": "",
    "text": "All the existing tests were developed using open-source Python and R libraries.\nThe Developer Framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema."
  },
  {
    "objectID": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "href": "guide/faq-testing.html#can-tests-be-configured-or-customized-and-can-we-add-our-own-tests",
    "title": "Testing",
    "section": "Can tests be configured or customized, and can we add our own tests?",
    "text": "Can tests be configured or customized, and can we add our own tests?\nValidMind allows tests to be configured at several levels:\n\nAdministrators can configure which tests are required to run programmatically depending on the model use case\nYou can change the thresholds and parameters for tests already available in the Developer Framework (for instance, changing the threshold parameter for class imbalance flag).\nIn addition, ValidMind is implementing a feature that allows you to add your own tests to the Developer Framework. You will also be able to connect your own custom tests with the Developer Framework. These custom tests will be configurable and able to run programmatically, just like the rest of the Developer Framework libraries (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "href": "guide/faq-testing.html#do-you-include-explainability-related-testing-and-documentation",
    "title": "Testing",
    "section": "Do you include explainability-related testing and documentation?",
    "text": "Do you include explainability-related testing and documentation?\nOur Developer Framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.\n\nIn addition, ValidMind is implementing standard documentation via the Developer Framework for the following items and modeling techniques:\n\nConceptual soundness\n\nModel use case description (Q2’2023)\nModel selection rationale (Q2’2023)\n\nData evaluation\n\nData quality metrics\nSampling method validation\nPopulation distribution (PSI)\nCorrelations & interactions\nData lineage (Q3’2023)\nFeature engineering (Q3’2023)\n\nModel Evaluation\n\nPerformance & accuracy evaluation\nGoodness of fit (Q2’2023)\nStability & sensitivity to perturbations (Q3’2023)\nModel robustness & weak regions (Q3’2023)\nGlobal explainability - permutation feature importance, SHAP\nLocal explainability- LIME (Q3’2023)\nModel testing at implementation / post-production (2024)\n\nModel techniques\n\nTime series (ARIMA, Error correction)\nRegression (OLS, Logistic, GLM, XGBoost)\nDecision trees (tree-based ML models)\nRandom forests\nK-means clustering (Q2 2023)\nNLP (2024)\nDeep learning (2024)\nComputer vision (2024)"
  },
  {
    "objectID": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "href": "guide/faq-testing.html#is-there-a-use-case-for-synthetic-data-on-the-platform",
    "title": "Testing",
    "section": "Is there a use case for synthetic data on the platform?",
    "text": "Is there a use case for synthetic data on the platform?\nValidMind’s Developer Framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.\nWe are happy to discuss exploring specific use cases for synthetic data generation with you further."
  },
  {
    "objectID": "guide/view-validation-guidelines.html",
    "href": "guide/view-validation-guidelines.html",
    "title": "View validation guidelines",
    "section": "",
    "text": "Learn how to view the guidelines for the validation report associated with a template. This topic is relevant for model validaators who need to ensure that they are following the guidelines for their validation report."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#prerequisites",
    "href": "guide/view-validation-guidelines.html#prerequisites",
    "title": "View validation guidelines",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been submitted for review by the model validation team for this project\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-validation-guidelines.html#steps",
    "href": "guide/view-validation-guidelines.html#steps",
    "title": "View validation guidelines",
    "section": "Steps",
    "text": "Steps\n\nFrom the Documentation Projects page, select a model and go to the Validation Report page.\nIn any section of the validation report, click the ValidMind Insights button in the top-right to expand the ValidMind Insights right sidebar \n\nThe Validation Report Guidelines tab shows the guidelines associated with this model that have been configured by the model validation team.\nThe Comments tab shows the comment threads associated with this section of the validation report.\n\n\n\n\n\n\n\n\n\n\n\nThe validation guidelines for each template can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-validation-guidelines.html#whats-next",
    "href": "guide/view-validation-guidelines.html#whats-next",
    "title": "View validation guidelines",
    "section": "What’s next",
    "text": "What’s next\n\n\nWork with validation reports"
  },
  {
    "objectID": "guide/join-community.html",
    "href": "guide/join-community.html",
    "title": "ValidMind",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "ValidMind overview",
    "section": "",
    "text": "ValidMind is a platform designed to streamline the management of risk for AI models, including those used in:\nThe platform caters to both model developers and validators, and automates key aspects of the model lifecycle, including model documentation, validation, and testing.\nIn addition, the platform comes with built-in communication and tracking features that enable all stakeholders to collaborate and communicate effectively throughout the model risk management process, ensuring that everyone is informed and up-to-date."
  },
  {
    "objectID": "guide/overview.html#model-risk",
    "href": "guide/overview.html#model-risk",
    "title": "ValidMind overview",
    "section": "Model risk",
    "text": "Model risk\nAs the 1st line of defense against model risk, model developers have a strong incentive and obligation to document and test the models they build, especially when they are subject to regulations. As the 2nd line of defense, model validators must independently validate models and ensure the firm’s model risk management principles are implemented at all stages of the model lifecycle.\nTo achieve this purpose, ValidMind provides different tools for developers to document and validate models regardless of the technology platform used to build them.\nModel documentation provides a comprehensive record and description of a quantitative model. This documentation should encompass all relevant information about the model in accordance with:\n\nRegulatory requirements, as set by regulatory bodies\nModel risk policie, as set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\nIntended use\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model."
  },
  {
    "objectID": "guide/overview.html#validmind-benefits",
    "href": "guide/overview.html#validmind-benefits",
    "title": "ValidMind overview",
    "section": "ValidMind Benefits",
    "text": "ValidMind Benefits\n\nModel Documentation Automation\n\nThe platform enables the automation of model documentation using configurable documentation templates and model test plans. The automation helps ensure all necessary information is captured and recorded consistently and accurately and reduces the risk of errors or omissions in the documentation. In addition, this helps streamline the model lifecycle and reduce the amount of time and resources required to produce documentation while increasing confidence in the accuracy and reliability of the models.\n\nCommunication and tracking\n\nThe AI risk platform comes with built-in functionality for communicating and tracking communication between stakeholders. It offers an effective way of communication that helps ensure all stakeholders have a clear understanding of the models being used, their limitations, and the potential risks associated with their use. The built-in communication facilitates collaboration between stakeholders and by working together, stakeholders can develop effective strategies for managing model risk and ensuring the models are used appropriately.\n\nModel risk management lifecycle and workflow\n\nThe AI risk platform helps manage your model lifecycle and provides a configurable workflow that simplifies management and control over the model risk management process. You can customize the workflow to fit your organization’s needs and requirements, providing a structured approach to identify and manage the risks associated with its models, and enhance stakeholder confidence in its ability to manage risk.\n\n\n\n\n\nThe ValidMind Platform\n\n\nOur solution comprises two primary architectural components: the ValidMind Developer Framework and the cloud-based ValidMind platform.\n\nValidMind Developer Framework\n\nThe ValidMind Developer Framework is designed to offer model developers a systematic approach to documenting and testing risk models with repeatability and consistency. The Developer Framework streamlines the process of generating documentation for AI/ML and algorithmic models, helping you proactively identify potential risks through rigorous testing.\nThe Developer Framework consists of a client-side library, API endpoints for models and testing, and validation tests that streamline the model development process. Implemented as a series of independent libraries in Python and R, our framework ensures compatibility and flexibility with diverse set of developer environments and requirements.\nWith the Developer Framework, you can:\n\nAutomate documentation — Add comprehensive documentation as metadata while you build models to be shared with model validators, streamlining and speeding up the process.\nRun test plans — Identify potential risks for a diverse range of statistical and AI/LLM/ML models by assessing data quality, model outcomes, robustness, and explainability.\nIntegrate with your development environment — Seamlessly incorporate the framework into your existing model development environment, connecting to your existing model code and data sets.\nUpload documentation data — Send qualitative and quantitative test data to the AI risk platform to generate the model documentation for review and approval, fostering effective collaboration with model reviewers and validators.\n\n\n\nValidMind AI risk Platform\n\nThe ValidMind Platform provides a comprehensive suite of tools, guidelines, and best practices. You use the platform to review and evaluate models and model documentation to ensure they comply with organizational and regulatory requirements.\nThe platform employs a multi-tenant architecture, hosting the cloud-based user interface, APIs, databases, and internal services. The design ensures efficient resource utilization and offers a highly scalable solution for organizations of varying sizes.\nWith the ValidMind Platform, you can:\n\nTrack your model inventory — Manage the model lifecycle, track the workflow status for models, plan for upcoming validation dates, and more.\nWork on validation projects — Collaborate with developers and validators to review documentation, add findings, keep track of review statuses, and generate validation reports.\nConfigure workflows — Set up ValidMind to follow your existing model risk management processes, manage statuses for different parts of the workflow, and get an end-to-end view of workflows and who is involved.\nUse, create, or edit tests, test plans, and templates — Create and/or configure required validation tests, test plans, and documentation templates for specific model use cases, tailoring it to your own specific needs.\nIntegrate with your stack — Import and export model documentation and validation reports."
  },
  {
    "objectID": "guide/overview.html#related-topics",
    "href": "guide/overview.html#related-topics",
    "title": "ValidMind overview",
    "section": "Related Topics",
    "text": "Related Topics\nReady to try out ValidMind? Try the Quickstart."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html",
    "href": "guide/upload-to-documentation-project.html",
    "title": "Upload to Your Documentation Project",
    "section": "",
    "text": "You are now ready to modify the Quickstart notebook to upload to your own project that you created earlier."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html#steps",
    "href": "guide/upload-to-documentation-project.html#steps",
    "title": "Upload to Your Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nReopen the Quickstart notebook you accessed earlier.\nIn the Quickstart notebook, replace the vm.init() lines that look like the following with your own client integration information from the earlier step when you created your new documentation project:\n\n\n\n\n\nRun each cell in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will install the ValidMind library and then connect to your own documentation project in the ValidMind Platform.\n\nYou can now switch back to the Platform UI and view the documentation that has been created by the data and artifacts provided by the Developer Framework."
  },
  {
    "objectID": "guide/upload-to-documentation-project.html#whats-next",
    "href": "guide/upload-to-documentation-project.html#whats-next",
    "title": "Upload to Your Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nReady to learn more about how you can use ValidMind? Check out Next Steps."
  },
  {
    "objectID": "guide/view-templates.html",
    "href": "guide/view-templates.html",
    "title": "View templates",
    "section": "",
    "text": "Learn how to view the structure and configuration of existing documentation templates on the ValidMind Platform."
  },
  {
    "objectID": "guide/view-templates.html#prerequisites",
    "href": "guide/view-templates.html#prerequisites",
    "title": "View templates",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/view-templates.html#steps",
    "href": "guide/view-templates.html#steps",
    "title": "View templates",
    "section": "Steps",
    "text": "Steps\n\nFrom the ValidMind Platform homepage, go to Templates on the left.\nClick on one of the available templates to view the YAML configuration file.\nIn the configuration file that opens, you can view information about the template, such as:\n\nName and description of the template\nVersion of the templates\nSections in the template and how they are structured\nGuidelines associated with each section\nMetrics from the Developer Framework that feed into the template\n\n\n\n\n\n\n\n\n\n\n\nTemplates can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-templates.html#related-topics",
    "href": "guide/view-templates.html#related-topics",
    "title": "View templates",
    "section": "Related topics",
    "text": "Related topics\n\nCreate documentation projects\nDocument models with the Developer Framework\nReview and comment on documentation projects"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Guides",
    "section": "",
    "text": "Find how-to instructions for many common user tasks for the following user roles:"
  },
  {
    "objectID": "guide/guide.html#related-topics",
    "href": "guide/guide.html#related-topics",
    "title": "Guides",
    "section": "Related Topics",
    "text": "Related Topics\nFor model developers, refer to our Developer Framework section."
  },
  {
    "objectID": "guide/submit-for-approval.html",
    "href": "guide/submit-for-approval.html",
    "title": "Submit for approval",
    "section": "",
    "text": "Learn how to use the ValidMind UI to view the approval workflow configured by an administrator and to submit projects for review and approval according to that workflow. This topic is relevant for:"
  },
  {
    "objectID": "guide/submit-for-approval.html#prerequisites",
    "href": "guide/submit-for-approval.html#prerequisites",
    "title": "Submit for approval",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nFor Model Developers submitting their documentation for review: model documentation is complete\nFor Model Validators submitting their validation report for review: validation report is complete"
  },
  {
    "objectID": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "href": "guide/submit-for-approval.html#view-the-current-status-and-workflow",
    "title": "Submit for approval",
    "section": "View the current status and workflow",
    "text": "View the current status and workflow\n\nFrom the Documentation Projects page, select a project.\nOn the Overview page, the current status of the project is displayed under Status. \nClick See workflow under Status to visualize the entire workflow that this project will go through."
  },
  {
    "objectID": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "href": "guide/submit-for-approval.html#submit-for-review-for-validation-or-to-advance-to-a-workflow",
    "title": "Submit for approval",
    "section": "Submit for review, for validation, or to advance to a workflow",
    "text": "Submit for review, for validation, or to advance to a workflow\n\nFrom the Documentation Projects page, select a project.\nUnder Actions on the right, initiate the transition from the current state to the next workflow state.\nFor example, change the state from In Documentation to In Validation to indicate that a model developer has completed the initial model documentation and is ready to go through the model validation step. \n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow states and transitions can only be configured by an administrator."
  },
  {
    "objectID": "guide/view-all-test-plans.html",
    "href": "guide/view-all-test-plans.html",
    "title": "View all test plans",
    "section": "",
    "text": "Learn how to use list_plans(), list_test(), and describe_plan() methods to view and describe test plans and tests available in the Developer Framework."
  },
  {
    "objectID": "guide/view-all-test-plans.html#prerequisites",
    "href": "guide/view-all-test-plans.html#prerequisites",
    "title": "View all test plans",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nYou are working on an active documentation project\nYou have already installed the ValidMind client library in your developer environment"
  },
  {
    "objectID": "guide/view-all-test-plans.html#steps",
    "href": "guide/view-all-test-plans.html#steps",
    "title": "View all test plans",
    "section": "Steps",
    "text": "Steps\n\nInitialize the client library.\nUse list_plans() and list_tests() to view the list of all available test plans and tests.\nExamples:\n\nList all available test plans currently available in the the Developer Framework:\nvm.test_plans.list_plans()\nList all available individual tests currently available in the Developer Framework:\nvm.test_plans.list_tests() \n\nUse describe_testplan() to list all the tests included in a specific test plan:\nExample: The following code will list tests included in the tabular_data_quality test plan:\nvm.test_plans.describe_plan(\"tabular_data_quality\")"
  },
  {
    "objectID": "guide/view-all-test-plans.html#related-topics",
    "href": "guide/view-all-test-plans.html#related-topics",
    "title": "View all test plans",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html",
    "href": "guide/use-test-plans-and-tests.html",
    "title": "When to use test plans and tests",
    "section": "",
    "text": "This topic provides an overview about:"
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "href": "guide/use-test-plans-and-tests.html#what-tests-test-plans-and-test-suites-are",
    "title": "When to use test plans and tests",
    "section": "What tests, test plans, and test suites are",
    "text": "What tests, test plans, and test suites are\n\nTests are designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates.\nTest plans are collections of tests which are meant to be run together to address specific aspects of the documentation.\nExample: the tabular_dataset test plan runs several descriptive and data quality tests on a structured dataset, and documents the results in the ValidMind Platform.\nTest suites are collection of test plans which are meant to run together to automate generate model documentation end-to-end for specific use-cases.\nExample: the binary_classifier_full_suite test suite runs the tabular_dataset and binary_classifier test plans to fully document the data and model sections for binary classification model use-cases."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "href": "guide/use-test-plans-and-tests.html#when-to-use-validmind-tests-test-plans-and-test-suites",
    "title": "When to use test plans and tests",
    "section": "When to use ValidMind Tests, Test plans, and Test suites",
    "text": "When to use ValidMind Tests, Test plans, and Test suites\nValidMind provides many built-in tests and test plans which make it easy for a model developer to document their work at any point during the model development lifecycle when they need to validate that their work satisfies model risk management requirements.\nWhile model developers have the flexibility to decide when to use ValidMind tests, we have identified a few typical scenarios which have their own characteristics and needs:\n\nWhen you want to document and validate your dataset:\n\nFor generic tabular datasets: use the tabular_dataset test plan.\nFor time-series datasets: use the time_series_dataset test plan.\n\nWhen you want to document and validate about your model:\n\nFor binary classification models: use the binary_classifier test plan.\nFor time series models: use the timeseries test plan.\n\nWhen you want to document a binary classification model and the relevant dataset end-to-end: use the binary_classifier_full_suite test suite."
  },
  {
    "objectID": "guide/use-test-plans-and-tests.html#api-reference",
    "href": "guide/use-test-plans-and-tests.html#api-reference",
    "title": "When to use test plans and tests",
    "section": "API Reference",
    "text": "API Reference\nSee the Reference pages for a list of all of the built-in tests and test plans for datasets and models."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html",
    "href": "guide/try-developer-framework-with-docker.html",
    "title": "Try it with Docker Desktop",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with the ValidMind Docker image."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#prerequisites",
    "href": "guide/try-developer-framework-with-docker.html#prerequisites",
    "title": "Try it with Docker Desktop",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have Docker Desktop installed on your machine."
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#steps",
    "href": "guide/try-developer-framework-with-docker.html#steps",
    "title": "Try it with Docker Desktop",
    "section": "Steps",
    "text": "Steps\n\nFrom the command line, pull the latest ValidMind Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nRun the ValidMind Docker image:\ndocker run -it -p 8888:8888 validmind/validmind-jupyter-demo\nAfter the command completes, you should see a message that Jupyter Server is running similar to this:\n[I 2023-05-18 21:53:06.030 ServerApp] Serving notebooks from local directory: /app\n    1 active kernel\n    Jupyter Server 2.5.0 is running at:\n    http://032c824982aa:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\n        http://127.0.0.1:8888/lab?token=89e415261acba66a0f897c0d06ffdacfff9340553e65a626\nCopy the browser URL that starts with http://127.0.0.1:8888 from the message and paste it into a new browser tab.\nAfter JupyterLab opens in your browser, you should see a link for our Quickstart_Customer Churn_full_suite.ipynb notebook.\nDouble click the notebook to open it:"
  },
  {
    "objectID": "guide/try-developer-framework-with-docker.html#whats-next",
    "href": "guide/try-developer-framework-with-docker.html#whats-next",
    "title": "Try it with Docker Desktop",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/login.html",
    "href": "guide/login.html",
    "title": "Log into the ValidMind UI",
    "section": "",
    "text": "Log into our cloud-hosted platform UI to collaborate with others."
  },
  {
    "objectID": "guide/login.html#prerequisites",
    "href": "guide/login.html#prerequisites",
    "title": "Log into the ValidMind UI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA valid email address registered with the ValidMind Platform.\nYour password associated with the registered email address.\n\n\nSteps\n\n\n\n\n\n\n\n\nUsing a company VPC?\n\n\n\nLog in through AWS PrivateLink: https://private.prod.vm.validmind.ai\n\n\n\nIn a web browser, go to https://app.prod.validmind.ai.\nClick Log In and enter your email address and password.\nClick Continue.\n\nAfter successful login, you are redirected to the main dashboard of the ValidMind UI where you can start exploring the features of the ValidMind Platform."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html",
    "href": "guide/release-notes-2023-may-30.html",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#release-highlights",
    "href": "guide/release-notes-2023-may-30.html#release-highlights",
    "title": "May 30, 2023",
    "section": "",
    "text": "Plots and visual outputs have been enhanced with the Plotly package. Users can now view how values change when hovering over the following plots:\n\nCorrelations matrix\nPopulation Stability Index\nConfusion matrices\nROC curve\nPR curve\n\nAdded support for Global test config definition. This allows you to share configuration and parameters across different tests.\n\n\n\n\n\nYou can now export documentation projects to Word documents from the Platform UI. This feature enables you make use of model documentation or validation report files outside of the ValidMind Platform. Learn more …\nWe added a new demo model that allows you to test the ValidMind developer framework with a time-series forecasting model. Learn more …"
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#bugfixes",
    "href": "guide/release-notes-2023-may-30.html#bugfixes",
    "title": "May 30, 2023",
    "section": "Bugfixes",
    "text": "Bugfixes\n\nFixed the display alignment in certain pages of the UI.\nFixed display issues related to Helvetica Neue font not available for Windows users.\nFixed an issue preventing users to drag & drop image files directly in the online editor.\nAdjusted filters for Model Inventory and Documentation Projects search boxes."
  },
  {
    "objectID": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "href": "guide/release-notes-2023-may-30.html#how-to-upgrade",
    "title": "May 30, 2023",
    "section": "How to upgrade",
    "text": "How to upgrade\nTo access the latest version of the ValidMind Platform UI, refresh your browser.\nTo upgrade the ValidMind Developer Framework:\n\nUsing Docker: pull the latest Docker image:\ndocker pull validmind/validmind-jupyter-demo:latest\nIn your own developer environment: restart your notebook and re-run:\n!pip install validmind"
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html",
    "href": "guide/load-credentials-to-env-file.html",
    "title": "Load project credentials to a .env file",
    "section": "",
    "text": "Learn how to store project identifier credentials in a .env file instead of using inline credentials. This topic is relevant for model developers who want to follow best practices for security when running notebooks."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#why-is-this-recommended",
    "href": "guide/load-credentials-to-env-file.html#why-is-this-recommended",
    "title": "Load project credentials to a .env file",
    "section": "Why is this recommended?",
    "text": "Why is this recommended?\nStoring credentials in a .env file is considered a best practice for security. Embedding credentials directly within the code makes them more susceptible to accidental exposure when sharing code or collaborating on projects. Keeing project credentials in a separate file also allows for precise access control and ensures that sensitive credentials are not publically accessible."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#prerequisites",
    "href": "guide/load-credentials-to-env-file.html#prerequisites",
    "title": "Load project credentials to a .env file",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory"
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#steps",
    "href": "guide/load-credentials-to-env-file.html#steps",
    "title": "Load project credentials to a .env file",
    "section": "Steps",
    "text": "Steps\n\nCreate a new file in the same folder as your notebook and name it .env. This is a hidden file, so you may need to change your settings to view it.\nLocate the project identifier credentials for your documentation project. These credentials can be found on the Getting started page. Copy the values from this page and paste them into your .env file in the following format:\n\n``` VM_API_PROJECT= VM_API_HOST= VM_API_KEY= VM_API_SECRET= ```\n\nInsert this code snippet above your project identifier credentials:\n\n``` %load_ext dotenv %dotenv dev.env ```\nThe updated notebook should look like this:\n``` %load_ext dotenv %dotenv .env\nimport validmind as vm\nvm.init( api_host = “http://localhost:3000/api/v1/tracking”, project = “…” ) ```\n\nRun the cell. Instead of using inline credentials, this cell will now load your project credentials from a .env file."
  },
  {
    "objectID": "guide/load-credentials-to-env-file.html#related-topics",
    "href": "guide/load-credentials-to-env-file.html#related-topics",
    "title": "Load project credentials to a .env file",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models with the ValidMind UI\nDocument models with the Developer Framework"
  },
  {
    "objectID": "guide/explore-example-documentation-project.html",
    "href": "guide/explore-example-documentation-project.html",
    "title": "Explore an Example Documentation Project",
    "section": "",
    "text": "Let’s take a look at how the Developer Framework works hand-in-hand with the ValidMind Platform and how documentation and test results get uploaded.\nThe ValidMind Platform is the central place to:"
  },
  {
    "objectID": "guide/explore-example-documentation-project.html#steps",
    "href": "guide/explore-example-documentation-project.html#steps",
    "title": "Explore an Example Documentation Project",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nFrom the side navigation, select Model Inventory.\nLocate or search for the [Quickstart] Customer Churn Model - Initial Validation and select it.\nOn the model details page that open, you can find important information about the model, such as:\n\nThe ID of the model and its specific use case\nThe owners, developers, validators, and business unit associated with the model\nThe risk tier and current version\nAnd more\n\nScroll down to Documentation Project History and select the model.\nOn the project overview page that opens, you can see what is included, such as model, project findings, recent activity, and project stakeholders, and more. In the left sidebar, you can find links to the documentation, project findings, validation report, audit trail, and client integration.\nFor this Quickstart, we will focus on the Documentation section to show you how content from the Developer Framework gets uploaded.\nNote that the model status is In Documentation. This is the status that a model starts in as part of a documentation project. You can click See workflow to look at what the full workflow is, from documentation, to validation, to review, and finally approval.\nFrom the left sidebar, select Documentation &gt; 2. Data preparation &gt; 2.1. Data description.\n\n\n\n\n\nThis content is generated by the ValidMind Developer Framework and provides information about the dataset used, including histograms, information about dataset quality, and test results.\nSections that need your attention get flagged with Requires Attention. These sections get flagged automatically by the Developer Framework whenever a test result is above or below a certain threshold.\nFrom the left sidebar, select 3. Model Development and any of the subsection to see information that has been uploaded by the Developer Framework about:\n\nModel training\nModel evaluation\nModel explainability and interpretability\nModel diagnosis\n\nThe Documentation Guidelines in the ValidMind Insights right sidebar can tell you more about what these sections mean and help you with the task of documenting the model.\nFinally, take a look at section 4. Monitoring and Governance.\nSections like 4.1 Monitoring Plan are not generated by the Developer Framework, but they get added by the model developer in the Platform UI."
  },
  {
    "objectID": "guide/explore-example-documentation-project.html#whats-next",
    "href": "guide/explore-example-documentation-project.html#whats-next",
    "title": "Explore an Example Documentation Project",
    "section": "What’s next",
    "text": "What’s next\nReady to learn about how you can use the ValidMind Platform? Continue with Create your first documentation project."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html",
    "href": "guide/configure-aws-privatelink.html",
    "title": "Configure AWS PrivateLink",
    "section": "",
    "text": "Learn how to configure AWS PrivateLink to establish a private connection between ValidMind and your company network without exposing traffic to the public internet. Using PrivateLink can improve the security and compliance of your applications and data by keeping traffic private and reducing the attack surface of your network.\nAWS PrivateLink is a networking service that allows secure and private communication between Amazon Virtual Private Cloud (VPC) resources and services hosted in other VPCs or in AWS partner services, such as ValidMind. With AWS PrivateLink, you can connect to services over the Amazon network, without needing to expose your network traffic to the public internet.\nPrivateLink works by creating a private VPC endpoint for a supported AWS service within your virtual private cloud. This endpoint acts as a proxy between your VPC and ValidMind, allowing traffic to be routed privately over the AWS network. To make the endpoint easier to use, ValidMind provides a private DNS name that model developers and validators can connect to in a browser.\nThe responsibility of setting up a VPC endpoint for AWS PrivateLink falls to your IT department, such as the cloud engineering, infrastructure, or security teams. To learn more, check Access an AWS service using an interface VPC endpoint."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#prerequisites",
    "href": "guide/configure-aws-privatelink.html#prerequisites",
    "title": "Configure AWS PrivateLink",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou must have access to the AWS Console for your company and the necessary expertise to set up, configure, and maintain AWS services.\nThese steps assume that you already have established connectivity between your own company network and AWS VPC and know which company VPC you want to connect to.\n\nVPC service information\n\n\n\n\nRegion\nService name\nPrivate DNS name\n\n\n\n\nus-west-2\nEmail support@validmind.com\nEmail support@validmind.com"
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#steps",
    "href": "guide/configure-aws-privatelink.html#steps",
    "title": "Configure AWS PrivateLink",
    "section": "Steps",
    "text": "Steps\n\nCreate a VPC endpoint for ValidMind:\n\nLog into the AWS Console.\nIn the VPC dashboard, click Endpoints in the navigation pane.\nClick Create endpoint.\nSelect Other endpoint services.\nEnter the service name from the VPC service information and click Verify service.\nSelect the company VPC that you want to create the endpoint in.\nSelect the subnets where you want to create the endpoint network interfaces.\nConfigure the security group for the VPC endpoint. Make sure to allow traffic between your network and the endpoint.\nClick Create endpoint.\n\nThe status for the endpoint should show Pending.\nContact ValidMind at support@validmind.ai to get your new VPC endpoint connection request accepted. Include the following information:\n\nThe owner or account ID\nThe VPC endpoint ID\n\nAfter ValidMind has accepted your endpoint connection request, verify the endpoint is available:\n\nIn the VPC console, go to the Endpoints section.\nVerify that status for the endpoint shows Available.\n\nEnable the private DNS name:\n\nCheck the VPC endpoint you created, click the Actions menu, and select Modify private DNS name.\nSelect Enable for this endpoint.\nClick Save changes.\nVerify that Private DNS names shows the name shown in the VPC service information.\n\nTest the connection:\n\nFrom your company network, access ValidMind using the private DNS name from the VPC service information.\nIn a browser, confirm that you can successfully connect to ValidMind and log in.\nFrom your developer environment, confirm that you can connect to ValidMind with the Developer Framework."
  },
  {
    "objectID": "guide/configure-aws-privatelink.html#whats-next",
    "href": "guide/configure-aws-privatelink.html#whats-next",
    "title": "Configure AWS PrivateLink",
    "section": "What’s next",
    "text": "What’s next\nAfter completing these steps, users on your company network can connect to ValidMind via AWS PrivateLink using the private DNS name from the VPC service information."
  },
  {
    "objectID": "guide/faq-models.html",
    "href": "guide/faq-models.html",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "href": "guide/faq-models.html#how-do-models-get-registered-in-validmind",
    "title": "Model registration",
    "section": "",
    "text": "Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:\n\nModel Name\nModel Use\nModel Owner\nModel Dependencies\nAnd more"
  },
  {
    "objectID": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "href": "guide/faq-models.html#can-the-fields-for-project-registration-questionnaires-be-configured",
    "title": "Model registration",
    "section": "Can the fields for project registration questionnaires be configured?",
    "text": "Can the fields for project registration questionnaires be configured?\nValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates.\nYou can modify these fields as needed and on an ongoing basis."
  },
  {
    "objectID": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "href": "guide/faq-models.html#can-we-leverage-content-from-historical-documentations",
    "title": "Model registration",
    "section": "Can we leverage content from historical documentations? ",
    "text": "Can we leverage content from historical documentations? \nValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:\n\nAllowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases\nOffering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation\n\nThese features are currently on the roadmap and under research, no release schedule is set yet."
  },
  {
    "objectID": "guide/faq-models.html#can-we-customize-illustrations",
    "href": "guide/faq-models.html#can-we-customize-illustrations",
    "title": "Model registration",
    "section": "Can we customize illustrations?",
    "text": "Can we customize illustrations?\nValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.\nThis feature is currently scheduled for Q4 2023.\nAdditionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code."
  },
  {
    "objectID": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "href": "guide/faq-models.html#can-validmind-manage-complex-model-hierarchies-or-use-cases-with-multiple-models",
    "title": "Model registration",
    "section": "Can ValidMind manage complex model hierarchies or use cases with multiple models?",
    "text": "Can ValidMind manage complex model hierarchies or use cases with multiple models?\nValidMind is enhancing support for complex or modular models in two ways:\n\nBy adding parent/sub-model relational attributes to the model inventory. This is a roadmap item currently scheduled for Q2’2023.\nBy enabling tests to run on multiple models simultaneously and aggregating the results. This is a roadmap item currently scheduled for Q3’2023."
  },
  {
    "objectID": "guide/glossary.html",
    "href": "guide/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This glossary of terms provides short definitions for technical terms you find commonly used in our product documentation grouped by terms related to:"
  },
  {
    "objectID": "guide/glossary.html#validmind-platform",
    "href": "guide/glossary.html#validmind-platform",
    "title": "Glossary",
    "section": "ValidMind platform",
    "text": "ValidMind platform\n\nclient library, Python client library\n\nEnables the interaction of your development environment with the ValidMind platform as part of the Developer Framework.\n\ndocumentation automation\n\nA core benefit of the ValidMind platform that allows for the automatic creation of model documentation using predefined templates and test plans.\n\ndocumentation project, project\n\nActs as a container for the model documentation and validation report of your model into which all documentation work in ValidMind is organized. Enables developers and validators to collaborate on model documentation review, status tracking, and generating validation reports.\n\nmodel inventory \n\nA feature of the ValidMind platform where you can track, manage, and oversee the lifecycle of models. Covers the full model lifecycle, including customizable documentation and approval workflows for different user roles, status and activity tracking, and periodic revalidation.\n\ntemplate, documentation template\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results. When rendered, produces a document that model developers can use for model validation.\n\ntest\n\nRuns a specific quantitative test provided by the ValidMind Developer Framework on the dataset or model. Test results are sent to the ValidMind platform to generate the model documentation according to the template that is associated with the documentation project.\n\ntest plan\n\nA collection of tests which are run together to address specific aspects of the documentation, such as the tabular_dataset test plan that runs several descriptive and data quality tests on a structured dataset.\n\ntest suite\n\nA collection of test plans which are run together to generate model documentation end-to-end for specific use-cases. For example, the binary_classifier_full_suite test suite runs the tabular_dataset and binary_classifier test plans to fully document the data and model sections for binary classification model use-cases.\n\nValidMind Developer Framework\n\nA suite of documentation tools and test plans designed to document models, test models for weaknesses, and identify overfit areas. Enables automating the generation of model documentation by uploading documentation, metrics, and test results to the ValidMind platform.\n\nValidMind Platform UI \n\nA hosted multi-tenant architecture that includes the cloud-based web interface, APIs, databases, documentation and validation engine, and various internal services."
  },
  {
    "objectID": "guide/glossary.html#developer-tools",
    "href": "guide/glossary.html#developer-tools",
    "title": "Glossary",
    "section": "Developer tools",
    "text": "Developer tools\n\npip\n\nA package manager for Python, used to install and manage software packages written in the Python programming language. ValidMind uses the pip command to install the Python client library that is part of the ValidMind Developer Framework so that model developers can make use of its features.\n\nJupyterHub\n\nA multi-user server provides a platform for users to interactively work with data science and scientific computing tools in a collaborative environment. ValidMind uses JupyterHub to share live code, how-to instructions, and visualizations via notebooks as part of our getting started experience for new users.\n\nJupyter notebook\n\nAllows users to create and share documents containing live code, data visualizations, and narrative text. Supports various programming languages, most notably Python, and is widely used for data analysis, machine learning, scientific research, and educational purposes. ValidMind uses notebooks to share sample code and how-to instructions with users that you can adapt to your own use case.\n\nGitHub\n\nA cloud-based platform that provides hosting for software development and version control using Git. GitHub offers collaboration tools such as bug tracking, feature requests, task management, and continuous integration pipelines. ValidMind uses GitHub to share open-source software with you."
  },
  {
    "objectID": "guide/glossary.html#ai-and-model-risk-management",
    "href": "guide/glossary.html#ai-and-model-risk-management",
    "title": "Glossary",
    "section": "AI and model risk management",
    "text": "AI and model risk management\n\n1st line of defense\n\nIn the context of model risk, the business unit(s) responsible for model development, validation, and implementation during the model lifecycle. As the 1st line of defense, model developers must document and test models to ensure that they are accurate, robust, and fit for purpose.\n\n2nd line of defense\n\nIn the context of model risk, an independent oversight function that provides a governance framework for the model lifecycle. As the 2nd line of defense, model validators must independently validate and challenge models created by model developers to ensure that model risk management principles are followed.\n\nmodel developer\n\nResponsible for the design, implementation, and maintenance of models to ensure they are fit-for-purpose, accurate, and aligned with business requirements. As subject matter experts, they collaborate with model validators and other business units, ensuring the models are conceptually sound and robust.\n\nmodel documentation\n\nA structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. Within the realm of model risk management, this documentation serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n\nmodel inventory\n\nA systematic and organized record of all quantitative and qualitative models used within an organization. This inventory facilitates oversight, tracking, and assessment by listing each model’s purpose, characteristics, owners, validation status, and associated risks. Also see ValidMind model inventory.\n\nmodel risk management (MRM)\n\nA structured approach to identifying, assessing, mitigating, and monitoring risks arising from the use of quantitative and qualitative models within an organization. Ensures that models are developed, validated, and used appropriately, with robust controls in place. Encompasses practices such as maintaining a model inventory, conducting periodic validations, and ensuring proper documentation.\n\nmodel risk\n\nThe potential for financial loss, incorrect decisions, or unintended consequences resulting from errors or inaccuracies in AI or machine learning models. Model risk typically arises from incorrect or inappropriate use of models, inaccurate assumptions, or limitations in data quality. Consequences of unmitigated model risk can include adverse outcomes such as financial loss, damage to reputation, and regulatory penalties, for example.\n\nmodel governance\n\nA framework of policies, procedures, and standards established to oversee the lifecycle of models within an organization. Ensures that models are developed, validated, implemented, and retired in a controlled and consistent manner, promoting accountability, transparency, and adherence to regulatory requirements.\n\nmodel validation\n\nA systematic process to evaluate and verify that a model is performing as intended, accurately represents the phenomena it is designed to capture, and is appropriate for its specified purpose. This assessment encompasses a review of the model’s conceptual soundness, data integrity, calibration, and performance outcomes, as well as testing against out-of-sample datasets. Within model risk management, model validation ensures that potential risks associated with model errors, misuse, or misunderstanding are identified and mitigated.\n\nmodel validator\n\nresponsible for conducting independent assessments of models to ensure their accuracy, reliability, and appropriateness for intended purposes. The role involves evaluating a model’s conceptual soundness, data integrity, calibration methods, and overall performance, typically using out-of-sample datasets. Model validators identify potential risks and weaknesses, ensuring that models within an organization meet established standards and regulatory requirements, and provide recommendations to model developers for improvements or modifications.\n\nthree lines of defense\n\nA structured approach to model risk management, consisting of three independent functions. The first line consists of business units responsible for model development, validation, and implementation. They ensure that models are accurate, robust, and fit for purpose. The second line is an independent model risk oversight function that provides a governance framework and guidance for model risk management. The third line is the internal or external audit function, which assesses the effectiveness of model risk management practices and controls.\n\nvalidation report\n\nA formal document produced after a model validation process, outlining the findings, assessments, and recommendations related to a specific model’s performance, appropriateness, and limitations. Provides a comprehensive review of the model’s conceptual framework, data sources and integrity, calibration methods, and performance outcomes. Within model risk management, the validation report is crucial for ensuring transparency, demonstrating regulatory compliance, and offering actionable insights for model refinement or adjustments."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html",
    "href": "guide/comment-on-documentation-projects.html",
    "title": "Comment on document projects",
    "section": "",
    "text": "Learn how a model validator can post comments on a model documentation project. This topic is relevant for model validators who want to provide feedback and ask questions to model developers on the basis of the model documentation provided."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#prerequisites",
    "href": "guide/comment-on-documentation-projects.html#prerequisites",
    "title": "Comment on document projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "href": "guide/comment-on-documentation-projects.html#posting-a-comment-on-the-documentation",
    "title": "Comment on document projects",
    "section": "Posting a comment on the documentation",
    "text": "Posting a comment on the documentation\n\nIn any section of the model documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your text comment and click Submit.\nYou can view the comment by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "href": "guide/comment-on-documentation-projects.html#responding-to-an-existing-comment",
    "title": "Comment on document projects",
    "section": "Responding to an existing comment",
    "text": "Responding to an existing comment\n\nSelect a highlighted text portion to view the associated comment thread, or click the Comments tab in the ValidMind Insights right sidebar.\nEnter your text comment and click Submit.\nYou can view the comment thread by highlighting the corresponding portion of text again, or by clicking the Comments tab in the ValidMind Insights side bar.\n\n\n\n\n\n\n\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/comment-on-documentation-projects.html#related-topics",
    "href": "guide/comment-on-documentation-projects.html#related-topics",
    "title": "Comment on document projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nView validation guidelines"
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html",
    "href": "guide/try-developer-framework-with-jupyterhub.html",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "",
    "text": "Learn how to access our introductory Jupyter notebook with Jupyter Hub (recommended)."
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html#steps",
    "href": "guide/try-developer-framework-with-jupyterhub.html#steps",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "Steps",
    "text": "Steps\n\nIn a web browser, go to https://jupyterhub.validmind.ai.\nClick Sign in with Auth0, enter your ValidMind email address and password credentials, and click Continue.\nIn the sidebar, double click the Quickstart_Customer Churn_full_suite.ipynb notebook:\n\nAfter the notebook opens, run the first few cells in the notebook:\n\nHover over each cell and click the  icon; OR\nPress Shift + Enter on Windows or Cmd + Enter if you are on a Mac\n\nThe notebook will guide you through installing the ValidMind Developer Framework, initializing the Python environment, and finally initializing the ValidMind Client Library by connecting to your own documentation project in the ValidMind Platform.\nNear the bottom of the Initialize ValidMind section, you should see a message like this:\nConnected to ValidMind. Project: Customer Churn Model - Initial Validation (xxxxxxxxxxxxxxxxxxxxxxxxx)\nThis message confirms that the Developer Framework works as expected.\n\nYou can now continue running the rest of the cells if you want to see how the demo notebook works or, to save some time, you can move on to the next section to explore the Platform UI."
  },
  {
    "objectID": "guide/try-developer-framework-with-jupyterhub.html#whats-next",
    "href": "guide/try-developer-framework-with-jupyterhub.html#whats-next",
    "title": "Try it with Jupyter Hub (recommended)",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Platform UI to learn how you can use the sample notebook."
  },
  {
    "objectID": "guide/faq-integrations.html",
    "href": "guide/faq-integrations.html",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "href": "guide/faq-integrations.html#can-you-integrate-with-jira-to-connect-with-our-model-development-pipeline",
    "title": "Integrations and support",
    "section": "",
    "text": "ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023)."
  },
  {
    "objectID": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "href": "guide/faq-integrations.html#what-libraries-beyond-xgboost-are-supported",
    "title": "Integrations and support",
    "section": "What libraries beyond XGBoost are supported?",
    "text": "What libraries beyond XGBoost are supported?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the Developer Framework for additional modeling techniques, check Do you include explainability-related testing and documentation? for more information."
  },
  {
    "objectID": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "href": "guide/faq-integrations.html#what-other-programming-languages-and-development-environments-do-you-support-beyond-python-and-jupyter-notebook-such-as-r-and-sas",
    "title": "Integrations and support",
    "section": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?",
    "text": "What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?\nValidMind’s Developer Framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.\nCurrently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).\nWe are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).\nSupport for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers."
  },
  {
    "objectID": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "href": "guide/faq-integrations.html#do-you-support-integration-with-data-lakes-and-etl-solutions",
    "title": "Integrations and support",
    "section": "Do you support integration with data lakes and ETL solutions?",
    "text": "Do you support integration with data lakes and ETL solutions?\nSupport for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).\nWe will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake."
  },
  {
    "objectID": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "href": "guide/faq-integrations.html#which-model-development-packageslibraries-are-supported-by-the-developer-framework-what-about-complexdistributed-models-built-with-tensorflow",
    "title": "Integrations and support",
    "section": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?",
    "text": "Which model development packages/libraries are supported by the Developer Framework? What about complex/distributed models built with TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python and R, such as:\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training or evaluation pipeline, using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "href": "guide/faq-integrations.html#is-it-possible-for-us-to-integrate-the-tool-with-llms-like-gpt-3",
    "title": "Integrations and support",
    "section": "Is it possible for us to integrate the tool with LLMs like GPT-3?",
    "text": "Is it possible for us to integrate the tool with LLMs like GPT-3?\nValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:\n\nGenerating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).\nProviding insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled)."
  },
  {
    "objectID": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "href": "guide/faq-integrations.html#can-you-handle-more-sophisticated-aiml-libraries-such-as-pytorch-tensorflow",
    "title": "Integrations and support",
    "section": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?",
    "text": "Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?\nValidMind supports the most popular open-source model development libraries in Python, R, such as :\n\nscikit-learn\nXGBoost\nstatsmodels\nPyTorch\nTensorFlow\n\nFor distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code. ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms."
  },
  {
    "objectID": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "href": "guide/faq-integrations.html#does-validmind-support-data-dictionaries",
    "title": "Integrations and support",
    "section": "Does ValidMind support data dictionaries?",
    "text": "Does ValidMind support data dictionaries?\nYou can pass a data dictionary to ValidMind via the Developer Framework, such as in CSV format."
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html",
    "href": "guide/samples-jupyter-notebooks.html",
    "title": "Code samples",
    "section": "",
    "text": "Our code samples, based on Jupyter notebooks, are designed to showcase the capabilities and features of the Developer Framework and ValidMind Platform, while also providing you with useful examples that you can build on and adapt for your own use cases.\nTry the notebooks yourself:\nGoogle Colaboratory (Colab) is a free Jupyter notebook environment that runs in the cloud. There, you can work with our Jupyter notebooks by saving your own copy, write and execute code, share your work to collaborate with others in real-time, and download notebooks to try them out locally in your own developer environment."
  },
  {
    "objectID": "guide/samples-jupyter-notebooks.html#related-topics",
    "href": "guide/samples-jupyter-notebooks.html#related-topics",
    "title": "Code samples",
    "section": "Related topics",
    "text": "Related topics\nFor an introduction to how these notebooks get used with ValidMind, take a look at the Quickstart."
  },
  {
    "objectID": "guide/key-validmind-concepts.html",
    "href": "guide/key-validmind-concepts.html",
    "title": "ValidMind",
    "section": "",
    "text": "Model documentation\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model. Provides a comprehensive record and description of a quantitative model. Should encompass all relevant information about the model in accordance with:\n\n\n\nIntended use\nRegulatory requirements set by regulatory bodies\nModel risk policies set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\n\n\nDocumentation project\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s lifecycle constitutes a new project and can be configured with its own templates and workflows.\n\nTemplate\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results.\n\n\nModel documentation is populated when the tests contained in a template run successfully, completing the test suite. This process ensures that the model meets all the specified requirements. At the same time, template placeholders get populated with content that documents the model. This content is generated by the ValidMind Developer Framework, providing a coherent structure for model information.\nEssentially, our platform scans these templates, identifies all tests, and systematically organizes them into a well-structured test suite. This automation enhances the efficiency and consistency of the validation process.\nThe criteria for these templates are typically provided by your model risk management team. We provide some templates out of the box that you can use and programmatically customize to suit the requirements of each model use case. This task of customization is usually performed by an administrator, who ensures that the templates align with the organizational standards and specific needs of each model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates."
  },
  {
    "objectID": "guide/tutorials.html",
    "href": "guide/tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our tutorials provide a more targeted learning experience and cover specific scenarios or use cases."
  },
  {
    "objectID": "guide/tutorials.html#related-topics",
    "href": "guide/tutorials.html#related-topics",
    "title": "Tutorials",
    "section": "Related topics",
    "text": "Related topics\nBesides our tutorials, we also offer a Quickstart that walks you through the full experience from the Developer Framework to the Platform UI."
  },
  {
    "objectID": "guide/export-documentation.html",
    "href": "guide/export-documentation.html",
    "title": "Export documentation",
    "section": "",
    "text": "Learn how to export a model documentation project in Word or PDF format. This topic is relevant for both model developers and model validators who need to export the model documentation or validation report files to use them outside the ValidMind Platform."
  },
  {
    "objectID": "guide/export-documentation.html#prerequisites",
    "href": "guide/export-documentation.html#prerequisites",
    "title": "Export documentation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model is already registered in the model inventory\nA model document project is completed or in progress\nYou are logged into the ValidMind Platform\n\nValidMind supports Word 365, Word 2019, Word 2016, and Word 2013."
  },
  {
    "objectID": "guide/export-documentation.html#export-model-documentation",
    "href": "guide/export-documentation.html#export-model-documentation",
    "title": "Export documentation",
    "section": "Export Model Documentation",
    "text": "Export Model Documentation\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Documentation on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#export-validation-report",
    "href": "guide/export-documentation.html#export-validation-report",
    "title": "Export documentation",
    "section": "Export Validation Report",
    "text": "Export Validation Report\n\nFrom the Documentation Projects page, select the project you want to export.\nClick Validation Report on the left to view the model documentation table of contents.\nIn the table of contents sidebar, click Export.\nConfigure the export options:\n\n\nChoose the file format for export. We support exporting to .docx for Microsoft Word and .pdf for PDF format.\n\nClick Download file to download the file locally on your machine."
  },
  {
    "objectID": "guide/export-documentation.html#related-topics",
    "href": "guide/export-documentation.html#related-topics",
    "title": "Export documentation",
    "section": "Related topics",
    "text": "Related topics\n\nDocument models\nReview and comment on documentation projects\nSubmit for approval"
  },
  {
    "objectID": "guide/solutions.html",
    "href": "guide/solutions.html",
    "title": "ValidMind solutions",
    "section": "",
    "text": "These topics introduce the ValidMind architecture and basic requirements.\nKey Concepts & Architecture\nOverview of ValidMind architecture and basic concepts.\nSupported Cloud Platforms\nOverview of the cloud computing platforms on which ValidMind is offered.\nValdiMind Editions\nDescription of the services and features included with each edition of ValidMind.\nValidMind Releases\nDescription of the ValidMind release process and instructions for requesting 24-hour early access for Enterprise Edition (and higher) accounts.\nOverview of Key Features\nList of key/major features in the current release of ValidMind."
  },
  {
    "objectID": "guide/solutions.html#compliance",
    "href": "guide/solutions.html#compliance",
    "title": "ValidMind solutions",
    "section": "Compliance",
    "text": "Compliance\nTBD"
  },
  {
    "objectID": "guide/solutions.html#software-dependencies",
    "href": "guide/solutions.html#software-dependencies",
    "title": "ValidMind solutions",
    "section": "Software dependencies",
    "text": "Software dependencies\nTBD"
  },
  {
    "objectID": "guide/create-documentation-project.html",
    "href": "guide/create-documentation-project.html",
    "title": "Create documentation projects",
    "section": "",
    "text": "Learn how to create a new documentation project in the ValidMind Platform. You can use this new project to upload tests and documentation to the ValidMind Platform, review and validate models, and generate validation reports."
  },
  {
    "objectID": "guide/create-documentation-project.html#prerequisites",
    "href": "guide/create-documentation-project.html#prerequisites",
    "title": "Create documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory"
  },
  {
    "objectID": "guide/create-documentation-project.html#steps",
    "href": "guide/create-documentation-project.html#steps",
    "title": "Create documentation projects",
    "section": "Steps",
    "text": "Steps\n\nLog in to the ValidMind UI.\nOn the Documentation Projects page, click Create new project.\nSelect the relevant details in the form:\n\nSelect the relevant model\nSelect the relevant type of documentation you are looking to generate\nEnter a name for the project\n\nClick Create Project.\n\n\n\n\n\n\n\n\n\nThe documentation template is automatically applied based on the selected model details and documentation requirements configured by an administrator, such as your model risk management team.\n\n\n\nValidMind has now created an empty documentation project associated with the model. You can access this project from the UI on the Documentation Projects page or by navigating to the relevant model details page in the Model Inventory page.\n\nLocating the project identifier, API key and secret:\nOn the Getting started page of the newly created project, you can find the initialization code that enables the client library to associate documentation and tests with the appropriate project. The initialization code configures the following arguments:\n\n\napi_host: The location of the ValidMind API\napi_key: The account API key\napi_secret: The account secret key\nproject: The project identifier\n\nThe code snippet can be copied and pasted directly into your developer source code to integrate the ValidMind Developer Framework and to be able to upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/create-documentation-project.html#related-topics",
    "href": "guide/create-documentation-project.html#related-topics",
    "title": "Create documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nInstall and initialize the Developer Framework\nDocument models with the Developer Framework\nDocument models with the ValidMind UI"
  },
  {
    "objectID": "guide/troubleshooting.html",
    "href": "guide/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "Learn how to resolve commonly encountered issues with the Developer Framework."
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-install-the-validmind-developer-framework",
    "href": "guide/troubleshooting.html#cannot-install-the-validmind-developer-framework",
    "title": "Troubleshooting",
    "section": "Cannot install the ValidMind Developer Framework",
    "text": "Cannot install the ValidMind Developer Framework\nIssue: You cannot run pip install validmind or import validmind as vm in the ValidMind Developer Framework notebooks.\nFix: Make sure you are installing the latest version of the Developer Framework by running this command:\n%pip install --upgrade validmind"
  },
  {
    "objectID": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "href": "guide/troubleshooting.html#cannot-initialize-validmind-client-library",
    "title": "Troubleshooting",
    "section": "Cannot initialize ValidMind client library",
    "text": "Cannot initialize ValidMind client library\nIssue: When you run vm.init(), you encounter an error message like this:\nMissingAPICredentialsError: API key and secret must be provided either as environment variables or as arguments to init.\nor\nInvalidProjectError: Invalid project ID. Please ensure that you have provided a project ID that belongs to your organization.\nFix: Make sure that you are using the correct initialization credentials for the project you are trying to connect to.\nFollow the steps in Install and initialize the Developer Framework for detailed instructions on how to integrate the Developer Framework and upload to the ValidMind Platform."
  },
  {
    "objectID": "guide/troubleshooting.html#additional-resources",
    "href": "guide/troubleshooting.html#additional-resources",
    "title": "Troubleshooting",
    "section": "Additional resources",
    "text": "Additional resources\nCheck out our FAQ page to browse through common questions, or contact our support team for more help troubleshooting technical issues."
  },
  {
    "objectID": "guide/review-documentation-project.html",
    "href": "guide/review-documentation-project.html",
    "title": "Review and comment on documentation projects",
    "section": "",
    "text": "Learn how to use the ValidMind UI editor to review, and comment on a documentation project. This topic is relevant for:"
  },
  {
    "objectID": "guide/review-documentation-project.html#prerequisites",
    "href": "guide/review-documentation-project.html#prerequisites",
    "title": "Review and comment on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has already been created for this project\nA model developer has started generating documentation, either using the Developer Framework or via the online UI editor\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/review-documentation-project.html#view-model-documentation",
    "href": "guide/review-documentation-project.html#view-model-documentation",
    "title": "Review and comment on documentation projects",
    "section": "View model documentation",
    "text": "View model documentation\n\nNavigate to the relevant model documentation project:\nIn the Documentation Projects page, select the project corresponding to the model for which you want to view documentation.\nFrom the Overview page, select Documentation on the left.\nYou can now jump to any section of the model documentation by expanding the table of contents on the left and selecting the relevant section you would like to view."
  },
  {
    "objectID": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "href": "guide/review-documentation-project.html#post-comments-on-the-documentation",
    "title": "Review and comment on documentation projects",
    "section": "Post comments on the documentation",
    "text": "Post comments on the documentation\n\nIn any section of the documentation, select a portion of text that you would like to comment on, and click the Add comment button that appears.\n\n\n\nEnter your comment and click Submit.\nYou can now view the comment by highlighting the corresponding portion of text or by clicking the Comments tab in the ValidMind Insights right sidebar."
  },
  {
    "objectID": "guide/review-documentation-project.html#whats-next",
    "href": "guide/review-documentation-project.html#whats-next",
    "title": "Review and comment on documentation projects",
    "section": "What’s next",
    "text": "What’s next\n\nDocument models with the ValidMind UI\nView documentation guidelines\nSubmit for approval"
  },
  {
    "objectID": "guide/before-you-begin.html",
    "href": "guide/before-you-begin.html",
    "title": "Before you begin",
    "section": "",
    "text": "Don’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#prerequisites",
    "href": "guide/before-you-begin.html#prerequisites",
    "title": "Before you begin",
    "section": "",
    "text": "Don’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\nTo connect the ValidMind Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai.\n\n\nTo follow the Quickstart, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\n\n\nYou need to be able to access our ValidMind Web UI from a modern browser such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox."
  },
  {
    "objectID": "guide/before-you-begin.html#whats-next",
    "href": "guide/before-you-begin.html#whats-next",
    "title": "Before you begin",
    "section": "What’s next",
    "text": "What’s next\nContinue with Explore the Developer Framework. We recommend using Jupyter Hub."
  },
  {
    "objectID": "guide/quickstart.html",
    "href": "guide/quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "The easiest way to get started with ValidMind is to try out our Developer Framework in Jupyter Hub and to explore the ValidMind Platform UI online."
  },
  {
    "objectID": "guide/quickstart.html#before-you-begin",
    "href": "guide/quickstart.html#before-you-begin",
    "title": "Quickstart",
    "section": "Before you begin",
    "text": "Before you begin\nTo try out ValidMind, you need to be a registered user on the ValidMind Platform.\n\n\n\n\n\n\nDon’t have access?\n\n\n\nRequest a trial to try out ValidMind for free.\n\n\n\nValidMind Developer Framework\nTo try the ValidMind Developer Framework, you need access to ONE of the following:\n\nJupyter Hub (recommended)\nGoogle Colaboratory (Colab)\nDocker Desktop\n\nTo run our sample Jupyter notebooks locally, your developer environment must support Python 3.8+.\n\n\nValidMind Platform UI\nWe support most modern browsers, such as Microsoft Edge, Google Chrome, Apple Safari, or Mozilla Firefox.\nTo upload from the Developer Framework to the ValidMind Platform and to access the web user interface, you must be able to access our domains:\n\nvalidmind.ai\nvalidmind.com\n\nIf necessary, ask a network administrator to add these domains to your firewall allowlist (whitelist).\nIf your company has strict security requirements and requires you to connect via VPN or AWS PrivateLink, please contact your IT/InfoSec team. For additional help setting up a VPN or PrivateLink with ValidMind’s MRM platform please visit configure AWS PrivateLink or send an email to support@validmind.ai."
  },
  {
    "objectID": "guide/quickstart.html#steps",
    "href": "guide/quickstart.html#steps",
    "title": "Quickstart",
    "section": "Steps",
    "text": "Steps\n\nTry the Developer Framework (10 minutes)\nTry our introductory Jupyter notebook to see the Developer Framework in action.\nExplore the Platform UI (15 minutes):\n\nExplore an Example Documentation Project\nCreate Your First Documentation Project\nUpload to Your Documentation Project\n\n\n\nWhat’s next\nIf you’re ready to do more with ValidMind, check out Next steps."
  },
  {
    "objectID": "guide/next-steps.html",
    "href": "guide/next-steps.html",
    "title": "Next steps",
    "section": "",
    "text": "Ready to use ValidMind for production with your own use cases? Our documentation includes how-to instructions for the following user roles:\n\nFor platform administrators — Learn how to configure the platform, from setting up connectivity via AWS PrivateLink, to customizing the ValidMind Platform to suit your existing workflows, and more.\nFor model developers — Find information for ValidMind test plans and tests, additional Jupyter notebooks, and the ValidMind Developer Framework reference.\nWe have more code samples available that you can download and try out yourself.\nAlso check the Guides for how to integrate the Developer Framework in your own environment, add documentation, and collaborate with model validators.\nFor model validators — Learn how to step through the approval process after review and generate validation reports as you collaborate with model developers.\nCollaborate on documentation projects\n\n\nHave more questions?\nWe curate several lists of frequently asked questions (FAQs) that might be of help:\n\nModel registration, configuration, and customization\nModel inventory, tracking, and reporting\nDocumentation and templates\nWorkflows and collaboration\nTesting and thresholds\nIntegrations and support\nData handling and privacy\n\nDon’t see what you are looking for? Email support@validmind.com to get help from a human.\n\n\nNeed help?\nIf you would like help from a human, check our Support page. You can also send us your feedback on product features or our documentation."
  },
  {
    "objectID": "guide/get-started-developer-framework.html",
    "href": "guide/get-started-developer-framework.html",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "",
    "text": "This section introduces you to the ValidMind Developer Framework and its functionalities. This topic is relevant for model developers who want to learn how to use the framework to document and test their models."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#what-is-the-developer-framework",
    "href": "guide/get-started-developer-framework.html#what-is-the-developer-framework",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "What is the Developer Framework?",
    "text": "What is the Developer Framework?\nValidMind’s Developer Framework provides a rich suite of documentation tools and test plans, from documenting descriptions of your dataset to validation testing your models for weak spots and overfit areas.\nYou use the framework to automate the generation of model documentation by uploading documentation artifacts and test results to the ValidMind platform.\n\n\n\n\n\n\nValidMind offers two primary methods for documenting model risk:\n\nBy generating model documentation: Through automation, the framework extracts metadata from associated datasets and models for you and generates model documentation. You can also add more documentation and tests manually using the documentation editing capabilities in the ValidMind UI.\nBy running pre-built validation tests: The framework provides a suite of validation tests for common financial services use cases. For cases where these tests do not cover everything you need, you can also extend existing test suites with your own proprietary tests or testing providers.\n\nThe Developer Framework is designed to be model agnostic. If your model is built in Python, ValidMind’s Python client library already provides all the standard functionality you might need without requiring your developers to rewrite any functions.\n\n\n\n\n\n\n Expand to learn about key ValidMind concepts\n\n\n\n\n\n\n\nModel documentation\n\nThe purpose of model documentation is to provide transparency, facilitate understanding, and enable effective governance and oversight of the model. Provides a comprehensive record and description of a quantitative model. Should encompass all relevant information about the model in accordance with:\n\n\n\nIntended use\nRegulatory requirements set by regulatory bodies\nModel risk policies set by your institution\nAssumptions\nMethodologies\nData and inputs\nModel performance evaluation\nLimitations\n\n\nDocumentation project\n\nAll documentation work in ValidMind is organized into projects which act as a container for the model documentation and validation report of your model. Each stage of the model’s lifecycle constitutes a new project and can be configured with its own templates and workflows.\n\nTemplate\n\nFunctions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. ValidMind templates come with pre-defined sections, similar to test placeholders, including boilerplates and spaces designated for documentation and test results.\n\n\nModel documentation is populated when the tests contained in a template run successfully, completing the test suite. This process ensures that the model meets all the specified requirements. At the same time, template placeholders get populated with content that documents the model. This content is generated by the ValidMind Developer Framework, providing a coherent structure for model information.\nEssentially, our platform scans these templates, identifies all tests, and systematically organizes them into a well-structured test suite. This automation enhances the efficiency and consistency of the validation process.\nThe criteria for these templates are typically provided by your model risk management team. We provide some templates out of the box that you can use and programmatically customize to suit the requirements of each model use case. This task of customization is usually performed by an administrator, who ensures that the templates align with the organizational standards and specific needs of each model.\n\nTests\n\nA function contained in the ValidMind Developer Framework, which is designed to run a specific quantitative test on the dataset or model. Test results are sent to the ValidMind Platform to generate the model documentation according to the relevant templates."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#how-do-i-use-the-framework",
    "href": "guide/get-started-developer-framework.html#how-do-i-use-the-framework",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "How do I use the framework?",
    "text": "How do I use the framework?\nA typical high-level workflow for model developers consists of four major steps:\n\n\n\n\ngraph LR\n    A[Develop&lt;br&gt;model] --&gt; B[Generate model&lt;br&gt;documentation]\n    B --&gt; C[Refine model&lt;br&gt;documentation]\n    C --&gt; D[Submit for review]\n    C --&gt; B\n\n\n\n\n\n\n\nDevelop model\n\nIn your existing developer environment, build one or more candidate models that need to be validated. This step includes all the usual activities you already follow as a model developer.\n\nGenerate model documentation\n\nWith the ValidMind Developer Framework, generate automated model documentation and run validation tests. This step includes making use of the automation and testing functionality provided by the framework and uploading the output to the Platform UI. You can iteratively regenerate the documentation as you work though the next step of refining your documentation.\n\nRefine model documentation\n\nIn the ValidMind Platform UI, review the generated documentation and test output. Iterate over the documentation and test output to refine your model documentation. Collaborate with other developers and model validators to finalize the model documentation and get it ready for review.\n\nSubmit for review\n\nIn the ValidMind Platform UI, you submit the model documentation for review which moves the documentation workflow moves to the next phase where a model validator will review it."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#how-do-i-generate-model-documentation",
    "href": "guide/get-started-developer-framework.html#how-do-i-generate-model-documentation",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "How do I generate model documentation?",
    "text": "How do I generate model documentation?\n\nBefore you can use the Developer Framework, you need to verify that the current documentation template contains all the necessary tests for the model you are developing:\n\nThe template might already be sufficient and you only need to run the template within the Developer Framework to populate documentation.\nOr, more likely, the template might need additional tests that you can add these tests via the Developer Framework.\n\nThis process of verifying the suitability of the the current documentation template and adding more tests to the template is an iterative process:\n\n\n\n\ngraph LR\n    A[Verify template] --&gt; B[Build template]\n    B --&gt; D[Add tests and&lt;br&gt;content blocks]\n    D --&gt; E[Add external&lt;br&gt;test providers]\n    E --&gt; C[Run template]\n    C --&gt; B\n\n\n\n\n\n\n\n\n\nBuild the template\n\nWhen the documentation template requires more tests to be added, or if the documentation template does not include a specific content or test block you need:\n\n\n\nFor functionality provided by the Developer Framework: Add the relevant tests or content blocks for the model use case.\nFor tests not provided by the framework: Add your own external test provider.\n\nRun the template : When you have registered all the required tests as content blocks in the documentation template, populate the necessary model documentation by adding this call to your model:\nrun_documentation_tests()\n\n\n\n\n\n\nValidMind may not support all potential use cases or provide a universally applicable documentation template. Typically, you initiate the process of putting ValidMind into production by constructing a template specific for your own use case and then refine your the documentation project.\n\n\n\n\nEnd-to end workflow\n\n\n\nIn your modeling environment\n\n\nBuild your model.\nExport the datasets and model.\n\nNext, go to With the Developer Framework, Step 2. \n\n\nWith the Developer Framework\n\nCreate a notebook to select and build the relevant tests.\n From your modeling environment, load the trained datasets and models.\n Use the instructions from In the Platform UI, Step 3, initialize the ValidMind Developer Framework.\nSelect the relevant tests.\nReview if all tests are covered by ValidMind or your external test provider:\n\nIf all tests are NOT covered: Create and register additional tests.\nIf all tests are covered:\n\nRun the selected tests.\nReview your test results.\n\n\n\nNext, go to In the ValidMind Platform UI, Step 5. \n\n\n\nIn the ValidMind Platform UI\n\nRegister a new model.\nReview the template structure.\nLocate the framework integration instructions.\nGo to With the Developer Framework, Step 3. \n After With the Developer Framework, Step 6, add content blocks to your model documentation:\nSelect the block type:\n\nFor test-driven blocks: Select from available test provider results\nFor text blocks:\n\nFor new block:\n\nAdd new editable text content block\nReview and collaborate on the content block \n\nFor existing blocks: Select from available texts from content provider\n\n\nSubmit your documentation project for review."
  },
  {
    "objectID": "guide/get-started-developer-framework.html#related-topics",
    "href": "guide/get-started-developer-framework.html#related-topics",
    "title": "Get started with the ValidMind Developer Framework",
    "section": "Related Topics",
    "text": "Related Topics\n\nCode samples\nValidMind Developer Framework Reference"
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html",
    "href": "guide/collaborate-on-documentation-projects.html",
    "title": "Collaborate on documentation projects",
    "section": "",
    "text": "Learn how ValidMind enhances collaboration between model validators and developers on documentation projects. This topic is relevant for model validators who want to track changes across projects, add comments, and access revision history for real-time collaboration with model developers."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#prerequisites",
    "href": "guide/collaborate-on-documentation-projects.html#prerequisites",
    "title": "Collaborate on documentation projects",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou are a registered user on the ValidMind Platform\nThe model you are documenting is registered in the model inventory\nA documentation project has submitted for review or validation by the model validation team\nYou are logged into the ValidMind Platform"
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#commenting",
    "href": "guide/collaborate-on-documentation-projects.html#commenting",
    "title": "Collaborate on documentation projects",
    "section": "Commenting",
    "text": "Commenting\n\nPosting comments to the documentation\n\nIn any section of the model documentation, highlight the portion of text you would like to comment on, and click the Comment button in the toolbar.\nEnter your comment and click Comment.\nYou can view the comment by clicking the highlighted text. Comments will also appear in the right sidebar.\n\n\n\nResponding to an existing comment\n\nClick the highlighted text portion to view the comment thread.\nEnter your comment and click Reply.\nYou can view the comment thread by clicking the highlighted text.\n\n\n\nResolving comment threads and viewing archived comments\n\nClick the highlighted text portion to view the thread, then click  to resolve the thread.\nTo view the resolved comment thread, click the Comment archive button in the toolbar. You can view a history of all archived comments in the Comment archive.\nTo reopen a comment thread, reply to the comment thread in the Comment archive or click the Reopen button that appears next to the highlighted text portion.\n\n\n\nEditing and deleting comments\n\nClick the highlighted text portion to access the comment thread.\nTo edit a comment in the thread, click the More options icon for the corresponding comment and click Edit.\nEdit your comment and click Save.\nTo edit a comment in a resolved thread, follow the same steps but click the Comments archive button first to access the resolved thread.\n\n\n\n\n\n\n\n\n\n\nAll users associated with a project, such as model developers and model validators, will see a notification that a comment has been posted in their Recent Activity feed, accessible via the ValidMind Home page."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#tracking-changes",
    "href": "guide/collaborate-on-documentation-projects.html#tracking-changes",
    "title": "Collaborate on documentation projects",
    "section": "Tracking changes",
    "text": "Tracking changes\n\nSuggesting a change\n\nClick the Track changes button in the toolbar to turn on suggestion mode.\nMake your changes to the documentation project. When changes tracking is enabled, other project contributers can accept or decline the suggested changes.\n\n\n\nResolving changes\n\nSuggested changes appear in green or red highlighted text, depending on if the change is adding or removing content. To accept or decline a change, click the highlighted text, then click  or . You can also reply to a suggested change.\nTo mass accept or decline suggestions, click the dropdown arror next to the Track changes button and click the desired option."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#revision-history",
    "href": "guide/collaborate-on-documentation-projects.html#revision-history",
    "title": "Collaborate on documentation projects",
    "section": "Revision history",
    "text": "Revision history\n\nSaving a version\n\nClick the Revision history button in the toolbar.\nIn the dropdown, click Save current version. Optionally, enter a version name. The default name is the date and time the latest change was made.\n\n\n\nViewing revision history\n\nClick the Revision history button in the toolbar, then click Open revision history. Here, you can view a history of all saved versions and your current version.\nTo see the the change made with each version, select the version in the right sidebar. Changes made in that version are highlighted. Hover over the highlighted content to see who made the change.\n\n\n\nRestoring a version\n\nTo restore a version, select the desired version and click Restore this version.\nThe restored version will now appear under revision history with the name: “Restored: ‘version name’”. To exit revision history without restoring a version, click Back to editing."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#real-time-collaboration",
    "href": "guide/collaborate-on-documentation-projects.html#real-time-collaboration",
    "title": "Collaborate on documentation projects",
    "section": "Real-time collaboration",
    "text": "Real-time collaboration\nUsers can simultaneously edit the documentation project, leave and respond to comments suggestions, and access revision history. Changes to the documentation project are also automatically added to ValidMind’s activity feed."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#additional-features",
    "href": "guide/collaborate-on-documentation-projects.html#additional-features",
    "title": "Collaborate on documentation projects",
    "section": "Additional features",
    "text": "Additional features\n\nSpell and grammar checker.\nMath formulas. Add math formulas to documentation by clicking the MathType button and using the toolbar, or switch to handwriting."
  },
  {
    "objectID": "guide/collaborate-on-documentation-projects.html#related-topics",
    "href": "guide/collaborate-on-documentation-projects.html#related-topics",
    "title": "Collaborate on documentation projects",
    "section": "Related topics",
    "text": "Related topics\n\nWork with Validation Reports \nComment on Documentation Projects"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The guide to elevating your AI model risk workflow",
    "section": "",
    "text": "Trial 2\n    \n    \n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Documentation\n                            The guide to elevating your AI model risk workflow\n                            Need help? Find all the information you need to use our platform for model risk management (AI model risk).\n                            \n                            \n                                \n                                    \n                                        \n                                            \n                                        \n                                        \n                                                                \n                                        \n                                    \n                                \n                            \n\n                        \n                    \n                \n                \n                \n            \n        \n    \n    \n        \n            \n                \n                    Overview\n                    Automating key aspects of the model risk management process, ValidMind is an AI model risk solution designed for the unique documentation and validation needs of model developers and validators.\n                    Model Documentation Automation\n                    AI Model Risk & Lifecycle Management\n                    Communication & TrackingInstructional GuidesGet Started\n                \n                \n                \n                    \n                        \n                            Model Developers\n                            Automate your model documentation and testing tasks with our Developer Framework.Get started\n                        \n                    \n                    \n                \n                \n                \n                    \n                        \n                            Model Validators\n                            Review and evaluate models and documentation to ensure they comply with organizational & regulatory requirements.Collaborate on projects\n                        \n                    \n                \n            \n        \n    \n    \n        \n            \n                \n                    Support & Training\n                    You can learn more about effective model risk management by requesting a demo with the ValidMind Platform.\n                    Need some help? Try our self-service documentation or email us at: support@validmind.comFrequently Asked QuestionsRequest A Demo"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "",
    "text": "This interactive notebook will guide you through documenting a model using the ValidMind Developer framework. We will use sample datasets provided by the library and train a simple classification model.\nFor this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\nWe will train a sample model and demonstrate the following documentation functionalities:"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#before-you-begin",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#before-you-begin",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Before you begin",
    "text": "Before you begin\nClick File &gt; Save a copy in Drive &gt; to make your own copy in Google Drive so that you can modify the notebook.\nAlternatively, you can download the notebook source and work with it in your own developer environment.\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#install-the-client-library",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#install-the-client-library",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind\n\nNote: Colab may generate the following warning after running the first cell:\nWARNING [...]\nYou must restart the runtime in order to use newly installed versions\nIf you see this, please click on “Restart runtime” and continue with the next cell."
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#initialize-the-client-library",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#initialize-the-client-library",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\nThis step requires a documentation project. Learn how you can create one.\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"API_SECRET\",\n  project = \"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n##Initializing the Python environment\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#load-the-demo-dataset",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#load-the-demo-dataset",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\nFor the purpose of this demonstration, we will use a sample dataset provided by the ValidMind library.\n\n# Import the sample dataset from the library\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# You can try a different dataset with:\n#from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nInitialize a dataset object for ValidMind\nBefore running the test plan, we must first initialize a ValidMind dataset object using the init_dataset function from the vm module. This function takes in arguements: dataset which is the dataset that we want to analyze; target_column which is used to identify the target variable; class_labels which is used to identify the labels used for classification model training.\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/Quickstart_Customer Churn_full_suite.html#run-the-full-data-and-model-validation-test-suite",
    "href": "notebooks/Quickstart_Customer Churn_full_suite.html#run-the-full-data-and-model-validation-test-suite",
    "title": "Quickstart - Customer Churn Full Suite Model Documentation",
    "section": "Run the Full Data and Model Validation Test Suite",
    "text": "Run the Full Data and Model Validation Test Suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the Raw Dataset\nFor demonstration purposes, we simplified the preprocessing using demo_dataset.preprocess which executes the following operations:\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\nWe can now initialize the training and test datasets into dataset objects using vm.init_dataset():\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nWe also initialize a model object using vm.init_model():\n\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the Full Suite\nWe are now ready to run the test suite for binary classifier with tabular datasets. This function will run test plans on the dataset and model objects, and will document the results in the ValidMind UI.\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\n\nvm.test_plans.list_tests()\n\n\nvm.test_plans.describe_plan(\"tabular_data_quality\")\n\n\nfull_suite = vm.run_test_suite(\n    \"binary_classifier_full_suite\",\n    dataset=vm_dataset,\n    model=vm_model\n)\n\nYou can access and review the resulting documentation in the ValidMind UI, in the “Model Development” section of the model documentation."
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html",
    "title": "Credit Risk Scorecard Demo",
    "section": "",
    "text": "Import Libraries\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy.stats import chi2_contingency\n%matplotlib inline\n\nConnect to ValidMind Project\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n  project = \"clk00h0u800x9qjy67gduf5om\"\n)\n  \n  \n\n2023-07-13 14:58:07,771 - INFO(validmind.api_client): Connected to ValidMind. Project: [6] Credit Risk Scorecard - Initial Validation (clk00h0u800x9qjy67gduf5om)\n\n\nList of Tests used in this Demo"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#introduction",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#introduction",
    "title": "Credit Risk Scorecard Demo",
    "section": "",
    "text": "Import Libraries\n\n# Load API key and secret from environment variables\n%load_ext dotenv\n%dotenv .env\n\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy.stats import chi2_contingency\n%matplotlib inline\n\nConnect to ValidMind Project\n\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n  project = \"clk00h0u800x9qjy67gduf5om\"\n)\n  \n  \n\n2023-07-13 14:58:07,771 - INFO(validmind.api_client): Connected to ValidMind. Project: [6] Credit Risk Scorecard - Initial Validation (clk00h0u800x9qjy67gduf5om)\n\n\nList of Tests used in this Demo"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#data-description",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#data-description",
    "title": "Credit Risk Scorecard Demo",
    "section": "Data Description",
    "text": "Data Description\n\nImport Dataset\n\nfilepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\ndf = pd.read_csv(filepath)\n\n/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_26246/861735461.py:2: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(filepath)\n\n\n\ndef get_numerical_columns(df):\n        numerical_columns = df.select_dtypes(\n            include=[\"int\", \"float\", \"uint\"]\n        ).columns.tolist()\n        return numerical_columns\n\ndef get_categorical_columns(df):\n        categorical_columns = df.select_dtypes(\n            include=[\"object\", \"category\"]\n        ).columns.tolist()\n        return categorical_columns\n\n\n\nDescribe Dataset\n\n# from validmind.vm_models.test_context import TestContext\n #from validmind.tests.data_validation.DatasetDescription import DatasetDescription\n\n# vm_df = vm.init_dataset(dataset=df)\n# test_context = TestContext(dataset=vm_df)\n\n# metric = DatasetDescription(test_context)\n# metric.run()\n# metric.result.show()\n\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.tests.data_validation.DescriptiveStatistics import DescriptiveStatistics\n\nvm_df = vm.init_dataset(dataset=df)\ntest_context = TestContext(dataset=vm_df)\n\nmetric = DescriptiveStatistics(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 14:58:10,093 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 14:58:10,094 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n# from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n\n# metric = TabularDescriptionTables(test_context)\n# metric.run()\n# metric.result.show()\n\n\n\nIdentify Missing Values\n\nfrom validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n\nvm_df = vm.init_dataset(dataset=df)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"threshold\": 70,\n          \"fig_height\": 1100}\n\nmetric = MissingValuesBarPlot(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 14:58:16,676 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 14:58:16,677 - INFO(validmind.vm_models.dataset): Inferring dataset types..."
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#data-preparation",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#data-preparation",
    "title": "Credit Risk Scorecard Demo",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nIdentify Target Variable\nDefinition of Default\nWe categorizing Fully Paid loans as “default = 0” and Charged Off loans as “default = 1”. This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk).\nLoans with Current status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis.\nAdd default Variable\n\ndef add_target_column(df, target_column):\n    # Assuming the column name is 'loan_status'\n    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n    # Remove rows where the target column is NaN\n    df = df.dropna(subset=[target_column])\n    # Convert target column to integer\n    df[target_column] = df[target_column].astype(int)\n    return df\n\ntarget_column = 'default'\ndf = add_target_column(df, target_column)\n\n# Drop 'loan_status' variable \ndf.drop(columns='loan_status', axis=1, inplace=True)\n\n/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_26246/2473939582.py:7: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_26246/2473939582.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\nClass Imbalance\nClass imbalance is a common issue in credit risk scorecards and datasets like the Lending Club’s. This imbalance arises when the number of defaulting loans (negative class) is significantly smaller than the number of loans that are paid off (positive class). Such imbalance can lead to biased models that favor the majority class, thus affecting predictive performance.\nSpecial techniques like oversampling, undersampling, or cost-sensitive learning are often needed to ensure that the minority class is appropriately represented during model training.\n\nfrom validmind.tests.data_validation.ClassImbalance import ClassImbalance\n\nvm_df = vm.init_dataset(dataset=df,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nmetric = ClassImbalance(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 14:58:24,605 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 14:58:24,605 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n\nRemove Unused Variables\n\nunused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n                    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n                    'earliest_cr_line', 'issue_d']\n\ndf = df.drop(columns=unused_variables)\n\n\n\nRemove Variables with Large Number of Missing Values\n\ndef variables_with_min_missing(df, min_missing_percentage):\n    # Calculate the percentage of missing values in each column\n    missing_percentages = df.isnull().mean() * 100\n\n    # Get the variables where the percentage of missing values is greater than the specified minimum\n    variables_to_drop = missing_percentages[missing_percentages &gt; min_missing_percentage].index.tolist()\n\n    # Also add any columns where all values are missing\n    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n\n    # Remove duplicates (if any)\n    variables_to_drop = list(set(variables_to_drop))\n\n    return variables_to_drop\n\nmin_missing_count = 80\nvariables_to_drop = variables_with_min_missing(df, min_missing_count)\ndf.drop(columns=variables_to_drop, axis=1, inplace=True)\n\ndf.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\ndf.dropna(axis=0, subset=[\"revol_util\"], inplace=True)\n\n\n\nFormat Type of Variables\n\nfrom typing import List\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport re\n\ndef clean_term_column(df, column):\n    \"\"\"\n    Function to remove 'months' string from the 'term' column and convert it to categorical\n    \"\"\"\n    # Ensure the column exists in the dataframe\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n    \n    df[column] = df[column].str.replace(' months', '')\n    \n    # Convert to categorical\n    df[column] = df[column].astype('object')\n\ndef clean_emp_length_column(df, column):\n    \"\"\"\n    Function to clean 'emp_length' column and convert it to categorical.\n    \"\"\"\n    # Ensure the column exists in the dataframe\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n    \n    df[column] = df[column].replace('n/a', np.nan)\n    df[column] = df[column].str.replace('&lt; 1 year', str(0))\n    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n    df[column].fillna(value = 0, inplace=True)\n\n    # Convert to categorical\n    df[column] = df[column].astype('object')\n\ndef clean_inq_last_6mths(df, column):\n    \"\"\"\n    Function to convert 'inq_last_6mths' column into categorical.\n    \"\"\"\n    # Ensure the column exists in the dataframe\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n\n    # Convert to categorical\n    df[column] = df[column].astype('category')\n\nclean_emp_length_column(df, 'emp_length')\nclean_term_column(df, 'term')\nclean_inq_last_6mths(df, 'inq_last_6mths')\n\n\n\nHandle Outliers\nIdentify Outliers\n\nfrom validmind.tests.data_validation.IQROutliersPlots import IQROutliersPlots\n\nvm_df = vm.init_dataset(dataset=df)\ntest_context = TestContext(dataset=vm_df)\n\nnum_features = get_numerical_columns(df)\nparams = {\"num_features\": num_features,\n          \"threshold\": 1.5,\n          \"fig_width\": 500}\n\nmetric = IQROutliersPlots(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 14:58:27,358 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 14:58:27,358 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\nRemove Outliers using IQR Method\n\ndef compute_outliers(series, threshold=1.5):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - threshold * IQR\n    upper_bound = Q3 + threshold * IQR\n    return series[(series &lt; lower_bound) | (series &gt; upper_bound)]\n\ndef remove_iqr_outliers(df, target_column, threshold=1.5):\n    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n    for col in num_cols:\n        outliers = compute_outliers(df[col], threshold)\n        df = df[~df[col].isin(outliers)]\n    return df\n\ndf = remove_iqr_outliers(df, target_column, threshold=1.5)"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#data-sampling",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#data-sampling",
    "title": "Credit Risk Scorecard Demo",
    "section": "Data Sampling",
    "text": "Data Sampling\n\nSampling Method\nWe employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the stratify = y parameter is set, it ensures that the distribution of the target variable (y) in the test set is the same as that in the original dataset.\nThis is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards.\n\n# Split data into train and test \nX = df.drop(target_column, axis = 1)\ny = df[target_column]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n                                                    random_state = 42, stratify = y)\n\n# Concatenate X_train with y_train to form df_train\ndf_train = pd.concat([X_train, y_train], axis=1)\n\n# Concatenate X_test with y_test to form df_test\ndf_test = pd.concat([X_test, y_test], axis=1)"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#univariate-analysis",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#univariate-analysis",
    "title": "Credit Risk Scorecard Demo",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\n\nHistograms of Numerical Features\n\nfrom validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n\nvm_df_train = vm.init_dataset(dataset=df_train)\ntest_context = TestContext(dataset=vm_df_train)\n\nmetric = TabularNumericalHistograms(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 15:00:01,001 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:00:01,001 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n\nHigh Cardinality of Categorical Features\n\nfrom validmind.tests.data_validation.HighCardinality import HighCardinality\nmetric = HighCardinality(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\nBar Plots of Categorical Features\n\nfrom validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\nmetric = TabularCategoricalBarPlots(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\nDefault Rate by Categorical Feature\n\nfrom validmind.tests.data_validation.TargetRateBarPlots import TargetRateBarPlots\n\n# Configure the metric\nparams = {\n    \"default_column\": target_column,\n    \"columns\": None\n}\n\nmetric = TargetRateBarPlots(test_context, params=params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\nThe column default is correct and contains only 1 and 0.\n\n\n\n\n\n\n\nChi-Squared Test on Categorical Features\n\nfrom validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\ncat_features = get_categorical_columns(df_train)\nparams = {\"cat_features\": cat_features,\n          \"p_threshold\": 0.05}\n\nmetric = ChiSquaredFeaturesTable(test_context, params)\nmetric.run()\n# await metric.result.log() - throws error for this metric\nmetric.result.show()\n\n2023-07-13 15:01:20,408 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:20,408 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n\nANOVA Test on Numerical Features\n\nfrom validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nnum_features = get_numerical_columns(df_train)\nparams = {\"num_features\": num_features,\n          \"p_threshold\": 0.05}\n\nmetric = ANOVAOneWayTable(test_context, params)\nmetric.run()\n# await metric.result.log() - throws error for this metric\nmetric.result.show()\n\n2023-07-13 15:01:21,074 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:21,074 - INFO(validmind.vm_models.dataset): Inferring dataset types..."
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#multivariate-analysis",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#multivariate-analysis",
    "title": "Credit Risk Scorecard Demo",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\n\nHeatmap Correlation of Numerical Features\n\nfrom validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"declutter\": False,\n          \"features\": None,\n          \"fontsize\": 13}\n\nmetric = PearsonCorrelationMatrix(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 15:01:21,799 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:21,799 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n\nCorrelations of Numerical Features with Target Variable\n\nfrom validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n\nvm_df = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\ntest_context = TestContext(dataset=vm_df)\n\nparams = {\"features\": None,\n          \"fig_height\": 600}\n\nmetric = FeatureTargetCorrelationPlot(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 15:01:24,453 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:24,453 - INFO(validmind.vm_models.dataset): Inferring dataset types..."
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#feature-selection",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#feature-selection",
    "title": "Credit Risk Scorecard Demo",
    "section": "Feature Selection",
    "text": "Feature Selection\n\ndrop_categorical_features = ['addr_state']\ndrop_numerical_features = ['total_rec_int', 'loan_amnt',\n                           'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n                           'total_pymnt_inv', 'last_pymnt_amnt']\n\ndf_train.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)\n\n# Update df_test \ndf_test.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#feature-engineering",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#feature-engineering",
    "title": "Credit Risk Scorecard Demo",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nClass Binning\n\nimport pandas as pd\nimport numpy as np\n\ndef encode_numerical_features(df):\n    \n    # term\n    df['term'] = df['term'].replace({' 36': '36M', ' 60': '60M'})\n\n    # emp_length_int\n    df['emp_length'] = df['emp_length'].replace('10+', '10')  # Replace '10+' with '10'\n    df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')  # Convert to numeric\n    df['emp_length'].fillna(-1, inplace=True)\n    bins = [0,1,2,3,5,8,10,999]\n    df['emp_length_bucket'] = pd.cut(df['emp_length'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='emp_length', inplace=True)\n\n    # inq_last_6mths\n    df['inq_last_6mths'].fillna(-1, inplace=True)\n    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n    df['inq_last_6mths_bucket'] = pd.cut(df['inq_last_6mths'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='inq_last_6mths', inplace=True)\n    \n    # total_acc\n    df['total_acc'].fillna(-1, inplace=True)\n    bins = [-1, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 999]\n    df['total_acc_bucket'] = pd.cut(df['total_acc'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='total_acc', inplace=True)\n\n    # annual_inc\n    df['annual_inc'].fillna(-1, inplace=True)\n    df['annual_inc_1000'] = df['annual_inc']/1000\n    bins = [-1, 0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 1000, 10000]\n    df['annual_inc_bucket'] = pd.cut(df['annual_inc_1000'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='annual_inc', inplace=True)\n    df.drop(columns='annual_inc_1000', inplace=True)\n    \n    # int_rate\n    df['int_rate'].fillna(-1, inplace=True)\n    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n    df['int_rate_bucket'] = pd.cut(df['int_rate'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='int_rate', inplace=True)\n\n    # installment\n    df['installment'].fillna(-1, inplace=True)\n    bins = [-1, 0, 100, 200, 300, 400, 500, 750, 1000, 1500]\n    df['installment_bucket'] = pd.cut(df['installment'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='installment', inplace=True)\n\n    # open_acc\n    df['open_acc'].replace(\"N/A\", 1, inplace=True)\n    df['open_acc'].fillna(-1, inplace=True)\n    bins = [-1, 0, 1, 2, 3, 4, 5, 8, 10, 100]\n    df['open_acc_bucket'] = pd.cut(df['open_acc'], bins=bins, right=False, include_lowest=True)\n    df.drop(columns='open_acc', inplace=True)\n\ndef find_categorical_features(df):\n    # Get the column names of features with the data type \"category\"\n    categorical_features = df.select_dtypes(include='category').columns.tolist()\n\n    return categorical_features\n\n\ndef convert_categorical_to_object(df):\n    # Find the categorical features\n    categorical_features = find_categorical_features(df)\n\n    # Convert the categorical features to object type\n    df[categorical_features] = df[categorical_features].astype(str)\n\n\nencode_numerical_features(df_train)\nconvert_categorical_to_object(df_train)\n\n# Update df_test\nencode_numerical_features(df_test)\nconvert_categorical_to_object(df_test)\n\n\n\nWeight of Evidence (WoE) and Infomation Value (IV)\n\nfrom validmind.tests.data_validation.WOEIVTable import WOEIVTable\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\n# Configure test parameters\n\nparams = {\n    \"features\": None,\n    \"order_by\": [\"Feature\", \"WoE\"]\n}\n\n# Run test\nmetric = WOEIVTable(test_context, params=params)\nmetric.run()\n# await metric.result.log() - throws error for this metric\nwoe_iv_df = metric.result.metric.value['woe_iv']\nmetric.result.show()\n\n2023-07-13 15:01:26,850 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:26,850 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n\nClass Aggregation\n\nimport pandas as pd\n\ndef coarse_classing(df, mappings):\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_new = df.copy()\n\n    # Loop through each feature and merge set\n    for feature, merge_sets in mappings.items():\n        for merge_set in merge_sets:\n            # Merge the specified categories into a new category\n            df_new[feature] = df_new[feature].apply(lambda x: f\"[{','.join(merge_set)}]\" if x in merge_set else x)\n\n    return df_new\n\ndef shorten_category_names(df, max_length=20, suffix=\"...\"):\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_new = df.copy()\n    \n    # Iterate over each column in the DataFrame\n    for feature in df_new.columns:\n        # Check if the column has the \"object\" data type\n        if df_new[feature].dtype.name == 'object':\n            # Shorten long category names\n            df_new[feature] = df_new[feature].apply(lambda x: x[:max_length] + suffix if len(x) &gt; max_length else x)\n    \n    return df_new\n\n# Create a dictionary of features and the sets to merge\nmappings = {\n    'sub_grade': [['B2','B3','B4','B5','C3','D1'], ['C1','C2','C4','C5'], ['D3','D4','D5','E3','G4'], ['E1','E2','E4','E5','F1','F2','F3','F4','G1','G2','G3','G5','F5']],\n    'grade': [['F','G']],\n    'purpose': [['wedding','major_purchase'], ['credit_card','car'], ['debt_consolidation','other','vacation'], ['medical','moving','house','educational'], ['renewable_energy','small_business']],\n    'home_ownership': [['MORTGAGE','OWN','RENT']],\n    'annual_inc_bucket': [['[250, 1000)','[100, 150)','[150, 250)','[1000, 10000)'], ['[50, 75)','[40, 50)'], ['[10, 20)','[0, 10)']],\n    'emp_length_bucket': [['[2, 3)','[40, 50)','[3, 5)','[1, 2)','[0, 1)','[5, 8)','[8, 10)']],\n    'inq_last_6mths_bucket': [['[4, 5)','[1, 2)'], ['[5, 10)','[3, 4)']],\n    'installment_bucket': [['[300, 400)','[200, 300)','[0, 100)'], ['[400, 500)', '[500, 750)']],\n    'total_acc_bucket': [['[20, 25)','[30, 35)','[15, 20)','[45, 50)','[40, 45)','[35, 40)','[10, 15)','[5, 10)']],\n    'open_acc_bucket': [['[5, 8)','[8, 10)','[10, 100)','[4, 5)'], ['[1, 2)','[2, 3)']]\n}\n\ndf_train = coarse_classing(df_train, mappings)\ndf_train = df_train[~df_train['home_ownership'].isin(['OTHER', 'NONE'])]\ndf_train.drop(columns=\"home_ownership\", inplace=True)\ndf_train = shorten_category_names(df_train, max_length=15, suffix=\"...\")\n\n# Update df_test\ndf_test = coarse_classing(df_test, mappings)\ndf_test = df_test[~df_test['home_ownership'].isin(['OTHER', 'NONE'])]\ndf_test.drop(columns=\"home_ownership\", inplace=True)\ndf_test = shorten_category_names(df_test, max_length=15, suffix=\"...\")\n\n\nfrom validmind.tests.data_validation.WOEIVPlots import WOEIVPlots\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\nparams = {\n    \"features\": None,\n    \"fig_height\": 500,\n    \"fig_height\": 500,\n}\n\n# Run test\nmetric = WOEIVPlots(test_context, params=params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n2023-07-13 15:01:33,980 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:33,981 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\n\n\n\nEncode Features with WoE\n\n# Update vm dataset and test context\nvm_df_train = vm.init_dataset(dataset=df_train, \n                              target_column=target_column)\ntest_context = TestContext(dataset=vm_df_train)\n\n# Configure test parameters\nparams = {\n    \"features\": None,\n    \"order_by\": [\"Feature\", \"WoE\"]\n}\n\n# Run test\nmetric = WOEIVTable(test_context, params=params)\nmetric.run()\n# await metric.result.log() - throws error for this metric\nwoe_iv_df = metric.result.metric.value['woe_iv']\n\n2023-07-13 15:01:58,209 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:01:58,210 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\ndef check_categories(woe_df, original_df):\n    for feature in woe_df['Feature'].unique():\n        woe_categories = woe_df[woe_df['Feature'] == feature]['Category'].unique()\n        original_categories = original_df[feature].unique()\n        \n        # Check categories in WoE table that are not in original DataFrame\n        for category in woe_categories:\n            if category not in original_categories:\n                print(f\"Category '{category}' not found in feature '{feature}' in original DataFrame.\")\n                \n        # Check categories in original DataFrame that are not in WoE table\n        for category in original_categories:\n            if category not in woe_categories:\n                print(f\"Category '{category}' in feature '{feature}' not found in WoE table.\")\n\n                \ncheck_categories(woe_iv_df, df_train)\n\n\nimport pandas as pd\nimport numpy as np\n\ndef woe_encoder(woe_df, original_df, target):\n    # Create a new DataFrame with the same columns as original_df\n    woe_encoded_df = pd.DataFrame(columns=original_df.columns, index=original_df.index)\n\n    # Loop through each feature-category and assign the corresponding WoE value as float\n    for feature in woe_df['Feature'].unique():\n        # Check that the feature exists in the original DataFrame\n        if feature not in original_df.columns:\n            print(f\"Feature {feature} not found in original DataFrame. Skipping...\")\n            continue\n\n        feature_woe = woe_df[woe_df['Feature'] == feature]\n        woe_dict = dict(zip(feature_woe['Category'], feature_woe['WoE']))\n\n        # Check that the categories exist in the original DataFrame\n        # Converting both to strings to avoid datatype issues\n        original_categories = original_df[feature].astype(str).unique()\n        woe_categories = feature_woe['Category'].astype(str).unique()\n        \n        # Two-way check:\n        # 1. For each category in the original DataFrame, check if it exists in the WoE DataFrame\n        missing_from_woe = [category for category in original_categories if category not in woe_categories]\n        if missing_from_woe:\n            print(f\"Categories {missing_from_woe} from original DataFrame not found in WoE DataFrame for feature {feature}.\")\n            \n        # 2. For each category in the WoE DataFrame, check if it exists in the original DataFrame\n        missing_from_original = [category for category in woe_categories if category not in original_categories]\n        if missing_from_original:\n            print(f\"Categories {missing_from_original} from WoE DataFrame not found in original DataFrame for feature {feature}.\")\n        \n        # Also converting original dataframe feature to string before replacement\n        woe_encoded_df[feature] = original_df[feature].astype(str).replace(woe_dict).astype(float)\n\n    # Check that the target exists in the original DataFrame\n    if target not in original_df.columns:\n        print(f\"Target {target} not found in original DataFrame. Returning None...\")\n        return None\n\n    # Add the target column to the new DataFrame\n    woe_encoded_df[target] = original_df[target]\n\n    return woe_encoded_df\n\n\ndf_train = woe_encoder(woe_iv_df, df_train, target='default')\n\n# Update df_test\ndf_test = woe_encoder(woe_iv_df, df_test, target='default')"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#model-training",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#model-training",
    "title": "Credit Risk Scorecard Demo",
    "section": "Model Training",
    "text": "Model Training\n\nFit GLM Logistic Regression Model\n\nimport statsmodels.api as sm\n\n# Create X_train, y_train and X_test, y_test\ndf_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\ny_train = df_train[target_column]\nX_train = df_train.drop(target_column, axis=1)\n\n# Add constant to X_train for intercept term\nX_train = sm.add_constant(X_train)\ndf_train = pd.concat([X_train, y_train], axis=1)\n\n# Update df_test\ny_test = df_test[target_column]\nX_test = df_test.drop(target_column, axis=1)\nX_test = sm.add_constant(X_test)\ndf_test = pd.concat([X_test, y_test], axis=1)\ndf_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n\n# Define the model\nmodel = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n\n# Fit the model\nmodel_fit_glm = model.fit()\n\n# Print out the statistics\nprint(model_fit_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                default   No. Observations:               137441\nModel:                            GLM   Df Residuals:                   137428\nModel Family:                Binomial   Df Model:                           12\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -59834.\nDate:                Thu, 13 Jul 2023   Deviance:                   1.1967e+05\nTime:                        15:02:04   Pearson chi2:                 1.38e+05\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.07997\nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                    -1.4910      0.007   -199.474      0.000      -1.506      -1.476\nterm                     -0.6012      0.017    -35.602      0.000      -0.634      -0.568\ngrade                    -0.5400      0.029    -18.866      0.000      -0.596      -0.484\nsub_grade                -0.1907      0.031     -6.200      0.000      -0.251      -0.130\nverification_status      -0.2329      0.036     -6.557      0.000      -0.302      -0.163\npurpose                  -0.4942      0.049    -10.150      0.000      -0.590      -0.399\nemp_length_bucket        -2.0902      2.683     -0.779      0.436      -7.349       3.169\ninq_last_6mths_bucket    -0.4292      0.050     -8.595      0.000      -0.527      -0.331\ntotal_acc_bucket         -0.3940      0.222     -1.777      0.076      -0.829       0.040\nannual_inc_bucket        -1.2091      0.037    -33.107      0.000      -1.281      -1.138\nint_rate_bucket          -0.1150      0.024     -4.721      0.000      -0.163      -0.067\ninstallment_bucket       -1.3242      0.047    -28.064      0.000      -1.417      -1.232\nopen_acc_bucket          -0.9450      0.736     -1.285      0.199      -2.387       0.497\n=========================================================================================\n\n\n\n\nRemove Non-Significant Features\n\nfeatures_to_drop = ['emp_length_bucket', 'total_acc_bucket', 'open_acc_bucket']\ndf_train.drop(columns = features_to_drop, inplace=True)\n\n# Update df_test \ndf_test.drop(columns = features_to_drop, inplace=True)\n\n\n\nUpdate GLM Model Fit\n\n# Create X_train and y_train\ny_train = df_train[target_column]\nX_train = df_train.drop(target_column, axis=1)\n\n# Update df_test\ny_test = df_test[target_column]\nX_test = df_test.drop(target_column, axis=1)\n\n# Define the model\nmodel = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n\n# Fit the model\nmodel_fit_glm = model.fit()\n\n# Print out the statistics\nprint(model_fit_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                default   No. Observations:               137441\nModel:                            GLM   Df Residuals:                   137431\nModel Family:                Binomial   Df Model:                            9\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -59837.\nDate:                Thu, 13 Jul 2023   Deviance:                   1.1967e+05\nTime:                        15:02:04   Pearson chi2:                 1.38e+05\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.07993\nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                    -1.4910      0.007   -199.480      0.000      -1.506      -1.476\nterm                     -0.6005      0.017    -35.614      0.000      -0.634      -0.567\ngrade                    -0.5406      0.029    -18.888      0.000      -0.597      -0.485\nsub_grade                -0.1917      0.031     -6.234      0.000      -0.252      -0.131\nverification_status      -0.2325      0.035     -6.551      0.000      -0.302      -0.163\npurpose                  -0.4939      0.049    -10.142      0.000      -0.589      -0.398\ninq_last_6mths_bucket    -0.4254      0.050     -8.525      0.000      -0.523      -0.328\nannual_inc_bucket        -1.2106      0.036    -33.387      0.000      -1.282      -1.139\nint_rate_bucket          -0.1143      0.024     -4.693      0.000      -0.162      -0.067\ninstallment_bucket       -1.3212      0.047    -28.045      0.000      -1.414      -1.229\n=========================================================================================\n\n\n\n\nCreate ValidMind Model\n\n# Create VM dataset\nvm_train_ds = vm.init_dataset(dataset=df_train,\n                        target_column=target_column)\nvm_test_ds = vm.init_dataset(dataset=df_test,\n                        target_column=target_column)\n\n# Create VM model\nvm_model_glm = vm.init_model(\n    model = model_fit_glm, \n    train_ds=vm_train_ds, \n    test_ds=vm_test_ds)\n\n2023-07-13 15:02:04,511 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:02:04,513 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n2023-07-13 15:02:05,282 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:02:05,282 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\n\nModel Coefficients\n\nfrom validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n\ntest_context = TestContext(models= [vm_model_glm])\nmetric = RegressionModelsCoeffs(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\nfrom validmind.tests.model_validation.statsmodels.RegressionCoeffsPlot import RegressionCoeffsPlot\n\ntest_context = TestContext(models= [vm_model_glm])\nmetric = RegressionCoeffsPlot(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()"
  },
  {
    "objectID": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#model-evaluation",
    "href": "notebooks/probability_of_default/credit_risk_scorecard_demo.html#model-evaluation",
    "title": "Credit Risk Scorecard Demo",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nScorecard Development\n\nConfusion Matrix\n\nfrom validmind.tests.model_validation.statsmodels.LogRegressionConfusionMatrix import LogRegressionConfusionMatrix\n\ntest_context = TestContext(model= vm_model_glm)\n\n# Configure test parameters\nparams = {\n    \"cut_off_threshold\": 0.5,\n}\n\nmetric = LogRegressionConfusionMatrix(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\nROC Curve\n\nfrom validmind.tests.model_validation.statsmodels.RegressionROCCurve import RegressionROCCurve\n\ntest_context = TestContext(model= vm_model_glm)\nmetric = RegressionROCCurve(test_context)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\nGINI and Kolmogorov-Smirnov (KS)\n\nfrom validmind.tests.model_validation.statsmodels.GINITable import GINITable\n\ntest_context = TestContext(model= vm_model_glm)\nmetric = GINITable(test_context)\nmetric.run()\n# await metric.result.log() - throws error for this metric\nmetric.result.show()\n\n\n\n\n\n\nProbability of Default\n\nfrom validmind.tests.model_validation.statsmodels.LogisticRegPredictionHistogram import LogisticRegPredictionHistogram\n\n# Configure test parameters\nparams = {\n    \"title\": \"Histogram of Probability of Default\",\n}\ntest_context = TestContext(model= vm_model_glm)\nmetric = LogisticRegPredictionHistogram(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\nfrom validmind.tests.model_validation.statsmodels.LogisticRegCumulativeProb import LogisticRegCumulativeProb\n\n# Configure test parameters\nparams = {\n    \"title\": \"Cumulative Probability of Default\",\n}\ntest_context = TestContext(model= vm_model_glm)\nmetric = LogisticRegCumulativeProb(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\nHistogram of Credit Scores\n\nfrom validmind.tests.model_validation.statsmodels.ScorecardHistogram import ScorecardHistogram\n\n# Configure test parameters\nparams = {\n    \"target_score\": 600,\n    \"target_odds\": 50,\n    \"pdo\": 20,\n    \"title\": \"Histogram of Credit Scores\",\n}\ntest_context = TestContext(model= vm_model_glm)\nmetric = ScorecardHistogram(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\n\nScorecard Calibration\nPoint-in-Time (PiT) calibration is a method used to adjust the predicted probabilities of a model so that they align with the actual observed default rates. If the observed default rate at the time of calibration is the same as it was when the scorecard was built, then the calibration process would not lead to any changes in the predicted probabilities, and hence, no improvement in the model’s performance as measured by the confusion matrix or other metrics.\nThe purpose of calibration is to ensure that the model’s predicted probabilities accurately reflect the actual likelihood of the event (in this case, default). If the underlying default rate hasn’t changed since the model was built, then the model’s predictions are already well-calibrated, and further calibration isn’t necessary.\n\ndef create_scorecard(model, X, target_score, target_odds, pdo):\n    # Compute probability of default\n    X_copy = X.copy()\n    X_copy[\"probabilities\"] = model.predict(X_copy)\n\n    # Compute scores\n    factor = pdo / np.log(2)\n    offset = target_score - (factor * np.log(target_odds))\n\n    X_copy[\"score\"] = offset + factor * np.log(\n        X_copy[\"probabilities\"] / (1 - X_copy[\"probabilities\"])\n    )\n    return X_copy\n\ntarget_score = 600\ntarget_odds = 50\npdo = 20\nX_train_scores = create_scorecard(model_fit_glm, X_train, target_score, target_odds, pdo)\nX_train_scores = X_train_scores['score']\n\n\n# Define the model\nX_train_scores = sm.add_constant(X_train_scores)\ncalibrated_model = sm.GLM(y_train, X_train_scores, family=sm.families.Binomial())\n\n# Fit the model\ncalibrated_model_fit_glm = calibrated_model.fit()\n\n# Print out the statistics\nprint(calibrated_model_fit_glm.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                default   No. Observations:               137441\nModel:                            GLM   Df Residuals:                   137439\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -59837.\nDate:                Thu, 13 Jul 2023   Deviance:                   1.1967e+05\nTime:                        15:02:58   Pearson chi2:                 1.38e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):            0.07993\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -16.8824      0.155   -108.778      0.000     -17.187     -16.578\nscore          0.0347      0.000    100.269      0.000       0.034       0.035\n==============================================================================\n\n\n\n# Create df_calibrated_train and df_calibrated_test\ndf_calibrated_train = pd.concat([X_train_scores, y_train], axis=1)\n\n# Update test data \nX_test_scores = create_scorecard(model_fit_glm, X_test, target_score, target_odds, pdo)\nX_test_scores = X_test_scores['score']\nX_test_scores = sm.add_constant(X_test_scores)\ndf_calibrated_test = pd.concat([X_test_scores, y_test], axis=1)\n\nCreate VM Datasets and Model\n\n# Create VM dataset\nvm_train_calibrated_ds = vm.init_dataset(dataset=df_calibrated_train,\n                        target_column=target_column)\nvm_test_calibrated_ds = vm.init_dataset(dataset=df_calibrated_test,\n                        target_column=target_column)\n\n# Create VM model\nvm_model_glm_calibrated = vm.init_model(\n    model = calibrated_model_fit_glm, \n    train_ds=vm_train_calibrated_ds, \n    test_ds=vm_test_calibrated_ds)\n\n2023-07-13 15:02:58,116 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:02:58,119 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n2023-07-13 15:02:58,325 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-13 15:02:58,326 - INFO(validmind.vm_models.dataset): Inferring dataset types...\n\n\n\nConfusion Matrix\n\nfrom validmind.tests.model_validation.statsmodels.LogRegressionConfusionMatrix import LogRegressionConfusionMatrix\n\ntest_context = TestContext(model= vm_model_glm_calibrated)\n\n# Configure test parameters\nparams = {\n    \"cut_off_threshold\": 0.5,\n}\n\nmetric = LogRegressionConfusionMatrix(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()\n\n\n\n\n\n\n\nProbability of Default by Rating Class\n\nfrom validmind.tests.model_validation.statsmodels.PDRatingClassPlot import PDRatingClassPlot\n\n# Configure test parameters\nparams = {\n    \"rating_classes\": ['A','B','C','D'],\n    \"title\": \"PD by Rating Class\",\n}\ntest_context = TestContext(model= vm_model_glm_calibrated)\nmetric = PDRatingClassPlot(test_context, params)\nmetric.run()\nawait metric.result.log()\nmetric.result.show()"
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html",
    "href": "notebooks/how_to/run_a_template.html",
    "title": "Run a Documentation Template",
    "section": "",
    "text": "This notebook shows how to run all tests defined in a documentation template."
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#before-you-begin",
    "href": "notebooks/how_to/run_a_template.html#before-you-begin",
    "title": "Run a Documentation Template",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#install-the-client-library",
    "href": "notebooks/how_to/run_a_template.html#install-the-client-library",
    "title": "Run a Documentation Template",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_a_template.html#initialize-the-client-library",
    "title": "Run a Documentation Template",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    api_secret=\"API_SECRET\",\n    project=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#structure-of-a-documentation-template",
    "href": "notebooks/how_to/run_a_template.html#structure-of-a-documentation-template",
    "title": "Run a Documentation Template",
    "section": "Structure of a documentation template",
    "text": "Structure of a documentation template\nAll projects created in ValidMind are based on a documentation template. A documentation template is a collection of content blocks that, when rendered, produce a document that model developers can use for model validation.\nThe template structure is a simple combination of content sections where each section can have one or more content blocks. We currently support text-based content blocks (that are populated with the ValidMind UI) and test-driven content blocks. Under the hood, a template is represented as a YAML file. For more information about a template’s structure, please refer to this page."
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#previewing-a-template",
    "href": "notebooks/how_to/run_a_template.html#previewing-a-template",
    "title": "Run a Documentation Template",
    "section": "Previewing a template",
    "text": "Previewing a template\nWe can use the preview_template() function to preview the content blocks that will be populated by the developer framework.\n\nvm.preview_template()"
  },
  {
    "objectID": "notebooks/how_to/run_a_template.html#running-a-template",
    "href": "notebooks/how_to/run_a_template.html#running-a-template",
    "title": "Run a Documentation Template",
    "section": "Running a template",
    "text": "Running a template\nWe can use the run_template() function to run all tests defined in a documentation template. Note that each test in the template will have specific context requirements. Each context object can be passed as a function argument. For example, if a test requires a model object, we can pass it as a function argument:\nvm.run_template(model=vm_model) # vm_model is a validmind Model instance\nLet’s run through our customer churn demo to illustrate how to run a template and pass a model and dataset context to the run_template() function.\n\n# Load a demo dataset and train a simple model\nimport xgboost as xgb\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)\n\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\nNow we can run this project’s documentation template by passing the required dataset and model context:\n\nmodel_suite = vm.run_template(model=vm_model, dataset=vm_dataset)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html",
    "href": "notebooks/how_to/run_a_test_suite.html",
    "title": "Running an Individual Test Suite",
    "section": "",
    "text": "This notebook shows how to run an individual test suite."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#before-you-begin",
    "href": "notebooks/how_to/run_a_test_suite.html#before-you-begin",
    "title": "Running an Individual Test Suite",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#install-the-client-library",
    "href": "notebooks/how_to/run_a_test_suite.html#install-the-client-library",
    "title": "Running an Individual Test Suite",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_a_test_suite.html#initialize-the-client-library",
    "title": "Running an Individual Test Suite",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n    api_key=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    api_secret=\"API_SECRET\",\n    project=\"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n\n%matplotlib inline\n\nimport xgboost as xgb"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test_suite.html#load-the-demo-dataset",
    "title": "Running an Individual Test Suite",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import taiwan_credit like this:\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nvm_dataset = vm.init_dataset(\n    dataset=df,\n    target_column=demo_dataset.target_column,\n    class_labels=demo_dataset.class_labels\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "href": "notebooks/how_to/run_a_test_suite.html#list-available-test-suites",
    "title": "Running an Individual Test Suite",
    "section": "List available test suites",
    "text": "List available test suites\n\nvm.test_suites.list_suites()"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-data-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the Data Validation Test suite",
    "text": "Run the Data Validation Test suite\n\ntabular_suite = vm.run_test_suite(\"tabular_dataset\", dataset=vm_dataset)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#run-the-model-validation-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Run the Model Validation Test suite",
    "text": "Run the Model Validation Test suite\nWe will need to preprocess the dataset and produce the training, test and validation splits first.\n\nPrepocess the Raw Dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)\n\n\n\nTrain a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "href": "notebooks/how_to/run_a_test_suite.html#import-and-run-the-individual-test-suite",
    "title": "Running an Individual Test Suite",
    "section": "Import and Run the Individual Test Suite",
    "text": "Import and Run the Individual Test Suite\n\nInitialize ValidMind objects\nWe initize the objects required to run test plans using the ValidMind framework\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nRun the Binary Classification Test Suite\n\nmodel_suite = vm.run_test_suite(\"binary_classifier_model_validation\", model=vm_model)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html",
    "href": "notebooks/how_to/run_a_test.html",
    "title": "Running an Individual Test",
    "section": "",
    "text": "This notebook shows how to run individual metrics or thresholds tests."
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#before-you-begin",
    "href": "notebooks/how_to/run_a_test.html#before-you-begin",
    "title": "Running an Individual Test",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#install-the-client-library",
    "href": "notebooks/how_to/run_a_test.html#install-the-client-library",
    "title": "Running an Individual Test",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#initialize-the-client-library",
    "href": "notebooks/how_to/run_a_test.html#initialize-the-client-library",
    "title": "Running an Individual Test",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n\n\n\n\n\n\n\n\nThis step requires a documentation project. Learn how you can create one.\n\n\n\nNext, replace this placeholder with your own code snippet:\n\n## Replace with code snippet from your documentation project ##\n\nimport validmind as vm\n\nvm.init(\n  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n  api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  api_secret = \"API_SECRET\",\n  project = \"xxxxxxxxxxxxxxxxxxxxxxxxx\"\n)\n\n\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "href": "notebooks/how_to/run_a_test.html#load-the-demo-dataset",
    "title": "Running an Individual Test",
    "section": "Load the Demo Dataset",
    "text": "Load the Demo Dataset\n\n# You can also import customer_churn like this:\nfrom validmind.datasets.classification import customer_churn as demo_dataset\n# from validmind.datasets.classification import taiwan_credit as demo_dataset\n\ndf = demo_dataset.load_data()\n\n\nPrepocess the Raw Dataset\n\ntrain_df, validation_df, test_df = demo_dataset.preprocess(df)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "href": "notebooks/how_to/run_a_test.html#train-a-model-for-testing",
    "title": "Running an Individual Test",
    "section": "Train a Model for Testing",
    "text": "Train a Model for Testing\nWe train a simple customer churn model for our test.\n\nx_train = train_df.drop(demo_dataset.target_column, axis=1)\ny_train = train_df[demo_dataset.target_column]\nx_val = validation_df.drop(demo_dataset.target_column, axis=1)\ny_val = validation_df[demo_dataset.target_column]\n\nmodel = xgb.XGBClassifier(early_stopping_rounds=10)\nmodel.set_params(\n    eval_metric=[\"error\", \"logloss\", \"auc\"],\n)\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_val, y_val)],\n    verbose=False,\n)"
  },
  {
    "objectID": "notebooks/how_to/run_a_test.html#import-and-run-the-individual-test",
    "href": "notebooks/how_to/run_a_test.html#import-and-run-the-individual-test",
    "title": "Running an Individual Test",
    "section": "Import and Run the Individual Test",
    "text": "Import and Run the Individual Test\n\nInitialize ValidMind objects\n\nvm_train_ds = vm.init_dataset(\n    dataset=train_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_test_ds = vm.init_dataset(\n    dataset=test_df,\n    type=\"generic\",\n    target_column=demo_dataset.target_column\n)\n\nvm_model = vm.init_model(\n    model,\n    train_ds=vm_train_ds,\n    test_ds=vm_test_ds,\n)\n\n\n\nImport the individual test\n\nfrom validmind.vm_models.test_context import TestContext\nfrom validmind.tests.model_validation.sklearn.TrainingTestDegradation import TrainingTestDegradation\n\n\n\nPass the required context and config parameters\n\ntest_context = TestContext(model=vm_model)\nws_diagnostic = TrainingTestDegradation(test_context)\n\n\n\nRun the test\n\nws_diagnostic.run()\n\n\n\nDisplay results\n\nws_diagnostic.result.show()"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "This notebook aims to provide an introduction to documenting an NLP model using the ValidMind Developer Framework. The use case presented is a sentiment analysis of tweets related to COVID-19 into “positive” and “negative”; the model is a binary text classification using the CatBoost library.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nInitializing the ValidMind Developer Framework\nUsing a sample datasets provided by the library to train a simple nlp classification model using CatBoost library\nRunning a test various tests to quickly generate document about the data and model"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#natural-language-processing-analysis-binary-classification-using-catboost",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#natural-language-processing-analysis-binary-classification-using-catboost",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "This notebook aims to provide an introduction to documenting an NLP model using the ValidMind Developer Framework. The use case presented is a sentiment analysis of tweets related to COVID-19 into “positive” and “negative”; the model is a binary text classification using the CatBoost library.\nWe will train a sample model and demonstrate the following documentation functionalities:\n\nInitializing the ValidMind Developer Framework\nUsing a sample datasets provided by the library to train a simple nlp classification model using CatBoost library\nRunning a test various tests to quickly generate document about the data and model"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#before-you-begin",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#before-you-begin",
    "title": "Sensitivity Analysis",
    "section": "Before you begin",
    "text": "Before you begin\nTo use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\nIf you don’t already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#install-the-client-library",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#install-the-client-library",
    "title": "Sensitivity Analysis",
    "section": "Install the client library",
    "text": "Install the client library\n\n%pip install --upgrade validmind"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#initialize-the-client-library",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#initialize-the-client-library",
    "title": "Sensitivity Analysis",
    "section": "Initialize the client library",
    "text": "Initialize the client library\nIn a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\nThis step requires a documentation project. Learn how you can create one.\nNext, replace this placeholder with your own code snippet:\n\n%load_ext dotenv\n%dotenv .env\nimport validmind as vm\n\nvm.init(\n  api_host = \"http://localhost:3000/api/v1/tracking\",\n  api_key = \"dd4abeb23264f4784e1932204a47965d\",\n  api_secret = \"1aba00ce6500a58b4605c59e42e0c5c83526080a648855b988f99a7827e4a06e\",\n  project = \"cliop8llc003x32rlklophmdl\"\n)\n\n/Users/anilsorathiya/Library/Caches/pypoetry/virtualenvs/validmind-pPj8dHa5-py3.9/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n2023-07-06 12:52:29,172 - INFO - api_client - Connected to ValidMind. Project: nlp model sensitivity analysis - Initial Validation (cliop8llc003x32rlklophmdl)"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#explorary-data-analysis-of-covid-tweets-data",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#explorary-data-analysis-of-covid-tweets-data",
    "title": "Sensitivity Analysis",
    "section": "1. Explorary Data Analysis of Covid tweets data",
    "text": "1. Explorary Data Analysis of Covid tweets data\nThe emphasis in this section is on the in-depth analysis and preprocessing of the text data (tweets). In this section, we introduce the manually tagged COVID-19 tweets, which range from Highly Negative to Highly Positive, representing five distinct classes. In this Exploratory Data Analysis (EDA), these five classes will be simplified to two classes: Positive and Negative.\n\nLoad library\n\n%set_env PYTORCH_MPS_HIGH_WATERMARK_RATIO 0.8\n\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\n\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\ndevice = \"cpu\"\n\ntrain_model = True\n\nenv: PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.8\n\n\n\n\nLoad covid-19 tweets data\n\nfrom validmind.datasets.nlp import twitter_covid_19 as demo_data\ndf = demo_data.load_data()\ndf.head(10)\n\n\n\n\n\n\n\n\nOriginalTweet\nSentiment\n\n\n\n\n0\n@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\nNeutral\n\n\n1\nadvice Talk to your neighbours family to excha...\nPositive\n\n\n2\nCoronavirus Australia: Woolworths to give elde...\nPositive\n\n\n3\nMy food stock is not the only one which is emp...\nPositive\n\n\n4\nMe, ready to go at supermarket during the #COV...\nExtremely Negative\n\n\n5\nAs news of the region’s first confirmed COVID-...\nPositive\n\n\n6\nCashier at grocery store was sharing his insig...\nPositive\n\n\n7\nWas at the supermarket today. Didn't buy toile...\nNeutral\n\n\n8\nDue to COVID-19 our retail store and classroom...\nPositive\n\n\n9\nFor corona prevention,we should stop to buy th...\nNegative\n\n\n\n\n\n\n\n\n\nRun text data quality test plan\nIn this section we use the ValidMind Developer Framework to run various data quality checks on the dataset, and send the results to the model document on the ValidMind Platform UI.\n\nvm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='OriginalTweet', target_column=\"Sentiment\")\n\n2023-07-06 12:52:30,028 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-06 12:52:30,028 - INFO - dataset - Inferring dataset types...\n\n\n\nconfig = {\n    \"class_imbalance\":{\"min_percent_threshold\": 3}\n}\ntext_data_test_plan = vm.run_test_plan(\"text_data_quality\",\n                                       dataset=vm_ds,\n                                       config=config)\n\n\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/anilsorathiya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/anilsorathiya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#preprocess-data",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#preprocess-data",
    "title": "Sensitivity Analysis",
    "section": "2. Preprocess data",
    "text": "2. Preprocess data\n\nHandle class bias\nOne way to handle class bias is to merge a specific class data with related class. Here, we will copy the text and class lables in separate columns so that the original text is also there for comparison.\n\nprint(\"Original Classes:\", df.Sentiment.unique())\n\ndf['text'] = df.OriginalTweet\ndf[\"text\"] = df[\"text\"].astype(str)\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"positive\"\n    elif x == \"Extremely Negative\":\n        return \"negative\"\n    elif x == \"Negative\":\n        return \"negative\"\n    elif x ==  \"Positive\":\n        return \"positive\"\n    else:\n        return \"neutral\"\n    \ndf['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\ntarget=df['sentiment']\n\nprint(df.sentiment.value_counts(normalize= True))\nprint(\"Modified Classes:\", df.sentiment.unique())\n\nOriginal Classes: ['Neutral' 'Positive' 'Extremely Negative' 'Negative' 'Extremely Positive']\npositive    0.435814\nnegative    0.378846\nneutral     0.185341\nName: sentiment, dtype: float64\nModified Classes: ['neutral' 'positive' 'negative']\n\n\n\n\nRemove neutral class\n\ndf = df[df[\"sentiment\"] != \"neutral\"]\nprint(df.sentiment.unique())\nprint(df.sentiment.value_counts(normalize= True))\nprint(df.shape)\n\n['positive' 'negative']\npositive    0.534964\nnegative    0.465036\nName: sentiment, dtype: float64\n(36623, 4)\n\n\n\ndf\n\n\n\n\n\n\n\n\nOriginalTweet\nSentiment\ntext\nsentiment\n\n\n\n\n1\nadvice Talk to your neighbours family to excha...\nPositive\nadvice Talk to your neighbours family to excha...\npositive\n\n\n2\nCoronavirus Australia: Woolworths to give elde...\nPositive\nCoronavirus Australia: Woolworths to give elde...\npositive\n\n\n3\nMy food stock is not the only one which is emp...\nPositive\nMy food stock is not the only one which is emp...\npositive\n\n\n4\nMe, ready to go at supermarket during the #COV...\nExtremely Negative\nMe, ready to go at supermarket during the #COV...\nnegative\n\n\n5\nAs news of the region’s first confirmed COVID-...\nPositive\nAs news of the region’s first confirmed COVID-...\npositive\n\n\n...\n...\n...\n...\n...\n\n\n44949\n@RicePolitics @MDCounties Craig, will you call...\nNegative\n@RicePolitics @MDCounties Craig, will you call...\nnegative\n\n\n44950\nMeanwhile In A Supermarket in Israel -- People...\nPositive\nMeanwhile In A Supermarket in Israel -- People...\npositive\n\n\n44951\nDid you panic buy a lot of non-perishable item...\nNegative\nDid you panic buy a lot of non-perishable item...\nnegative\n\n\n44953\nGov need to do somethings instead of biar je r...\nExtremely Negative\nGov need to do somethings instead of biar je r...\nnegative\n\n\n44954\nI and @ForestandPaper members are committed to...\nExtremely Positive\nI and @ForestandPaper members are committed to...\npositive\n\n\n\n\n36623 rows × 4 columns\n\n\n\n\n\nRemove urls and html links\n\n#Remove Urls and HTML links\nimport re\n\ndef remove_urls(text):\n    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\n\ndf['text']=df['text'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'&lt;.*?&gt;')\n    return html.sub(r'',text)\n\ndf['text']=df['text'].apply(lambda x:remove_html(x))\n\n\n\nConvert text to lower case\n\n# Lower casing\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['text']=df['text'].apply(lambda x:lower(x))\n\n\n\nRemove numbers\n\n# Number removal\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['text']=df['text'].apply(lambda x:remove_num(x))\n\n\n\nRemove stopwords\n\n#Remove stopwords\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['text']=df['text'].apply(lambda x:remove_stopwords(x))\n\n\n\nRemove Punctuations\n\n#Remove Punctuations\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['text']=df['text'].apply(lambda x:punct_remove(x))\n\n\n\nRemove mentions\n\n#Remove mentions \ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['text']=df['text'].apply(lambda x:remove_mention(x))\n\n\n\nRemove hashtags\n\n#Remove hashtags \n\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['text']=df['text'].apply(lambda x:remove_hash(x))\n\n\n\nRemove extra white space left while removing stuff\n\n#Remove extra white space left while removing stuff\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['text']=df['text'].apply(lambda x:remove_space(x))\n\n\ndf\n\n\n\n\n\n\n\n\nOriginalTweet\nSentiment\ntext\nsentiment\n\n\n\n\n1\nadvice Talk to your neighbours family to excha...\nPositive\nadvice talk neighbours family exchange phone n...\npositive\n\n\n2\nCoronavirus Australia: Woolworths to give elde...\nPositive\ncoronavirus australia woolworths give elderly ...\npositive\n\n\n3\nMy food stock is not the only one which is emp...\nPositive\nfood stock one empty please panic enough food ...\npositive\n\n\n4\nMe, ready to go at supermarket during the #COV...\nExtremely Negative\nme ready go supermarket covid outbreak im para...\nnegative\n\n\n5\nAs news of the region’s first confirmed COVID-...\nPositive\nnews regions first confirmed covid case came s...\npositive\n\n\n...\n...\n...\n...\n...\n\n\n44949\n@RicePolitics @MDCounties Craig, will you call...\nNegative\nricepolitics mdcounties craig call general ass...\nnegative\n\n\n44950\nMeanwhile In A Supermarket in Israel -- People...\nPositive\nmeanwhile supermarket israel people dance sing...\npositive\n\n\n44951\nDid you panic buy a lot of non-perishable item...\nNegative\npanic buy lot nonperishable items echo needs f...\nnegative\n\n\n44953\nGov need to do somethings instead of biar je r...\nExtremely Negative\ngov need somethings instead biar je rakyat ass...\nnegative\n\n\n44954\nI and @ForestandPaper members are committed to...\nExtremely Positive\nforestandpaper members committed safety employ...\npositive\n\n\n\n\n36623 rows × 4 columns\n\n\n\n\n\nRun text data quality tests again\nHere, we are checking the quality of the data again by running data quality tests so verify that we have preprocess data well and tests are passing according to our requirements.\n\nvm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='text', target_column=\"sentiment\")\n\nconfig = {\n    \"class_imbalance\":{\"min_percent_threshold\": 3}\n}\ntext_data_test_plan = vm.run_test_plan(\"text_data_quality\",\n                                       dataset=vm_ds,\n                                       config=config)\n\n2023-07-06 12:52:42,024 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-06 12:52:42,024 - INFO - dataset - Inferring dataset types...\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/anilsorathiya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/anilsorathiya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#modeling",
    "href": "notebooks/nlp/nlp_sentiment_analysis_catboost_demo.html#modeling",
    "title": "Sensitivity Analysis",
    "section": "4. Modeling",
    "text": "4. Modeling\n\nTraining, validation, test\nWith our data in nice shape, we’ll split it into training, validation, and test sets.\n\n\ndf = df[df['sentiment'] != \"neutral\"]\ndf.loc[df['sentiment'] == \"positive\", 'sentiment'] = 1\ndf.loc[df['sentiment'] == \"negative\", 'sentiment'] = 0\nprint(np.unique(df['sentiment']))\n\nprint(df.head())\ntrain, test = train_test_split(df[['text','sentiment']], test_size=0.33, random_state=42)\ntrain = train[['text','sentiment']]\ntest = test[['text','sentiment']]\n\ntrain, valid = train_test_split(\n    train,\n    train_size=0.7,\n    random_state=0,\n    stratify=train['sentiment'])\ny_train, X_train = \\\n    train['sentiment'], train.drop(['sentiment'], axis=1)\ny_valid, X_valid = \\\n    valid['sentiment'], valid.drop(['sentiment'], axis=1)\ny_test, X_test= \\\n    test['sentiment'], test.drop(['sentiment'], axis=1)\n\n[0 1]\n                                       OriginalTweet           Sentiment  \\\n1  advice Talk to your neighbours family to excha...            Positive   \n2  Coronavirus Australia: Woolworths to give elde...            Positive   \n3  My food stock is not the only one which is emp...            Positive   \n4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n5  As news of the regions first confirmed COVID-...            Positive   \n\n                                                text sentiment  \n1  advice talk neighbours family exchange phone n...         1  \n2  coronavirus australia woolworths give elderly ...         1  \n3  food stock one empty please panic enough food ...         1  \n4  me ready go supermarket covid outbreak im para...         0  \n5  news regions first confirmed covid case came s...         1  \n\n\n\n\nBuild model\n\ndef fit_model(X_train, y_train,val_data, **kwargs):\n    model = CatBoostClassifier(\n        task_type='CPU',\n        iterations=5000,\n        eval_metric='Accuracy',\n        od_type='Iter',\n        od_wait=500,\n        **kwargs\n    )\n    return model.fit(\n        X=X_train,\n        y=y_train,\n        eval_set=val_data,\n        verbose=100,\n        plot=True,\n        use_best_model=True\n        )\n\n\nmodel = fit_model(\n    X_train, y_train,\n    val_data=(X_valid,y_valid),\n    text_features=['text'],\n    learning_rate=0.35,\n    tokenizers=[\n        {\n            'tokenizer_id': 'Sense',\n            'separator_type': 'BySense',\n            'lowercasing': 'True',\n            'token_types':['Word', 'Number', 'SentenceBreak'],\n            'sub_tokens_policy':'SeveralTokens'\n        }      \n    ],\n    dictionaries = [\n        {\n            'dictionary_id': 'Word',\n            'max_dictionary_size': '5000'\n        }\n    ],\n    feature_calcers = [\n        'BoW:top_tokens_count=10000'\n    ]\n)\n\n\n\n\n0:  learn: 0.6037263    test: 0.6062211 best: 0.6062211 (0) total: 101ms    remaining: 8m 25s\n100:    learn: 0.8601456    test: 0.8288509 best: 0.8291225 (98)    total: 4.36s    remaining: 3m 31s\n200:    learn: 0.9101601    test: 0.8515349 best: 0.8528932 (193)   total: 8.52s    remaining: 3m 23s\n300:    learn: 0.9349054    test: 0.8562891 best: 0.8564249 (297)   total: 12.8s    remaining: 3m 19s\n400:    learn: 0.9557496    test: 0.8566965 best: 0.8583265 (348)   total: 17s  remaining: 3m 15s\n500:    learn: 0.9712955    test: 0.8569682 best: 0.8583265 (348)   total: 21.2s    remaining: 3m 10s\n600:    learn: 0.9795633    test: 0.8576474 best: 0.8583265 (348)   total: 25.3s    remaining: 3m 5s\n700:    learn: 0.9865502    test: 0.8533007 best: 0.8583265 (348)   total: 29.6s    remaining: 3m 1s\n800:    learn: 0.9915575    test: 0.8572399 best: 0.8583265 (348)   total: 33.8s    remaining: 2m 57s\n900:    learn: 0.9949345    test: 0.8603640 best: 0.8610432 (869)   total: 38.1s    remaining: 2m 53s\n1000:   learn: 0.9970306    test: 0.8604999 best: 0.8610432 (869)   total: 42.4s    remaining: 2m 49s\n1100:   learn: 0.9980786    test: 0.8590057 best: 0.8611790 (1038)  total: 46.7s    remaining: 2m 45s\n1200:   learn: 0.9985444    test: 0.8602282 best: 0.8611790 (1038)  total: 51s  remaining: 2m 41s\n1300:   learn: 0.9987773    test: 0.8626732 best: 0.8632165 (1261)  total: 55.2s    remaining: 2m 37s\n1400:   learn: 0.9990684    test: 0.8621299 best: 0.8632165 (1261)  total: 59.5s    remaining: 2m 32s\n1500:   learn: 0.9991266    test: 0.8624015 best: 0.8636240 (1464)  total: 1m 3s    remaining: 2m 28s\n1600:   learn: 0.9991849    test: 0.8614507 best: 0.8636240 (1464)  total: 1m 8s    remaining: 2m 24s\n1700:   learn: 0.9992431    test: 0.8617224 best: 0.8636240 (1464)  total: 1m 12s   remaining: 2m 20s\n1800:   learn: 0.9993013    test: 0.8613149 best: 0.8638957 (1756)  total: 1m 16s   remaining: 2m 16s\n1900:   learn: 0.9994178    test: 0.8614507 best: 0.8638957 (1756)  total: 1m 21s   remaining: 2m 12s\n2000:   learn: 0.9994760    test: 0.8604999 best: 0.8638957 (1756)  total: 1m 25s   remaining: 2m 8s\n2100:   learn: 0.9994760    test: 0.8600924 best: 0.8638957 (1756)  total: 1m 29s   remaining: 2m 3s\n2200:   learn: 0.9994760    test: 0.8607715 best: 0.8638957 (1756)  total: 1m 34s   remaining: 1m 59s\nStopped by overfitting detector  (500 iterations wait)\n\nbestTest = 0.8638956805\nbestIteration = 1756\n\nShrink model to first 1757 iterations.\n\n\n\n\nInitialize validmind objects\n\nvm_train_ds = vm.init_dataset(dataset=pd.concat([X_train, y_train], axis=1), type=\"generic\", target_column=\"sentiment\")\nvm_test_ds = vm.init_dataset(dataset=pd.concat([X_test, y_test], axis=1), type=\"generic\",target_column=\"sentiment\")\nvm_model = vm.init_model(model, train_ds=vm_train_ds, test_ds=vm_test_ds)\n\n2023-07-06 12:54:24,980 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-06 12:54:24,982 - INFO - dataset - Inferring dataset types...\n2023-07-06 12:54:24,998 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n2023-07-06 12:54:24,999 - INFO - dataset - Inferring dataset types...\n\n\n\nRun model metrics test plan\n\nmodel_metrics_test_plan = vm.run_test_plan(\"binary_classifier_metrics\", \n                                             model=vm_model\n                                            )\n\n\n\n\n2023-07-06 12:54:25,525 - INFO - PermutationFeatureImportance - Skiping PFI for catboost models\n2023-07-06 12:54:25,555 - INFO - PopulationStabilityIndex - Skiping PSI for catboost models\n2023-07-06 12:54:25,556 - INFO - SHAPGlobalImportance - Skiping SHAP for catboost models\n\n\n\n\n\n\n\nRun model validation test plan\n\nmodel_validation_test_plan = vm.run_test_plan(\"binary_classifier_validation\", \n                                             model=vm_model\n                                            )"
  },
  {
    "objectID": "guide/join-closed-beta.html",
    "href": "guide/join-closed-beta.html",
    "title": "ValidMind",
    "section": "",
    "text": "Image and Iframe\n\n\n    \n        \n        Loading…"
  }
]