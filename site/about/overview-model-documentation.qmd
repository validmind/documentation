---
title: "Automated model testing & documentation"
date: last-modified
filters:
  - tachyons
aliases:
  - ../guide/overview-model-documentation.html
listing:
  id: quickstart
  type: grid
  grid-columns: 2
  # image-height: 100%
  contents:
    - path: ../get-started/quickstart.qmd
      title: "QuickStart"
      description: "Our QuickStart is the quickest and easiest way to try out our product features."
  fields: [title, description, reading-time]
---

The {{< var validmind.developer >}} streamlines the process of documenting various types of models. {{< var vm.product >}} automates the documentation process, ensuring that your model documentation and testing aligns with regulatory and compliance standards.

::: {.prereq}

## {{< fa code >}} The {{< var validmind.developer >}}

The {{< var validmind.developer >}} is a {{< var vm.developer >}} and documentation engine designed to streamline the process of documenting various types of models, including traditional statistical models, legacy systems, artificial intelligence/machine learning models, and large language models (LLMs). 

It offers model developers a systematic approach to documenting and testing risk models with repeatability and consistency, ensuring alignment with regulatory and compliance standards.

<!-- Using the variable in alt text messes up the image display  -->

![The two main components of {{< var vm.product >}}. The {{< var validmind.developer >}} that integrates with your existing developer environment, and the {{< var validmind.platform >}}.](overview-validmind-library.png){fig-align="center" fig-alt="An image showing the two main components of ValidMind. The ValidMind Library that integrates with your existing developer environment, and the ValidMind Platform."}

The {{< var validmind.developer >}} consists of a client-side library, a {{< var vm.api >}} integration for models and testing, and validation tests that streamline the model development process. Implemented as a series of independent libraries in Python and R, our {{< var vm.developer >}} ensures compatibility and flexibility with diverse sets of developer environments and requirements.

With the {{< var validmind.developer >}}, you can:

- **Automate documentation** — Add comprehensive documentation as metadata while you build models to be shared with model validators, streamlining and speeding up the process.
- **Run test suites** — Identify potential risks for a diverse range of statistical and AI/LLM/ML models by assessing data quality, model outcomes, robustness, and explainability.
- **Integrate with your development environment** — Seamlessly incorporate the {{< var validmind.developer >}} into your existing model development environment, connecting to your existing model code and data sets.
- **Upload documentation data** — Send qualitative and quantitative test data to the {{< var validmind.platform >}}[^1] to generate the model documentation for review and approval, fostering effective collaboration with model reviewers and validators.

:::

## Simple installation

Install the {{< var vm.developer >}} with: 

```python
%pip install validmind
```

## Docs-as-code

:::: {.flex .flex-wrap .justify-around}

::: {.w-40-ns}

What the {{< var validmind.developer >}} offers:

- Generates documentation artifacts utilizing the context of the model and dataset, the model's metadata, and the chosen documentation template.
- Can be easily imported into your local model development environment. The supported platforms include Python and R.
- Dual-licensed — The {{< var vm.developer >}} is available as open-source under AGPL v3 license and also with a commercial software license.

:::

::: {.w-50-ns}

```python
import validmind as vm

vm.init(model="MODEL_IDENTIFIER")
```

```python
vm_dataset = vm. log_dataset(
      df,
      "training",
      targets=targets,
)
vm. run_dataset_tests(df, vm_dataset=vm_dataset)
```

```python
vm. Log_model (model)
vm. log_training_metrics (model, x_train, y_train)
vm. run_model_tests (model, x_test, y_test)
```

:::

:::


How the {{< var validmind.developer >}} works:

- The tests and functions are executed automatically, following pre-configured templates tailored for specific model use cases. This ensures that minimum documentation requirements are consistently fulfilled.
- The {{< var vm.developer >}} integrates with ETL/data processing pipelines using connector interfaces. This enables the extraction of relationships between raw data sources and their corresponding post-processed datasets, such as those preloaded session instances received from platforms like Spark and Snowflake.

## Extensible by design

{{< var vm.product >}} supports various model types, including:[^2]

- Traditional machine learning models (ML) such as tree-based models and neural network models.
- Natural language processing models (NLP) for text analysis and understanding.
- Large language models (LLMs) in beta testing phase, offering advanced language capabilities.
- Traditional statistical models like Ordinary Least Squares (OLS) regression, Logistic regression, Time Series models, and more.

{{< var vm.product >}} is designed to be highly extensible to cater to our customers' specific requirements. You can expand its functionality in the following ways:

- You can easily add support for new models and data types by defining new classes within the {{< var validmind.developer >}}. We provide templates to guide you through this process.[^3] 
- To include custom tests in the library, you can define new functions. We offer templates to help you create these custom tests.[^4] 
- You have the flexibility to integrate third-party test libraries seamlessly. These libraries can be hosted either locally within your infrastructure or remotely, for example, on GitHub. Leverage additional testing capabilities and resources as needed.[^5] 

## {{< var validmind.api >}} integration

{{< var vm.product >}} imports the following artifacts into the documentation via our {{< var validmind.api >}} integration:

- Metadata about datasets and models, used to lookup programmatic documentation content, such as the stored definition for _common logistic regression limitations_ when a logistic regression model has been passed to the {{< var vm.product >}} test plan to be run.
- Quality and performance metrics collected from datasets and models.
- Output from test and test suites that have been run.
- Images, plots, visuals that were generated as part of extracting metrics and running tests.

![Artifacts imported into the documentation via our {{< var vm.api >}}](fine-print/overview-api-integration.png){width=90% fig-alt="A representation of artifacts imported into the documentation via our Python API"}

::: {.callout-important}
## {{< var vm.product >}} does NOT: 
- Send any personal identifiable information (PII) when generating documentation reports.
- Store any customer datasets or models. 
:::

## Ready to try out {{< var vm.product >}}?

:::{#quickstart}
:::



<!-- FOOTNOTES -->

[^1]: [Model risk management](overview-model-risk-management.qmd)

[^2]: [Supported models](/developer/model-documentation/supported-models.qmd)

[^3]: [Customize documentation templates](/guide/model-documentation/customize-documentation-templates.qmd)

[^4]: [Implement custom tests](/notebooks/code_samples/custom_tests/implement_custom_tests.ipynb)

[^5]: [Integrate external test providers](/notebooks/code_samples/custom_tests/integrate_external_test_providers.ipynb)


