---
title: "Testing"
date: last-modified
aliases:
  - ../guide/faq-testing.html
---

## How do the out-of-the-box tests developed by {{< var vm.product >}} work?

All the default tests are developed using open-source Python and R libraries.

The {{< var validmind.developer >}} test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and contains other functions to collect and post results to the {{< var validmind.platform >}} using a generic results schema.

## Can we configure, customize, or add our own tests?

Yes, {{< var vm.product >}} allows tests to be manipulated at several levels:

- You can configure which tests are required to run programmatically depending on the model use case.
- You can change the thresholds and parameters for default tests already available in the {{< var vm.developer >}} — for instance, changing the threshold parameter for the class imbalance flag.
- You can also connect your own custom tests with the {{< var validmind.developer >}}. These custom tests are configurable and are able to run programmatically, just like the rest of the {{< var vm.developer >}}.

## Do you include explainability-related testing and documentation? 

Our {{< var vm.developer >}} currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.

<!-- BELOW REMOVED ON REQUEST AS PER SC-6528 -->

In addition, {{< var vm.product >}} is implementing standard documentation via the {{< var vm.developer >}} for the following items and modeling techniques:

- Conceptual soundness
    - Model use case description (Q2’2023)
    - Model selection rationale (Q2’2023)
- Data evaluation
    - Data quality metrics
    - Sampling method validation
    - Population distribution (PSI)
    - Correlations & interactions
    - Data lineage (Q3’2023)
    - Feature engineering (Q3’2023)
- Model Evaluation
    - Performance & accuracy evaluation
    - Goodness of fit (Q2’2023)
    - Stability & sensitivity to perturbations (Q3’2023)
    - Model robustness & weak regions (Q3’2023)
    - Global explainability - permutation feature importance, SHAP
    - Local explainability- LIME (Q3’2023)
    - Model testing at implementation / post-production (2024)
- Model techniques
    - Time series (ARIMA, Error correction)
    - Regression (OLS, Logistic, GLM, XGBoost)
    - Decision trees (tree-based ML models)
    - Random forests
    - K-means clustering (Q2 2023)
    - NLP (2024)
    - Deep learning (2024)
    - Computer vision (2024)

## Is there a use case for synthetic data within {{< var vm.product >}}?

The {{< var validmind.developer >}} supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.

We are happy to discuss exploring specific use cases for synthetic data generation with you further.

{{< include _faq-monitoring.qmd >}}

