---
title: "Testing"
date: last-modified
aliases:
  - ../guide/faq-testing.html
listing:
  - id: faq-testing
    type: grid
    grid-columns: 3
    max-description-length: 250
    sort: false
    fields: [title, description]
    contents:
    - ../developer/model-testing/testing-overview.qmd
    - ../developer/model-testing/test-descriptions.qmd
    - ../guide/monitoring/ongoing-monitoring.qmd
---

## How do the out-of-the-box tests developed by {{< var vm.product >}} work?

All the default tests are developed using open-source Python and R libraries.

The {{< var validmind.developer >}}[^1] test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and contains other functions to collect and post results to the {{< var validmind.platform >}}[^2] using a generic results schema.

## When do I use tests and tests suites?

While you have the flexibility to decide when to use which {{< var vm.product >}} tests, here are a few typical scenarios:[^3]

- **Dataset testing** — To document and validate your dataset.
- **Model testing** — To document and validate your model.
- **End-to-end testing** — To document a binary classification model and the relevant dataset end-to-end.

## Can we configure, customize, or add our own tests?

Yes, {{< var vm.product >}} allows tests to be manipulated at several levels:

- You can configure which tests are required to run programmatically depending on the model use case.[^4]
- You can change the thresholds and parameters for default tests already available in the {{< var vm.developer >}} — for instance, changing the threshold parameter for the class imbalance flag.[^5]
- You can also connect your own custom tests with the {{< var validmind.developer >}}. These custom tests are configurable and are able to run programmatically, just like the rest of the {{< var vm.developer >}}.[^6]

## Do you include explainability-related testing and documentation? 
<span id="explanability"></span>

Yes, {{< var vm.product >}} includes explainability-related testing and documentation as part of our offerings. Our approach incorporates a comprehensive suite of tests designed to evaluate model interpretability and identify potential risks, ensuring transparency and reliability in model outcomes. 

Below is an overview of our key explainability-related tests:

- **Features AUC** — Assesses the discriminatory power of individual features in binary classification models, providing insights into how well each feature differentiates between classes. This test supports explainability by isolating the contribution of each feature to the classification task.
- **Feature Importance** — Generates feature importance scores to identify and compare impactful features across different models and datasets. By highlighting the relative significance of features, this test clarifies how inputs influence model predictions.
- **Overfit Diagnosis** — Detects potential overfitting by comparing performance between training and testing sets for specific feature segments, highlighting areas of significant deviation. This test aids explainability by revealing where model behavior is inconsistent, offering insights into its generalization capability.
Permutation Feature Importance: Measures feature significance by analyzing the impact of randomly rearranging feature values on model performance. This test quantifies the dependency of model performance on each feature, making it clear which inputs drive the predictions.
- **SHAP Global Importance** — Uses SHAP (SHapley Additive exPlanations) values to assign global importance to features, offering a clear explanation of model outcomes and supporting risk identification. SHAP values provide a mathematically sound attribution of model predictions to specific features, enhancing interpretability.
- **Weakspots Diagnosis** — Identifies and visualizes regions of suboptimal model performance across the feature space, highlighting areas that may require further attention. This test explains where and why the model struggles by connecting poor performance to specific feature regions.
In terms of documentation, each test automatically generates a comprehensive report as soon as it is executed. ValidMind leverages generative AI to produce tailored, detailed summaries that include the test description, key insights, and a concise summary of results. This automated documentation ensures that every test outcome is transparently recorded, clearly communicated, and immediately actionable.

<!-- BELOW REMOVED ON REQUEST AS PER SC-6528 -->

<!-- In addition, {{< var vm.product >}} is implementing standard documentation via the {{< var vm.developer >}} for the following items and modeling techniques:

- Conceptual soundness
    - Model use case description (Q2’2023)
    - Model selection rationale (Q2’2023)
- Data evaluation
    - Data quality metrics
    - Sampling method validation
    - Population distribution (PSI)
    - Correlations & interactions
    - Data lineage (Q3’2023)
    - Feature engineering (Q3’2023)
- Model Evaluation
    - Performance & accuracy evaluation
    - Goodness of fit (Q2’2023)
    - Stability & sensitivity to perturbations (Q3’2023)
    - Model robustness & weak regions (Q3’2023)
    - Global explainability - permutation feature importance, SHAP
    - Local explainability- LIME (Q3’2023)
    - Model testing at implementation / post-production (2024)
- Model techniques
    - Time series (ARIMA, Error correction)
    - Regression (OLS, Logistic, GLM, XGBoost)
    - Decision trees (tree-based ML models)
    - Random forests
    - K-means clustering (Q2 2023)
    - NLP (2024)
    - Deep learning (2024)
    - Computer vision (2024) -->

{{< include _faq-synthetic-datasets.qmd >}}

{{< include _faq-monitoring.qmd >}}

## Learn more

:::{#faq-testing}
:::


<!-- FOOTNOTES -->

[^1]: [Get started with the {{< var validmind.developer >}}](/developer/get-started-validmind-library.qmd)

[^2]: [Accessing {{< var vm.product >}}](/guide/configuration/accessing-validmind.qmd)

[^3]: [When do I use tests and test suites?](/developer/model-testing/testing-overview.qmd#when-do-i-use-tests-and-test-suites)

[^4]: [`run_documentation_tests()`](/validmind/validmind.html#run_documentation_tests)

[^5]: [`ClassImbalance()`](/validmind/validmind/tests/data_validation/ClassImbalance.html#ClassImbalance)

[^6]: [Can I use my own tests?](/developer/model-testing/testing-overview.qmd#can-i-use-my-own-tests)