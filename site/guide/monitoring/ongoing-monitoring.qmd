---
title: "Ongoing monitoring"
date: last-modified
listing:
  - id: ongoing-monitoring-listing
    type: grid
    max-description-length: 250
    sort: false
    fields: [title, description]
    contents:
      - ../../notebooks/code_samples/ongoing_monitoring/quickstart_customer_churn_ongoing_monitoring.ipynb
  - id: ongoing-monitoring
    contents: "../../tests/ongoing_monitoring/**"
    type: grid
    max-description-length: 250
    page-size: 150
    fields: [title, description]  
filters:
  - tachyons
---

{{< include /about/glossary/monitoring/_monitoring-intro.qmd >}}

## Prerequisites

- You are logged into the {{< var validmind.platform >}}.
- You hold the `Developer` or `Validator` role.[^1]
- The model is in the pre-approval stage for performance testing or the model is has been approved and is in production.

## Monitoring scenarios

Scenarios where ongoing monitoring is warranted include:

- **Pre-approval monitoring of new models**: New models should undergo a trial phase of monitoring before full approval and subsequent deployment, particularly for high-risk or regulatory models, to ensure reliability before deployment. Trial phases where a model is subject to ongoing monitoring are typically fairly short, ranging from a few days to several weeks.

- **Monitoring during significant updates**: When a model undergoes a significant update, ongoing monitoring should compare the updated model's performance to the original. This process, called parallel runs, involves running both models simultaneously for a set period. Model outputs should be closely monitored to assess whether the update improves performance or introduces new risks. The results help determine if the updated model should replace the original or if further adjustments are needed. Parallel runs are especially important for regulatory or critical models, ensuring changes don't harm performance.

- **Post-production monitoring**: After deployment into production, models should be regularly assessed against performance benchmarks to identify deviations, enabling timely recalibrations or adjustments. The model's output should be continuously assessed against predefined performance benchmarks to ensure it meets the required standards. Any deviations from expected performance should be quickly identified, allowing for timely intervention

## Ongoing monitoring plan

A robust ongoing monitoring plan is crucial for maintaining model accuracy and reliability. Model developers should start regular monitoring from the outset, ideally during the pre-approval phase of model development, refining the plan as new insights are gained. This plan should be included in the initial model documentation, with clear instructions for execution and use of results.[^2]

As monitoring progresses, it's important to report each model's status to the model risk management committee. Regular updates with summary metrics should keep the committee informed of findings and emerging risks, highlighting significant trends or issues that may need action.

Key components of your ongoing monitoring plan should include:

- **Defined metrics and models**: Specify which models and performance metrics will be monitored.
- **Monitoring frequency**: Set how often each metric will be monitored, based on the model's risk and importance.
- **Risk thresholds**: Establish thresholds that trigger alerts or actions when metrics deviate from expected ranges.
- **Notification system**: Implement a system to notify stakeholders when significant issues or deviations occur.
- **Regular reports**: Present monitoring results using visual tools for clear and accessible decision-making.

### Key concepts

<!-- TO EDIT THESE KEY CONCEPTS, SEE `about/glossary/_monitoring.qmd` -->


:::: {.flex .flex-wrap .justify-around}

::: {.w-40-ns}

{{< include /about/glossary/monitoring/_backtesting.qmd >}}

{{< include /about/glossary/monitoring/_compliance-and-regulatory-adherence.qmd >}}

{{< include /about/glossary/monitoring/_model-drift.qmd >}}

{{< include /about/glossary/monitoring/_model-performance.qmd >}}

:::

::: {.w-40-ns}

{{< include /about/glossary/monitoring/_ongoing-monitoring.qmd >}}

{{< include /about/glossary/monitoring/_recalibrating-models.qmd >}}

{{< include /about/glossary/monitoring/_reporting-and-governance.qmd >}}

:::

::::

### Design and implementation

The design of your ongoing monitoring plan overall should be a collaborative effort between the first line of defense, typically business units or model owners, and the second line of defense, namely your risk management team. Together, they should select the performance metrics, determine monitoring frequency, and tailor the ongoing monitoring plan to the model's specific use case and risk profile.

Practically, this entails that the ongoing monitoring plan is primarily designed and implemented by the developers involved in the model's development and deployment. Their work is then reviewed during model validation to ensure the effectiveness of the ongoing monitoring plan and alignment with risk management goals. 

The implementation of the ongoing monitoring plan typically also falls on model developers. This effort includes executing the monitoring activities, collecting performance data, and generating reports. The model developers also ensure that the monitoring is carried out according to the established schedule and that any anomalies or deviations are promptly identified and addressed.

### Testing

Model monitoring should incorporate a variety of tests to ensure ongoing accuracy and reliability. Key test areas to pay attention to:

- **Benchmarking** — compare model estimates with alternative estimates to validate their accuracy.
- **Sensitivity testing** — reaffirm the model's robustness and stability under different conditions.
- **Analysis of overrides** — evaluate any adjustments made to the model to understand their impact.
- **Parallel outcomes analysis** — assess whether new data should be included in the model’s calibration to enhance its performance.

### {{< fa desktop >}} Available tests

:::{#ongoing-monitoring}
:::

## Enable monitoring for models

 To enable monitoring and start uploading ongoing monitoring results, you need to:

1. [Add `monitoring=True` to your code snippet](#add-monitoringtrue-to-your-code-snippet)
2. [Select a monitoring template](#select-a-monitoring-template)
3. [Review the monitoring results](#review-the-monitoring-results)

You can enable monitoring on both new and existing models.

::: {.callout}
Want to try out how monitoring works in a notebook? Check out out our code sample for ongoing monitoring of models.[^4]
:::

### Add `monitoring=True` to your code snippet 

ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook. 

To enable ongoing monitoring for a model, add `monitoring=True` to your code snippet: 

1. In the {{< var validmind.platform >}}, navigate to **Model Inventory** in the left sidebar. 

2. Either register a new model[^3] first or locate an existing model:

   a. In the left sidebar, click **Model Inventory**.

   b. Select a model by clicking on it or find your model by applying a filter or searching for it.

3. Copy the code snippet for the model: 

   a. In the left sidebar that appears for your model, click **Getting Started**.

   b. Locate the code snippet and click **{{< fa regular copy >}} Copy snippet to clipboard**. 

3. Paste the code snippet into your code and add `monitoring=True` to the `vm.init` method, similar to this example:
   
   ```python
   import validmind as vm
   
   vm.init(
       api_host="https://api.prod.validmind.ai/api/v1/tracking",
       api_key="...",
       api_secret="...",
       project="...",
       monitoring=True
   )
   ```

### Select a monitoring template

Before you can start sending ongoing monitoring data from the {{< var validmind.developer >}}, you must select a monitoring template:

1. In the {{< var validmind.platform >}}, navigate to **Model Inventory** in the left sidebar.

2. Select a model by clicking on it or find your model by applying a filter or searching for it.

3. In the left sidebar, click **Ongoing Monitoring**.

4. From the **Template** drop-down, select one of the available monitoring templates. 

5. Click **Use Template**.

## Review the monitoring results 

As the monitoring template for your model gets populated, review the monitoring results:

1. In the {{< var validmind.platform >}}, navigate to **Model Inventory** in the left sidebar.

2. Select a model by clicking on it or find your model by applying a filter or searching for it.

3. In the left sidebar, click **Ongoing Monitoring**.

<!-- ## Troubleshooting -->

## What's next

:::{#ongoing-monitoring-listing}
:::

<!-- FOOTNOTES -->

[^1]: [Managing users (Supported roles)](/guide/configuration/managing-users.qmd#supported-roles)

[^2]: [Working with model documentation](/guide/model-documentation/working-with-model-documentation.qmd)

[^3]: [Register models in the inventory](/guide/model-inventory/register-models-in-inventory.qmd)

[^4]: [Quickstart for ongoing monitoring of models with ValidMind](../../notebooks/code_samples/ongoing_monitoring/quickstart_customer_churn_ongoing_monitoring.ipynb)