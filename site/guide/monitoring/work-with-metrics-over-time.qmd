---
title: "Work with metrics over time"
date: last-modified
---

Once generated via the {{< var validmind.developer >}}, view and add metrics over time to your ongoing monitoring plans in the {{< var validmind.platform >}}.

Metrics over time refers to the continued monitoring of a model's performance once it is deployed. Tracking how a model performs as new data is introduced or conditions change ensures that it remains accurate and reliable in real-world environments where data distributions or market conditions shift. 

- Model performance is determined by continously measuring metrics and comparing them over time to detect degration, bias, or shifts in the model's output. 
- Performance data is collected and tracked over time, often using a rolling window approach or real-time monitoring tools with the same metrics used in testing, but observed across different periods. 
- Continuous tracking helps to identify if and when a model needs to be recalibrated, retrained, or even replaced due to performance deterioration or changing conditions.

::: {.prereq}

## Prerequisites

- [x] {{< var link.login >}}
- [x] Metrics over time have already been logged via the {{< var validmind.developer >}} for your model.[^1] 
- [x] You are a `Developer` or `Validator`, or assigned another role with sufficient permissions to perform the tasks in this guide.[^2]

:::

## Add metrics over time

1. In the left sidebar, click **Model Inventory**.

2. Select a model by clicking on it or find your model by applying a filter or searching for it.[^3]

3. In the left sidebar that appears for your model, click **Documentation** or **Ongoing Monitoring**.

   You can now jump to any section of the model documentation or ongoing monitoring plan by expanding the table of contents on the left and selecting the relevant section you would like to add content to, such as **1.1 Model Monitoring Performance History**. 

4. Hover your mouse over the space where you want your new block to go until a horizontal dashed line with a {{< fa square-plus >}} sign appears that indicates you can insert a new block:

    ![Adding a content block in the UI](/guide/model-documentation/add-content-block.gif){width=90% fig-alt="A gif showing the process of adding a content block in the UI" class="screenshot"}

5. Click {{< fa square-plus >}} and then select **Metric Over Time**.[^4]

    By default, only the `Developer` role can add metrics over time within model documentation or ongoing monitoring plans.

6. Select metric over time results using one of these methods:

   - Select the metrics over time to insert into the model documentation from the list of available metrics.
   - Search by name using **{{<fa magnifying-glass >}} Search** on the top-left.

    ![](test-driven-block-menu.png){width=85% fig-alt="A screenshot showing several test-driven blocks that have been selected for insertion into the model documentation" class="screenshot"}

   To preview what is included in a test, select it. By default, selected tests are previewed.

7. Click **Insert # Test Results to Document** when you are ready.

8. After inserting the results into your documentation, click on the text to make changes or add comments.[^5]


## View metric over time metadata

After you have added metrics over time to your documentation, you can view the following information attached to the result:

- History of values for the result
- What users wrote those results
- Relevant inputs associated with the result

1. In the left sidebar, click **Model Inventory**.

2. Select a model by clicking on it or find your model by applying a filter or searching for it.[^6]

3. In the left sidebar that appears for your model, click **Documentation** or **Ongoing Monitoring**.

4. Locate the test result whose metadata you want to view. 

5. Under the test result's name, click on the row indicating the date of the latest changes.

    - On the test result timeline, click on the **{{< fa chevron-down >}}** associated with a test run to expand for details.
    - When you are done, you can either click **Cancel** or **{{< fa x >}}** to close the metadata menu.

    ![](test-run-details.gif){width=85% fig-alt="A gif showcasing detail expansion of test runs on the test result timeline" class="screenshot"}


## What's next

- [Working with model documentation](/guide/model-documentation/working-with-model-documentation.qmd)
- [Work with content blocks](/guide/model-documentation/work-with-content-blocks.qmd)
- [Collaborate with others](/guide/model-documentation/collaborate-with-others.qmd)
- [View model activity](/guide/model-inventory/view-model-activity.qmd)


<!-- FOOTNOTES -->

[^1]: [Intro to Unit Metrics](/notebooks/how_to/run_unit_metrics.ipynb)

[^2]: [Manage permissions](/guide/configuration/manage-permissions.qmd)

[^3]: [Working with the model inventory](/guide/model-inventory/working-with-model-inventory.qmd#search-filter-and-sort-models)

[^4]: [Work with content blocks](/guide/model-documentation/work-with-content-blocks.qmd)

[^5]: [Collaborate with others](/guide/model-documentation/collaborate-with-others.qmd)

[^6]: [Working with the model inventory](/guide/model-inventory/working-with-model-inventory.qmd#search-filter-and-sort-models)