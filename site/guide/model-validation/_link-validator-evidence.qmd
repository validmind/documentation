:::: {.content-visible unless-format="revealjs"}
::: {.callout}
To link validator evidence to a report, you must first log tests as a validator with the {{< var validmind.developer >}}.^[[{{< var validmind.developer >}}](/developer/validmind-library.qmd)]
:::

1. In the left sidebar, click **{{< fa cubes >}} Inventory**.

1. Select a model or find your model by applying a filter or searching for it.^[[Working with the model inventory](/guide/model-inventory/working-with-model-inventory.qmd#search-filter-and-sort-models)]

1. In the left sidebar that appears for your model, click **Validation Report** under {{< fa file >}} Documents.^[[Working with model documents](/guide/templates/working-with-model-documents.qmd)]

   You can now expand any subsection of the validation report you would like to work with.

   For example, locate 2. Validation then select **2.1.1. Assumptions**.

1. In any section of the report where the button is available, click **{{< fa link >}} Link Evidence to Report**. 

   ![Validation report section 2.1.1. that shows a compliance assessment with the option to link evidence](/guide/model-validation/link-validator-evidence.png){fig-alt="A screenshot of the validation report section 2.1.1. that shows a compliance assessment with the option to link evidence" .screenshot}

1. On the **Link Validator Evidence to Validation Report** page that opens, select the evidence that is related to your assessment. 
   
   If you are not sure if something is relevant, you can expand the section by clicking **{{< fa chevron-down >}}** for more details.

1. Click **Update Linked Evidence**.

   - Confirm that the newly linked-to evidence shown under Developer Evidence is accurate.
   - Evidence that needs your attention gets flagged with [{{< fa triangle-exclamation >}} Requires Attention]{.bubble .yellow-bg}. These sections get flagged automatically whenever a test result is above or below a certain threshold.

::::


:::: {.content-hidden unless-format="revealjs"}
With some test results logged, let's head to the model we connected to at the beginning of this notebook and insert our test results into the validation report as evidence.

While the example below focuses on a specific test result, you can follow the same general procedure for your other results:

::: {.panel-tabset}

### 1. Link data quality test results
a. From the **{{< fa cubes >}} Inventory** in the {{< var validmind.platform >}}, go to the model you connected to earlier.

a. In the left sidebar that appears for your model, click **Validation Report** under {{< fa file >}} Documents.

a. Locate the Data Preparation section and click on **2.2.1. Data Quality** to expand that section.

a. Under the Class Imbalance Assessment section, locate Validator Evidence then click **Link Evidence to Report**.

a. Select the Class Imbalance test results we logged: **ValidMind Data Validation Class Imbalance** 

    ![The ClassImbalance tests selected](/notebooks/tutorials/model_validation/selecting-class-imbalance-results.png){fig-alt="Screenshot showing the ClassImbalance tests selected" .screenshot}

a. Click **Update Linked Evidence** to add the test results to the validation report.

    Confirm that the results for the Class Imbalance test you inserted has been correctly inserted into section **2.2.1. Data Quality** of the report.

### 2. Review Class Imbalance test results

- Once linked as evidence to section **2.2.1. Data Quality** note that the ValidMind Data Validation Class Imbalance test results are flagged as **Requires Attention** â€” as they include comparative results from our initial raw dataset.
- Click **See evidence details** to review the LLM-generated description that summarizes the test results, that confirm that our final preprocessed dataset actually passes our test:

  ![ClassImbalance test generated description in the text editor](/notebooks/tutorials/model_validation/class-imbalance-results-detail.png){fig-alt="Screenshot showing the ClassImbalance test generated description in the text editor" .screenshot}

:::


::::