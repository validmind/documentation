---
title: "Frequently asked questions (FAQ)"
keywords: "faq, model risk management, ValidMind"
---

Find answers to questions, grouped by topic:

- Model registration, configuration, and customization
- Model inventory, tracking, and reporting
- Documentation and templates
- Workflows and collaboration
- Testing and thresholds
- Integrations and support
- Data handling and privacy

## Model registration, configuration, and customization

### How do models get registered in ValidMind?

Models get registered into ValidMind via the Model Inventory. To add a model into the Inventory, you need to fill out a customizable registration questionnaire capturing the required registration metadata, such as:

- Model Name
- Model Use
- Model Owner
- Model Dependencies
- And more

### Can the fields for project registration questionnaires be configured?

ValidMind enables you to configure project registration fields, including dropdown options for model risk tiers, model use cases, and documentation templates. 

You can modify these fields as needed and on an ongoing basis. See @sec-introduction

### Can we leverage content from historical documentations? 

ValidMind is in the process of developing features that allow you to benefit from content in historical documentation by:

- Allowing users to select definitions and specific documentation artifacts from previous model documentation for particular model use cases
- Offering users AI-generated content suggestions for specific areas of the documentation (e.g., qualitative sections) based on high-quality historical documentation

These features are currently on the roadmap and under research, no release schedule is set yet.

### What is the cost to configure or customize the solution?

ValidMind offers its solution in multiple editions:

- Standard Edition serves as our entry-level offering.
- Enterprise Edition includes all features and services of the Standard Edition, plus additional features tailored to the needs of large-scale organizations.
- Virtual Private ValidMind (VPV) provides the highest level of security for organizations requiring a stricter trust model, such as financial services organizations handling highly sensitive data. It encompasses all features and services of the Enterprise Edition but within a separate ValidMind environment, isolated from other ValidMind accounts (VPV accounts do not share resources with non-VPV accounts).

Each edition is priced on an annual subscription basis, depending on the number of models on the platform and support requirements.

The platform is designed to allow customers to configure it directly using a user-friendly SaaS UI. Functionality includes user and role-based administration, documentation template configuration, workflow configuration, test-kit customization, and more.

### Can we customize illustrations?

ValidMind utilizes open-source libraries (such as Seaborn and Matplotlib) to generate plots and illustrations. We are working on implementing the ability for model developers to customize styling parameters for these libraries directly within the Developer Framework.

This feature is currently scheduled for Q4 2023.

Additionally, ValidMind is developing a feature that enables developers to create custom visualization widgets by writing JavaScript-based rendering code.

### 
Can ValidMind manage complex model hierarchies or use cases with multiple models?
ValidMind is enhancing support for complex or modular models in two ways:

- By adding parent/sub-model relational attributes to the model inventory
- By enabling tests to run on multiple models simultaneously and aggregating the results

## Model inventory, tracking, and reporting

### Can permissions for the model inventory be configured?

ValidMind allows you to configure view and edit permissions for the model inventory and documentation or validation reports based on user roles.

### Is it possible to track or view a summary of questions asked by validators?

Questions, comments, and findings from model validations are centrally tracked and accessible within the ValidMind SaaS UI.

### Can the model inventory track revalidation, periodic validation dates, and more?

In addition to initial validation exercises, ValidMind can manage activities throughout the entire model risk management lifecycle, including periodic reviews, change validations, and ongoing monitoring deadlines (roadmap item – Q3 2023).

### Do you support executive reporting for senior leaders in our BUs?

ValidMind is working on a dashboard feature that provides executive metrics, such as model documentation compliance reporting across BUs, findings by status and model use case, and more.

These metrics can be exported into a customizable report for the customer.

## Documentation and templates

### Can documentation templates be configured per model use case or to match our existing templates?

ValidMind's platform allows you to configure multiple templates based on documentation requirements for each model use case. During the model registration process, the platform automatically selects the template based on the provided model use case information.

Documentation templates can be modified by configuring a YAML file in the backend.

ValidMind is working on a UI feature that will enable User Administrators (such as the Model Validation team) to modify existing templates and upload new templates to the platform (target roadmap item – Q3 2023).

### Can the documentation be exported?

ValidMind supports exporting documentation and validation reports in Word (.docx) or PDF formats.

### Can we attach files to the documentation on the UI? What file formats are supported?

You can attach image files to documentation cells and comments on the UI. The following file formats are supported:

- JPEG
- PNG
- GIF
- TIFF
- BMP
- SVG
- RAW
- WebP
- HEIF
- PSD

Additionally, ValidMind is working on enabling you to attach Excel, CSV, Word, and PDF files to the documentation in the UI (Roadmap item – Q2 2023).

### Can the documentation be initialized from the UI instead of the developer framework?

ValidMind allows you to writr documentation directly in the online UI editor, without having to use the developer framework.

From the online UI editor, you can edit text and tables and upload your test results, including images. Using the developer framework, you can execute test plans and generate the corresponding documentation.

### Can we export the documentation produced by ValidMind to the storage/workflow system used by the model validation team?

Documentation and validation reports produced in ValidMind can be exported to Word and PDF formats. Depending on the integration requirements of the systems used by your validation teams, such as connectivity via API, SharePoint, and more, ValidMind can work with you to automate the export and storage of documentation into these systems.

## Workflows and collaboration

### How are parallel editing and version control handled?

ValidMind currently allows multiple users to simultaneously edit documentation in the SaaS UI. If two users are editing the same cell on the UI, the most recently saved version of the content will prevail. 

ValidMind is implementing more sophisticated version control features:

- ValidMind will provide a detailed version and revision history, and notification system, for you to view what changes have been applied to the documentation (roadmap item for Q2’2023).
- The platform will provide an indication if another user is currently editing the same cell on the online UI (roadmap item for Q3’2023).
- Administrators will be given the ability to configure content syncing and session management preferences (roadmap item currently scheduled for Q4’2023).

### Can we work with disconnected workflows?

ValidMind supports disconnected workflows natively at the data-collection level since the developer framework creates individual test runs every time a new test iteration is executed. This allows for running parallel/disconnected tests that individually send results to the ValidMind API.

Visualizing the disconnected workflow in terms of model testing and documentation will depend on requirements at the use-case level.

### Can the workflow accommodate an additional review step, before the documentation gets sent to the 2nd line model validation team?

With ValidMind, administrators can create custom workflows for the review and approval of documentation.

These workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders will be involved at each stage. 

ValidMind is also implementing the ability for administrators to configure default user roles and user groups or teams as part of initial onboarding onto the tool (roadmap item – Q2 2023).

Sample screenshots from ValidMind’s Workflow Configuration tool:

<!--- TO DO Add screenshot --->

### How flexible is ValidMind to accommodate our own model development and review workflows?

ValidMind allows administrators to create custom workflows for the review and approval of documentation, once the user decides it is ready for review.

These workflows can be configured to include any number of review stages before submission, and administrators can configure which stakeholders are involved at each stage.

You can also leverage ValidMind’s developer framework once you are ready to document a specific model for review and validation. That is, you do not need to use ValidMind while you are in the exploration or R&D phase of model development.

## Testing and thresholds

### How did ValidMind develop the tests that are currently in the library?

All the existing tests were developed using open-source Python and R libraries.

The developer framework test interface is a light wrapper that defines some utility functions to interact with different dataset and model backends in an agnostic way, and other functions to collect and post results to the ValidMind backend using a generic results schema.

### Can tests be configured or customized, and can we add our own tests?

ValidMind allows tests to be configured at several levels:

- Administrators can configure which tests are required to run programmatically depending on the model use case
- You can change the thresholds and parameters for tests already available in the developer framework (for instance, changing the threshold parameter for class imbalance flag).
- In addition, ValidMind is implementing a feature that allows you to add your own tests to the developer framework. You will also be able to connect your own custom tests with the developer framework. These custom tests will be configurable and able to run programmatically, just like the rest of the developer framework libraries (roadmap item – Q3’2023).

### Do you include explainability-related testing and documentation? 

Our developer framework currently includes test kits to test and document global explainability features of the model, specifically, permutation feature importance and Shapley values.

<span id="testing-techniques" />

In addition, ValidMind is implementing standard documentation via the developer framework for the following items and modeling techniques:

- Conceptual soundness
    - Model use case description (Q2’2023)
    - Model selection rationale (Q2’2023)
- Data evaluation
    - Data quality metrics
    - Sampling method validation
    - Population distribution (PSI)
    - Correlations & interactions
    - Data lineage (Q3’2023)
    - Feature engineering (Q3’2023)
- Model Evaluation
    - Performance & accuracy evaluation
    - Goodness of fit (Q2’2023)
    - Stability & sensitivity to perturbations (Q3’2023)
    - Model robustness & weak regions (Q3’2023)
    - Global explainability - permutation feature importance, SHAP
    - Local explainability- LIME (Q3’2023)
    - Model testing at implementation / post-production (2024)
- Model techniques
    - Time series (ARIMA, Error correction)
    - Regression (OLS, Logistic, GLM, XGBoost)
    - Decision trees (tree-based ML models)
    - Random forests
    - K-means clustering (Q2 2023)
    - NLP (2024)
    - Deep learning (2024)
    - Computer vision (2024)

### Is there a use case for synthetic data on the platform?

ValidMind's developer framework supports you bringing your own datasets, including synthetic datasets, for testing and benchmarking purposes, such as for fair lending and bias testing.

We are happy to discuss exploring specific use cases for synthetic data generation with you further.
<!--- TO DO We need a contact email here --->

## Integrations and support

### Can you integrate with JIRA to connect with our Model Development pipeline?

ValidMind is planning to provide integration with JIRA tickets via the JIRA Python API. You will be able to configure ValidMind to update the status of a particular JIRA ticket when a specific state or approval is triggered from the workflow (roadmap item – Q3’2023).

### What libraries beyond XGBoost are supported?

ValidMind supports the most popular open-source model development libraries in Python and R, such as:

- scikit-learn
- XGBoost
- statsmodels
- PyTorch
- TensorFlow

ValidMind supports ingesting metrics and test results from your training and evaluation pipeline, such as using batch prediction or online prediction mechanisms. We are also implementing standard documentation via the developer framework for additional modeling techniques, check [Do you include explainability-related testing and documentation?](#testing-techniques) for more information.

### What other programming languages and development environments do you support beyond Python and Jupyter notebook, such as R and SAS?

ValidMind's developer framework is designed to be platform-agnostic and compatible with the most popular open-source programming languages and model development environments.

Currently, we support Python 3.8+ and the most popular AI/ML and data science libraries (scikit-learn, XGBoost, statsmodels, PyTorch, TensorFlow).

We are working on deploying support for R 4.0+ and associated libraries (roadmap item – Q2’2023).

Support for commercial and closed-source programming languages such as SAS and Matlab depends on specific deployment details and commercial agreements with customers.

### Do you support integration with data lakes and ETL solutions?

Support for connecting to data lakes and data processing or ETL pipelines is on our roadmap (Q3’2023+).

We will be implementing connector interfaces allowing extraction of relationships between raw data sources and final post-processed datasets for preloaded session instances received from Spark and Snowflake.

### Which model development packages/libraries are supported by the developer framework? What about complex/distributed models built with TensorFlow?

ValidMind supports the most popular open-source model development libraries in Python and R, such as:

- scikit-learn
- XGBoost
- statsmodels
- PyTorch
- TensorFlow

For distributed training pipelines built with frameworks like TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object, if the framework is imported from within the pipeline's code. ValidMind can also ingest metrics and test results from the customer's training or evaluation pipeline, using batch prediction or online prediction mechanisms.

### Is it possible for us to integrate the tool with LLMs like GPT-3?

ValidMind is integrating LLMs tools into our documentation features, enabling the following documentation features:

- Generating content recommendations (or “starting points”) for model developers for specific sections of the documentation, based on historical documentations (roadmap item — Q3’2023).
- Providing insights to model developers and model reviewers on possible model risks, and mitigation actions/improvements to the model, based on historical model documentations (roadmap item currently in research – not scheduled).

### Can you handle more sophisticated AI/ML libraries such as Pytorch, TensorFlow?

ValidMind supports the most popular open-source model development libraries in Python, R, such as :

- scikit-learn
- XGBoost
- statsmodels
- PyTorch
- TensorFlow

For distributed training pipelines built with frameworks, such as TensorFlow, ValidMind can directly access the trained model instance to extract metadata stored in the model object if the framework is imported from within the pipeline’s code.
ValidMind can also ingest metrics and test results from the customer’s training/evaluation pipeline, such as using batch prediction or online prediction mechanisms.

### Does ValidMind support data dictionaries?

You can pass a data dictionary to ValidMind via the developer framework, such as in CSV format.

## Data handling and privacy

### How are users added to ValidMind?

ValidMind provides a built-in user management interface that allows new users to be registered on the platform and assigned user roles. User roles and access permissions are configured during initial onboarding. In addition, ValidMind also provides support for Single Sign-On (SSO) integration as part of our Enterprise and our Virtual Private ValidMind (VPV) edition.

### How does ValidMind handle end-user computing and spreadsheet models?

Customers can register spreadsheet models in the model inventory and centralize tracking of the associated documentation files with the inventory metadata (roadmap item – Q3’2023). However, ValidMind cannot automate documentation generation for spreadsheet models.

### What model artifacts are automatically imported into documentation and how are they retained?

ValidMind stores the following artifacts in the documentation via our API:

- Dataset and model metadata which allow generating documentation snippets programmatically (example: stored definition for "common logistic regression limitations" when a logistic regression model has been passed to the ValidMind test plan execution)
- Quality and performance metrics collected from the dataset and model
- Outputs from executed test plans
- Images, plots, and visuals generated as part of extracting metrics and running tests

ValidMind is a multi-tenant solution hosted on AWS. For organizations requiring the highest degree of data security, ValidMind offers a "Virtual Private ValidMind" option to host the solution in a dedicated single-tenant cloud instance on the ValidMind AWS account. Furthermore, ValidMind's data retention policy complies with the SOC 2 security standard.

### How does ValidMind handle large datasets? What about the confidentiality of data sent to ValidMind?

ValidMind does not send datasets outside the client's environment. The developer framework executes test plans and functions locally in your environment and is not limited by dataset size.

Additionally, ValidMind adheres to a strict data confidentiality and retention policy, compliant with the SOC 2 security standard.

### What solutions do you offer and how do you handle privacy?

ValidMind is a SaaS platform and developer framework available in multiple editions catering to different organizational needs:

- **Standard Edition**: Our introductory offering, providing essential features and services.
- **Enterprise Edition**: Builds upon the Standard Edition by adding features tailored for large-scale organizations.
- **Virtual Private ValidMind (VPV)**: Our most secure offering for organizations requiring a higher level of privacy, such as financial services handling sensitive data. Includes all Enterprise Edition features but in a separate, isolated ValidMind environment. VPV accounts do not share resources with accounts outside the VPV.

Access to any SaaS edition is facilitated through AWS PrivateLink, which provides private connectivity between ValidMind and your on-premises networks without exposing your traffic to the public internet. To learn more, check [Configure AWS PrivateLink](configure-aws-privatelink.qmd). ValidMind does not send any personally identifiable information (PII) through our API.

### Can the tool automatically document other non-standard ETL steps or performance metrics from notebooks?

Support for more complex data processing pipelines is on our roadmap, currently scheduled for Q4'2023. We are implementing connector interfaces that will allow us to extract relationships between raw data sources and final post-processed datasets for Spark and Snowflake.

### How does the tool manage model changes?

ValidMind allows model developers to re-run documentation functions with the developer framework to capture changes in the model, such as changes in the number of features or hyperparameters.

After a model developer has made a change in their development environment, such as to a Jupyter notebook, they can execute the relevant ValidMind documentation function to update the corresponding documentation section. ValidMind will then automatically recreate the relevant figures and tables and update them in the online documentation.

ValidMind is currently working on a version history function, which will allow users to see the history of changes made to the documentation.

### Can you accommodate Spark DataFrames?

Our developer framework can extract dataset quality metrics on Pandas DataFrame, NumPy arrays, or Spark DataFrame instances using standard metrics provided by popular open-source frameworks such as scikit-learn, statsmodels, and more.
Each test defines a mapping to the different supported dataset and/or model interfaces: when passing a Spark DataFrame, our framework will directly call native evaluation metrics provided by the SparkML API or custom ones built by the developer (such as via UDFs).
