{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate release notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook guides you through the process of creating release notes. Using `generate_release_objects.py` and the OpenAI API, we are able to automate the release notes authoring process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents<a id='toc0_'></a>    \n",
    "- [Import necessary libraries](#toc1_)    \n",
    "- [Set up OpenAI API](#toc2_)    \n",
    "- [Set labels](#toc3_)    \n",
    "- [Collect GitHub URLs](#toc4_)    \n",
    "- [Set the release date](#toc5_)    \n",
    "- [Create release folder](#toc6_)    \n",
    "- [Start writing to release notes file](#toc7_)    \n",
    "- [Set up release notes components](#toc8_)    \n",
    "- [Set the repository and tag name](#toc9_)    \n",
    "- [Extract PRs from each URL](#toc10_)    \n",
    "- [Load PR data](#toc11_)    \n",
    "- [Compile and edit release notes body](#toc12_)    \n",
    "- [Edit each title](#toc13_)    \n",
    "- [Set labels for each PR](#toc14_)    \n",
    "- [Assign PR details to PR](#toc15_)    \n",
    "- [Combine all PR data into the same release notes components](#toc16_)    \n",
    "- [Write release notes to file](#toc17_)    \n",
    "- [Update sidebar](#toc18_)    \n",
    "- [Show files to commit](#toc19_)    \n",
    "- [Preview and edit changes](#toc20_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=4\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "You should be on a separate branch associated to the story for the release notes. See [our release notes guide](https://www.notion.so/validmind/On-release-notes-20de4e7ea03f402587514f6c9eda3bb1) for the steps needed before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>Import necessary libraries [](#toc0_)\n",
    "\n",
    "This cell imports any dependencies and some functions from `generate_release_objects.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import datetime\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from generate_release_objects import ReleaseURL, PR\n",
    "from generate_release_objects import get_release_date, write_prs_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>Set up OpenAI API [](#toc0_)\n",
    "\n",
    "Running this cell grabs your OpenAI API secret key from your `.env` file. If the relative path to your `.env` file is not `../.env`, change it to your relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_openai_api():\n",
    "    \"\"\"Loads .env file and updates the OpenAI API key. \n",
    "    \n",
    "    Replace '../.env' with the relative path to your .env file.\n",
    "\n",
    "    Modifies:\n",
    "        openai.api_key\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv('../.env') # replace to match your correct path\n",
    "\n",
    "    # Get the OpenAI API key\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        raise EnvironmentError(\"OpenAI API key is not set in .env file.\")\n",
    "\n",
    "    # Set the API key for the OpenAI library\n",
    "    openai.api_key = api_key\n",
    "\n",
    "setup_openai_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>Set labels [](#toc0_)\n",
    "\n",
    "This cell creates the main sections of the release notes. `label_hierarchy` shows the order in which updates will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_category = {\n",
    "    \"highlight\": \"## Release highlights\",\n",
    "    \"enhancement\": \"## Enhancements\",\n",
    "    \"deprecation\": \"## Deprecations\",\n",
    "    \"bug\": \"## Bug fixes\",\n",
    "    \"documentation\": \"## Documentation\"\n",
    "}\n",
    "\n",
    "categories = { \n",
    "    \"highlight\": [],\n",
    "    \"enhancement\": [],\n",
    "    \"deprecation\": [],\n",
    "    \"bug\": [],\n",
    "    \"documentation\": []\n",
    "}\n",
    "\n",
    "label_hierarchy = [\"highlight\", \"deprecation\", \"bug\", \"enhancement\", \"documentation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>Collect GitHub URLs [](#toc0_)\n",
    "\n",
    "Running this cell will prompt you to enter your GitHub release URLs. Keep pasting them in until you're done, then press enter again.\n",
    "\n",
    "Example release URL: https://github.com/validmind/documentation/releases/tag/v2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_github_urls(): \n",
    "    \"\"\"Collects release URLs from user.\n",
    "\n",
    "    Returns:\n",
    "        List[ReleaseURL]: A list of ReleaseURL objects\n",
    "\n",
    "    Exits:\n",
    "        If the user presses enter and no URLs were entered\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    while True:\n",
    "        url = input(\"Enter a full GitHub release URL (leave empty to finish): \")\n",
    "        if not url:\n",
    "            if not urls:  # Check if no URLs have been added yet\n",
    "                print(\"Error: You must specify at least one full GitHub release URL.\")\n",
    "                exit(1)  # Exit the script with an error code\n",
    "            break\n",
    "        urls.append(ReleaseURL(url))\n",
    "    return urls \n",
    "\n",
    "github_urls = collect_github_urls() # the only big global variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>Set the release date [](#toc0_)\n",
    "Running this cell will prompt you to enter the desired release date. \n",
    "The default is 3 business days from today if you leave the prompt empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "release_datetime = get_release_date()\n",
    "formatted_release_date = release_datetime.strftime(\"%Y-%b-%d\").lower()\n",
    "original_release_date = release_datetime.strftime(\"%B %-d, %Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>Create release folder [](#toc0_)\n",
    "\n",
    "These lines will create a folder inside of `~/site/releases` for the release notes. The folder name is the release date, as per our convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory_path = f\"../site/releases/{formatted_release_date}/\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "output_file = f\"{directory_path}release-notes.qmd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>Start writing to release notes file [](#toc0_)\n",
    "This block writes the title of the release notes into the final release notes file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating & editing release notes ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Generating & editing release notes ...\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(f\"---\\ntitle: \\\"{original_release_date}\\\"\\n---\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_'></a>Set up release notes components [](#toc0_)\n",
    "`release_components` will contain all the components of the release notes in the form of a dictionary. Later, we will merge everything together to create the release notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_components = dict()\n",
    "release_components.update(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_'></a>Set the repository and tag name [](#toc0_)\n",
    "This block checks every URL and assigns its repo name, such as `documentation` or `backend`, and its tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in github_urls:\n",
    "    url.set_repo_and_tag_name() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc10_'></a>Extract PRs from each URL [](#toc0_)\n",
    "This block finds all the pull requests from each URL and stores them somewhere safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansi_escape = re.compile(r'\\x1B\\[[0-?]*[ -/]*[@-~]')\n",
    "\n",
    "def notebook_extract_prs(self):\n",
    "    \"\"\"Extracts PRs from the release URL.\n",
    "\n",
    "    Modifies:\n",
    "        self.prs\n",
    "        self.data_json\n",
    "    \"\"\"\n",
    "    cmd_release = ['gh', 'api', f'repos/{self.repo_name}/releases/tags/{self.tag_name}']\n",
    "    result_release = subprocess.run(cmd_release, capture_output=True, text=True)\n",
    "    output_release = result_release.stdout.strip()\n",
    "\n",
    "    output_release_clean = ansi_escape.sub('', output_release)\n",
    "\n",
    "    try:\n",
    "        self.data_json = json.loads(output_release_clean)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Unable to parse release data for URL '{self.url}'.\")      \n",
    "    \n",
    "    if 'body' in self.data_json:\n",
    "        body = self.data_json['body']\n",
    "        pr_numbers = re.findall(r\"https://github\\.com/.+/pull/(\\d+)\", body)\n",
    "\n",
    "        for pr_number in pr_numbers: # initialize PR objects using pr_numbers and add to list of PRs\n",
    "            curr_PR = PR(self.repo_name, pr_number)\n",
    "            self.prs.append(curr_PR)\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: No body found in release data for URL '{self.url}'.\")\n",
    "\n",
    "ReleaseURL.extract_prs = notebook_extract_prs\n",
    "\n",
    "for url in github_urls:\n",
    "    url.extract_prs() # initializes PR objects into a list for each URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc11_'></a>Load PR data [](#toc0_)\n",
    "\n",
    "Using the JSON data from the PRs, this block extracts and stores data from each PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_load_data_json(self):\n",
    "    \"\"\"Loads the JSON data from a PR to self.data_json, sets to None if any labels are 'internal'\n",
    "\n",
    "    Modifies:\n",
    "        self.data_json\n",
    "    \"\"\"\n",
    "    cmd = ['gh', 'pr', 'view', self.pr_number, '--json', 'title,body,url,labels', '--repo', self.repo_name]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    output = result.stdout.strip()\n",
    "\n",
    "    output_clean = ansi_escape.sub('', output)\n",
    "\n",
    "    try:\n",
    "        self.data_json = json.loads(output_clean)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Unable to parse PR data for PR number {self.pr_number} in repository {self.repo_name}.\")\n",
    "        return None\n",
    "    \n",
    "    if any(label['name'] == 'internal' for label in self.data_json['labels']):\n",
    "        self.data_json = None  # Ignore PRs with the 'internal' label\n",
    "\n",
    "PR.load_data_json = notebook_load_data_json\n",
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        pr.load_data_json() # loads json file into object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc12_'></a>Compile and edit release notes body [](#toc0_)\n",
    "\n",
    "(20s)\n",
    "Using the prompt below, this block feeds the body of each PR to ChatGPT for editing. If you find that the output is not quite right, edit the prompt and play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_instructions_body = \"\"\"\n",
    "    Please edit the provided technical content according to the following guidelines:\n",
    "\n",
    "    - Use simple and neutral language in the active voice.\n",
    "    - Address users directly in the second person with \"you\".\n",
    "    - Use present tense by avoiding the use of \"will\".\n",
    "    - Apply sentence-style capitalization to text\n",
    "    - Always capitalize the first letter of text on each line.\n",
    "    - Rewrite sentences that are longer than 25 words as multiple sentences.\n",
    "    - Only split text across multiple lines if the text contains more than three sentences.\n",
    "    - Avoid handwaving references to \"it\" or \"this\" by including the text referred to. \n",
    "    - Treat short text of less than ten words without a period at the end as a heading. \n",
    "    - Enclose any words joined by underscores in backticks (`) if they aren't already.\n",
    "    - Remove exclamation marks from text.\n",
    "    - Remove quotes around non-code words.\n",
    "    - Remove the text \"feat:\" from the output\n",
    "    - Maintain existing punctuation at the end of sentences.\n",
    "    - Maintain all original hyperlinks for reference.\n",
    "    - Preserve all comments in the format <!--- COMMENT ---> as they appear in the text.\n",
    "    \"\"\"\n",
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json: \n",
    "            if pr.extract_external_release_notes(): pr.edit_text_with_openai(False, editing_instructions_body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Git diff\n",
    "\n",
    "Here, we will load the differences between the code for each PR, to be interpreted by ChatGPT later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_git_diff_notebook(self):\n",
    "    \"\"\"Fetches the git diff between the pr and the last commit and stores it in self.git_diff\n",
    "\n",
    "    Modifies:\n",
    "        self.git_diff\n",
    "    \"\"\"\n",
    "    cmd = ['gh', 'pr', 'diff', self.pr_number, '--repo', self.repo_name]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    output = result.stdout.strip()\n",
    "\n",
    "    output_clean = ansi_escape.sub('', output)\n",
    "\n",
    "    self.git_diff = output_clean\n",
    "    \n",
    "PR.load_git_diff = load_git_diff_notebook\n",
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json: \n",
    "            pr.load_git_diff()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI API to interpret the Git Diff\n",
    "Now, we'll have ChatGPT explain the code differences for each PR. \n",
    "\n",
    "If we like this version better, we can toggle the release notes to use this instead of the text generated from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pr\u001b[38;5;241m.\u001b[39mdata_json \u001b[38;5;129;01mand\u001b[39;00m pr\u001b[38;5;241m.\u001b[39mgit_diff: \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret_git_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_explain_instructions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ValidMind/documentation/release-scripts/generate_release_objects.py:94\u001b[0m, in \u001b[0;36mPR.interpret_git_diff\u001b[0;34m(self, diff_instructions)\u001b[0m\n\u001b[1;32m     92\u001b[0m explanations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m---> 94\u001b[0m     explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret_git_diff_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_instructions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     explanations\u001b[38;5;241m.\u001b[39mappend(explanation)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplained_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(explanations)\n",
      "File \u001b[0;32m~/Desktop/ValidMind/documentation/release-scripts/generate_release_objects.py:107\u001b[0m, in \u001b[0;36mPR.interpret_git_diff_chunk\u001b[0;34m(self, chunk, diff_instructions)\u001b[0m\n\u001b[1;32m    104\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_instructions\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "code_explain_instructions = \"\"\"\n",
    "I have the following git diff output from a pull request. \n",
    "Can you explain the changes in a way that is suitable for inclusion in release notes?\n",
    "The explanation should summarize what was modified, added, or removed, and the purpose of these changes.\n",
    "\"\"\"\n",
    "\n",
    "for url in github_urls:\n",
    "    # for pr in url.prs:\n",
    "    pr = url.prs[7]\n",
    "    if pr.data_json and pr.git_diff: \n",
    "        print(f\"yay\")\n",
    "        pr.interpret_git_diff(code_explain_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/notebooks/code_samples/time_series/quickstart_time_series_full_suite.ipynb b/notebooks/code_samples/time_series/quickstart_time_series_full_suite.ipynb\n",
      "new file mode 100644\n",
      "index 000000000..27bbd5e28\n",
      "--- /dev/null\n",
      "+++ b/notebooks/code_samples/time_series/quickstart_time_series_full_suite.ipynb\n",
      "@@ -0,0 +1,639 @@\n",
      "+{\n",
      "+ \"cells\": [\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"# Document a time series forecasting model\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Use the [FRED](https://fred.stlouisfed.org/) sample dataset to train a simple time series model and document that model with the ValidMind Developer Framework.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"As part of the notebook, you will learn how to train a simple model while exploring how the documentation process works:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"- Initializing the ValidMind Developer Framework\\n\",\n",
      "+    \"- Loading a sample dataset provided by the library to train a simple time series model\\n\",\n",
      "+    \"- Running a ValidMind test suite to quickly generate documentation about the data and model\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc0_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Contents    \\n\",\n",
      "+    \"- [About ValidMind](#toc1_)    \\n\",\n",
      "+    \"  - [Before you begin](#toc1_1_)    \\n\",\n",
      "+    \"  - [New to ValidMind?](#toc1_2_)    \\n\",\n",
      "+    \"  - [Key concepts](#toc1_3_)    \\n\",\n",
      "+    \"- [Install the client library](#toc2_)\\n\",\n",
      "+    \"  - [Get your code snippet](#toc3_1_)\\n\",\n",
      "+    \"- [Initialize the client library](#toc3_)    \\n\",\n",
      "+    \"- [Initialize the Python environment](#toc4_)    \\n\",\n",
      "+    \"  - [Preview the documentation template](#toc4_1_)    \\n\",\n",
      "+    \"- [Load the sample dataset](#toc5_)    \\n\",\n",
      "+    \"- [Document the model](#toc6_)    \\n\",\n",
      "+    \"  - [Prepocess the raw dataset](#toc6_1_)    \\n\",\n",
      "+    \"  - [Initialize the ValidMind datasets](#toc6_2_)    \\n\",\n",
      "+    \"  - [Initialize a model object](#toc6_3_)    \\n\",\n",
      "+    \"  - [Assign predictions to the datasets](#toc6_4_)    \\n\",\n",
      "+    \"  - [Run the full suite of tests](#toc6_5_)    \\n\",\n",
      "+    \"- [Next steps](#toc7_)    \\n\",\n",
      "+    \"  - [Work with your model documentation](#toc7_1_)    \\n\",\n",
      "+    \"  - [Discover more learning resources](#toc7_2_)    \\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<!-- vscode-jupyter-toc-config\\n\",\n",
      "+    \"\\tnumbering=false\\n\",\n",
      "+    \"\\tanchor=true\\n\",\n",
      "+    \"\\tflat=false\\n\",\n",
      "+    \"\\tminLevel=2\\n\",\n",
      "+    \"\\tmaxLevel=4\\n\",\n",
      "+    \"\\t/vscode-jupyter-toc-config -->\\n\",\n",
      "+    \"<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc1_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## About ValidMind\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"ValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<a id='toc1_1_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Before you begin\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \\n\",\n",
      "+    \"\\n\",\n",
      "+    \"If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<a id='toc1_2_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### New to ValidMind?\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"If you haven't already seen our [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html), we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<div class=\\\"alert alert-block alert-info\\\" style=\\\"background-color: #f7e4ee; color: black; border: 1px solid black;\\\">For access to all features available in this notebook, create a free ValidMind account.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Signing up is FREE — <a href=\\\"https://app.prod.validmind.ai\\\"><b>Sign up now</b></a></div>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<a id='toc1_3_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Key concepts\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Tests**: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Metrics**: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Custom metrics**: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Inputs**: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"  - **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\\n\",\n",
      "+    \"  - **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\\n\",\n",
      "+    \"  - **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\\n\",\n",
      "+    \"  - **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Outputs**: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Example: the [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases.\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc2_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Install the client library\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"The client library provides Python support for the ValidMind Developer Framework. To install it:\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"%pip install -q validmind\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc3_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Initialize the client library\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<a id='toc3_1_'></a>\\n\",\n",
      "+    \"### Get your code snippet\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"   For example, to register a model for use with this notebook, select:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"   - Documentation template: `Binary classification`\\n\",\n",
      "+    \"   - Use case: `Marketing/Sales - Attrition/Churn Management`\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"   You can fill in other options according to your preference.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"4. Go to **Getting Started** and click **Copy snippet to clipboard**.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Next, replace this placeholder with your own code snippet:\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"# Replace with your code snippet\\n\",\n",
      "+    \"import validmind as vm\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm.init(\\n\",\n",
      "+    \"  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "+    \"  api_key = \\\"...\\\",\\n\",\n",
      "+    \"  api_secret = \\\"...\\\",\\n\",\n",
      "+    \"  project = \\\"...\\\"\\n\",\n",
      "+    \")\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc4_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Initialize the Python environment\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Next, let's import the necessary libraries and set up your Python environment for data analysis:\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n",
      "+    \"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\\n\",\n",
      "+    \"from sklearn.metrics import mean_squared_error, r2_score\\n\",\n",
      "+    \"from sklearn.model_selection import train_test_split\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"%matplotlib inline\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc4_1_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Preview the documentation template\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"You will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the `vm.preview_template()` function from the ValidMind library and note the empty sections:\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"vm.preview_template()\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc5_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Load the sample dataset\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"The sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), a two-dimensional tabular data structure that makes use of rows and columns:\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"from validmind.datasets.regression import fred_timeseries \\n\",\n",
      "+    \"\\n\",\n",
      "+    \"target_column = fred_timeseries.target_column\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"print(\\n\",\n",
      "+    \"    f\\\"Loaded demo dataset with: \\\\n\\\\n\\\\t• Target column: '{target_column}'\\\"\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"raw_df = fred_timeseries.load_data()\\n\",\n",
      "+    \"raw_df.head()\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc6_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Document the model\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"As part of documenting the model with the ValidMind Developer Framework, you need to preprocess the raw dataset, initialize some training and test datasets, initialize a model object you can use for testing, and then run the full suite of tests.\\n\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc6_1_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Prepocess the raw dataset\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Preprocessing performs a number of operations to get ready for the subsequent steps:\\n\",\n",
      "+    \"- **Split the dataset**: Divide the original dataset into training and test sets for the primary model with an 80/20 split, without shuffling.\\n\",\n",
      "+    \"- **Difference the data**: Calculate the first difference of the train and test datasets to remove trends and seasonality, then drop any resulting NaN values.\\n\",\n",
      "+    \"- **Extract features and target variables**: Separate the feature columns (predictors) and the target variable from the differenced train and test datasets.\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"# Split the raw dataset into training and test sets \\n\",\n",
      "+    \"train_df, test_df = train_test_split(raw_df, test_size=0.2, shuffle=False)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Take the first difference of the training and test sets\\n\",\n",
      "+    \"train_diff_df = train_df.diff().dropna()\\n\",\n",
      "+    \"test_diff_df = test_df.diff().dropna()\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Extract the features and target variable from the training set\\n\",\n",
      "+    \"X_diff_train = train_diff_df.drop(target_column, axis=1)\\n\",\n",
      "+    \"y_diff_train = train_diff_df[target_column]\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Extract the features and target variable from the test set\\n\",\n",
      "+    \"X_diff_test = test_diff_df.drop(target_column, axis=1)\\n\",\n",
      "+    \"y_diff_test = test_diff_df[target_column]\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"### Train random forests and gradient boosting regressor models\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"This section trains random forest and gradient boosting models on differenced data, transforms predictions back to the original scale, and evaluates model performance using Mean Squared Error (MSE) and R-squared (R²) scores. \\n\",\n",
      "+    \"\\n\",\n",
      "+    \"The following helper functions are used to post-process predictions and evaluate model performance:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"- `transform_to_levels`: Reconstructs the original values from differenced predictions by cumulatively summing them, starting from a given initial value.\\n\",\n",
      "+    \"- `evaluate_model`: Calculates the Mean Squared Error (MSE) and R-squared (R²) score to evaluate the accuracy of the predictions against the true values.\\n\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"def transform_to_levels(y_diff_pred, first_value=0): \\n\",\n",
      "+    \"    y_pred = [first_value]\\n\",\n",
      "+    \"    for pred in y_diff_pred:\\n\",\n",
      "+    \"        y_pred.append(y_pred[-1] + pred)\\n\",\n",
      "+    \"    return y_pred\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"def evaluate_model(y_true, y_pred):\\n\",\n",
      "+    \"    mse = mean_squared_error(y_true, y_pred)\\n\",\n",
      "+    \"    r2 = r2_score(y_true, y_pred)\\n\",\n",
      "+    \"    return mse, r2\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"# Fit the random forest model\\n\",\n",
      "+    \"model_rf = RandomForestRegressor(n_estimators=1500, random_state=0)\\n\",\n",
      "+    \"model_rf.fit(X_diff_train, y_diff_train)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Make predictions on the training and test sets\\n\",\n",
      "+    \"y_diff_train_pred = model_rf.predict(X_diff_train)\\n\",\n",
      "+    \"y_diff_test_pred = model_rf.predict(X_diff_test)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Transform the predictions back to the original scale\\n\",\n",
      "+    \"y_train_rf_pred = transform_to_levels(y_diff_train_pred, first_value=train_df[target_column].iloc[0])\\n\",\n",
      "+    \"y_test_rf_pred = transform_to_levels(y_diff_test_pred, first_value=test_df[target_column].iloc[0])\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Evaluate the model's performance on the training and test sets\\n\",\n",
      "+    \"mse_train, r2_train = evaluate_model(train_df[target_column], y_train_rf_pred)\\n\",\n",
      "+    \"mse_test, r2_test = evaluate_model(test_df[target_column], y_test_rf_pred)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"print(f\\\"Train Mean Squared Error: {mse_train}\\\")\\n\",\n",
      "+    \"print(f\\\"Train R-Squared: {r2_train}\\\")\\n\",\n",
      "+    \"print(f\\\"Test Mean Squared Error: {mse_test}\\\")\\n\",\n",
      "+    \"print(f\\\"Test R-Squared: {r2_test}\\\")\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"# Fit the gradient boost model\\n\",\n",
      "+    \"model_gb = GradientBoostingRegressor(n_estimators=1500, random_state=0)\\n\",\n",
      "+    \"model_gb.fit(X_diff_train, y_diff_train)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Make predictions on the training and test sets\\n\",\n",
      "+    \"y_diff_train_pred = model_gb.predict(X_diff_train)\\n\",\n",
      "+    \"y_diff_test_pred = model_gb.predict(X_diff_test)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Transform the predictions back to the original scale\\n\",\n",
      "+    \"y_train_gb_pred = transform_to_levels(y_diff_train_pred, first_value=train_df[target_column].iloc[0])\\n\",\n",
      "+    \"y_test_gb_pred = transform_to_levels(y_diff_test_pred, first_value=test_df[target_column].iloc[0])\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"# Evaluate the model's performance on the training and test sets\\n\",\n",
      "+    \"mse_train, r2_train = evaluate_model(train_df[target_column], y_train_gb_pred)\\n\",\n",
      "+    \"mse_test, r2_test = evaluate_model(test_df[target_column], y_test_gb_pred)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"print(f\\\"Train Mean Squared Error: {mse_train}\\\")\\n\",\n",
      "+    \"print(f\\\"Train R-Squared: {r2_train}\\\")\\n\",\n",
      "+    \"print(f\\\"Test Mean Squared Error: {mse_test}\\\")\\n\",\n",
      "+    \"print(f\\\"Test R-Squared: {r2_test}\\\")\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc6_2_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Initialize the ValidMind datasets\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"This function takes a number of arguments:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"- `dataset` — the raw dataset that you want to provide as input to tests\\n\",\n",
      "+    \"- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\\n\",\n",
      "+    \"- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"With all dataframes ready, you can now initialize the ValidMind datasets objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"- `vm_raw_ds`: contains the raw, unprocessed data with the specified target column.\\n\",\n",
      "+    \"- `vm_train_diff_ds`: contains the training data with the differenced target column, excluding the first row to remove NaN values caused by differencing.\\n\",\n",
      "+    \"- `vm_test_diff_ds`: contains the test data with the differenced target column, excluding the first row to remove NaN values caused by differencing.\\n\",\n",
      "+    \"- `vm_train_ds`:  contains the training data, excluding the first row to align with the differenced data.\\n\",\n",
      "+    \"- `vm_test_ds`: includes the test data split from the raw dataset.\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"vm_raw_ds = vm.init_dataset(\\n\",\n",
      "+    \"    input_id=\\\"raw_ds\\\",\\n\",\n",
      "+    \"    dataset=raw_df,\\n\",\n",
      "+    \"    target_column=target_column,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_train_diff_ds = vm.init_dataset(\\n\",\n",
      "+    \"    input_id=\\\"train_diff_ds\\\",\\n\",\n",
      "+    \"    dataset=train_diff_df,\\n\",\n",
      "+    \"    target_column=target_column,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_test_diff_ds = vm.init_dataset(\\n\",\n",
      "+    \"    input_id=\\\"test_diff_ds\\\",\\n\",\n",
      "+    \"    dataset=test_diff_df,\\n\",\n",
      "+    \"    target_column=target_column,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_train_ds = vm.init_dataset(\\n\",\n",
      "+    \"    input_id=\\\"train_ds\\\",\\n\",\n",
      "+    \"    dataset=train_df,\\n\",\n",
      "+    \"    target_column=target_column,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_test_ds = vm.init_dataset(\\n\",\n",
      "+    \"    input_id=\\\"test_ds\\\",\\n\",\n",
      "+    \"    dataset=test_df,\\n\",\n",
      "+    \"    target_column=target_column,\\n\",\n",
      "+    \")\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc6_3_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Initialize the model objects\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Additionally, you need to initialize a ValidMind model object (`vm_model`) for each model, that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"vm_model_rf = vm.init_model(\\n\",\n",
      "+    \"    model_rf,\\n\",\n",
      "+    \"    input_id=\\\"random_forests_model\\\",\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_model_gb = vm.init_model(\\n\",\n",
      "+    \"    model_gb,\\n\",\n",
      "+    \"    input_id=\\\"gradient_boosting_model\\\",\\n\",\n",
      "+    \")\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc6_4_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Assign predictions to the datasets\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"We can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\\n\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"vm_train_ds.assign_predictions(\\n\",\n",
      "+    \"    model=vm_model_rf,\\n\",\n",
      "+    \"    prediction_values=y_train_rf_pred,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_test_ds.assign_predictions(\\n\",\n",
      "+    \"    model=vm_model_rf,\\n\",\n",
      "+    \"    prediction_values=y_test_rf_pred,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_train_ds.assign_predictions(\\n\",\n",
      "+    \"    model=vm_model_gb,\\n\",\n",
      "+    \"    prediction_values=y_train_gb_pred,\\n\",\n",
      "+    \")\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"vm_test_ds.assign_predictions(\\n\",\n",
      "+    \"    model=vm_model_gb,\\n\",\n",
      "+    \"    prediction_values=y_test_gb_pred,\\n\",\n",
      "+    \")\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc6_5_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Run the full suite of tests\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"This is where it all comes together: you are now ready to run the documentation tests for the model as defined by the documentation template you looked at earlier.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"The [`vm.run_documentation_tests`](https://docs.validmind.ai/validmind/validmind.html#run_documentation_tests) function finds and runs every test specified in the template and then uploads all the documentation and test artifacts that get generated to the ValidMind AI Risk Platform.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"The function requires information about the inputs to use on every test. These inputs can be passed as an `inputs` argument if we want to use the same inputs for all tests. It's also possible to pass a `config` argument that has information about the `params` and `inputs` that each test requires. The `config` parameter is a dictionary with the following structure:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"```python\\n\",\n",
      "+    \"config = {\\n\",\n",
      "+    \"    \\\"<test-id>\\\": {\\n\",\n",
      "+    \"        \\\"params\\\": {\\n\",\n",
      "+    \"            \\\"param1\\\": \\\"value1\\\",\\n\",\n",
      "+    \"            \\\"param2\\\": \\\"value2\\\",\\n\",\n",
      "+    \"            ...\\n\",\n",
      "+    \"        },\\n\",\n",
      "+    \"        \\\"inputs\\\": {\\n\",\n",
      "+    \"            \\\"input1\\\": \\\"value1\\\",\\n\",\n",
      "+    \"            \\\"input2\\\": \\\"value2\\\",\\n\",\n",
      "+    \"            ...\\n\",\n",
      "+    \"        }\\n\",\n",
      "+    \"    },\\n\",\n",
      "+    \"    ...\\n\",\n",
      "+    \"}\\n\",\n",
      "+    \"```\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Each `<test-id>` above corresponds to the test driven block identifiers shown by `vm.preview_template()`. For this model, we will use the default parameters for all tests, but we'll need to specify the input configuration for each one. The method `get_demo_test_config()` below constructs the default input configuration for our demo.\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"from validmind.utils import preview_test_config\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"test_config = fred_timeseries.get_demo_test_config()\\n\",\n",
      "+    \"preview_test_config(test_config)\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"Now we can pass the input configuration to `vm.run_documentation_tests()` and run the full suite of tests. The variable `full_suite` then holds the result of these tests.\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": [\n",
      "+    \"full_suite = vm.run_documentation_tests(config=test_config)\"\n",
      "+   ]\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"markdown\",\n",
      "+   \"metadata\": {},\n",
      "+   \"source\": [\n",
      "+    \"<a id='toc7_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"## Next steps\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"You can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<a id='toc7_1_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Work with your model documentation\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"1. From the [**Model Inventory**](https://app.prod.validmind.ai/model-inventory) in the ValidMind Platform UI, go to the model you registered earlier.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"2. Click and expand the **Model Development** section.\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"What you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it's ready. [Learn more ...](https://docs.validmind.ai/guide/working-with-model-documentation.html)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"<a id='toc7_2_'></a>\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"### Discover more learning resources\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"We offer many interactive notebooks to help you document models:\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"- [Run tests & test suites](https://docs.validmind.ai/guide/testing-overview.html)\\n\",\n",
      "+    \"- [Code samples](https://docs.validmind.ai/guide/samples-jupyter-notebooks.html)\\n\",\n",
      "+    \"\\n\",\n",
      "+    \"Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind.\"\n",
      "+   ]\n",
      "+  }\n",
      "+ ],\n",
      "+ \"metadata\": {\n",
      "+  \"kernelspec\": {\n",
      "+   \"display_name\": \"validmind-eEL8LtKG-py3.10\",\n",
      "+   \"language\": \"python\",\n",
      "+   \"name\": \"python3\"\n",
      "+  },\n",
      "+  \"language_info\": {\n",
      "+   \"codemirror_mode\": {\n",
      "+    \"name\": \"ipython\",\n",
      "+    \"version\": 3\n",
      "+   },\n",
      "+   \"file_extension\": \".py\",\n",
      "+   \"mimetype\": \"text/x-python\",\n",
      "+   \"name\": \"python\",\n",
      "+   \"nbconvert_exporter\": \"python\",\n",
      "+   \"pygments_lexer\": \"ipython3\",\n",
      "+   \"version\": \"3.10.13\"\n",
      "+  }\n",
      "+ },\n",
      "+ \"nbformat\": 4,\n",
      "+ \"nbformat_minor\": 2\n",
      "+}\n",
      "diff --git a/notebooks/code_samples/time_series/tutorial_time_series_forecasting.ipynb b/notebooks/code_samples/time_series/tutorial_time_series_forecasting.ipynb\n",
      "deleted file mode 100644\n",
      "index 3a6d8fae9..000000000\n",
      "--- a/notebooks/code_samples/time_series/tutorial_time_series_forecasting.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,718 +0,0 @@\n",
      "-{\n",
      "- \"cells\": [\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"# Document a time series forecasting model\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Automatically document time series forecasting models by running the test suite for time series datasets.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"This interactive notebook shows you how to use the ValidMind Developer Framework to import and prepare data and before running a data validation test suite, followed by loading a pre-trained model and running a model validation test suite.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"As part of the notebook, you will learn how to:\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"- Import raw data\\n\",\n",
      "-    \"- Run data validation test suite on raw data\\n\",\n",
      "-    \"- Preprocess data\\n\",\n",
      "-    \"- Run data validation test suite on processed data\\n\",\n",
      "-    \"- Load pre-trained models\\n\",\n",
      "-    \"- Run model validation test suite on models\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## Contents\\n\",\n",
      "-    \"- [About ValidMind](#toc1_)    \\n\",\n",
      "-    \"  - [Before you begin](#toc1_1_)    \\n\",\n",
      "-    \"  - [New to ValidMind?](#toc1_2_)    \\n\",\n",
      "-    \"  - [Key concepts](#toc1_3_)    \\n\",\n",
      "-    \"- [Before you begin](#toc2_)    \\n\",\n",
      "-    \"  - [New to ValidMind?](#toc2_1_)    \\n\",\n",
      "-    \"- [Install the client library](#toc3_)    \\n\",\n",
      "-    \"- [Initialize the client library](#toc4_)    \\n\",\n",
      "-    \"    - [Explore available test suites](#toc4_1_1_)    \\n\",\n",
      "-    \"- [Import raw data](#toc5_)    \\n\",\n",
      "-    \"  - [Import FRED dataset](#toc5_1_)    \\n\",\n",
      "-    \"- [Run data validation test suite on raw data](#toc6_)    \\n\",\n",
      "-    \"  - [Explore the time series dataset test suites](#toc6_1_)    \\n\",\n",
      "-    \"  - [Initialize the dataset](#toc6_2_)    \\n\",\n",
      "-    \"  - [Run time series dataset test suite on raw dataset](#toc6_3_)    \\n\",\n",
      "-    \"- [Preprocess data](#toc7_)    \\n\",\n",
      "-    \"  - [Handle frequencies, missing values and stationairty](#toc7_1_)    \\n\",\n",
      "-    \"- [Run data validation test suite on processed data](#toc8_)    \\n\",\n",
      "-    \"- [Load pre-trained models](#toc9_)    \\n\",\n",
      "-    \"  - [Load pre-trained models](#toc9_1_)    \\n\",\n",
      "-    \"  - [Initialize ValidMind models](#toc9_2_)    \\n\",\n",
      "-    \"- [Run model validation test suite on models](#toc10_)    \\n\",\n",
      "-    \"  - [Explore the time series model validation test suite](#toc10_1_)    \\n\",\n",
      "-    \"  - [Run model validation test suite on a list of models](#toc10_2_)    \\n\",\n",
      "-    \"- [Next steps](#toc11_)    \\n\",\n",
      "-    \"  - [Work with your model documentation](#toc11_1_)    \\n\",\n",
      "-    \"  - [Discover more learning resources](#toc11_2_)    \\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<!-- vscode-jupyter-toc-config\\n\",\n",
      "-    \"\\tnumbering=false\\n\",\n",
      "-    \"\\tanchor=true\\n\",\n",
      "-    \"\\tflat=false\\n\",\n",
      "-    \"\\tminLevel=2\\n\",\n",
      "-    \"\\tmaxLevel=4\\n\",\n",
      "-    \"\\t/vscode-jupyter-toc-config -->\\n\",\n",
      "-    \"<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## About ValidMind\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"ValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<a id='toc1_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Before you begin\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \\n\",\n",
      "-    \"\\n\",\n",
      "-    \"If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<a id='toc1_2_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### New to ValidMind?\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"If you haven't already seen our [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html), we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<div class=\\\"alert alert-block alert-info\\\" style=\\\"background-color: #f7e4ee; color: #222425; border: 1px solid #222425;\\\">For access to all features available in this notebook, create a free ValidMind account.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Signing up is FREE — <a href=\\\"https://app.prod.validmind.ai\\\"><b>Sign up now</b></a></div>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<a id='toc1_3_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Key concepts\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Tests**: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Metrics**: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Custom metrics**: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Inputs**: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"  - **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\\n\",\n",
      "-    \"  - **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\\n\",\n",
      "-    \"  - **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\\n\",\n",
      "-    \"  - **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Outputs**: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Example: the [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases.\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc2_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Before you begin\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<div class=\\\"alert alert-block alert-info\\\" style=\\\"background-color: #f7e4ee; color: #222425; border: 1px solid #222425;\\\">For access to all features available in this notebook, create a free ValidMind account.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Signing up is FREE — <a href=\\\"https://app.prod.validmind.ai\\\"><b>Sign up now</b></a></div>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc3_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Install the client library\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"The client library provides Python support for the ValidMind Developer Framework. To install it:\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"%pip install -q validmind\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc4_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Initialize the client library\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Get your code snippet:\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"3. Enter the model details, making sure to select **Time Series Forecasting** as the template and **Credit Risk - Underwriting - Loan** as the use case, and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"4. Go to **Getting Started** and click **Copy snippet to clipboard**.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Next, replace this placeholder with your own code snippet:\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Replace with your code snippet\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"import validmind as vm\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"vm.init(\\n\",\n",
      "-    \"    api_host=\\\"...\\\",\\n\",\n",
      "-    \"    api_host=\\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-    \"    api_key=\\\"...\\\",\\n\",\n",
      "-    \"    api_secret=\\\"...\\\",\\n\",\n",
      "-    \"    project=\\\"...\\\"\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc4_1_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"#### Explore available test suites\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"In this notebook we will run a collection of test suites that are available in the ValidMind Developer Framework. Test suites group together a collection of tests that are relevant for a specific use case. In our case, we will run test different test suites for time series forecasting models. Once a test suite runs successfully, its results will be automatically uploaded to the ValidMind platform.\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"vm.test_suites.list_suites()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"For our example use case we will run the following test suites:\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"- `time_series_dataset`\\n\",\n",
      "-    \"- `time_series_model_validation`\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc5_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Import raw data\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc5_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Import FRED dataset\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Federal Reserve Economic Data, or FRED, is a comprehensive database maintained by the Federal Reserve Bank of St. Louis. It offers a wide array of economic data from various sources, including U.S. government agencies and international organizations. The dataset encompasses numerous economic indicators across various categories such as employment, consumer price indices, money supply, and gross domestic product, among others.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"FRED provides a valuable resource for researchers, policymakers, and anyone interested in understanding economic trends and conducting economic analysis. The platform also includes tools for data visualization, which can help users interpret complex economic data and identify trends over time.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"The following code snippet imports a sample FRED dataset into a Pandas dataframe:\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"from validmind.datasets.regression import fred as demo_dataset\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"target_column = demo_dataset.target_column\\n\",\n",
      "-    \"feature_columns = demo_dataset.feature_columns\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"df = demo_dataset.load_data()\\n\",\n",
      "-    \"df.tail(10)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc6_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Run data validation test suite on raw data\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc6_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Explore the time series dataset test suites\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Let's see what tests are included on each test suite:\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"vm.test_suites.describe_suite(\\\"time_series_data_quality\\\")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"vm.test_suites.describe_suite(\\\"time_series_univariate\\\")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc6_2_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Initialize the dataset\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Use the ValidMind Developer Framework to initialize the dataset object:\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-    \"    input_id=\\\"raw_dataset\\\",\\n\",\n",
      "-    \"    dataset=df,\\n\",\n",
      "-    \"    target_column=demo_dataset.target_column,\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc6_3_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Run time series dataset test suite on raw dataset\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Next, use the ValidMind Developer Framework to run the test suite for time series datasets:\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"config = {\\n\",\n",
      "-    \"    # TIME SERIES DATA QUALITY PARAMS\\n\",\n",
      "-    \"    \\\"validmind.data_validation.TimeSeriesOutliers\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"zscore_threshold\\\": 3\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.TimeSeriesMissingValues\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"min_threshold\\\": 2\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # TIME SERIES UNIVARIATE PARAMS\\n\",\n",
      "-    \"    \\\"validmind.data_validation.RollingStatsPlot\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"window_size\\\": 12\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.SeasonalDecompose\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"seasonal_model\\\": \\\"additive\\\"\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.AutoSeasonality\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"min_period\\\": 1,\\n\",\n",
      "-    \"            \\\"max_period\\\": 3\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.AutoStationarity\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"max_order\\\": 3,\\n\",\n",
      "-    \"            \\\"threshold\\\": 0.05\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.AutoAR\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"max_ar_order\\\": 2\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.AutoMA\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"max_ma_order\\\": 2\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # TIME SERIES MULTIVARIATE PARAMS\\n\",\n",
      "-    \"    \\\"validmind.data_validation.LaggedCorrelationHeatmap\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"target_col\\\": demo_dataset.target_column,\\n\",\n",
      "-    \"            \\\"independent_vars\\\": demo_dataset.feature_columns\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.data_validation.EngleGrangerCoint\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"threshold\\\": 0.05\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"}\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"full_suite = vm.run_test_suite(\\n\",\n",
      "-    \"    \\\"time_series_dataset\\\",\\n\",\n",
      "-    \"    inputs={\\n\",\n",
      "-    \"        \\\"dataset\\\": vm_dataset,\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    config=config,\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc7_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Preprocess data\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc7_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Handle frequencies, missing values and stationairty\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Sample frequencies to Monthly\\n\",\n",
      "-    \"resampled_df = df.resample(\\\"MS\\\").last()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"#  Remove all missing values\\n\",\n",
      "-    \"nona_df = resampled_df.dropna()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"#  Take the first different across all variables\\n\",\n",
      "-    \"preprocessed_df = nona_df.diff().dropna()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc8_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Run data validation test suite on processed data\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-    \"    input_id=\\\"preprocess_dataset\\\",\\n\",\n",
      "-    \"    dataset=preprocessed_df,\\n\",\n",
      "-    \"    target_column=demo_dataset.target_column,\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"full_suite = vm.run_test_suite(\\n\",\n",
      "-    \"    \\\"time_series_dataset\\\",\\n\",\n",
      "-    \"    inputs={\\\"dataset\\\": vm_dataset},\\n\",\n",
      "-    \"    config=config,\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc9_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Load pre-trained models\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc9_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Load pre-trained models\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"from validmind.datasets.regression import fred as demo_dataset\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"model_A, train_df_A, test_df_A = demo_dataset.load_model('fred_loan_rates_model_3')\\n\",\n",
      "-    \"model_B, train_df_B, test_df_B = demo_dataset.load_model('fred_loan_rates_model_4')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc9_2_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Initialize ValidMind models\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Initialize training and testing datasets for model A\\n\",\n",
      "-    \"vm_train_ds_A = vm.init_dataset(\\n\",\n",
      "-    \"    input_id=\\\"train_a_dataset\\\",\\n\",\n",
      "-    \"    dataset=train_df_A,\\n\",\n",
      "-    \"    target_column=demo_dataset.target_column\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"vm_test_ds_A = vm.init_dataset(\\n\",\n",
      "-    \"    input_id=\\\"test_a_dataset\\\",\\n\",\n",
      "-    \"    dataset=test_df_A,\\n\",\n",
      "-    \"    target_column=demo_dataset.target_column\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Initialize training and testing datasets for model B\\n\",\n",
      "-    \"vm_train_ds_B = vm.init_dataset(\\n\",\n",
      "-    \"    input_id=\\\"train_b_dataset\\\",\\n\",\n",
      "-    \"    dataset=train_df_B,\\n\",\n",
      "-    \"    target_column=demo_dataset.target_column\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"vm_test_ds_B = vm.init_dataset(\\n\",\n",
      "-    \"    input_id=\\\"test_b_dataset\\\",\\n\",\n",
      "-    \"    dataset=test_df_B,\\n\",\n",
      "-    \"    target_column=demo_dataset.target_column\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Initialize model A\\n\",\n",
      "-    \"vm_model_A = vm.init_model(\\n\",\n",
      "-    \"    input_id=\\\"model_a\\\",\\n\",\n",
      "-    \"    model=model_A,\\n\",\n",
      "-    \"\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"vm_train_ds_A.assign_predictions(model=vm_model_A)\\n\",\n",
      "-    \"vm_test_ds_A.assign_predictions(model=vm_model_A)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Initialize model B\\n\",\n",
      "-    \"vm_model_B = vm.init_model(\\n\",\n",
      "-    \"    input_id=\\\"model_b\\\",\\n\",\n",
      "-    \"    model=model_B,\\n\",\n",
      "-    \")\\n\",\n",
      "-    \"vm_train_ds_B.assign_predictions(model=vm_model_B)\\n\",\n",
      "-    \"vm_test_ds_B.assign_predictions(model=vm_model_B)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"models = [vm_model_A, vm_model_B]\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc10_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Run model validation test suite on models\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc10_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Explore the time series model validation test suite\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"vm.test_suites.describe_test_suite(\\\"time_series_model_validation\\\")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc10_2_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Run model validation test suite on a list of models\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"config = {\\n\",\n",
      "-    \"    \\\"validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"transformation\\\": \\\"integrate\\\",\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\\\": {\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"transformation\\\": \\\"integrate\\\",\\n\",\n",
      "-    \"            \\\"shocks\\\": [0.3],\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.model_validation.statsmodels.RegressionModelsPerformance\\\": {\\n\",\n",
      "-    \"        \\\"inputs\\\": {\\n\",\n",
      "-    \"            \\\"in_sample_datasets\\\": (vm_train_ds_A, vm_train_ds_B),\\n\",\n",
      "-    \"            \\\"out_of_sample_datasets\\\": (vm_test_ds_A, vm_test_ds_B),\\n\",\n",
      "-    \"            \\\"models\\\": models,\\n\",\n",
      "-    \"        },\\n\",\n",
      "-    \"        \\\"params\\\": {\\n\",\n",
      "-    \"            \\\"transformation\\\": \\\"integrate\\\",\\n\",\n",
      "-    \"            \\\"shocks\\\": [0.3],\\n\",\n",
      "-    \"        }\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels\\\": {\\n\",\n",
      "-    \"        \\\"inputs\\\": {\\n\",\n",
      "-    \"            \\\"datasets\\\": (vm_train_ds_A, vm_test_ds_A),\\n\",\n",
      "-    \"            \\\"models\\\": [vm_model_A],\\n\",\n",
      "-    \"        },\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    \\\"validmind.model_validation.statsmodels.RegressionModelSensitivityPlot\\\": {\\n\",\n",
      "-    \"        \\\"inputs\\\": {\\n\",\n",
      "-    \"            \\\"datasets\\\": (vm_train_ds_A, vm_test_ds_A),\\n\",\n",
      "-    \"            \\\"models\\\": [vm_model_A],\\n\",\n",
      "-    \"        },\\n\",\n",
      "-    \"    }\\n\",\n",
      "-    \"}\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"full_suite = vm.run_test_suite(\\n\",\n",
      "-    \"    \\\"time_series_model_validation\\\",\\n\",\n",
      "-    \"    inputs={\\n\",\n",
      "-    \"        \\\"dataset\\\": vm_train_ds_A,\\n\",\n",
      "-    \"        \\\"datasets\\\": (vm_train_ds_A, vm_test_ds_A),\\n\",\n",
      "-    \"        \\\"model\\\": vm_model_A,\\n\",\n",
      "-    \"        \\\"models\\\": models,\\n\",\n",
      "-    \"    },\\n\",\n",
      "-    \"    config=config,\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"<a id='toc11_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Next steps\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"You can look at the results of this test suite right in the notebook where you ran the code, as you would expect. But there is a better way — use the ValidMind platform to work with your model documentation.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<a id='toc11_1_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Work with your model documentation\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"1. From the [**Model Inventory**](https://app.prod.validmind.ai/model-inventory) in the ValidMind Platform UI, go to the model you registered earlier.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"2. Click and expand the **Model Development** section.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"What you see is the full draft of your model documentation in a more easily consumable version. From here, you can make qualitative edits to model documentation, view guidelines, collaborate with validators, and submit your model documentation for approval when it's ready. [Learn more ...](https://docs.validmind.ai/guide/working-with-model-documentation.html)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"<a id='toc11_2_'></a>\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"### Discover more learning resources\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"We offer many interactive notebooks to help you document models:\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"- [Run tests & test suites](https://docs.validmind.ai/guide/testing-overview.html)\\n\",\n",
      "-    \"- [Code samples](https://docs.validmind.ai/guide/samples-jupyter-notebooks.html)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"Or, visit our [documentation](https://docs.validmind.ai/) to learn more about ValidMind.\"\n",
      "-   ]\n",
      "-  }\n",
      "- ],\n",
      "- \"metadata\": {\n",
      "-  \"kernelspec\": {\n",
      "-   \"display_name\": \"validmind-1QuffXMV-py3.9\",\n",
      "-   \"language\": \"python\",\n",
      "-   \"name\": \"python3\"\n",
      "-  },\n",
      "-  \"language_info\": {\n",
      "-   \"codemirror_mode\": {\n",
      "-    \"name\": \"ipython\",\n",
      "-    \"version\": 3\n",
      "-   },\n",
      "-   \"file_extension\": \".py\",\n",
      "-   \"mimetype\": \"text/x-python\",\n",
      "-   \"name\": \"python\",\n",
      "-   \"nbconvert_exporter\": \"python\",\n",
      "-   \"pygments_lexer\": \"ipython3\",\n",
      "-   \"version\": \"3.8.13\"\n",
      "-  }\n",
      "- },\n",
      "- \"nbformat\": 4,\n",
      "- \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/loan_rates_forecast_poc.ipynb b/notebooks/code_sharing/time_series/loan_rates_forecast_poc.ipynb\n",
      "deleted file mode 100644\n",
      "index 343f91a31..000000000\n",
      "--- a/notebooks/code_sharing/time_series/loan_rates_forecast_poc.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,2351 +0,0 @@\n",
      "-{\n",
      "- \"cells\": [\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"# POC Loan Rates Forecast Model\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## 4.1. Data Engineering\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.1.1. Data Collection\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Setup\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# System libraries\\n\",\n",
      "-    \"import glob\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# ML libraries\\n\",\n",
      "-    \"import pandas as pd\\n\",\n",
      "-    \"import numpy as np\\n\",\n",
      "-    \"from scipy import stats\\n\",\n",
      "-    \"import statsmodels.api as sm\\n\",\n",
      "-    \"from statsmodels.tsa.seasonal import seasonal_decompose\\n\",\n",
      "-    \"from statsmodels.tsa.ar_model import AutoReg\\n\",\n",
      "-    \"from statsmodels.tsa.stattools import adfuller, kpss\\n\",\n",
      "-    \"from statsmodels.tsa.arima.model import ARIMA\\n\",\n",
      "-    \"from statsmodels.tsa.stattools import coint\\n\",\n",
      "-    \"from arch.unitroot import PhillipsPerron, DFGLS\\n\",\n",
      "-    \"import xgboost as xgb\\n\",\n",
      "-    \"from numpy import argmax\\n\",\n",
      "-    \"from sklearn.metrics import accuracy_score, precision_recall_curve\\n\",\n",
      "-    \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Plotting libraries\\n\",\n",
      "-    \"import matplotlib.pyplot as plt\\n\",\n",
      "-    \"import seaborn as sns\\n\",\n",
      "-    \"%matplotlib inline\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Load FRED Data\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"from validmind.datasets.regression import fred as demo_dataset\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"target_column = demo_dataset.target_column\\n\",\n",
      "-    \"feature_columns = demo_dataset.feature_columns\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Split the dataset into test and training\\n\",\n",
      "-    \"df = demo_dataset.load_data()\\n\",\n",
      "-    \"df.tail(10)\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Plot time series.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_time_series(df, cols_to_plot=None, title=''):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Plot multiple time-series in the same axes using seaborn.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param df: DataFrame with time-series data\\n\",\n",
      "-    \"    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\\n\",\n",
      "-    \"    :param title: Title of the plot, default is ''\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    if cols_to_plot is None:\\n\",\n",
      "-    \"        cols_to_plot = df.columns.tolist()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Create a new DataFrame with the columns to plot\\n\",\n",
      "-    \"    plot_df = df[cols_to_plot]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Set seaborn plot style\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Plot the time-series data\\n\",\n",
      "-    \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-    \"    for col in plot_df.columns:\\n\",\n",
      "-    \"        sns.lineplot(data=plot_df[col], label=col)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    plt.xlabel('Date')\\n\",\n",
      "-    \"    plt.ylabel('Value')\\n\",\n",
      "-    \"    plt.title(title)\\n\",\n",
      "-    \"    plt.legend()\\n\",\n",
      "-    \"    plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_time_series(df_diff[['GS10']], title='All Variables')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.1.2. Data Description\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"df.info()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.1.3. Data Quality\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Frequency of Series \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Distribution of frequencies in the data.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_time_difference_frequency(df):\\n\",\n",
      "-    \"    # Calculate the time differences between consecutive entries\\n\",\n",
      "-    \"    time_diff = df.index.to_series().diff().dropna()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Convert the time differences to a suitable unit (e.g., days)\\n\",\n",
      "-    \"    time_diff_days = time_diff.dt.total_seconds() / (60 * 60 * 24)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Create a DataFrame with the time differences\\n\",\n",
      "-    \"    time_diff_df = pd.DataFrame({'Time Differences (Days)': time_diff_days})\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Plot the frequency distribution of the time differences\\n\",\n",
      "-    \"    sns.histplot(data=time_diff_df, x='Time Differences (Days)', bins=50, kde=False)\\n\",\n",
      "-    \"    plt.xlabel('Time Differences (Days)')\\n\",\n",
      "-    \"    plt.ylabel('Frequency')\\n\",\n",
      "-    \"    plt.show()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_time_difference_frequency(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Identify frequencies for each variable.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def identify_frequencies(df):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Identify the frequency of each series in the DataFrame.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param df: Time-series DataFrame\\n\",\n",
      "-    \"    :return: DataFrame with two columns: 'Variable' and 'Frequency'\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    frequencies = []\\n\",\n",
      "-    \"    for column in df.columns:\\n\",\n",
      "-    \"        series = df[column].dropna()\\n\",\n",
      "-    \"        if not series.empty:\\n\",\n",
      "-    \"            freq = pd.infer_freq(series.index)\\n\",\n",
      "-    \"            if freq == 'MS' or freq == 'M':\\n\",\n",
      "-    \"                label = 'Monthly'\\n\",\n",
      "-    \"            elif freq == 'Q':\\n\",\n",
      "-    \"                label = 'Quarterly'\\n\",\n",
      "-    \"            elif freq == 'A':\\n\",\n",
      "-    \"                label = 'Yearly'\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                label = freq\\n\",\n",
      "-    \"        else:\\n\",\n",
      "-    \"            label = None\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        frequencies.append({'Variable': column, 'Frequency': label})\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    freq_df = pd.DataFrame(frequencies)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return freq_df\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"frequencies = identify_frequencies(df)\\n\",\n",
      "-    \"display(frequencies)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Handling frequencies.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"df = df.resample('MS').last()\\n\",\n",
      "-    \"frequencies = identify_frequencies(df)\\n\",\n",
      "-    \"display(frequencies)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Missing Values\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Identify Missing Values**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Total number of missing values.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_missing_values_bar(df):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Plot a bar chart displaying the total number of missing values per variable (column) in a time-series DataFrame using seaborn.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param df: Time-series DataFrame\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Calculate the total number of missing values per column\\n\",\n",
      "-    \"    missing_values = df.isnull().sum()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Set seaborn plot style\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Plot the bar chart\\n\",\n",
      "-    \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-    \"    sns.barplot(x=missing_values.index, y=missing_values.values)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    plt.xticks(rotation=45, ha='right')\\n\",\n",
      "-    \"    plt.xlabel('Variables (Columns)')\\n\",\n",
      "-    \"    plt.ylabel('Number of Missing Values')\\n\",\n",
      "-    \"    plt.title('Total Number of Missing Values per Variable')\\n\",\n",
      "-    \"    plt.show()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_missing_values_bar(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Heatmap of missing values.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_missing_values_heatmap(df, start_year=None, end_year=None):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Plot a heatmap of missing values with actual years in rows using seaborn.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param df: Time-series DataFrame\\n\",\n",
      "-    \"    :param start_year: Start year for zooming in, defaults to None\\n\",\n",
      "-    \"    :param end_year: End year for zooming in, defaults to None\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Filter the DataFrame based on the specified start_year and end_year\\n\",\n",
      "-    \"    if start_year:\\n\",\n",
      "-    \"        df = df[df.index.year >= start_year]\\n\",\n",
      "-    \"    if end_year:\\n\",\n",
      "-    \"        df = df[df.index.year <= end_year]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Create a boolean mask for missing values\\n\",\n",
      "-    \"    missing_mask = df.isnull()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Set seaborn plot style\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Plot the heatmap\\n\",\n",
      "-    \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-    \"    sns.heatmap(missing_mask.T, cmap='viridis', cbar=False, xticklabels=False)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Add actual years on the x-axis\\n\",\n",
      "-    \"    years = df.index.year.unique()\\n\",\n",
      "-    \"    xticks = [df.index.get_loc(df.index[df.index.year == year][0]) for year in years]\\n\",\n",
      "-    \"    plt.xticks(xticks, years, rotation=45, ha='right')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    plt.ylabel('Columns')\\n\",\n",
      "-    \"    plt.xlabel('Rows (Years)')\\n\",\n",
      "-    \"    plt.title('Missing Values Heatmap with Actual Years in Rows')\\n\",\n",
      "-    \"    plt.show()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_missing_values_heatmap(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Handling Missing Values**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Drop missing values.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"df = df.dropna()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_missing_values_bar(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_missing_values_heatmap(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Outliers\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Identify Outliers**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def identify_outliers(df, threshold=3):\\n\",\n",
      "-    \"    z_scores = pd.DataFrame(stats.zscore(df), index=df.index, columns=df.columns)\\n\",\n",
      "-    \"    outliers = z_scores[(z_scores.abs() > threshold).any(axis=1)]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    outlier_table = []\\n\",\n",
      "-    \"    for idx, row in outliers.iterrows():\\n\",\n",
      "-    \"        for col in df.columns:\\n\",\n",
      "-    \"            if abs(row[col]) > threshold:\\n\",\n",
      "-    \"                outlier_table.append({\\\"Variable\\\": col, \\\"z-score\\\": row[col], \\\"Threshold\\\": threshold, \\\"Date\\\": idx})\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return pd.DataFrame(outlier_table)\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"outliers_table = identify_outliers(df, threshold=3)\\n\",\n",
      "-    \"display(outliers_table)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Plot outliers.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"import pandas as pd\\n\",\n",
      "-    \"import seaborn as sns\\n\",\n",
      "-    \"import matplotlib.pyplot as plt\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"def plot_outliers(df, outliers_table, use_subplots=False):\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    if use_subplots:\\n\",\n",
      "-    \"        n_variables = len(df.columns)\\n\",\n",
      "-    \"        fig, axes = plt.subplots(n_variables, 1, figsize=(12, 3 * n_variables), sharex=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        for i, col in enumerate(df.columns):\\n\",\n",
      "-    \"            sns.lineplot(data=df, x=df.index, y=col, ax=axes[i], label=col)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            variable_outliers = outliers_table[outliers_table[\\\"Variable\\\"] == col]\\n\",\n",
      "-    \"            for idx, row in variable_outliers.iterrows():\\n\",\n",
      "-    \"                date = row[\\\"Date\\\"]\\n\",\n",
      "-    \"                outlier_value = df.loc[date, col]\\n\",\n",
      "-    \"                axes[i].scatter(date, outlier_value, marker=\\\"o\\\", s=100, c=\\\"red\\\", label=\\\"Outlier\\\" if idx == 0 else \\\"\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            axes[i].legend()\\n\",\n",
      "-    \"            axes[i].set_ylabel(\\\"Value\\\")\\n\",\n",
      "-    \"            axes[i].set_title(f\\\"Time Series with Outliers for {col}\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        plt.xlabel(\\\"Date\\\")\\n\",\n",
      "-    \"        plt.tight_layout()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    else:\\n\",\n",
      "-    \"        plt.figure(figsize=(12, 3))\\n\",\n",
      "-    \"        for col in df.columns:\\n\",\n",
      "-    \"            sns.lineplot(data=df, x=df.index, y=col, label=col)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        plotted_outlier_variables = set()\\n\",\n",
      "-    \"        for idx, row in outliers_table.iterrows():\\n\",\n",
      "-    \"            date = row[\\\"Date\\\"]\\n\",\n",
      "-    \"            variable = row[\\\"Variable\\\"]\\n\",\n",
      "-    \"            outlier_value = df.loc[date, variable]\\n\",\n",
      "-    \"            if variable not in plotted_outlier_variables:\\n\",\n",
      "-    \"                plt.scatter(date, outlier_value, marker=\\\"o\\\", s=100, c=\\\"red\\\", label=f\\\"Outlier ({variable})\\\")\\n\",\n",
      "-    \"                plotted_outlier_variables.add(variable)\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                plt.scatter(date, outlier_value, marker=\\\"o\\\", s=100, c=\\\"red\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        plt.legend()\\n\",\n",
      "-    \"        plt.xlabel(\\\"Date\\\")\\n\",\n",
      "-    \"        plt.ylabel(\\\"Value\\\")\\n\",\n",
      "-    \"        plt.title(\\\"Time Series with Outliers\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_outliers(df, outliers_table, use_subplots=True)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Handling Outliers**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": []\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## 4.2. Exploratory Data Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.2.1. Univariate Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Visual Inspection\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Line plots.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_time_series(df, title='All Variables')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Seasonality \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Compute Seasonal Decomposition**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def compute_seasonal_decomposition(data, model='additive'):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Compute seasonal decomposition for all time-series in a DataFrame and store all the components in a new DataFrame.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param data: DataFrame with time-series data\\n\",\n",
      "-    \"    :param period: Number of observations in each seasonal period\\n\",\n",
      "-    \"    :return: DataFrame with seasonal, trend, and residual components for all time-series in the input DataFrame\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Initialize an empty DataFrame to store the components for each time-series\\n\",\n",
      "-    \"    decomp_df = pd.DataFrame()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Loop over each column in the input DataFrame and perform seasonal decomposition\\n\",\n",
      "-    \"    for col in data.columns:\\n\",\n",
      "-    \"        res = seasonal_decompose(data[col], model=model)\\n\",\n",
      "-    \"        decomp_df[f'{col}_seasonal'] = res.seasonal\\n\",\n",
      "-    \"        decomp_df[f'{col}_trend'] = res.trend\\n\",\n",
      "-    \"        decomp_df[f'{col}_residual'] = res.resid\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Set the index of the decomposed DataFrame to be the same as the input DataFrame\\n\",\n",
      "-    \"    decomp_df.index = data.index\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return decomp_df\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"decomp_df = compute_seasonal_decomposition(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Visualize Seasonal Decomposition**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_seasonal_components(decomp_df):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Plot all seasonal, trend, and residual components for each variable in a DataFrame.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param decomp_df: DataFrame with seasonal, trend, and residual components for each variable\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Initialize a figure with subplots for each variable and component\\n\",\n",
      "-    \"    fig, axs = plt.subplots(nrows=len(decomp_df.columns) // 3, ncols=3, figsize=(12, 4 * (len(decomp_df.columns) // 3)))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Loop over each variable in the input DataFrame and plot the seasonal, trend, and residual components\\n\",\n",
      "-    \"    for i, col in enumerate(decomp_df.columns[::3]):\\n\",\n",
      "-    \"        axs[i, 0].plot(decomp_df.index, decomp_df[f'{col}'])\\n\",\n",
      "-    \"        axs[i, 0].set_title(f'Seasonal: {col[:-9]}')\\n\",\n",
      "-    \"        axs[i, 1].plot(decomp_df.index, decomp_df[f'{col[:-9]}_trend'])\\n\",\n",
      "-    \"        axs[i, 1].set_title(f'Trend: {col[:-9]}')\\n\",\n",
      "-    \"        axs[i, 2].plot(decomp_df.index, decomp_df[f'{col[:-9]}_residual'])\\n\",\n",
      "-    \"        axs[i, 2].set_title(f'Residual: {col[:-9]}')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Set the figure title\\n\",\n",
      "-    \"    fig.suptitle('Seasonal Decomposition', fontsize=16)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Adjust the spacing between subplots\\n\",\n",
      "-    \"    fig.tight_layout()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show the plot\\n\",\n",
      "-    \"    plt.show()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_seasonal_components(decomp_df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 3: Residual Analysis**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": []\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Stationarity\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Auto Stationarity**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def test_stationarity(data, threshold=0.05):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Perform multiple stationarity tests on each time series in a DataFrame.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param data: DataFrame with time-series data\\n\",\n",
      "-    \"    :return: DataFrame with test results (Variable, Test, p-value, Threshold, Pass/Fail, Decision)\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Initialize an empty DataFrame to store the test results\\n\",\n",
      "-    \"    test_results = pd.DataFrame(columns=['Variable', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Loop over each column in the input DataFrame and perform stationarity tests\\n\",\n",
      "-    \"    for col in data.columns:\\n\",\n",
      "-    \"        # Perform the ADF test\\n\",\n",
      "-    \"        adf_result = adfuller(data[col], autolag='AIC')\\n\",\n",
      "-    \"        adf_pvalue = adf_result[1]\\n\",\n",
      "-    \"        adf_pass_fail = adf_pvalue < threshold\\n\",\n",
      "-    \"        adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\\n\",\n",
      "-    \"        test_results = test_results.append({\\n\",\n",
      "-    \"            'Variable': col,\\n\",\n",
      "-    \"            'Test': 'ADF',\\n\",\n",
      "-    \"            'p-value': adf_pvalue,\\n\",\n",
      "-    \"            'Threshold': threshold,\\n\",\n",
      "-    \"            'Pass/Fail': adf_pass_fail,\\n\",\n",
      "-    \"            'Decision': adf_decision\\n\",\n",
      "-    \"        }, ignore_index=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Perform the KPSS test\\n\",\n",
      "-    \"        kpss_result = kpss(data[col], regression='c', nlags='auto')\\n\",\n",
      "-    \"        kpss_pvalue = kpss_result[1]\\n\",\n",
      "-    \"        kpss_pass_fail = kpss_pvalue > threshold\\n\",\n",
      "-    \"        kpss_decision = 'Stationary' if kpss_pass_fail else 'Non-stationary'\\n\",\n",
      "-    \"        test_results = test_results.append({\\n\",\n",
      "-    \"            'Variable': col,\\n\",\n",
      "-    \"            'Test': 'KPSS',\\n\",\n",
      "-    \"            'p-value': kpss_pvalue,\\n\",\n",
      "-    \"            'Threshold': threshold,\\n\",\n",
      "-    \"            'Pass/Fail': kpss_pass_fail,\\n\",\n",
      "-    \"            'Decision': kpss_decision\\n\",\n",
      "-    \"        }, ignore_index=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Perform the Phillips-Perron test\\n\",\n",
      "-    \"        pp_result = PhillipsPerron(data[col], trend='ct')\\n\",\n",
      "-    \"        pp_pvalue = pp_result.pvalue\\n\",\n",
      "-    \"        pp_threshold = threshold\\n\",\n",
      "-    \"        pp_pass_fail = pp_pvalue < pp_threshold\\n\",\n",
      "-    \"        pp_decision = 'Stationary' if pp_pass_fail else 'Non-stationary'\\n\",\n",
      "-    \"        test_results = test_results.append({\\n\",\n",
      "-    \"            'Variable': col,\\n\",\n",
      "-    \"            'Test': 'PhillipsPerron',\\n\",\n",
      "-    \"            'p-value': pp_pvalue,\\n\",\n",
      "-    \"            'Threshold': pp_threshold,\\n\",\n",
      "-    \"            'Pass/Fail': pp_pass_fail,\\n\",\n",
      "-    \"            'Decision': pp_decision\\n\",\n",
      "-    \"        }, ignore_index=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Perform the DF-GLS test\\n\",\n",
      "-    \"        dfgls_result = DFGLS(data[col], trend='ct')\\n\",\n",
      "-    \"        dfgls_pvalue = dfgls_result.pvalue\\n\",\n",
      "-    \"        dfgls_threshold = threshold\\n\",\n",
      "-    \"        dfgls_pass_fail = dfgls_pvalue < dfgls_threshold\\n\",\n",
      "-    \"        dfgls_decision = 'Stationary' if dfgls_pass_fail else 'Non-stationary'\\n\",\n",
      "-    \"        test_results = test_results.append({\\n\",\n",
      "-    \"            'Variable': col,\\n\",\n",
      "-    \"            'Test': 'DFGLS',\\n\",\n",
      "-    \"            'p-value': dfgls_pvalue,\\n\",\n",
      "-    \"            'Threshold': dfgls_threshold,\\n\",\n",
      "-    \"            'Pass/Fail': dfgls_pass_fail,\\n\",\n",
      "-    \"            'Decision': dfgls_decision\\n\",\n",
      "-    \"        }, ignore_index=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return test_results\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def auto_stationarity(data, max_order=5, threshold=0.05):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Perform the Augmented Dickey-Fuller (ADF) stationarity test on each time series in a DataFrame,\\n\",\n",
      "-    \"    testing for different integration orders until the series is stationary.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param data: DataFrame with time-series data\\n\",\n",
      "-    \"    :param max_order: Maximum integration order to test\\n\",\n",
      "-    \"    :param threshold: Significance level for the ADF test\\n\",\n",
      "-    \"    :return: DataFrame with test results (Variable, Integration Order, Test, p-value, Threshold, Pass/Fail, Decision)\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Initialize an empty DataFrame to store the test results\\n\",\n",
      "-    \"    test_results = pd.DataFrame(columns=['Variable', 'Integration Order', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Loop over each column in the input DataFrame and perform stationarity tests\\n\",\n",
      "-    \"    for col in data.columns:\\n\",\n",
      "-    \"        is_stationary = False\\n\",\n",
      "-    \"        order = 0\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        while not is_stationary and order <= max_order:\\n\",\n",
      "-    \"            series = data[col]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            if order == 0:\\n\",\n",
      "-    \"                adf_result = adfuller(series)\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                adf_result = adfuller(np.diff(series, n=order-1))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            adf_pvalue = adf_result[1]\\n\",\n",
      "-    \"            adf_pass_fail = adf_pvalue < threshold\\n\",\n",
      "-    \"            adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            test_results = test_results.append({\\n\",\n",
      "-    \"                'Variable': col,\\n\",\n",
      "-    \"                'Integration Order': order,\\n\",\n",
      "-    \"                'Test': 'ADF',\\n\",\n",
      "-    \"                'p-value': adf_pvalue,\\n\",\n",
      "-    \"                'Threshold': threshold,\\n\",\n",
      "-    \"                'Pass/Fail': 'Pass' if adf_pass_fail else 'Fail',\\n\",\n",
      "-    \"                'Decision': adf_decision\\n\",\n",
      "-    \"            }, ignore_index=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            if adf_pass_fail:\\n\",\n",
      "-    \"                is_stationary = True\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            order += 1\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return test_results\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"auto_stationarity(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Rolling Statistics**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_rolling_statistics(df, window_size=12):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Plot rolling mean and rolling standard deviation in different subplots for each variable.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    :param df: DataFrame with time-series data\\n\",\n",
      "-    \"    :param window_size: Window size for the rolling calculations\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    for col_name in df.columns:\\n\",\n",
      "-    \"        rolling_mean = df[col_name].rolling(window=window_size).mean()\\n\",\n",
      "-    \"        rolling_std = df[col_name].rolling(window=window_size).std()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        ax1.plot(rolling_mean, label=f'{col_name} Rolling Mean')\\n\",\n",
      "-    \"        ax1.legend()\\n\",\n",
      "-    \"        ax1.set_ylabel('Value')\\n\",\n",
      "-    \"        ax1.set_title(f'Rolling Mean for {col_name}')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        ax2.plot(rolling_std, label=f'{col_name} Rolling Standard Deviation', color='orange')\\n\",\n",
      "-    \"        ax2.legend()\\n\",\n",
      "-    \"        ax2.set_xlabel('Time')\\n\",\n",
      "-    \"        ax2.set_ylabel('Value')\\n\",\n",
      "-    \"        ax2.set_title(f'Rolling Standard Deviation for {col_name}')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_rolling_statistics(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"df_diff = df.diff().dropna()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### AR Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Calculate AR Orders**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def calculate_ar_orders(dataset, max_order=3):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    This function calculates the autoregressive order of all time series in a dataset.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    Parameters:\\n\",\n",
      "-    \"    dataset (pd.DataFrame): The dataset containing the time series.\\n\",\n",
      "-    \"    max_order (int): The maximum order to consider for the autoregressive models.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    Returns:\\n\",\n",
      "-    \"    pd.DataFrame: A table with the autoregressive order, AIC, and BIC for orders 0 up to max_order.\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Initialize an empty list to store the results\\n\",\n",
      "-    \"    results = []\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Iterate over each column (time series) in the dataset\\n\",\n",
      "-    \"    for col in dataset.columns:\\n\",\n",
      "-    \"        time_series = dataset[col]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Test for stationarity using Augmented Dickey-Fuller test\\n\",\n",
      "-    \"        adf_result = adfuller(time_series)\\n\",\n",
      "-    \"        if adf_result[1] > 0.05:\\n\",\n",
      "-    \"            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Test different autoregressive orders and store the AIC and BIC values\\n\",\n",
      "-    \"        for order in range(max_order + 1):\\n\",\n",
      "-    \"            model = AutoReg(time_series, lags=order, old_names=False)\\n\",\n",
      "-    \"            result = model.fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Add the current time series, order, AIC, and BIC to the results list\\n\",\n",
      "-    \"            results.append({'Variable': col, 'AR order': order, 'AIC': result.aic, 'BIC': result.bic})\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Convert the results list to a DataFrame and return it\\n\",\n",
      "-    \"    return pd.DataFrame(results)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"calculate_ar_orders(df_diff)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Selection of AR Order**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### MA Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Calculate MA Orders**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def calculate_ma_orders(dataset, max_order=3):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    This function calculates the moving average order of all time series in a dataset.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    Parameters:\\n\",\n",
      "-    \"    dataset (pd.DataFrame): The dataset containing the time series.\\n\",\n",
      "-    \"    max_order (int): The maximum order to consider for the moving average models.\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    Returns:\\n\",\n",
      "-    \"    pd.DataFrame: A table with the moving average order, AIC, and BIC for orders 0 up to max_order.\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    # Initialize an empty list to store the results\\n\",\n",
      "-    \"    results = []\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Iterate over each column (time series) in the dataset\\n\",\n",
      "-    \"    for col in dataset.columns:\\n\",\n",
      "-    \"        time_series = dataset[col]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Test for stationarity using Augmented Dickey-Fuller test\\n\",\n",
      "-    \"        adf_result = adfuller(time_series)\\n\",\n",
      "-    \"        if adf_result[1] > 0.05:\\n\",\n",
      "-    \"            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Test different moving average orders and store the AIC and BIC values\\n\",\n",
      "-    \"        for order in range(max_order + 1):\\n\",\n",
      "-    \"            model = ARIMA(time_series, order=(0, 0, order))\\n\",\n",
      "-    \"            result = model.fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Add the current time series, order, AIC, and BIC to the results list\\n\",\n",
      "-    \"            results.append({'Variable': col, 'MA order': order, 'AIC': result.aic, 'BIC': result.bic})\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Convert the results list to a DataFrame and return it\\n\",\n",
      "-    \"    return pd.DataFrame(results)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"calculate_ma_orders(df_diff)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Selection of MA Order**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.2.2. Multivariate Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Correlations\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Compute Correlation Matrix on Levels**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_corr_heatmap(df):\\n\",\n",
      "-    \"    # Compute correlation matrix\\n\",\n",
      "-    \"    corr_matrix = df.corr()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Plot heatmap\\n\",\n",
      "-    \"    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Set plot title\\n\",\n",
      "-    \"    plt.title('Correlation Matrix Heatmap')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show plot\\n\",\n",
      "-    \"    plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Corrrelations across Levels.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_corr_heatmap(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Compute Correlation Matrix on First Difference**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Correlations across First Differences.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_corr_heatmap(df_diff)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 3: Reasoning**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Scatter Plots\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Compute Scatter Plots on Levels**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_scatter_pairs(df):\\n\",\n",
      "-    \"    # Compute pairwise scatter plots\\n\",\n",
      "-    \"    sns.pairplot(df, kind='scatter')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show plot\\n\",\n",
      "-    \"    plt.show()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_scatter_pairs(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Compute Scatter Plots on First Difference**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Compute first difference.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_scatter_pairs(df_diff)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 3: Reasoning**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Lag Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Compute Correlations at Multiple Lags**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_heatmap_correlations(df, target_col, independent_vars, num_lags=10):\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"    Calculate the correlation between the target variable and the lags of independent variables in the dataset,\\n\",\n",
      "-    \"    and plot a heatmap of these correlations.\\n\",\n",
      "-    \"    :param df: DataFrame containing the target variable and independent variables of interest\\n\",\n",
      "-    \"    :param target_col: Column name of the target variable in the DataFrame\\n\",\n",
      "-    \"    :param independent_vars: List of column names of the independent variables in the DataFrame\\n\",\n",
      "-    \"    :param num_lags: Number of lags to calculate (default is 10)\\n\",\n",
      "-    \"    \\\"\\\"\\\"\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    correlations = np.zeros((len(independent_vars), num_lags + 1))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    for i, ind_var_col in enumerate(independent_vars):\\n\",\n",
      "-    \"        for lag in range(num_lags + 1):\\n\",\n",
      "-    \"            # Create a new DataFrame with the original and lagged variable\\n\",\n",
      "-    \"            temp_df = pd.DataFrame({target_col: df[target_col],\\n\",\n",
      "-    \"                                    f'{ind_var_col}_lag{lag}': df[ind_var_col].shift(lag)})\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Drop NaN rows\\n\",\n",
      "-    \"            temp_df = temp_df.dropna()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Calculate the correlation between the target variable and the lagged independent variable\\n\",\n",
      "-    \"            corr = temp_df[target_col].corr(temp_df[f'{ind_var_col}_lag{lag}'])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Store the correlation in the correlations matrix\\n\",\n",
      "-    \"            correlations[i, lag] = corr\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Create a DataFrame with the correlations matrix\\n\",\n",
      "-    \"    correlation_df = pd.DataFrame(correlations, columns=[f'lag_{i}' for i in range(num_lags + 1)], index=independent_vars)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Plot the heatmap\\n\",\n",
      "-    \"    plt.figure(figsize=(12, 3))\\n\",\n",
      "-    \"    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\\n\",\n",
      "-    \"    plt.title('Heatmap of Correlations between Target Variable and Lags of Independent Variables')\\n\",\n",
      "-    \"    plt.xlabel('Lags')\\n\",\n",
      "-    \"    plt.ylabel('Independent Variables')\\n\",\n",
      "-    \"    plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"target_var = 'MORTGAGE30US'\\n\",\n",
      "-    \"independent_vars = ['GS10', 'UNRATE', 'FEDFUNDS']\\n\",\n",
      "-    \"plot_heatmap_correlations(df_diff, target_col=target_var, independent_vars=independent_vars, num_lags=10)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Reasoning**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Colinearity\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Cointegration\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Compute Cointegration Test**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Function to calculate cointegration for each pair of variables in a DataFrame\\n\",\n",
      "-    \"def calculate_cointegration(dataframe, test=\\\"Engle-Granger\\\", threshold=0.05):\\n\",\n",
      "-    \"    coint_df = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\\n\",\n",
      "-    \"    for i in range(len(dataframe.columns)):\\n\",\n",
      "-    \"        for j in range(i+1, len(dataframe.columns)):\\n\",\n",
      "-    \"            var1 = dataframe.columns[i]\\n\",\n",
      "-    \"            var2 = dataframe.columns[j]\\n\",\n",
      "-    \"            _, p_value, _ = coint(dataframe[var1], dataframe[var2])\\n\",\n",
      "-    \"            pass_fail = \\\"Pass\\\" if p_value <= threshold else \\\"Fail\\\"\\n\",\n",
      "-    \"            decision = \\\"Cointegrated\\\" if pass_fail == \\\"Pass\\\" else \\\"Not Cointegrated\\\"\\n\",\n",
      "-    \"            coint_df = coint_df.append({\\n\",\n",
      "-    \"                'Variable 1': var1,\\n\",\n",
      "-    \"                'Variable 2': var2,\\n\",\n",
      "-    \"                'Test': test,\\n\",\n",
      "-    \"                'p-value': p_value,\\n\",\n",
      "-    \"                'Threshold': threshold,\\n\",\n",
      "-    \"                'Pass/Fail': pass_fail,\\n\",\n",
      "-    \"                'Decision': decision\\n\",\n",
      "-    \"            }, ignore_index=True)\\n\",\n",
      "-    \"    return coint_df\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Calculate cointegration for pairs of variables in the DataFrame\\n\",\n",
      "-    \"coint_results = calculate_cointegration(df)\\n\",\n",
      "-    \"display(coint_results)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Plot Spread between Variables**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Function to plot the spread between all pairs of variables in a DataFrame\\n\",\n",
      "-    \"def plot_spread(dataframe):\\n\",\n",
      "-    \"    num_vars = len(dataframe.columns)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    for i in range(num_vars):\\n\",\n",
      "-    \"        for j in range(i+1, num_vars):\\n\",\n",
      "-    \"            var1 = dataframe.columns[i]\\n\",\n",
      "-    \"            var2 = dataframe.columns[j]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Calculate the spread between the two variables\\n\",\n",
      "-    \"            dataframe['spread'] = dataframe[var1] - dataframe[var2]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Plot the difference (spread) using seaborn\\n\",\n",
      "-    \"            plt.figure(figsize=(10, 4))\\n\",\n",
      "-    \"            sns.lineplot(data=dataframe['spread'], label=f'Spread ({var1} - {var2})')\\n\",\n",
      "-    \"            plt.title(f'Spread ({var1} - {var2})')\\n\",\n",
      "-    \"            plt.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Display the plot\\n\",\n",
      "-    \"            plt.tight_layout()\\n\",\n",
      "-    \"            plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_spread(df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.2.3. Feature Selection\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## 4.3. Model Methodology\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## 4.4. Training Data\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.4.1. Sampling \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Split dataset into Training and Test**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"split_index = int(len(df) * 0.8)   # use 80% of the data for training\\n\",\n",
      "-    \"df_train, df_test = df[:split_index], df[split_index:]\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Create a Stationary Train and Test Dataset**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Apply first difference to both training and test df\\n\",\n",
      "-    \"df_train_diff = df_train.diff().dropna()\\n\",\n",
      "-    \"df_test_diff = df_test.diff().dropna()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"df_train_diff.head()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## 4.5. Model Training\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### Model 1: Loan Rates and FEDFUNDS\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Fit Model**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Add the independent variables with no intercept\\n\",\n",
      "-    \"X = df_train_diff['FEDFUNDS']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Define the dependent variable\\n\",\n",
      "-    \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Fit the linear regression model\\n\",\n",
      "-    \"model_1 = sm.OLS(y, X).fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Display the model summary\\n\",\n",
      "-    \"print(model_1.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Reasoning**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": []\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### Model 2: Loan Rates, constant and FEDFUNDS\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Fit Model**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-    \"X = sm.add_constant(df_train_diff['FEDFUNDS'])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Define the dependent variable\\n\",\n",
      "-    \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Fit the linear regression model\\n\",\n",
      "-    \"model_2 = sm.OLS(y, X).fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Display the model summary\\n\",\n",
      "-    \"print(model_2.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Reasoning**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": []\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### Model 3: Loan Rates and GS10\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-    \"X = df_train_diff['GS10']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Define the dependent variable\\n\",\n",
      "-    \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Fit the linear regression model\\n\",\n",
      "-    \"model_3 = sm.OLS(y, X).fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Display the model summary\\n\",\n",
      "-    \"print(model_3.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### Model 4: Loan Rates, FEDFUNDS and GS10\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Fit Model**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-    \"X = df_train_diff[['GS10', 'FEDFUNDS']]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Define the dependent variable\\n\",\n",
      "-    \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Fit the linear regression model\\n\",\n",
      "-    \"model_4 = sm.OLS(y, X).fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Display the model summary\\n\",\n",
      "-    \"print(model_4.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: Reasoning**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### Model 5: Loan Rates, FEDFUNDS, GS10 and UNRATE\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: Fit Model**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-    \"X = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Define the dependent variable\\n\",\n",
      "-    \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Fit the linear regression model\\n\",\n",
      "-    \"model_5 = sm.OLS(y, X).fit()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Display the model summary\\n\",\n",
      "-    \"print(model_5.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Model Selection\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 1: In-Sample Performance**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def in_sample_performance_ols(models):\\n\",\n",
      "-    \"    evaluation_results = []\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    for i, model in enumerate(models):\\n\",\n",
      "-    \"        X = model.model.exog\\n\",\n",
      "-    \"        X_columns = model.model.exog_names\\n\",\n",
      "-    \"        y = model.model.endog\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Calculate the predicted values using the model\\n\",\n",
      "-    \"        y_pred = model.predict(X)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Calculate the residuals\\n\",\n",
      "-    \"        residuals = y - y_pred\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Extract R-squared and Adjusted R-squared\\n\",\n",
      "-    \"        r2 = model.rsquared\\n\",\n",
      "-    \"        adj_r2 = model.rsquared_adj\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Calculate the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\\n\",\n",
      "-    \"        mse = model.mse_resid\\n\",\n",
      "-    \"        rmse = mse ** 0.5\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Append the results to the evaluation_results list\\n\",\n",
      "-    \"        evaluation_results.append({\\n\",\n",
      "-    \"            'Model': f'Model_{i + 1}',\\n\",\n",
      "-    \"            'Independent Variables': ', '.join(X_columns),\\n\",\n",
      "-    \"            'R-Squared': r2,\\n\",\n",
      "-    \"            'Adjusted R-Squared': adj_r2,\\n\",\n",
      "-    \"            'MSE': mse,\\n\",\n",
      "-    \"            'RMSE': rmse\\n\",\n",
      "-    \"        })\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Convert the evaluation_results list to a DataFrame\\n\",\n",
      "-    \"    results_df = pd.DataFrame(evaluation_results)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return results_df\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"models = [model_1, model_2, model_3, model_4, model_5]\\n\",\n",
      "-    \"results_df = in_sample_performance_ols(models)\\n\",\n",
      "-    \"display(results_df)\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 2: In-Sample Forecast First Difference**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def in_sample_forecast(models, observed_data, separate_subplots=False):\\n\",\n",
      "-    \"    # Extract the observed data and dates\\n\",\n",
      "-    \"    y = observed_data\\n\",\n",
      "-    \"    x = observed_data.index\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    if separate_subplots:\\n\",\n",
      "-    \"        # Calculate the number of rows and columns for the subplots\\n\",\n",
      "-    \"        n_models = len(models)\\n\",\n",
      "-    \"        n_cols = 2\\n\",\n",
      "-    \"        n_rows = n_models // n_cols + (n_models % n_cols > 0)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\\n\",\n",
      "-    \"        axes = axes.ravel()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data and in-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            ax = axes[i]\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            X = model.model.exog\\n\",\n",
      "-    \"            y_pred = model.predict(X)\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Get the independent variable names\\n\",\n",
      "-    \"            ind_var_names = ', '.join(model.model.exog_names)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\\n\",\n",
      "-    \"            ax.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Remove unused subplots\\n\",\n",
      "-    \"        for j in range(i+1, n_rows * n_cols):\\n\",\n",
      "-    \"            fig.delaxes(axes[j])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    else:\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        plt.figure(figsize=(10, 6))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data\\n\",\n",
      "-    \"        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the in-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            X = model.model.exog\\n\",\n",
      "-    \"            y_pred = model.predict(X)\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        plt.ylabel('Value')\\n\",\n",
      "-    \"        plt.title('Observed Data and In-sample Predictions')\\n\",\n",
      "-    \"        plt.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show the plot\\n\",\n",
      "-    \"    plt.tight_layout()\\n\",\n",
      "-    \"    plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"models = [model_1, model_2, model_3, model_4, model_5]\\n\",\n",
      "-    \"observed_data = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"in_sample_forecast(models, observed_data, separate_subplots=True)  # For separate subplots\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"**Step 3: In-Sample Forecast Levels**\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def in_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False):\\n\",\n",
      "-    \"    # Extract the observed data (levels) and dates\\n\",\n",
      "-    \"    y = original_data\\n\",\n",
      "-    \"    x = original_data.index\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    if separate_subplots:\\n\",\n",
      "-    \"        # Calculate the number of rows and columns for the subplots\\n\",\n",
      "-    \"        n_models = len(models)\\n\",\n",
      "-    \"        n_cols = 2\\n\",\n",
      "-    \"        n_rows = n_models // n_cols + (n_models % n_cols > 0)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\\n\",\n",
      "-    \"        axes = axes.ravel()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data (levels) and in-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            ax = axes[i]\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            X = model.model.exog\\n\",\n",
      "-    \"            y_diff_pred = model.predict(X)\\n\",\n",
      "-    \"            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            # Get the independent variable names\\n\",\n",
      "-    \"            ind_var_names = ', '.join(model.model.exog_names)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\\n\",\n",
      "-    \"            ax.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Remove unused subplots\\n\",\n",
      "-    \"        for j in range(i+1, n_rows * n_cols):\\n\",\n",
      "-    \"            fig.delaxes(axes[j])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        fig.text(0.5, 0.04, 'DATE', ha='center')\\n\",\n",
      "-    \"        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    else:\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        plt.figure(figsize=(10, 6))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data (levels)\\n\",\n",
      "-    \"        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the in-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            X = model.model.exog\\n\",\n",
      "-    \"            y_diff_pred = model.predict(X)\\n\",\n",
      "-    \"            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        plt.xlabel('DATE')\\n\",\n",
      "-    \"        plt.ylabel('Value')\\n\",\n",
      "-    \"        plt.title('Observed Data and In-sample Predictions (Levels)')\\n\",\n",
      "-    \"        plt.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show the plot\\n\",\n",
      "-    \"    plt.tight_layout()\\n\",\n",
      "-    \"    plt.show()\\n\",\n",
      "-    \"\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"models = [model_1, model_2, model_3, model_4, model_5]\\n\",\n",
      "-    \"original_data = df_train['MORTGAGE30US']\\n\",\n",
      "-    \"diff_data = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-    \"in_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False)  # For a single plot with all series\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## 4.6. Model Evaluation\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.6.1. Out-of-Sample Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Out-of-Sample Performance\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def out_of_sample_performance(model_list, model_names, test_data, target_col):\\n\",\n",
      "-    \"    # Initialize a list to store results\\n\",\n",
      "-    \"    results = []\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    for fitted_model, model_name in zip(model_list, model_names):\\n\",\n",
      "-    \"        # Extract the column names of the independent variables from the model\\n\",\n",
      "-    \"        independent_vars = fitted_model.model.exog_names\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Separate the target variable and features in the test dataset\\n\",\n",
      "-    \"        X_test = test_data[independent_vars]\\n\",\n",
      "-    \"        y_test = test_data[target_col]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Predict the test data\\n\",\n",
      "-    \"        y_pred = fitted_model.predict(X_test)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Calculate the residuals\\n\",\n",
      "-    \"        residuals = y_test - y_pred\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Calculate the mean squared error and root mean squared error\\n\",\n",
      "-    \"        mse = np.mean(residuals ** 2)\\n\",\n",
      "-    \"        rmse_val = np.sqrt(mse)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Store the results\\n\",\n",
      "-    \"        model_name_with_vars = f\\\"{model_name} ({', '.join(independent_vars)})\\\"\\n\",\n",
      "-    \"        results.append([model_name_with_vars, mse, rmse_val])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Create a DataFrame to display the results\\n\",\n",
      "-    \"    results_df = pd.DataFrame(results, columns=['Model', 'MSE', 'RMSE'])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return results_df\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"model_list = [model_3, model_4]\\n\",\n",
      "-    \"model_names = ['model_3', 'model_4']\\n\",\n",
      "-    \"results_df = out_of_sample_performance(model_list, model_names=model_names, test_data=df_test_diff, target_col='MORTGAGE30US')\\n\",\n",
      "-    \"display(results_df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Out-of-Sample Forecast\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def out_of_sample_forecast(models, model_names, test_data, target_col, separate_subplots=False):\\n\",\n",
      "-    \"    # Extract the observed data and dates\\n\",\n",
      "-    \"    y = test_data[target_col]\\n\",\n",
      "-    \"    x = test_data.index\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    if separate_subplots:\\n\",\n",
      "-    \"        # Calculate the number of rows and columns for the subplots\\n\",\n",
      "-    \"        n_models = len(models)\\n\",\n",
      "-    \"        n_cols = 2\\n\",\n",
      "-    \"        n_rows = n_models // n_cols + (n_models % n_cols > 0)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\\n\",\n",
      "-    \"        axes = axes.ravel()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data and out-of-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            ax = axes[i]\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            exog_names = model.model.exog_names\\n\",\n",
      "-    \"            if 'const' in exog_names and 'const' not in test_data.columns:\\n\",\n",
      "-    \"                X_test = test_data[[name for name in exog_names if name != 'const']]\\n\",\n",
      "-    \"                X_test.insert(0, 'const', 1)\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                X_test = test_data[exog_names]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            y_pred = model.predict(X_test)\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            ax.set_title(f'{model_names[i]} ({\\\", \\\".join(exog_names)})')\\n\",\n",
      "-    \"            ax.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Remove unused subplots\\n\",\n",
      "-    \"        for j in range(i + 1, n_rows * n_cols):\\n\",\n",
      "-    \"            fig.delaxes(axes[j])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    else:\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        plt.figure(figsize=(10, 6))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data\\n\",\n",
      "-    \"        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the out-of-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            exog_names = model.model.exog_names\\n\",\n",
      "-    \"            if 'const' in exog_names and 'const' not in test_data.columns:\\n\",\n",
      "-    \"                X_test = test_data[[name for name in exog_names if name != 'const']]\\n\",\n",
      "-    \"                X_test.insert(0, 'const', 1)\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                X_test = test_data[exog_names]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            y_pred = model.predict(X_test)\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        plt.ylabel('Value')\\n\",\n",
      "-    \"        plt.title('Observed Data and Out-of-sample Predictions')\\n\",\n",
      "-    \"        plt.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show the plot\\n\",\n",
      "-    \"    plt.tight_layout()\\n\",\n",
      "-    \"    plt.show()\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"models = [model_3, model_4]\\n\",\n",
      "-    \"out_of_sample_forecast(models, model_names=['model_3', 'model_4'], test_data=df_test_diff, target_col='MORTGAGE30US', separate_subplots=True)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": []\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def out_of_sample_forecast_levels(models, model_names, test_data, original_data, target_col, separate_subplots=False):\\n\",\n",
      "-    \"    # Extract the observed data and dates\\n\",\n",
      "-    \"    y_test = test_data[target_col]\\n\",\n",
      "-    \"    y_orig = original_data[original_data.index.isin(test_data.index)][target_col]\\n\",\n",
      "-    \"    x = y_orig.index\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    if separate_subplots:\\n\",\n",
      "-    \"        # Calculate the number of rows and columns for the subplots\\n\",\n",
      "-    \"        n_models = len(models)\\n\",\n",
      "-    \"        n_cols = 2\\n\",\n",
      "-    \"        n_rows = n_models // n_cols + (n_models % n_cols > 0)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\\n\",\n",
      "-    \"        axes = axes.ravel()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data and out-of-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            ax = axes[i]\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey', ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            exog_names = model.model.exog_names\\n\",\n",
      "-    \"            if 'const' in exog_names and 'const' not in test_data.columns:\\n\",\n",
      "-    \"                X_test = test_data[[name for name in exog_names if name != 'const']]\\n\",\n",
      "-    \"                X_test.insert(0, 'const', 1)\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                X_test = test_data[exog_names]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            y_pred_diff = model.predict(X_test)\\n\",\n",
      "-    \"            y_pred = y_pred_diff + y_orig.shift(1).values\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            ax.set_title(f'{model_names[i]} ({\\\", \\\".join(exog_names)})')\\n\",\n",
      "-    \"            ax.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Remove unused subplots\\n\",\n",
      "-    \"        for j in range(i + 1, n_rows * n_cols):\\n\",\n",
      "-    \"            fig.delaxes(axes[j])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    else:\\n\",\n",
      "-    \"        # Set up the plot\\n\",\n",
      "-    \"        plt.figure(figsize=(10, 6))\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the observed data\\n\",\n",
      "-    \"        sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Plot the out-of-sample predictions for each model\\n\",\n",
      "-    \"        for i, model in enumerate(models):\\n\",\n",
      "-    \"            exog_names = model.model.exog_names\\n\",\n",
      "-    \"            if 'const' in exog_names and 'const' not in test_data.columns:\\n\",\n",
      "-    \"                X_test = test_data[[name for name in exog_names if name != 'const']]\\n\",\n",
      "-    \"                X_test.insert(0, 'const', 1)\\n\",\n",
      "-    \"            else:\\n\",\n",
      "-    \"                X_test = test_data[exog_names]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"            y_pred_diff = model.predict(X_test)\\n\",\n",
      "-    \"            y_pred = y_pred_diff + y_orig.shift(1).values\\n\",\n",
      "-    \"            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Customize the plot\\n\",\n",
      "-    \"        plt.ylabel('Value')\\n\",\n",
      "-    \"        plt.title('Observed Data and Out-of-sample Predictions')\\n\",\n",
      "-    \"        plt.legend()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Show the plot\\n\",\n",
      "-    \"    plt.tight_layout()\\n\",\n",
      "-    \"    plt.show()\\n\",\n",
      "-    \"\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"models = [model_3, model_4]\\n\",\n",
      "-    \"model_names = ['model_3', 'model_4']\\n\",\n",
      "-    \"original_data = df_test\\n\",\n",
      "-    \"out_of_sample_forecast_levels(models, model_names, test_data=df_test_diff, original_data=original_data, target_col='MORTGAGE30US', separate_subplots=False)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.6.2. Forecast Performance \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### One-Step Ahead Forecast \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Five-Step Ahead Forecast \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.6.3. Scenario Analysis\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"#### Parallel Interest Rates Shocks \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.6.4. Stress Testing\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"### 4.6.5. Uncertainty Analysis\"\n",
      "-   ]\n",
      "-  }\n",
      "- ],\n",
      "- \"metadata\": {\n",
      "-  \"kernelspec\": {\n",
      "-   \"display_name\": \"validmind-eEL8LtKG-py3.10\",\n",
      "-   \"language\": \"python\",\n",
      "-   \"name\": \"python3\"\n",
      "-  },\n",
      "-  \"language_info\": {\n",
      "-   \"codemirror_mode\": {\n",
      "-    \"name\": \"ipython\",\n",
      "-    \"version\": 3\n",
      "-   },\n",
      "-   \"file_extension\": \".py\",\n",
      "-   \"mimetype\": \"text/x-python\",\n",
      "-   \"name\": \"python\",\n",
      "-   \"nbconvert_exporter\": \"python\",\n",
      "-   \"pygments_lexer\": \"ipython3\",\n",
      "-   \"version\": \"3.10.9\"\n",
      "-  },\n",
      "-  \"orig_nbformat\": 4\n",
      "- },\n",
      "- \"nbformat\": 4,\n",
      "- \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_data_quality_test_plan.ipynb b/notebooks/code_sharing/time_series/time_series_data_quality_test_plan.ipynb\n",
      "deleted file mode 100644\n",
      "index c3fdcb4b8..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_data_quality_test_plan.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,281 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Time Series Data Quality Test Plan Demo\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Setup ValidMind environment\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Load API key and secret from environment variables\\n\",\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Load libraries\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"import glob\\n\",\n",
      "-        \"# ML libraries\\n\",\n",
      "-        \"import pandas as pd\\n\",\n",
      "-        \"# Plotting libraries\\n\",\n",
      "-        \"import matplotlib.pyplot as plt\\n\",\n",
      "-        \"import seaborn as sns\\n\",\n",
      "-        \"%matplotlib inline\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Load data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def merge_fred_csv_files(file_pattern):\\n\",\n",
      "-        \"    # Use glob to find all files matching the specified pattern\\n\",\n",
      "-        \"    file_list = glob.glob(file_pattern)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Initialize an empty list to store individual DataFrames\\n\",\n",
      "-        \"    dataframes = []\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Iterate through each file in the file list\\n\",\n",
      "-        \"    for file in file_list:\\n\",\n",
      "-        \"        # Read the CSV file into a DataFrame\\n\",\n",
      "-        \"        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"        # Add the DataFrame to the list of DataFrames\\n\",\n",
      "-        \"        dataframes.append(df)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Merge all the DataFrames in the list into a single DataFrame\\n\",\n",
      "-        \"    merged_df = pd.concat(dataframes, axis=1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    return merged_df\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"file_pattern = './../../notebooks/datasets/time_series/raw/fred/*.csv'\\n\",\n",
      "-        \"df = merge_fred_csv_files(file_pattern)\\n\",\n",
      "-        \"display(df)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"selected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS']\\n\",\n",
      "-        \"df = df[selected_cols]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def plot_time_series(df, cols_to_plot=None, title=''):\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    Plot multiple time-series in the same axes using seaborn.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    :param df: DataFrame with time-series data\\n\",\n",
      "-        \"    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\\n\",\n",
      "-        \"    :param title: Title of the plot, default is ''\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    if cols_to_plot is None:\\n\",\n",
      "-        \"        cols_to_plot = df.columns.tolist()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Create a new DataFrame with the columns to plot\\n\",\n",
      "-        \"    plot_df = df[cols_to_plot]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Set seaborn plot style\\n\",\n",
      "-        \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Plot the time-series data\\n\",\n",
      "-        \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-        \"    for col in plot_df.columns:\\n\",\n",
      "-        \"        sns.lineplot(data=plot_df[col], label=col)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    plt.xlabel('Date')\\n\",\n",
      "-        \"    plt.ylabel('Value')\\n\",\n",
      "-        \"    plt.title(title)\\n\",\n",
      "-        \"    plt.legend()\\n\",\n",
      "-        \"    plt.show()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"plot_time_series(df, title='All Variables')\\n\",\n",
      "-        \"df.info()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Examine data quality using ValidMind framework\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df,\\n\",\n",
      "-        \"    target_column=\\\"MORTGAGE30US\\\"\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_suites()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.head()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"config={\\n\",\n",
      "-        \"    \\\"time_series_outliers\\\": {\\n\",\n",
      "-        \"        \\\"zscore_threshold\\\": 3.5,\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_missing_values\\\":{\\n\",\n",
      "-        \"        \\\"min_threshold\\\": 2,\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"test_suite = vm.run_test_suite(\\\"time_series_dataset\\\", dataset=vm_dataset, config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Adjust Frequencies, Remove missing values\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.resample('MS').last()\\n\",\n",
      "-        \"df = df.dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Examin data quality again\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"\\n\",\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df,\\n\",\n",
      "-        \"    target_column=\\\"MORTGAGE30US\\\"\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"test_suite = vm.run_test_suite(\\\"time_series_dataset\\\", dataset=vm_dataset, config=config)\"\n",
      "-      ]\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    }\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 4\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_data_validation_full_suite.ipynb b/notebooks/code_sharing/time_series/time_series_data_validation_full_suite.ipynb\n",
      "deleted file mode 100644\n",
      "index 976763634..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_data_validation_full_suite.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,507 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# Time Series Data Validation Full Suite\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 1. Introduction\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"The Time Series Data Validation Demo notebook aims to demonstrate the application of various data validation tests using the **ValidMind MRM Platform** and **Developer Framework**. Ensuring the quality and an a robust exploratory data analysis of time series data is essential for accurate model predictions and robust decision-making processes.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"In this demo, we will walk through different **data validation suites of tests** tailored for time series data, showcasing how these tools can assist you in identifying potential issues and inconsistencies in the data. \\n\",\n",
      "-        \"\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 2. Setup \\n\",\n",
      "-        \"\\n\",\n",
      "-        \"Prepare the environment for our analysis. First, **import** all necessary libraries and modules required for our analysis. Next, **connect** to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"Finally, define and **configure** the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis. \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Import Libraries\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Load API key and secret from environment variables\\n\",\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# ValidMind libraries\\n\",\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"from validmind.datasets.regression import (\\n\",\n",
      "-        \"    identify_frequencies,\\n\",\n",
      "-        \"    resample_to_common_frequency\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Connect to the ValidMind Library\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Find All Test Suites and Plans Available in the Developer Framework\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"We can find all the **test suites** and **test plans** available in the developer framework by calling the following functions:\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- All test suites: `vm.test_suites.list_suites()`\\n\",\n",
      "-        \"- All test plans: `vm.test_suites.list_plans()`\\n\",\n",
      "-        \"- Describe a test plan: `vm.test_suites.describe_plan(\\\"time_series_data_quality\\\")`\\n\",\n",
      "-        \"- List all available tests: `vm.tests.list_tests()`\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_suites()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 3. Load Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Conigure your use case.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# from validmind.datasets.classification import lending_club as demo_dataset\\n\",\n",
      "-        \"from validmind.datasets.regression import fred as demo_dataset\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"target_column = demo_dataset.target_column\\n\",\n",
      "-        \"feature_columns = demo_dataset.feature_columns\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Split the dataset into test and training\\n\",\n",
      "-        \"df = demo_dataset.load_data()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 4. Data Description\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.info()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 5. Data Validation\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### User Configuration of Test Suite \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Users can input the configuration to a test suite using **`config`**, allowing fine-tuning the suite according to their specific data requirements. \\n\",\n",
      "-        \"\\n\",\n",
      "-        \"**Time Series Data Quality params**\\n\",\n",
      "-        \"- `time_series_outliers` is set to identify outliers using a specific Z-score threshold\\n\",\n",
      "-        \"- `time_series_missing_values` defines a minimum threshold to identify missing data points.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"**Time Series Univariate params**\\n\",\n",
      "-        \"- *Visualization*: `time_series_line_plot` and `time_series_histogram` are designed to generate line and histogram plots respectively for each column in a DataFrame.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- *Seasonality*:  `seasonal_decompose` and `auto_seasonality` are dedicated to analyzing the seasonal component of the time series. `seasonal_decompose` performs a seasonal decomposition of the data, while `auto_seasonality` aids in the automatic detection of seasonality.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- *Stationarity*: `window_size` determines the number of consecutive data points used for calculating the rolling mean and standard deviation.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- *ARIMA*: `acf_pacf_plot`, `auto_ar`, and `auto_ma` are part of the ARIMA (Autoregressive Integrated Moving Average) model analysis. `acf_pacf_plot` generates autocorrelation and partial autocorrelation plots, `auto_ar` determines the order of the autoregressive part of the model, and `auto_ma` does the same for the moving average part.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"**Time Series Multivariate params**\\n\",\n",
      "-        \"- *Visualization*: `scatter_plot` is used to create scatter plots for each column in the DataFrame, offering a visual tool to understand the relationship between different variables in the dataset.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- *Correlation*: `lagged_correlation_heatmap` facilitates the creation of a heatmap, which visually represents the lagged correlation between the target column and the feature columns of a demo dataset. This provides a convenient way to examine the time-delayed correlation between different series.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- *Cointegration*: `engle_granger_coint` sets a threshold for conducting the Engle-Granger cointegration test, which is a statistical method used to identify the long-term correlation between two or more time series.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"config={\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # TIME SERIES DATA QUALITY PARAMS\\n\",\n",
      "-        \"    \\\"time_series_outliers\\\": {\\n\",\n",
      "-        \"        \\\"zscore_threshold\\\": 3,\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_missing_values\\\":{\\n\",\n",
      "-        \"        \\\"min_threshold\\\": 2,\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # TIME SERIES UNIVARIATE PARAMS\\n\",\n",
      "-        \"    \\\"rolling_stats_plot\\\": {\\n\",\n",
      "-        \"        \\\"window_size\\\": 12\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"     \\\"seasonal_decompose\\\": {\\n\",\n",
      "-        \"        \\\"seasonal_model\\\": 'additive'\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"     \\\"auto_seasonality\\\": {\\n\",\n",
      "-        \"        \\\"min_period\\\": 1,\\n\",\n",
      "-        \"        \\\"max_period\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"auto_stationarity\\\": {\\n\",\n",
      "-        \"        \\\"max_order\\\": 3,\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ar\\\": {\\n\",\n",
      "-        \"        \\\"max_ar_order\\\": 4\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ma\\\": {\\n\",\n",
      "-        \"        \\\"max_ma_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # TIME SERIES MULTIVARIATE PARAMS\\n\",\n",
      "-        \"    \\\"lagged_correlation_heatmap\\\": {\\n\",\n",
      "-        \"        \\\"target_col\\\": demo_dataset.target_column,\\n\",\n",
      "-        \"        \\\"independent_vars\\\": demo_dataset.feature_columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"engle_granger_coint\\\": {\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Validation of Raw Dataset\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### **Run the Time Series Dataset Test Suite**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df,\\n\",\n",
      "-        \"    target_column=demo_dataset.target_column,\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"full_suite = vm.run_test_suite(\\n\",\n",
      "-        \"    \\\"time_series_dataset\\\",\\n\",\n",
      "-        \"    dataset=vm_dataset,\\n\",\n",
      "-        \"    config = config,\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Handle Dataset Frequencies\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Show the frequencies of each variable in the raw dataset.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"frequencies = identify_frequencies(df)\\n\",\n",
      "-        \"display(frequencies)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Handle frequencies by resampling all variables to a common frequency.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"preprocessed_df = resample_to_common_frequency(df, common_frequency=demo_dataset.frequency)\\n\",\n",
      "-        \"frequencies = identify_frequencies(preprocessed_df)\\n\",\n",
      "-        \"display(frequencies)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### **Run the Time Series Dataset Test Suite**\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"Run the same suite again after handling frequencies.     \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=preprocessed_df,\\n\",\n",
      "-        \"    target_column=demo_dataset.target_column,\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"full_suite = vm.run_test_suite(\\n\",\n",
      "-        \"    \\\"time_series_dataset\\\",\\n\",\n",
      "-        \"    dataset=vm_dataset,\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Handle Missing Values\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Handle the missing values by droping all the `nan` values. \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"preprocessed_df = preprocessed_df.dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### **Run the Time Series Dataset Test Suite**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Run the same test suite to check there are no missing values and frequencies of all variables are the same.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=preprocessed_df,\\n\",\n",
      "-        \"    target_column=demo_dataset.target_column,\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"full_suite = vm.run_test_suite(\\n\",\n",
      "-        \"    \\\"time_series_dataset\\\",\\n\",\n",
      "-        \"    dataset=vm_dataset,\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Handle Stationarity\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Handle stationarity by taking the first difference. \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# preprocessed_df = preprocessed_df.diff().fillna(method='bfill')\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### **Run the Time Series Dataset Test Suite**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"#vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"#    dataset=preprocessed_df,\\n\",\n",
      "-        \"#    target_column=demo_dataset.target_column,\\n\",\n",
      "-        \"#)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"#full_suite = vm.run_test_suite(\\n\",\n",
      "-        \"#    \\\"time_series_dataset\\\",\\n\",\n",
      "-        \"#    dataset=vm_dataset,\\n\",\n",
      "-        \"#)\"\n",
      "-      ]\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    },\n",
      "-    \"orig_nbformat\": 4\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_demo.ipynb b/notebooks/code_sharing/time_series/time_series_demo.ipynb\n",
      "deleted file mode 100644\n",
      "index abfcce5b8..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_demo.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,157 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# Time Series Test Plan Demo\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# This environment variable can be set to silence the summarized output of test results, for testing purposes.\\n\",\n",
      "-        \"#\\n\",\n",
      "-        \"# %env VM_SUMMARIZE_test_suiteS = False\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"import pandas as pd\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"import dotenv\\n\",\n",
      "-        \"dotenv.load_dotenv()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = pd.read_csv(\\\"../datasets/lending_club_loan_rates.csv\\\")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.head()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.rename(columns={'Unnamed: 0': 'DATE'})\\n\",\n",
      "-        \"df = df.set_index(pd.to_datetime(df['DATE']))\\n\",\n",
      "-        \"df.drop([\\\"DATE\\\"], axis=1, inplace=True)\\n\",\n",
      "-        \"df\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.describe_plan(\\\"time_series_univariate\\\")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {\n",
      "-        \"scrolled\": false,\n",
      "-        \"tags\": []\n",
      "-      },\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"loan_rate_columns = [\\\"loan_rate_A\\\", \\\"loan_rate_B\\\", \\\"loan_rate_C\\\", \\\"loan_rate_D\\\"]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"columns\\\": loan_rate_columns,\\n\",\n",
      "-        \"    \\\"seasonal_decompose\\\": {\\\"seasonal_model\\\": \\\"additive\\\"},\\n\",\n",
      "-        \"    \\\"auto_seasonality\\\": {\\\"min_period\\\": 1, \\\"max_period\\\": 4},\\n\",\n",
      "-        \"    \\\"auto_stationarity\\\": {\\\"max_order\\\": 5, \\\"threshold\\\": 0.05},\\n\",\n",
      "-        \"    \\\"rolling_stats_plot\\\": {\\\"window_size\\\": 12},\\n\",\n",
      "-        \"    \\\"auto_ar\\\": {\\\"max_ar_order\\\": 3},\\n\",\n",
      "-        \"    \\\"auto_ma\\\": {\\\"max_ma_order\\\": 3},\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"plan = vm.run_test_suite(\\\"time_series_univariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    }\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 4\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_forecast_gls_algo.ipynb b/notebooks/code_sharing/time_series/time_series_forecast_gls_algo.ipynb\n",
      "deleted file mode 100644\n",
      "index 01ae9ea9f..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_forecast_gls_algo.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,784 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# GLS algorithm - Loan Rates Forecast Model\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Setup\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Load API key and secret from environment variables\\n\",\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\"\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# System libraries\\n\",\n",
      "-        \"import glob\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# ML libraries\\n\",\n",
      "-        \"import pandas as pd\\n\",\n",
      "-        \"import numpy as np\\n\",\n",
      "-        \"from scipy import stats\\n\",\n",
      "-        \"import statsmodels.api as sm\\n\",\n",
      "-        \"from statsmodels.tsa.seasonal import seasonal_decompose\\n\",\n",
      "-        \"from statsmodels.tsa.ar_model import AutoReg\\n\",\n",
      "-        \"from statsmodels.tsa.stattools import adfuller, kpss\\n\",\n",
      "-        \"from statsmodels.tsa.arima.model import ARIMA\\n\",\n",
      "-        \"from statsmodels.tsa.stattools import coint\\n\",\n",
      "-        \"from arch.unitroot import PhillipsPerron, DFGLS\\n\",\n",
      "-        \"import xgboost as xgb\\n\",\n",
      "-        \"from numpy import argmax\\n\",\n",
      "-        \"from sklearn.metrics import accuracy_score, precision_recall_curve\\n\",\n",
      "-        \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Plotting libraries\\n\",\n",
      "-        \"import matplotlib.pyplot as plt\\n\",\n",
      "-        \"import seaborn as sns\\n\",\n",
      "-        \"%matplotlib inline\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Load FRED Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def merge_fred_csv_files(file_pattern):\\n\",\n",
      "-        \"    # Use glob to find all files matching the specified pattern\\n\",\n",
      "-        \"    file_list = glob.glob(file_pattern)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Initialize an empty list to store individual DataFrames\\n\",\n",
      "-        \"    dataframes = []\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Iterate through each file in the file list\\n\",\n",
      "-        \"    for file in file_list:\\n\",\n",
      "-        \"        # Read the CSV file into a DataFrame\\n\",\n",
      "-        \"        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"        # Add the DataFrame to the list of DataFrames\\n\",\n",
      "-        \"        dataframes.append(df)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Merge all the DataFrames in the list into a single DataFrame\\n\",\n",
      "-        \"    merged_df = pd.concat(dataframes, axis=1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    return merged_df\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"file_pattern = '../datasets/time_series/raw/fred/*.csv'\\n\",\n",
      "-        \"df = merge_fred_csv_files(file_pattern)\\n\",\n",
      "-        \"display(df)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Preselection of variables.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"selected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS']\\n\",\n",
      "-        \"df = df[selected_cols]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Plot time series.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def plot_time_series(df, cols_to_plot=None, title=''):\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    Plot multiple time-series in the same axes using seaborn.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    :param df: DataFrame with time-series data\\n\",\n",
      "-        \"    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\\n\",\n",
      "-        \"    :param title: Title of the plot, default is ''\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    if cols_to_plot is None:\\n\",\n",
      "-        \"        cols_to_plot = df.columns.tolist()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Create a new DataFrame with the columns to plot\\n\",\n",
      "-        \"    plot_df = df[cols_to_plot]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Set seaborn plot style\\n\",\n",
      "-        \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Plot the time-series data\\n\",\n",
      "-        \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-        \"    for col in plot_df.columns:\\n\",\n",
      "-        \"        sns.lineplot(data=plot_df[col], label=col)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    plt.xlabel('Date')\\n\",\n",
      "-        \"    plt.ylabel('Value')\\n\",\n",
      "-        \"    plt.title(title)\\n\",\n",
      "-        \"    plt.legend()\\n\",\n",
      "-        \"    plt.show()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"plot_time_series(df, title='All Variables')\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.info()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Data Quality\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Handling frequencies.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.resample('MS').last()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Missing Values\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Drop missing values.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Outliers\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"config={\\n\",\n",
      "-        \"    \\\"time_series_outliers\\\": {\\n\",\n",
      "-        \"        \\\"zscore_threshold\\\": 3,\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_missing_values\\\":{\\n\",\n",
      "-        \"        \\\"min_threshold\\\": 2,\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"data_quality_testplan = vm.run_test_suite(\\\"time_series_data_quality\\\", dataset=vm_dataset, config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Exploratory Data Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Univariate Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df_diff = df.diff().dropna()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"time_series_line_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_histogram\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"acf_pacf_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ar\\\": {\\n\",\n",
      "-        \"        \\\"max_ar_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ma\\\": {\\n\",\n",
      "-        \"        \\\"max_ma_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"seasonal_decompose\\\": {\\n\",\n",
      "-        \"        \\\"seasonal_model\\\": 'additive',\\n\",\n",
      "-        \"         \\\"fig_size\\\": (40,30)\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_seasonality\\\": {\\n\",\n",
      "-        \"        \\\"min_period\\\": 1,\\n\",\n",
      "-        \"        \\\"max_period\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"auto_stationarity\\\": {\\n\",\n",
      "-        \"        \\\"max_order\\\": 3,\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"rolling_stats_plot\\\": {\\n\",\n",
      "-        \"        \\\"window_size\\\": 12\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df, target_column=\\\"MORTGAGE30US\\\"\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"univariate_testplan = vm.run_test_suite(\\\"time_series_univariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Multivariate Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.describe_plan(\\\"time_series_multivariate\\\")\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"scatter_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"lagged_correlation_heatmap\\\": {\\n\",\n",
      "-        \"        \\\"target_col\\\": 'MORTGAGE30US',\\n\",\n",
      "-        \"        \\\"independent_vars\\\": [\\\"GS10\\\", \\\"FEDFUNDS\\\", \\\"UNRATE\\\"]\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"engle_granger_coint\\\": {\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"multivariate_plan = vm.run_test_suite(\\\"time_series_multivariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Methodology\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Training Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Sampling \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Split dataset into Training and Test**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"split_index = int(len(df) * 0.8)   # use 80% of the data for training\\n\",\n",
      "-        \"df_train, df_test = df[:split_index], df[split_index:]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Create a Stationary Train and Test Dataset**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Apply first difference to both training and test df\\n\",\n",
      "-        \"df_train_diff = df_train.diff().dropna()\\n\",\n",
      "-        \"df_test_diff = df_test.diff().dropna()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Training\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 1: Loan Rates and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add the independent variables with no intercept\\n\",\n",
      "-        \"X_1 = df_train_diff['FEDFUNDS']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_1 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_1 = sm.GLS(y_1, X_1).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_1.summary())\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 2: Loan Rates, constant and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_training_dataset['const'] = 1.0\\n\",\n",
      "-        \"m2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_test_dataset['const'] = 1.0\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_2 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_2 = sm.GLS(y_2, X_2).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_2.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 3: Loan Rates and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"m3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_3 = df_train_diff['GS10']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_3 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_3 = sm.OLS(y_3, X_3).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_3.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 4: Loan Rates, FEDFUNDS and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X = df_train_diff[['GS10', 'FEDFUNDS']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_4 = sm.GLS(y, X).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_4.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 5: Loan Rates, FEDFUNDS, GS10 and UNRATE\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_5 = sm.GLS(y, X).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_5.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model Selection\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model performace Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_performance_test_suite = vm.run_test_suite(\\\"regression_model_performance\\\",\\n\",\n",
      "-        \"                                             model=vm_model_1\\n\",\n",
      "-        \"                                            )\\n\",\n",
      "-        \"\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model Comparison Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_comparison_test_suite = vm.run_test_suite(\\\"regression_models_comparison\\\",\\n\",\n",
      "-        \"                                             model = vm_model_1,\\n\",\n",
      "-        \"                                             models= [vm_model_2, vm_model_3],\\n\",\n",
      "-        \"                                            )\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model Forecasting Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"config= {\\n\",\n",
      "-        \"    \\\"regression_forecast_plot\\\": {\\n\",\n",
      "-        \"        \\\"start_date\\\": '2010-01-01',\\n\",\n",
      "-        \"        \\\"end_date\\\": '2022-01-01'\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"forcasting_testplan = vm.run_test_suite(\\\"time_series_forecast\\\",\\n\",\n",
      "-        \"                                        models=[vm_model_1],\\n\",\n",
      "-        \"                                        config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": []\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    },\n",
      "-    \"orig_nbformat\": 4\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_forecast_sklearn_linear_regression_algo.ipynb b/notebooks/code_sharing/time_series/time_series_forecast_sklearn_linear_regression_algo.ipynb\n",
      "deleted file mode 100644\n",
      "index 3cc87d066..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_forecast_sklearn_linear_regression_algo.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,792 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# Sklearn linear regression algorithm - Loan Rates Forecast Model\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Setup\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Load API key and secret from environment variables\\n\",\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\"\\n\",\n",
      "-        \")\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# System libraries\\n\",\n",
      "-        \"import glob\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# ML libraries\\n\",\n",
      "-        \"from sklearn.linear_model import LinearRegression\\n\",\n",
      "-        \"import numpy as np\\n\",\n",
      "-        \"from numpy import argmax\\n\",\n",
      "-        \"import pandas as pd\\n\",\n",
      "-        \"from sklearn.metrics import accuracy_score, precision_recall_curve\\n\",\n",
      "-        \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Plotting libraries\\n\",\n",
      "-        \"import matplotlib.pyplot as plt\\n\",\n",
      "-        \"import seaborn as sns\\n\",\n",
      "-        \"%matplotlib inline\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Load FRED Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def merge_fred_csv_files(file_pattern):\\n\",\n",
      "-        \"    # Use glob to find all files matching the specified pattern\\n\",\n",
      "-        \"    file_list = glob.glob(file_pattern)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Initialize an empty list to store individual DataFrames\\n\",\n",
      "-        \"    dataframes = []\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Iterate through each file in the file list\\n\",\n",
      "-        \"    for file in file_list:\\n\",\n",
      "-        \"        # Read the CSV file into a DataFrame\\n\",\n",
      "-        \"        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"        # Add the DataFrame to the list of DataFrames\\n\",\n",
      "-        \"        dataframes.append(df)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Merge all the DataFrames in the list into a single DataFrame\\n\",\n",
      "-        \"    merged_df = pd.concat(dataframes, axis=1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    return merged_df\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"file_pattern = '../datasets/time_series/raw/fred/*.csv'\\n\",\n",
      "-        \"df = merge_fred_csv_files(file_pattern)\\n\",\n",
      "-        \"display(df)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Preselection of variables.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"selected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS']\\n\",\n",
      "-        \"df = df[selected_cols]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Plot time series.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def plot_time_series(df, cols_to_plot=None, title=''):\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    Plot multiple time-series in the same axes using seaborn.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    :param df: DataFrame with time-series data\\n\",\n",
      "-        \"    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\\n\",\n",
      "-        \"    :param title: Title of the plot, default is ''\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    if cols_to_plot is None:\\n\",\n",
      "-        \"        cols_to_plot = df.columns.tolist()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Create a new DataFrame with the columns to plot\\n\",\n",
      "-        \"    plot_df = df[cols_to_plot]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Set seaborn plot style\\n\",\n",
      "-        \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Plot the time-series data\\n\",\n",
      "-        \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-        \"    for col in plot_df.columns:\\n\",\n",
      "-        \"        sns.lineplot(data=plot_df[col], label=col)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    plt.xlabel('Date')\\n\",\n",
      "-        \"    plt.ylabel('Value')\\n\",\n",
      "-        \"    plt.title(title)\\n\",\n",
      "-        \"    plt.legend()\\n\",\n",
      "-        \"    plt.show()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"plot_time_series(df, title='All Variables')\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.info()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Data Quality\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Handling frequencies.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.resample('MS').last()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Missing Values\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Drop missing values.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Outliers\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\"\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"config={\\n\",\n",
      "-        \"    \\\"time_series_outliers\\\": {\\n\",\n",
      "-        \"        \\\"zscore_threshold\\\": 3,\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_missing_values\\\":{\\n\",\n",
      "-        \"        \\\"min_threshold\\\": 2,\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"data_quality_testplan = vm.run_test_suite(\\\"time_series_data_quality\\\", dataset=vm_dataset, config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Exploratory Data Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Univariate Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df_diff = df.diff().dropna()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"time_series_line_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_histogram\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"acf_pacf_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ar\\\": {\\n\",\n",
      "-        \"        \\\"max_ar_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ma\\\": {\\n\",\n",
      "-        \"        \\\"max_ma_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"seasonal_decompose\\\": {\\n\",\n",
      "-        \"        \\\"seasonal_model\\\": 'additive',\\n\",\n",
      "-        \"         \\\"fig_size\\\": (40,30)\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_seasonality\\\": {\\n\",\n",
      "-        \"        \\\"min_period\\\": 1,\\n\",\n",
      "-        \"        \\\"max_period\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"auto_stationarity\\\": {\\n\",\n",
      "-        \"        \\\"max_order\\\": 3,\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"rolling_stats_plot\\\": {\\n\",\n",
      "-        \"        \\\"window_size\\\": 12\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\"\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"univariate_testplan = vm.run_test_suite(\\\"time_series_univariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Multivariate Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.describe_plan(\\\"time_series_multivariate\\\")\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"scatter_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"lagged_correlation_heatmap\\\": {\\n\",\n",
      "-        \"        \\\"target_col\\\": 'MORTGAGE30US',\\n\",\n",
      "-        \"        \\\"independent_vars\\\": [\\\"GS10\\\", \\\"FEDFUNDS\\\", \\\"UNRATE\\\"]\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"engle_granger_coint\\\": {\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"multivariate_plan = vm.run_test_suite(\\\"time_series_multivariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Methodology\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Training Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Sampling \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Split dataset into Training and Test**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"split_index = int(len(df) * 0.8)   # use 80% of the data for training\\n\",\n",
      "-        \"df_train, df_test = df[:split_index], df[split_index:]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Create a Stationary Train and Test Dataset**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Apply first difference to both training and test df\\n\",\n",
      "-        \"df_train_diff = df_train.diff().dropna()\\n\",\n",
      "-        \"df_test_diff = df_test.diff().dropna()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Training\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 1: Loan Rates and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add the independent variables with no intercept\\n\",\n",
      "-        \"X_1 = np.array(df_train_diff['FEDFUNDS']).reshape(-1,1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_1 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_1 = LinearRegression().fit(X_1, y_1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(repr(model_1))\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 2: Loan Rates, constant and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_training_dataset['const'] = 1.0\\n\",\n",
      "-        \"m2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_test_dataset['const'] = 1.0\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"temp_df = df_train_diff.copy(deep=True)\\n\",\n",
      "-        \"temp_df['const'] = 1.0\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_2 = temp_df[['const', 'FEDFUNDS']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_2 = temp_df['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_2 = LinearRegression().fit(X=X_2, y=y_2)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_2)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 3: Loan Rates and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"m3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_3 = np.array(df_train_diff['GS10']).reshape(-1,1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_3 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_3 = LinearRegression().fit(X_3, y_3)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_3)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 4: Loan Rates, FEDFUNDS and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m4_training_dataset = df_train_diff[['GS10', 'FEDFUNDS', 'MORTGAGE30US']]\\n\",\n",
      "-        \"m4_test_dataset = df_test_diff[['GS10', 'FEDFUNDS', 'MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_4 = df_train_diff[['GS10', 'FEDFUNDS']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_4 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_4 = LinearRegression().fit(X_4, y_4)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_4)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 5: Loan Rates, FEDFUNDS, GS10 and UNRATE\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m5_training_dataset = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE', 'MORTGAGE30US']]\\n\",\n",
      "-        \"m5_test_dataset = df_test_diff[['GS10', 'FEDFUNDS', 'UNRATE', 'MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_5 = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_5 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_5 = LinearRegression().fit(X=X_5, y=y_5)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_5)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model Selection\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model performace Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_performance_test_suite = vm.run_test_suite(\\\"regression_model_description\\\",\\n\",\n",
      "-        \"                                             model=vm_model_1\\n\",\n",
      "-        \"                                            )\\n\",\n",
      "-        \"\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model Comparison Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_4 = vm.init_dataset(dataset=m4_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_4 = vm.init_dataset(dataset=m4_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_4 = vm.init_model(model_4, train_ds=vm_train_ds_4, test_ds=vm_test_ds_4, validation_ds=vm_test_ds_4)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_comparison_test_suite = vm.run_test_suite(\\\"regression_models_evaluation\\\",\\n\",\n",
      "-        \"                                             model = vm_model_1,\\n\",\n",
      "-        \"                                             models= [vm_model_3, vm_model_4],\\n\",\n",
      "-        \"                                            )\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model Forcasting Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"config= {\\n\",\n",
      "-        \"    \\\"regression_forecast_plot\\\": {\\n\",\n",
      "-        \"        \\\"start_date\\\": '2010-01-01',\\n\",\n",
      "-        \"        \\\"end_date\\\": '2022-01-01'\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"forcasting_testplan = vm.run_test_suite(\\\"time_series_forecast\\\",\\n\",\n",
      "-        \"                                        models=[vm_model_4, vm_model_3],\\n\",\n",
      "-        \"                                        config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"\\n\"\n",
      "-      ]\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    },\n",
      "-    \"orig_nbformat\": 4\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_forecast_wls_algo.ipynb b/notebooks/code_sharing/time_series/time_series_forecast_wls_algo.ipynb\n",
      "deleted file mode 100644\n",
      "index 5b5aa9cd1..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_forecast_wls_algo.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,784 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# Statsmodels - WLS algorithm - Loan Rates Forecast Model\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Setup\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Load API key and secret from environment variables\\n\",\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\"\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# System libraries\\n\",\n",
      "-        \"import glob\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# ML libraries\\n\",\n",
      "-        \"import pandas as pd\\n\",\n",
      "-        \"import numpy as np\\n\",\n",
      "-        \"from scipy import stats\\n\",\n",
      "-        \"import statsmodels.api as sm\\n\",\n",
      "-        \"from statsmodels.tsa.seasonal import seasonal_decompose\\n\",\n",
      "-        \"from statsmodels.tsa.ar_model import AutoReg\\n\",\n",
      "-        \"from statsmodels.tsa.stattools import adfuller, kpss\\n\",\n",
      "-        \"from statsmodels.tsa.arima.model import ARIMA\\n\",\n",
      "-        \"from statsmodels.tsa.stattools import coint\\n\",\n",
      "-        \"from arch.unitroot import PhillipsPerron, DFGLS\\n\",\n",
      "-        \"import xgboost as xgb\\n\",\n",
      "-        \"from numpy import argmax\\n\",\n",
      "-        \"from sklearn.metrics import accuracy_score, precision_recall_curve\\n\",\n",
      "-        \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Plotting libraries\\n\",\n",
      "-        \"import matplotlib.pyplot as plt\\n\",\n",
      "-        \"import seaborn as sns\\n\",\n",
      "-        \"%matplotlib inline\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Load FRED Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def merge_fred_csv_files(file_pattern):\\n\",\n",
      "-        \"    # Use glob to find all files matching the specified pattern\\n\",\n",
      "-        \"    file_list = glob.glob(file_pattern)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Initialize an empty list to store individual DataFrames\\n\",\n",
      "-        \"    dataframes = []\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Iterate through each file in the file list\\n\",\n",
      "-        \"    for file in file_list:\\n\",\n",
      "-        \"        # Read the CSV file into a DataFrame\\n\",\n",
      "-        \"        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"        # Add the DataFrame to the list of DataFrames\\n\",\n",
      "-        \"        dataframes.append(df)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Merge all the DataFrames in the list into a single DataFrame\\n\",\n",
      "-        \"    merged_df = pd.concat(dataframes, axis=1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    return merged_df\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"file_pattern = '../datasets/time_series/raw/fred/*.csv'\\n\",\n",
      "-        \"df = merge_fred_csv_files(file_pattern)\\n\",\n",
      "-        \"display(df)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Preselection of variables.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"selected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS']\\n\",\n",
      "-        \"df = df[selected_cols]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Plot time series.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def plot_time_series(df, cols_to_plot=None, title=''):\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    Plot multiple time-series in the same axes using seaborn.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    :param df: DataFrame with time-series data\\n\",\n",
      "-        \"    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\\n\",\n",
      "-        \"    :param title: Title of the plot, default is ''\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    if cols_to_plot is None:\\n\",\n",
      "-        \"        cols_to_plot = df.columns.tolist()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Create a new DataFrame with the columns to plot\\n\",\n",
      "-        \"    plot_df = df[cols_to_plot]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Set seaborn plot style\\n\",\n",
      "-        \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Plot the time-series data\\n\",\n",
      "-        \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-        \"    for col in plot_df.columns:\\n\",\n",
      "-        \"        sns.lineplot(data=plot_df[col], label=col)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    plt.xlabel('Date')\\n\",\n",
      "-        \"    plt.ylabel('Value')\\n\",\n",
      "-        \"    plt.title(title)\\n\",\n",
      "-        \"    plt.legend()\\n\",\n",
      "-        \"    plt.show()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"plot_time_series(df, title='All Variables')\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.info()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Data Quality\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Handling frequencies.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.resample('MS').last()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Missing Values\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Drop missing values.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Outliers\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"config={\\n\",\n",
      "-        \"    \\\"time_series_outliers\\\": {\\n\",\n",
      "-        \"        \\\"zscore_threshold\\\": 3,\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_missing_values\\\":{\\n\",\n",
      "-        \"        \\\"min_threshold\\\": 2,\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"data_quality_testplan = vm.run_test_suite(\\\"time_series_data_quality\\\", dataset=vm_dataset, config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Exploratory Data Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Univariate Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df_diff = df.diff().dropna()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"time_series_line_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"time_series_histogram\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"acf_pacf_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ar\\\": {\\n\",\n",
      "-        \"        \\\"max_ar_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_ma\\\": {\\n\",\n",
      "-        \"        \\\"max_ma_order\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"seasonal_decompose\\\": {\\n\",\n",
      "-        \"        \\\"seasonal_model\\\": 'additive',\\n\",\n",
      "-        \"         \\\"fig_size\\\": (40,30)\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"auto_seasonality\\\": {\\n\",\n",
      "-        \"        \\\"min_period\\\": 1,\\n\",\n",
      "-        \"        \\\"max_period\\\": 3\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"auto_stationarity\\\": {\\n\",\n",
      "-        \"        \\\"max_order\\\": 3,\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"      \\\"rolling_stats_plot\\\": {\\n\",\n",
      "-        \"        \\\"window_size\\\": 12\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_dataset = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=df, target_column=\\\"MORTGAGE30US\\\"\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"univariate_testplan = vm.run_test_suite(\\\"time_series_univariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Multivariate Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.describe_plan(\\\"time_series_multivariate\\\")\\n\",\n",
      "-        \"test_suite_config = {\\n\",\n",
      "-        \"    \\\"scatter_plot\\\": {\\n\",\n",
      "-        \"        \\\"columns\\\": df.columns\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"lagged_correlation_heatmap\\\": {\\n\",\n",
      "-        \"        \\\"target_col\\\": 'MORTGAGE30US',\\n\",\n",
      "-        \"        \\\"independent_vars\\\": [\\\"GS10\\\", \\\"FEDFUNDS\\\", \\\"UNRATE\\\"]\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"engle_granger_coint\\\": {\\n\",\n",
      "-        \"        \\\"threshold\\\": 0.05\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"multivariate_plan = vm.run_test_suite(\\\"time_series_multivariate\\\", config=test_suite_config, dataset=vm_dataset)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Methodology\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Training Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Sampling \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Split dataset into Training and Test**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"split_index = int(len(df) * 0.8)   # use 80% of the data for training\\n\",\n",
      "-        \"df_train, df_test = df[:split_index], df[split_index:]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Create a Stationary Train and Test Dataset**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Apply first difference to both training and test df\\n\",\n",
      "-        \"df_train_diff = df_train.diff().dropna()\\n\",\n",
      "-        \"df_test_diff = df_test.diff().dropna()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Training\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 1: Loan Rates and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add the independent variables with no intercept\\n\",\n",
      "-        \"X_1 = df_train_diff['FEDFUNDS']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_1 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_1 = sm.WLS(y_1, X_1).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_1.summary())\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 2: Loan Rates, constant and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_training_dataset['const'] = 1.0\\n\",\n",
      "-        \"m2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_test_dataset['const'] = 1.0\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_2 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_2 = sm.WLS(y_2, X_2).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_2.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 3: Loan Rates and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"m3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_3 = df_train_diff['GS10']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_3 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_3 = sm.WLS(y_3, X_3).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_3.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 4: Loan Rates, FEDFUNDS and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X = df_train_diff[['GS10', 'FEDFUNDS']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_4 = sm.WLS(y, X).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_4.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 5: Loan Rates, FEDFUNDS, GS10 and UNRATE\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_5 = sm.WLS(y, X).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_5.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model Selection\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model performace Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_performance_test_suite = vm.run_test_suite(\\\"regression_model_description\\\",\\n\",\n",
      "-        \"                                             model=vm_model_1\\n\",\n",
      "-        \"                                            )\\n\",\n",
      "-        \"\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model Comparison Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_comparison_test_suite = vm.run_test_suite(\\\"regression_models_evaluation\\\",\\n\",\n",
      "-        \"                                             model = vm_model_1,\\n\",\n",
      "-        \"                                             models= [vm_model_2, vm_model_3],\\n\",\n",
      "-        \"                                            )\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model Forecasting Test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"config= {\\n\",\n",
      "-        \"    \\\"regression_forecast_plot\\\": {\\n\",\n",
      "-        \"        \\\"start_date\\\": '2010-01-01',\\n\",\n",
      "-        \"        \\\"end_date\\\": '2022-01-01'\\n\",\n",
      "-        \"    }\\n\",\n",
      "-        \"}\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"forcasting_testplan = vm.run_test_suite(\\\"time_series_forecast\\\",\\n\",\n",
      "-        \"                                        models=[vm_model_1],\\n\",\n",
      "-        \"                                        config=config)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": []\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    },\n",
      "-    \"orig_nbformat\": 4\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_model_validation_full_suite.ipynb b/notebooks/code_sharing/time_series/time_series_model_validation_full_suite.ipynb\n",
      "deleted file mode 100644\n",
      "index 7cffa30e8..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_model_validation_full_suite.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,289 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# Time Series Model Validation Full Suite\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 1. Introduction\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"The Time Series Data Validation Full Suite notebook aims to demonstrate the application of various data validation tests using the **ValidMind MRM Platform** and **Developer Framework**. \\n\",\n",
      "-        \"\\n\",\n",
      "-        \"In this demo, we will use the `time_series_model_validation` **test suite** to run multiple model validation tests on several pre-trained time series models.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 2. Setup \\n\",\n",
      "-        \"\\n\",\n",
      "-        \"Prepare the environment for our analysis. First, **import** all necessary libraries and modules required for our analysis. Next, **connect** to the ValidMind MRM platform, which provides a comprehensive suite of tools and services for model validation.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"Finally, define and **configure** the specific use case we are working on by setting up any required parameters, data sources, or other settings that will be used throughout the analysis. \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Import Libraries\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Load API key and secret from environment variables\\n\",\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# ValidMind libraries\\n\",\n",
      "-        \"import validmind as vm\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Connect to the ValidMind Library\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm.init(\\n\",\n",
      "-        \"    api_host=\\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"    project=\\\"clhhzo21s006wl9rl0swhv40h\\\",\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Find All Test Suites and Plans Available in the Developer Framework\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"We can find all the **test suites** and **test plans** available in the developer framework by calling the following functions:\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"- All test suites: `vm.test_suites.list_suites()`\\n\",\n",
      "-        \"- All test plans: `vm.test_suites.list_plans()`\\n\",\n",
      "-        \"- Describe a test plan: `vm.test_suites.describe_plan(\\\"time_series_data_quality\\\")`\\n\",\n",
      "-        \"- List all available tests: `vm.tests.list_tests()`\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_suites()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 3. Load Models\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Load Pre-Trained Time Series Models\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Currently only fred pre-trained models are available\\n\",\n",
      "-        \"from validmind.datasets.regression import fred as demo_dataset\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_A, train_df_A, test_df_A = demo_dataset.load_model(\\\"fred_loan_rates_model_3\\\")\\n\",\n",
      "-        \"model_B, train_df_B, test_df_B = demo_dataset.load_model(\\\"fred_loan_rates_model_4\\\")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Create ValidMind Datasets\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Initialize training and testing datasets for model A\\n\",\n",
      "-        \"vm_train_ds_A = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=train_df_A, type=\\\"generic\\\", target_column=demo_dataset.target_column\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"vm_test_ds_A = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=test_df_A, type=\\\"generic\\\", target_column=demo_dataset.target_column\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Initialize training and testing datasets for model B\\n\",\n",
      "-        \"vm_train_ds_B = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=train_df_B, type=\\\"generic\\\", target_column=demo_dataset.target_column\\n\",\n",
      "-        \")\\n\",\n",
      "-        \"vm_test_ds_B = vm.init_dataset(\\n\",\n",
      "-        \"    dataset=test_df_B, type=\\\"generic\\\", target_column=demo_dataset.target_column\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Create ValidMind Models\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Initialize model A\\n\",\n",
      "-        \"vm_model_A = vm.init_model(model=model_A, train_ds=vm_train_ds_A, test_ds=vm_test_ds_A)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Initialize model B\\n\",\n",
      "-        \"vm_model_B = vm.init_model(model=model_B, train_ds=vm_train_ds_B, test_ds=vm_test_ds_B)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"list_of_models = [vm_model_A, vm_model_B]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## 4. Model Validation\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### User Configuration of Test Suite \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Users can input the configuration to a test suite using **`config`**, allowing fine-tuning the suite according to their specific data requirements. \\n\",\n",
      "-        \"\\n\",\n",
      "-        \"**Time Series Forecast params**\\n\",\n",
      "-        \"- `transformation` specify the desired plotting settings for regression forecast evaluation. In this particular configuration, the chosen transformation is \\\"integrate.\\\"\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"config = {\\n\",\n",
      "-        \"    \\\"regression_forecast_plot_levels\\\": {\\n\",\n",
      "-        \"        \\\"transformation\\\": \\\"integrate\\\",\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"    \\\"regression_sensitivity_plot\\\": {\\n\",\n",
      "-        \"        \\\"transformation\\\": \\\"integrate\\\",\\n\",\n",
      "-        \"        \\\"shocks\\\": [0.1, 0.2],\\n\",\n",
      "-        \"    },\\n\",\n",
      "-        \"}\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"full_suite = vm.run_test_suite(\\n\",\n",
      "-        \"    \\\"time_series_model_validation\\\",\\n\",\n",
      "-        \"    model=vm_model_B,\\n\",\n",
      "-        \"    models=list_of_models,\\n\",\n",
      "-        \"    config=config,\\n\",\n",
      "-        \")\"\n",
      "-      ]\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    },\n",
      "-    \"orig_nbformat\": 4\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_model_validation_poc.ipynb b/notebooks/code_sharing/time_series/time_series_model_validation_poc.ipynb\n",
      "deleted file mode 100644\n",
      "index 290f92a1e..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_model_validation_poc.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,393 +0,0 @@\n",
      "-{\n",
      "- \"cells\": [\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"# Time Series Model Validation POC\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## Import Libraries \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# System libraries\\n\",\n",
      "-    \"import glob\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# ML libraries\\n\",\n",
      "-    \"import pickle\\n\",\n",
      "-    \"import pandas as pd\\n\",\n",
      "-    \"import numpy as np\\n\",\n",
      "-    \"from scipy import stats\\n\",\n",
      "-    \"import statsmodels.api as sm\\n\",\n",
      "-    \"from statsmodels.tsa.seasonal import seasonal_decompose\\n\",\n",
      "-    \"from statsmodels.tsa.ar_model import AutoReg\\n\",\n",
      "-    \"from statsmodels.tsa.stattools import adfuller, kpss\\n\",\n",
      "-    \"from statsmodels.tsa.arima.model import ARIMA\\n\",\n",
      "-    \"from statsmodels.tsa.stattools import coint\\n\",\n",
      "-    \"from arch.unitroot import PhillipsPerron, DFGLS\\n\",\n",
      "-    \"import xgboost as xgb\\n\",\n",
      "-    \"from numpy import argmax\\n\",\n",
      "-    \"from sklearn.metrics import accuracy_score, precision_recall_curve\\n\",\n",
      "-    \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Plotting libraries\\n\",\n",
      "-    \"import matplotlib.pyplot as plt\\n\",\n",
      "-    \"import seaborn as sns\\n\",\n",
      "-    \"%matplotlib inline\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## Load Time Series Models\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"with open('../models/time_series/fred_loan_rates_model_1.pkl', 'rb') as f:\\n\",\n",
      "-    \"    model_1 = pickle.load(f)\\n\",\n",
      "-    \"print(\\n\",\n",
      "-    \"\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"with open('../models/time_series/fred_loan_rates_model_2.pkl', 'rb') as f:\\n\",\n",
      "-    \"    model_2 = pickle.load(f)\\n\",\n",
      "-    \"print(model_2.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"with open('../models/time_series/fred_loan_rates_model_3.pkl', 'rb') as f:\\n\",\n",
      "-    \"    model_3 = pickle.load(f)\\n\",\n",
      "-    \"print(model_3.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"with open('../models/time_series/fred_loan_rates_model_4.pkl', 'rb') as f:\\n\",\n",
      "-    \"    model_4 = pickle.load(f)\\n\",\n",
      "-    \"print(model_4.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"with open('../models/time_series/fred_loan_rates_model_5.pkl', 'rb') as f:\\n\",\n",
      "-    \"    model_5 = pickle.load(f)\\n\",\n",
      "-    \"print(model_5.summary())\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def extract_coef_stats(summary, model_name):\\n\",\n",
      "-    \"    table = summary.tables[1].data\\n\",\n",
      "-    \"    headers = table.pop(0)\\n\",\n",
      "-    \"    headers[0] = 'Feature'\\n\",\n",
      "-    \"    df = pd.DataFrame(table, columns=headers)\\n\",\n",
      "-    \"    df['Model'] = model_name\\n\",\n",
      "-    \"    return df\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"def extract_coefficients_summary(summaries):\\n\",\n",
      "-    \"    coef_stats_df = pd.DataFrame()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    for i, summary in enumerate(summaries):\\n\",\n",
      "-    \"        model_name = f'Model {i+1}'\\n\",\n",
      "-    \"        coef_stats_df = pd.concat([coef_stats_df, extract_coef_stats(summary, model_name)])\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Reorder columns to have 'Model' as the first column and reset the index\\n\",\n",
      "-    \"    coef_stats_df = coef_stats_df.reset_index(drop=True)[['Model'] + [col for col in coef_stats_df.columns if col != 'Model']]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return coef_stats_df\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Example usage:\\n\",\n",
      "-    \"summaries = [model_1.summary(), model_2.summary(), model_3.summary()]\\n\",\n",
      "-    \"coef_stats_df = extract_coefficients_summary(summaries)\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"coef_stats_df\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## Load Training Datasets\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Extract the endogenous (target) variable from the model fit\\n\",\n",
      "-    \"train_df = pd.Series(model_1.model.endog, index=model_1.model.data.row_labels)\\n\",\n",
      "-    \"train_df = train_df.to_frame()\\n\",\n",
      "-    \"target_var_name = model_1.model.endog_names\\n\",\n",
      "-    \"train_df.columns = [target_var_name]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Extract the exogenous (explanatory) variables from the model fit\\n\",\n",
      "-    \"exog_df = pd.DataFrame(model_1.model.exog, index=model_1.model.data.row_labels, columns=model_1.model.exog_names)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"# Concatenate the endogenous (target) and exogenous (explanatory) variables\\n\",\n",
      "-    \"train_df = pd.concat([train_df, exog_df], axis=1)\\n\",\n",
      "-    \"train_df.head()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"train_df.tail()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## Load Test Datasets\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Load raw test dataset.\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"file = '../datasets/time_series/fred_loan_rates_test_1.csv'\\n\",\n",
      "-    \"raw_test_df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\\n\",\n",
      "-    \"display(raw_test_df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"Transform raw test dataset using same transformation used in the train dataset. \"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"transform_func = 'diff'\\n\",\n",
      "-    \"if transform_func == 'diff':\\n\",\n",
      "-    \"    test_df = raw_test_df.diff().dropna()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"attachments\": {},\n",
      "-   \"cell_type\": \"markdown\",\n",
      "-   \"metadata\": {},\n",
      "-   \"source\": [\n",
      "-    \"## Load Predictions\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def get_model_prediction(model_fits_dict, df_test):\\n\",\n",
      "-    \"    # Extract the training data from the first model fit\\n\",\n",
      "-    \"    first_model_fit = list(model_fits_dict.values())[0]\\n\",\n",
      "-    \"    train_data = pd.Series(first_model_fit.model.endog, index=first_model_fit.model.data.row_labels)\\n\",\n",
      "-    \"    train_data = train_data.to_frame()\\n\",\n",
      "-    \"    target_var_name = first_model_fit.model.endog_names\\n\",\n",
      "-    \"    train_data.columns = [f'{target_var_name}_train']\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Initialize an empty DataFrame to store the predictions\\n\",\n",
      "-    \"    prediction_df = pd.DataFrame(index=df_test.index)\\n\",\n",
      "-    \"    prediction_df[f'{target_var_name}_test'] = np.nan\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Concatenate the train_data and prediction_df\\n\",\n",
      "-    \"    combined_df = pd.concat([train_data, prediction_df], axis=0)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Loop through each model fit\\n\",\n",
      "-    \"    for model_name, model_fit in model_fits_dict.items():\\n\",\n",
      "-    \"        # Prepare the test dataset\\n\",\n",
      "-    \"        exog_names = model_fit.model.exog_names\\n\",\n",
      "-    \"        X_test = df_test.copy()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Add the constant if it's missing\\n\",\n",
      "-    \"        if 'const' in exog_names and 'const' not in X_test.columns:\\n\",\n",
      "-    \"            X_test['const'] = 1.0\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Select the necessary columns\\n\",\n",
      "-    \"        X_test = X_test[exog_names]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Generate the predictions\\n\",\n",
      "-    \"        predictions = model_fit.predict(X_test)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        # Add the predictions to the DataFrame\\n\",\n",
      "-    \"        combined_df[model_name] = np.nan\\n\",\n",
      "-    \"        combined_df[model_name].iloc[len(train_data):] = predictions\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    # Add the test data to the '<target_variable>_test' column\\n\",\n",
      "-    \"    combined_df[f'{target_var_name}_test'].iloc[len(train_data):] = df_test[target_var_name]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    return combined_df\\n\",\n",
      "-    \"\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# Replace with your list of model fits\\n\",\n",
      "-    \"model_fits = {\\n\",\n",
      "-    \"    'model_1': model_1,\\n\",\n",
      "-    \"    'model_3': model_3\\n\",\n",
      "-    \"}\\n\",\n",
      "-    \"prediction_df = get_model_prediction(model_fits, test_df)\\n\",\n",
      "-    \"display(prediction_df)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"def plot_predictions(prediction_df, subplot=True):\\n\",\n",
      "-    \"    n_models = prediction_df.shape[1] - 2\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    if subplot:\\n\",\n",
      "-    \"        fig, axes = plt.subplots(n_models, 1, figsize=(12, 6 * n_models), sharex=True)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        for i in range(n_models):\\n\",\n",
      "-    \"            axes[i].plot(prediction_df.index, prediction_df.iloc[:, 0], label=prediction_df.columns[0], color='grey')\\n\",\n",
      "-    \"            axes[i].plot(prediction_df.index, prediction_df.iloc[:, 1], label=prediction_df.columns[1], color='lightgrey')\\n\",\n",
      "-    \"            axes[i].plot(prediction_df.index, prediction_df.iloc[:, i + 2], label=prediction_df.columns[i + 2], linestyle='-')\\n\",\n",
      "-    \"            axes[i].set_ylabel('Target Variable')\\n\",\n",
      "-    \"            axes[i].set_title(f'Test Data vs. {prediction_df.columns[i + 2]}')\\n\",\n",
      "-    \"            axes[i].legend()\\n\",\n",
      "-    \"            axes[i].grid(True)\\n\",\n",
      "-    \"        plt.xlabel('Date')\\n\",\n",
      "-    \"        plt.tight_layout()\\n\",\n",
      "-    \"        plt.show()\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"    else:\\n\",\n",
      "-    \"        plt.figure(figsize=(12, 6))\\n\",\n",
      "-    \"        plt.plot(prediction_df.index, prediction_df.iloc[:, 0], label=prediction_df.columns[0], color='grey')\\n\",\n",
      "-    \"        plt.plot(prediction_df.index, prediction_df.iloc[:, 1], label=prediction_df.columns[1], color='lightgrey')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        for i in range(2, prediction_df.shape[1]):\\n\",\n",
      "-    \"            plt.plot(prediction_df.index, prediction_df.iloc[:, i], label=prediction_df.columns[i], linestyle='-')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"        plt.xlabel('Date')\\n\",\n",
      "-    \"        plt.ylabel('Target Variable')\\n\",\n",
      "-    \"        plt.title('Test Data vs. Model Forecasts')\\n\",\n",
      "-    \"        plt.legend()\\n\",\n",
      "-    \"        plt.grid(True)\\n\",\n",
      "-    \"        plt.show()\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"plot_predictions(prediction_df, subplot=True)\"\n",
      "-   ]\n",
      "-  }\n",
      "- ],\n",
      "- \"metadata\": {\n",
      "-  \"kernelspec\": {\n",
      "-   \"display_name\": \"validmind-eEL8LtKG-py3.10\",\n",
      "-   \"language\": \"python\",\n",
      "-   \"name\": \"python3\"\n",
      "-  },\n",
      "-  \"language_info\": {\n",
      "-   \"codemirror_mode\": {\n",
      "-    \"name\": \"ipython\",\n",
      "-    \"version\": 3\n",
      "-   },\n",
      "-   \"file_extension\": \".py\",\n",
      "-   \"mimetype\": \"text/x-python\",\n",
      "-   \"name\": \"python\",\n",
      "-   \"nbconvert_exporter\": \"python\",\n",
      "-   \"pygments_lexer\": \"ipython3\",\n",
      "-   \"version\": \"3.10.9\"\n",
      "-  },\n",
      "-  \"orig_nbformat\": 4\n",
      "- },\n",
      "- \"nbformat\": 4,\n",
      "- \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/notebooks/code_sharing/time_series/time_series_models_comparison_test_plan.ipynb b/notebooks/code_sharing/time_series/time_series_models_comparison_test_plan.ipynb\n",
      "deleted file mode 100644\n",
      "index 7a861929f..000000000\n",
      "--- a/notebooks/code_sharing/time_series/time_series_models_comparison_test_plan.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,567 +0,0 @@\n",
      "-{\n",
      "-  \"cells\": [\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"# Loan Rates Forecast Models Comparison test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Setup\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"%load_ext dotenv\\n\",\n",
      "-        \"%dotenv .env\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"import os\\n\",\n",
      "-        \"os.chdir(os.path.join(os.getcwd(), \\\"../..\\\"))\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"import validmind as vm\\n\",\n",
      "-        \"vm.init(  api_host = \\\"https://api.prod.validmind.ai/api/v1/tracking\\\",\\n\",\n",
      "-        \"  project = \\\"clhhzo21s006wl9rl0swhv40h\\\")\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# System libraries\\n\",\n",
      "-        \"import glob\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# ML libraries\\n\",\n",
      "-        \"import pandas as pd\\n\",\n",
      "-        \"import numpy as np\\n\",\n",
      "-        \"from scipy import stats\\n\",\n",
      "-        \"import statsmodels.api as sm\\n\",\n",
      "-        \"from statsmodels.tsa.seasonal import seasonal_decompose\\n\",\n",
      "-        \"from statsmodels.tsa.ar_model import AutoReg\\n\",\n",
      "-        \"from statsmodels.tsa.stattools import adfuller, kpss\\n\",\n",
      "-        \"from statsmodels.tsa.arima.model import ARIMA\\n\",\n",
      "-        \"from arch.unitroot import PhillipsPerron, DFGLS\\n\",\n",
      "-        \"import xgboost as xgb\\n\",\n",
      "-        \"from numpy import argmax\\n\",\n",
      "-        \"from sklearn.metrics import accuracy_score, precision_recall_curve\\n\",\n",
      "-        \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Plotting libraries\\n\",\n",
      "-        \"import matplotlib.pyplot as plt\\n\",\n",
      "-        \"import seaborn as sns\\n\",\n",
      "-        \"%matplotlib inline\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Load FRED Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def merge_fred_csv_files(file_pattern):\\n\",\n",
      "-        \"    # Use glob to find all files matching the specified pattern\\n\",\n",
      "-        \"    file_list = glob.glob(file_pattern)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Initialize an empty list to store individual DataFrames\\n\",\n",
      "-        \"    dataframes = []\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Iterate through each file in the file list\\n\",\n",
      "-        \"    for file in file_list:\\n\",\n",
      "-        \"        # Read the CSV file into a DataFrame\\n\",\n",
      "-        \"        df = pd.read_csv(file, parse_dates=['DATE'], index_col='DATE')\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"        # Add the DataFrame to the list of DataFrames\\n\",\n",
      "-        \"        dataframes.append(df)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Merge all the DataFrames in the list into a single DataFrame\\n\",\n",
      "-        \"    merged_df = pd.concat(dataframes, axis=1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    return merged_df\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"file_pattern = './notebooks/datasets/time_series/raw/fred/*.csv'\\n\",\n",
      "-        \"df = merge_fred_csv_files(file_pattern)\\n\",\n",
      "-        \"display(df)\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Preselection of variables.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"selected_cols = ['MORTGAGE30US', 'UNRATE', 'GS10', 'FEDFUNDS']\\n\",\n",
      "-        \"df = df[selected_cols]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"Plot time series.\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"def plot_time_series(df, cols_to_plot=None, title=''):\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    Plot multiple time-series in the same axes using seaborn.\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    :param df: DataFrame with time-series data\\n\",\n",
      "-        \"    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\\n\",\n",
      "-        \"    :param title: Title of the plot, default is ''\\n\",\n",
      "-        \"    \\\"\\\"\\\"\\n\",\n",
      "-        \"    if cols_to_plot is None:\\n\",\n",
      "-        \"        cols_to_plot = df.columns.tolist()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Create a new DataFrame with the columns to plot\\n\",\n",
      "-        \"    plot_df = df[cols_to_plot]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Set seaborn plot style\\n\",\n",
      "-        \"    sns.set(style=\\\"darkgrid\\\")\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    # Plot the time-series data\\n\",\n",
      "-        \"    plt.figure(figsize=(12, 6))\\n\",\n",
      "-        \"    for col in plot_df.columns:\\n\",\n",
      "-        \"        sns.lineplot(data=plot_df[col], label=col)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"    plt.xlabel('Date')\\n\",\n",
      "-        \"    plt.ylabel('Value')\\n\",\n",
      "-        \"    plt.title(title)\\n\",\n",
      "-        \"    plt.legend()\\n\",\n",
      "-        \"    plt.show()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"plot_time_series(df, title='All Variables')\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Data Description\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df.info()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Data Quality\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df = df.resample('MS').last()\\n\",\n",
      "-        \"df = df.dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Exploratory Data Analysis\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"df_diff = df.diff().dropna()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Training Data\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Sampling \"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Split dataset into Training and Test**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"split_index = int(len(df) * 0.8)   # use 80% of the data for training\\n\",\n",
      "-        \"df_train, df_test = df[:split_index], df[split_index:]\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Create a Stationary Train and Test Dataset**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"# Apply first difference to both training and test df\\n\",\n",
      "-        \"df_train_diff = df_train.diff().dropna()\\n\",\n",
      "-        \"df_test_diff = df_test.diff().dropna()\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model Training\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model 1: Loan Rates and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add the independent variables with no intercept\\n\",\n",
      "-        \"X_1 = df_train_diff['FEDFUNDS']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_1 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_1 = sm.OLS(y_1, X_1).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_1.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"## Model Methodology\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model Training\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model 1: Loan Rates and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m1_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m1_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add the independent variables with no intercept\\n\",\n",
      "-        \"X_1 = df_train_diff['FEDFUNDS']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_1 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_1 = sm.OLS(y_1, X_1).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_1.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 2: Loan Rates, constant and FEDFUNDS\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 1: Fit Model**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m2_training_dataset = df_train_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_training_dataset['const'] = 1.0\\n\",\n",
      "-        \"m2_test_dataset = df_test_diff[['FEDFUNDS','MORTGAGE30US']]\\n\",\n",
      "-        \"m2_test_dataset['const'] = 1.0\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_2 = sm.add_constant(df_train_diff['FEDFUNDS'])\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_2 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_2 = sm.OLS(y_2, X_2).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_2.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"**Step 2: Reasoning**\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": []\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"### Model 3: Loan Rates and GS10\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"m3_training_dataset = df_train_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"m3_test_dataset = df_test_diff[['GS10','MORTGAGE30US']]\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Add a constant to the independent variables for the linear regression model\\n\",\n",
      "-        \"X_3 = df_train_diff['GS10']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Define the dependent variable\\n\",\n",
      "-        \"y_3 = df_train_diff['MORTGAGE30US']\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Fit the linear regression model\\n\",\n",
      "-        \"model_3 = sm.OLS(y_3, X_3).fit()\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"# Display the model summary\\n\",\n",
      "-        \"print(model_3.summary())\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### VM model summary test plan\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm.test_suites.list_plans()\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"attachments\": {},\n",
      "-      \"cell_type\": \"markdown\",\n",
      "-      \"metadata\": {},\n",
      "-      \"source\": [\n",
      "-        \"#### Model perfomance Test\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": [\n",
      "-        \"vm_train_ds_1 = vm.init_dataset(dataset=m1_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_1 = vm.init_dataset(dataset=m1_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_1 = vm.init_model(model_1, train_ds=vm_train_ds_1, test_ds=vm_test_ds_1, validation_ds=vm_test_ds_1)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_performance_test_suite = vm.run_test_suite(\\\"regression_model_description\\\",\\n\",\n",
      "-        \"                                             model=vm_model_1\\n\",\n",
      "-        \"                                            )\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_train_ds_2 = vm.init_dataset(dataset=m2_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_2 = vm.init_dataset(dataset=m2_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_2 = vm.init_model(model_2, train_ds=vm_train_ds_2, test_ds=vm_test_ds_2, validation_ds=vm_test_ds_2)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"vm_train_ds_3 = vm.init_dataset(dataset=m3_training_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_test_ds_3 = vm.init_dataset(dataset=m3_test_dataset, type=\\\"generic\\\", target_column=\\\"MORTGAGE30US\\\")\\n\",\n",
      "-        \"vm_model_3 = vm.init_model(model_3, train_ds=vm_train_ds_3, test_ds=vm_test_ds_3, validation_ds=vm_test_ds_3)\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"\\n\",\n",
      "-        \"model_comparison_test_suite = vm.run_test_suite(\\\"regression_models_evaluation\\\",\\n\",\n",
      "-        \"                                             model = vm_model_1,\\n\",\n",
      "-        \"                                             models= [vm_model_2, vm_model_3],\\n\",\n",
      "-        \"                                            )\\n\",\n",
      "-        \"\\n\"\n",
      "-      ]\n",
      "-    },\n",
      "-    {\n",
      "-      \"cell_type\": \"code\",\n",
      "-      \"execution_count\": null,\n",
      "-      \"metadata\": {},\n",
      "-      \"outputs\": [],\n",
      "-      \"source\": []\n",
      "-    }\n",
      "-  ],\n",
      "-  \"metadata\": {\n",
      "-    \"kernelspec\": {\n",
      "-      \"display_name\": \"dev-framework\",\n",
      "-      \"language\": \"python\",\n",
      "-      \"name\": \"dev-framework\"\n",
      "-    },\n",
      "-    \"language_info\": {\n",
      "-      \"codemirror_mode\": {\n",
      "-        \"name\": \"ipython\",\n",
      "-        \"version\": 3\n",
      "-      },\n",
      "-      \"file_extension\": \".py\",\n",
      "-      \"mimetype\": \"text/x-python\",\n",
      "-      \"name\": \"python\",\n",
      "-      \"nbconvert_exporter\": \"python\",\n",
      "-      \"pygments_lexer\": \"ipython3\",\n",
      "-      \"version\": \"3.9.6\"\n",
      "-    },\n",
      "-    \"orig_nbformat\": 4\n",
      "-  },\n",
      "-  \"nbformat\": 4,\n",
      "-  \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/pyproject.toml b/pyproject.toml\n",
      "index 46e21aff7..020106b0c 100644\n",
      "--- a/pyproject.toml\n",
      "+++ b/pyproject.toml\n",
      "@@ -10,7 +10,7 @@ description = \"ValidMind Developer Framework\"\n",
      " license = \"Commercial License\"\n",
      " name = \"validmind\"\n",
      " readme = \"README.pypi.md\"\n",
      "-version = \"2.3.3\"\n",
      "+version = \"2.3.4\"\n",
      " \n",
      " [tool.poetry.dependencies]\n",
      " python = \">=3.8.1,<3.12\"\n",
      "diff --git a/tests/test_validmind_tests_module.py b/tests/test_validmind_tests_module.py\n",
      "index d340366b8..abac4624d 100644\n",
      "--- a/tests/test_validmind_tests_module.py\n",
      "+++ b/tests/test_validmind_tests_module.py\n",
      "@@ -20,19 +20,19 @@ def test_list_tests_filter(self):\n",
      " \n",
      "     def test_list_tests_filter_2(self):\n",
      "         tests = list_tests(\n",
      "-            filter=\"validmind.model_validation.ModelMetadata\", pretty=False\n",
      "+            filter=\"validmind.model_validation.ModelMetadataComparison\", pretty=False\n",
      "         )\n",
      "         self.assertTrue(len(tests) == 1)\n",
      " \n",
      "     def test_load_test(self):\n",
      "-        test = load_test(\"validmind.model_validation.ModelMetadata\")\n",
      "+        test = load_test(\"validmind.model_validation.ModelMetadataComparison\")\n",
      "         self.assertTrue(test is not None)\n",
      "         self.assertTrue(issubclass(test, Test))\n",
      " \n",
      "     def test_describe_test(self):\n",
      "-        describe_test(\"validmind.model_validation.ModelMetadata\")\n",
      "+        describe_test(\"validmind.model_validation.ModelMetadataComparison\")\n",
      "         description = describe_test(\n",
      "-            \"validmind.model_validation.ModelMetadata\", raw=True\n",
      "+            \"validmind.model_validation.ModelMetadataComparison\", raw=True\n",
      "         )\n",
      "         self.assertIsInstance(description, dict)\n",
      "         # check if description dict has \"ID\", \"Name\", \"Description\", \"Test Type\", \"Required Inputs\" and \"Params\" keys\n",
      "diff --git a/validmind/__version__.py b/validmind/__version__.py\n",
      "index ed5cc1722..4618fe650 100644\n",
      "--- a/validmind/__version__.py\n",
      "+++ b/validmind/__version__.py\n",
      "@@ -1 +1 @@\n",
      "-__version__ = \"2.3.3\"\n",
      "+__version__ = \"2.3.4\"\n",
      "diff --git a/validmind/datasets/regression/datasets/lending_club_loan_rates.csv b/validmind/datasets/regression/datasets/leanding_club_loan_rates.csv\n",
      "similarity index 100%\n",
      "rename from validmind/datasets/regression/datasets/lending_club_loan_rates.csv\n",
      "rename to validmind/datasets/regression/datasets/leanding_club_loan_rates.csv\n",
      "diff --git a/validmind/datasets/regression/fred_timeseries.py b/validmind/datasets/regression/fred_timeseries.py\n",
      "new file mode 100644\n",
      "index 000000000..9ecf98d3c\n",
      "--- /dev/null\n",
      "+++ b/validmind/datasets/regression/fred_timeseries.py\n",
      "@@ -0,0 +1,271 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import os\n",
      "+import pandas as pd\n",
      "+\n",
      "+current_path = os.path.dirname(os.path.abspath(__file__))\n",
      "+mortgage30us_path = os.path.join(current_path, \"datasets\", \"fred\", \"MORTGAGE30US.csv\")\n",
      "+fedfunds_path = os.path.join(current_path, \"datasets\", \"fred\", \"FEDFUNDS.csv\")\n",
      "+gs10_path = os.path.join(current_path, \"datasets\", \"fred\", \"GS10.csv\")\n",
      "+unrate_path = os.path.join(current_path, \"datasets\", \"fred\", \"UNRATE.csv\")\n",
      "+\n",
      "+target_column = \"MORTGAGE30US\"\n",
      "+feature_columns = [\"FEDFUNDS\", \"GS10\", \"UNRATE\"]\n",
      "+\n",
      "+\n",
      "+def get_common_date_range(dfs):\n",
      "+    start_dates = [df.index.min() for df in dfs]\n",
      "+    end_dates = [df.index.max() for df in dfs]\n",
      "+\n",
      "+    common_start_date = max(start_dates)\n",
      "+    common_end_date = min(end_dates)\n",
      "+\n",
      "+    return common_start_date, common_end_date\n",
      "+\n",
      "+\n",
      "+def align_date_range(dfs, start_date, end_date):\n",
      "+    return [df.loc[start_date:end_date] for df in dfs]\n",
      "+\n",
      "+\n",
      "+def load_data():\n",
      "+    mortgage30us = pd.read_csv(\n",
      "+        mortgage30us_path, parse_dates=[\"DATE\"], index_col=\"DATE\"\n",
      "+    )\n",
      "+    fedfunds = pd.read_csv(fedfunds_path, parse_dates=[\"DATE\"], index_col=\"DATE\")\n",
      "+    gs10 = pd.read_csv(gs10_path, parse_dates=[\"DATE\"], index_col=\"DATE\")\n",
      "+    unrate = pd.read_csv(unrate_path, parse_dates=[\"DATE\"], index_col=\"DATE\")\n",
      "+\n",
      "+    # Resample mortgage30us to monthly frequency\n",
      "+    mortgage30us = mortgage30us.resample(\"MS\").last()\n",
      "+\n",
      "+    # Get the common date range\n",
      "+    common_start_date, common_end_date = get_common_date_range(\n",
      "+        [mortgage30us, fedfunds, gs10, unrate]\n",
      "+    )\n",
      "+\n",
      "+    # Align the date range for all dataframes\n",
      "+    mortgage30us, fedfunds, gs10, unrate = align_date_range(\n",
      "+        [mortgage30us, fedfunds, gs10, unrate], common_start_date, common_end_date\n",
      "+    )\n",
      "+\n",
      "+    # Combine into a single DataFrame\n",
      "+    df = pd.concat([mortgage30us, fedfunds, gs10, unrate], axis=1, join=\"inner\")\n",
      "+    df.columns = [target_column] + feature_columns\n",
      "+\n",
      "+    return df\n",
      "+\n",
      "+\n",
      "+# Convert data back to levels\n",
      "+def convert_to_levels(diff_df, original_df, target_column):\n",
      "+    \"\"\"\n",
      "+    Convert differenced data back to original levels.\n",
      "+    \"\"\"\n",
      "+    previous_values = original_df[target_column].shift(1).dropna()\n",
      "+    levels_df = diff_df.add(previous_values, axis=0)\n",
      "+    return levels_df\n",
      "+\n",
      "+\n",
      "+def get_demo_test_config(test_suite=None):\n",
      "+\n",
      "+    default_config = {}\n",
      "+\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesDescription\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"raw_ds\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesLinePlot\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"raw_ds\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesMissingValues\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"raw_ds\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.SeasonalDecompose\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"raw_ds\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.data_validation.TimeSeriesDescriptiveStatistics:train_diff_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"train_diff_ds\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.data_validation.TimeSeriesDescriptiveStatistics:test_diff_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_diff_ds\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesOutliers:train_diff_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"train_diff_ds\",\n",
      "+        },\n",
      "+        \"params\": {\"zscore_threshold\": 4},\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesOutliers:test_diff_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_diff_ds\",\n",
      "+        },\n",
      "+        \"params\": {\"zscore_threshold\": 4},\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesHistogram:train_diff_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"train_diff_ds\",\n",
      "+        },\n",
      "+        \"params\": {\"nbins\": 100},\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.TimeSeriesHistogram:test_diff_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_diff_ds\",\n",
      "+        },\n",
      "+        \"params\": {\"nbins\": 100},\n",
      "+    }\n",
      "+    default_config[\"validmind.data_validation.DatasetSplit\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_diff_ds\", \"test_diff_ds\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.model_validation.ModelMetadataComparison\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.RegressionErrorsComparison:train_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_ds\", \"train_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.RegressionErrorsComparison:test_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"test_ds\", \"test_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.RegressionR2SquareComparison:train_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_ds\", \"train_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.RegressionR2SquareComparison:test_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"test_ds\", \"test_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.TimeSeriesR2SquareBySegments:train_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_ds\", \"train_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.TimeSeriesR2SquareBySegments:test_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"test_ds\", \"test_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        },\n",
      "+        \"params\": {\n",
      "+            \"segments\": {\n",
      "+                \"start_date\": [\"2012-11-01\", \"2018-02-01\"],\n",
      "+                \"end_date\": [\"2018-01-01\", \"2023-03-01\"],\n",
      "+            }\n",
      "+        },\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.TimeSeriesPredictionsPlot:train_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_ds\", \"train_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.model_validation.TimeSeriesPredictionsPlot:test_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"test_ds\", \"test_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.TimeSeriesPredictionWithCI:random_forests_model\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_ds\",\n",
      "+            \"model\": \"random_forests_model\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.TimeSeriesPredictionWithCI:gradient_boosting_model\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_ds\",\n",
      "+            \"model\": \"gradient_boosting_model\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.model_validation.ModelPredictionResiduals:train_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_ds\", \"train_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\"validmind.model_validation.ModelPredictionResiduals:test_data\"] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"test_ds\", \"test_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.FeatureImportanceComparison:train_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"train_ds\", \"train_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.FeatureImportanceComparison:test_data\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"datasets\": [\"test_ds\", \"test_ds\"],\n",
      "+            \"models\": [\"random_forests_model\", \"gradient_boosting_model\"],\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.PermutationFeatureImportance:random_forests_model\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_ds\",\n",
      "+            \"model\": \"random_forests_model\",\n",
      "+        }\n",
      "+    }\n",
      "+    default_config[\n",
      "+        \"validmind.model_validation.sklearn.PermutationFeatureImportance:gradient_boosting_model\"\n",
      "+    ] = {\n",
      "+        \"inputs\": {\n",
      "+            \"dataset\": \"test_ds\",\n",
      "+            \"model\": \"gradient_boosting_model\",\n",
      "+        }\n",
      "+    }\n",
      "+\n",
      "+    return default_config\n",
      "diff --git a/validmind/tests/data_validation/SeasonalDecompose.py b/validmind/tests/data_validation/SeasonalDecompose.py\n",
      "index 8d98f18b5..c8f709515 100644\n",
      "--- a/validmind/tests/data_validation/SeasonalDecompose.py\n",
      "+++ b/validmind/tests/data_validation/SeasonalDecompose.py\n",
      "@@ -4,10 +4,10 @@\n",
      " \n",
      " import warnings\n",
      " \n",
      "-import matplotlib.pyplot as plt\n",
      " import numpy as np\n",
      " import pandas as pd\n",
      "-import seaborn as sns\n",
      "+import plotly.graph_objects as go\n",
      "+from plotly.subplots import make_subplots\n",
      " from scipy import stats\n",
      " from statsmodels.tsa.seasonal import seasonal_decompose\n",
      " \n",
      "@@ -132,7 +132,6 @@ def run(self):\n",
      "                 inferred_freq = pd.infer_freq(series.index)\n",
      " \n",
      "                 if inferred_freq is not None:\n",
      "-                    logger.info(f\"Frequency of {col}: {inferred_freq}\")\n",
      " \n",
      "                     # Only take finite values to seasonal_decompose\n",
      "                     sd = seasonal_decompose(\n",
      "@@ -142,58 +141,87 @@ def run(self):\n",
      " \n",
      "                     results[col] = self.serialize_seasonal_decompose(sd)\n",
      " \n",
      "-                    # Create subplots\n",
      "-                    fig, axes = plt.subplots(3, 2)\n",
      "-                    width, _ = fig.get_size_inches()\n",
      "-                    fig.set_size_inches(width, 15)\n",
      "-                    fig.subplots_adjust(hspace=0.3)\n",
      "-                    fig.suptitle(\n",
      "-                        f\"Seasonal Decomposition for {col}\",\n",
      "-                        fontsize=20,\n",
      "-                        weight=\"bold\",\n",
      "-                        y=0.95,\n",
      "+                    # Create subplots using Plotly\n",
      "+                    fig = make_subplots(\n",
      "+                        rows=3,\n",
      "+                        cols=2,\n",
      "+                        subplot_titles=(\n",
      "+                            \"Observed\",\n",
      "+                            \"Trend\",\n",
      "+                            \"Seasonal\",\n",
      "+                            \"Residuals\",\n",
      "+                            \"Histogram and KDE of Residuals\",\n",
      "+                            \"Normal Q-Q Plot of Residuals\",\n",
      "+                        ),\n",
      "+                        vertical_spacing=0.1,\n",
      "                     )\n",
      " \n",
      "-                    # Original seasonal decomposition plots\n",
      "                     # Observed\n",
      "-                    sd.observed.plot(ax=axes[0, 0])\n",
      "-                    axes[0, 0].set_title(\"Observed\", fontsize=18)\n",
      "-                    axes[0, 0].set_xlabel(\"\")\n",
      "-                    axes[0, 0].tick_params(axis=\"both\", labelsize=18)\n",
      "+                    fig.add_trace(\n",
      "+                        go.Scatter(x=sd.observed.index, y=sd.observed, name=\"Observed\"),\n",
      "+                        row=1,\n",
      "+                        col=1,\n",
      "+                    )\n",
      " \n",
      "                     # Trend\n",
      "-                    sd.trend.plot(ax=axes[0, 1])\n",
      "-                    axes[0, 1].set_title(\"Trend\", fontsize=18)\n",
      "-                    axes[0, 1].set_xlabel(\"\")\n",
      "-                    axes[0, 1].tick_params(axis=\"both\", labelsize=18)\n",
      "+                    fig.add_trace(\n",
      "+                        go.Scatter(x=sd.trend.index, y=sd.trend, name=\"Trend\"),\n",
      "+                        row=1,\n",
      "+                        col=2,\n",
      "+                    )\n",
      " \n",
      "                     # Seasonal\n",
      "-                    sd.seasonal.plot(ax=axes[1, 0])\n",
      "-                    axes[1, 0].set_title(\"Seasonal\", fontsize=18)\n",
      "-                    axes[1, 0].set_xlabel(\"\")\n",
      "-                    axes[1, 0].tick_params(axis=\"both\", labelsize=18)\n",
      "+                    fig.add_trace(\n",
      "+                        go.Scatter(x=sd.seasonal.index, y=sd.seasonal, name=\"Seasonal\"),\n",
      "+                        row=2,\n",
      "+                        col=1,\n",
      "+                    )\n",
      " \n",
      "                     # Residuals\n",
      "-                    sd.resid.plot(ax=axes[1, 1])\n",
      "-                    axes[1, 1].set_title(\"Residuals\", fontsize=18)\n",
      "-                    axes[1, 1].set_xlabel(\"\")\n",
      "-                    axes[1, 1].tick_params(axis=\"both\", labelsize=18)\n",
      "+                    fig.add_trace(\n",
      "+                        go.Scatter(x=sd.resid.index, y=sd.resid, name=\"Residuals\"),\n",
      "+                        row=2,\n",
      "+                        col=2,\n",
      "+                    )\n",
      " \n",
      "                     # Histogram with KDE\n",
      "                     residuals = sd.resid.dropna()\n",
      "-                    sns.histplot(residuals, kde=True, ax=axes[2, 0])\n",
      "-                    axes[2, 0].set_title(\"Histogram and KDE of Residuals\", fontsize=18)\n",
      "-                    axes[2, 0].set_xlabel(\"\")\n",
      "-                    axes[2, 0].tick_params(axis=\"both\", labelsize=18)\n",
      "+                    fig.add_trace(\n",
      "+                        go.Histogram(x=residuals, nbinsx=100, name=\"Residuals\"),\n",
      "+                        row=3,\n",
      "+                        col=1,\n",
      "+                    )\n",
      " \n",
      "                     # Normal Q-Q plot\n",
      "-                    stats.probplot(residuals, plot=axes[2, 1])\n",
      "-                    axes[2, 1].set_title(\"Normal Q-Q Plot of Residuals\", fontsize=18)\n",
      "-                    axes[2, 1].set_xlabel(\"\")\n",
      "-                    axes[2, 1].tick_params(axis=\"both\", labelsize=18)\n",
      "+                    qq = stats.probplot(residuals, plot=None)\n",
      "+                    qq_line_slope, qq_line_intercept = stats.linregress(\n",
      "+                        qq[0][0], qq[0][1]\n",
      "+                    )[:2]\n",
      "+                    qq_line = qq_line_slope * np.array(qq[0][0]) + qq_line_intercept\n",
      "+\n",
      "+                    fig.add_trace(\n",
      "+                        go.Scatter(\n",
      "+                            x=qq[0][0], y=qq[0][1], mode=\"markers\", name=\"QQ plot\"\n",
      "+                        ),\n",
      "+                        row=3,\n",
      "+                        col=2,\n",
      "+                    )\n",
      "+                    fig.add_trace(\n",
      "+                        go.Scatter(\n",
      "+                            x=qq[0][0],\n",
      "+                            y=qq_line,\n",
      "+                            mode=\"lines\",\n",
      "+                            name=\"QQ line\",\n",
      "+                        ),\n",
      "+                        row=3,\n",
      "+                        col=2,\n",
      "+                    )\n",
      " \n",
      "-                    # Do this if you want to prevent the figure from being displayed\n",
      "-                    plt.close(\"all\")\n",
      "+                    fig.update_layout(\n",
      "+                        height=1000,\n",
      "+                        title_text=f\"Seasonal Decomposition for {col}\",\n",
      "+                        showlegend=False,\n",
      "+                    )\n",
      " \n",
      "                     figures.append(\n",
      "                         Figure(\n",
      "diff --git a/validmind/tests/data_validation/TimeSeriesDescription.py b/validmind/tests/data_validation/TimeSeriesDescription.py\n",
      "new file mode 100644\n",
      "index 000000000..19fc54834\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/data_validation/TimeSeriesDescription.py\n",
      "@@ -0,0 +1,74 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"time_series_data\", \"analysis\")\n",
      "+@tasks(\"regression\")\n",
      "+def TimeSeriesDescription(dataset):\n",
      "+    \"\"\"\n",
      "+    Generates a detailed analysis for the provided time series dataset.\n",
      "+\n",
      "+    **Purpose**: The purpose of the TimeSeriesDescription function is to analyze an individual time series\n",
      "+    by providing a summary of key statistics. This helps in understanding trends, patterns, and data quality issues\n",
      "+    within the time series.\n",
      "+\n",
      "+    **Test Mechanism**: The function extracts the time series data and provides a summary of key statistics.\n",
      "+    The dataset is expected to have a datetime index. The function checks this and raises an error if the index is\n",
      "+    not in datetime format. For each variable (column) in the dataset, appropriate statistics including start date,\n",
      "+    end date, frequency, number of missing values, count, min, and max values are calculated.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - If the index of the dataset is not in datetime format, it could lead to errors in time-series analysis.\n",
      "+    - Inconsistent or missing data within the dataset might affect the analysis of trends and patterns.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - This function provides a comprehensive summary of key statistics for each variable, helping to identify data quality\n",
      "+      issues such as missing values.\n",
      "+    - The function helps in understanding the distribution and range of the data by including min and max values.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - This function assumes that the dataset is provided as a DataFrameDataset object with a .df attribute to access\n",
      "+      the pandas DataFrame.\n",
      "+    - It only analyzes datasets with a datetime index and will raise an error for other types of indices.\n",
      "+    - The function does not handle large datasets efficiently, and performance may degrade with very large datasets.\n",
      "+    \"\"\"\n",
      "+\n",
      "+    summary = []\n",
      "+\n",
      "+    df = (\n",
      "+        dataset.df\n",
      "+    )  # Assuming DataFrameDataset objects have a .df attribute to get the pandas DataFrame\n",
      "+\n",
      "+    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
      "+        raise ValueError(f\"Dataset {dataset.input_id} must have a datetime index\")\n",
      "+\n",
      "+    for column in df.columns:\n",
      "+        start_date = df.index.min().strftime(\"%Y-%m-%d\")\n",
      "+        end_date = df.index.max().strftime(\"%Y-%m-%d\")\n",
      "+        frequency = pd.infer_freq(df.index)\n",
      "+        num_missing_values = df[column].isna().sum()\n",
      "+        count = df[column].count()\n",
      "+        min_value = df[column].min()\n",
      "+        max_value = df[column].max()\n",
      "+\n",
      "+        summary.append(\n",
      "+            {\n",
      "+                \"Variable\": column,\n",
      "+                \"Start Date\": start_date,\n",
      "+                \"End Date\": end_date,\n",
      "+                \"Frequency\": frequency,\n",
      "+                \"Num of Missing Values\": num_missing_values,\n",
      "+                \"Count\": count,\n",
      "+                \"Min Value\": min_value,\n",
      "+                \"Max Value\": max_value,\n",
      "+            }\n",
      "+        )\n",
      "+\n",
      "+    result_df = pd.DataFrame(summary)\n",
      "+\n",
      "+    return result_df\n",
      "diff --git a/validmind/tests/data_validation/TimeSeriesDescriptiveStatistics.py b/validmind/tests/data_validation/TimeSeriesDescriptiveStatistics.py\n",
      "new file mode 100644\n",
      "index 000000000..bdf0592cd\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/data_validation/TimeSeriesDescriptiveStatistics.py\n",
      "@@ -0,0 +1,76 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+from scipy.stats import skew, kurtosis\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"time_series_data\", \"analysis\")\n",
      "+@tasks(\"regression\")\n",
      "+def TimeSeriesDescriptiveStatistics(dataset):\n",
      "+    \"\"\"\n",
      "+    Generates a detailed table of descriptive statistics for the provided time series dataset.\n",
      "+\n",
      "+    **Purpose**: The purpose of the TimeSeriesDescriptiveStatistics function is to analyze an individual time series\n",
      "+    by providing a summary of key descriptive statistics. This helps in understanding trends, patterns, and data quality issues\n",
      "+    within the time series.\n",
      "+\n",
      "+    **Test Mechanism**: The function extracts the time series data and provides a summary of key descriptive statistics.\n",
      "+    The dataset is expected to have a datetime index. The function checks this and raises an error if the index is\n",
      "+    not in datetime format. For each variable (column) in the dataset, appropriate statistics including start date,\n",
      "+    end date, min, mean, max, skewness, kurtosis, and count are calculated.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - If the index of the dataset is not in datetime format, it could lead to errors in time-series analysis.\n",
      "+    - Inconsistent or missing data within the dataset might affect the analysis of trends and patterns.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - This function provides a comprehensive summary of key descriptive statistics for each variable, helping to identify data quality\n",
      "+      issues and understand the distribution of the data.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - This function assumes that the dataset is provided as a DataFrameDataset object with a .df attribute to access\n",
      "+      the pandas DataFrame.\n",
      "+    - It only analyzes datasets with a datetime index and will raise an error for other types of indices.\n",
      "+    - The function does not handle large datasets efficiently, and performance may degrade with very large datasets.\n",
      "+    \"\"\"\n",
      "+\n",
      "+    summary = []\n",
      "+\n",
      "+    df = (\n",
      "+        dataset.df\n",
      "+    )  # Assuming DataFrameDataset objects have a .df attribute to get the pandas DataFrame\n",
      "+\n",
      "+    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
      "+        raise ValueError(f\"Dataset {dataset.input_id} must have a datetime index\")\n",
      "+\n",
      "+    for column in df.columns:\n",
      "+        start_date = df.index.min().strftime(\"%Y-%m-%d\")\n",
      "+        end_date = df.index.max().strftime(\"%Y-%m-%d\")\n",
      "+        count = df[column].count()\n",
      "+        min_value = df[column].min()\n",
      "+        mean_value = df[column].mean()\n",
      "+        max_value = df[column].max()\n",
      "+        skewness_value = skew(df[column].dropna())\n",
      "+        kurtosis_value = kurtosis(df[column].dropna())\n",
      "+\n",
      "+        summary.append(\n",
      "+            {\n",
      "+                \"Variable\": column,\n",
      "+                \"Start Date\": start_date,\n",
      "+                \"End Date\": end_date,\n",
      "+                \"Min\": min_value,\n",
      "+                \"Mean\": mean_value,\n",
      "+                \"Max\": max_value,\n",
      "+                \"Skewness\": skewness_value,\n",
      "+                \"Kurtosis\": kurtosis_value,\n",
      "+                \"Count\": count,\n",
      "+            }\n",
      "+        )\n",
      "+\n",
      "+    result_df = pd.DataFrame(summary)\n",
      "+\n",
      "+    return result_df\n",
      "diff --git a/validmind/tests/data_validation/TimeSeriesHistogram.py b/validmind/tests/data_validation/TimeSeriesHistogram.py\n",
      "index 9822f8562..99b3a42f7 100644\n",
      "--- a/validmind/tests/data_validation/TimeSeriesHistogram.py\n",
      "+++ b/validmind/tests/data_validation/TimeSeriesHistogram.py\n",
      "@@ -2,14 +2,14 @@\n",
      " # See the LICENSE file in the root of this repository for details.\n",
      " # SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      " \n",
      "-import matplotlib.pyplot as plt\n",
      "-import pandas as pd\n",
      "-import seaborn as sns\n",
      "+import plotly.express as px\n",
      " \n",
      "-from validmind.vm_models import Figure, Metric\n",
      "+from validmind import tags, tasks\n",
      " \n",
      " \n",
      "-class TimeSeriesHistogram(Metric):\n",
      "+@tags(\"data_validation\", \"visualization\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def TimeSeriesHistogram(dataset, nbins=30):\n",
      "     \"\"\"\n",
      "     Visualizes distribution of time-series data using histograms and Kernel Density Estimation (KDE) lines.\n",
      " \n",
      "@@ -20,7 +20,7 @@ class TimeSeriesHistogram(Metric):\n",
      "     (kurtosis) underlying the data.\n",
      " \n",
      "     **Test Mechanism**: This test operates on a specific column within the dataset that is required to have a datetime\n",
      "-    type index. It goes through each column in the given dataset, creating a histogram with Seaborn's histplot\n",
      "+    type index. It goes through each column in the given dataset, creating a histogram with Plotly's histplot\n",
      "     function. In cases where the dataset includes more than one time-series (i.e., more than one column with a datetime\n",
      "     type index), a distinct histogram is plotted for each series. Additionally, a kernel density estimate (KDE) line is\n",
      "     drawn for each histogram, providing a visualization of the data's underlying probability distribution. The x and\n",
      "@@ -48,46 +48,30 @@ class TimeSeriesHistogram(Metric):\n",
      "     - The histogram's shape may be sensitive to the number of bins used.\n",
      "     \"\"\"\n",
      " \n",
      "-    name = \"time_series_histogram\"\n",
      "-    required_inputs = [\"dataset\"]\n",
      "-    metadata = {\n",
      "-        \"task_types\": [\"regression\"],\n",
      "-        \"tags\": [\"time_series_data\", \"visualization\"],\n",
      "-    }\n",
      "+    df = dataset.df\n",
      " \n",
      "-    def run(self):\n",
      "-        # Check if index is datetime\n",
      "-        if not pd.api.types.is_datetime64_any_dtype(self.inputs.dataset.df.index):\n",
      "-            raise ValueError(\"Index must be a datetime type\")\n",
      "+    columns = list(dataset.df.columns)\n",
      " \n",
      "-        columns = list(self.inputs.dataset.df.columns)\n",
      "+    if not set(columns).issubset(set(df.columns)):\n",
      "+        raise ValueError(\"Provided 'columns' must exist in the dataset\")\n",
      " \n",
      "-        df = self.inputs.dataset.df\n",
      "-\n",
      "-        if not set(columns).issubset(set(df.columns)):\n",
      "-            raise ValueError(\"Provided 'columns' must exist in the dataset\")\n",
      "-\n",
      "-        figures = []\n",
      "-        for col in columns:\n",
      "-            plt.figure()\n",
      "-            fig, _ = plt.subplots()\n",
      "-            ax = sns.histplot(data=df, x=col, kde=True)\n",
      "-            plt.title(f\"Histogram for {col}\", weight=\"bold\", fontsize=20)\n",
      "-\n",
      "-            plt.xticks(fontsize=18)\n",
      "-            plt.yticks(fontsize=18)\n",
      "-            ax.set_xlabel(\"\")\n",
      "-            ax.set_ylabel(\"\")\n",
      "-            figures.append(\n",
      "-                Figure(\n",
      "-                    for_object=self,\n",
      "-                    key=f\"{self.key}:{col}\",\n",
      "-                    figure=fig,\n",
      "-                )\n",
      "-            )\n",
      "-\n",
      "-        plt.close(\"all\")\n",
      "-\n",
      "-        return self.cache_results(\n",
      "-            figures=figures,\n",
      "+    figures = []\n",
      "+    for col in columns:\n",
      "+        fig = px.histogram(\n",
      "+            df, x=col, marginal=\"violin\", nbins=nbins, title=f\"Histogram for {col}\"\n",
      "+        )\n",
      "+        fig.update_layout(\n",
      "+            title={\n",
      "+                \"text\": f\"Histogram for {col}\",\n",
      "+                \"y\": 0.9,\n",
      "+                \"x\": 0.5,\n",
      "+                \"xanchor\": \"center\",\n",
      "+                \"yanchor\": \"top\",\n",
      "+            },\n",
      "+            xaxis_title=\"\",\n",
      "+            yaxis_title=\"\",\n",
      "+            font=dict(size=18),\n",
      "         )\n",
      "+        figures.append(fig)\n",
      "+\n",
      "+    return tuple(figures)\n",
      "diff --git a/validmind/tests/data_validation/TimeSeriesOutliers.py b/validmind/tests/data_validation/TimeSeriesOutliers.py\n",
      "index 4b5411a0d..b26f6cf43 100644\n",
      "--- a/validmind/tests/data_validation/TimeSeriesOutliers.py\n",
      "+++ b/validmind/tests/data_validation/TimeSeriesOutliers.py\n",
      "@@ -4,11 +4,8 @@\n",
      " \n",
      " from dataclasses import dataclass\n",
      " \n",
      "-import matplotlib.pyplot as plt\n",
      " import pandas as pd\n",
      "-import seaborn as sns\n",
      "-from ydata_profiling.config import Settings\n",
      "-from ydata_profiling.model.typeset import ProfilingTypeSet\n",
      "+import plotly.graph_objects as go\n",
      " \n",
      " from validmind.vm_models import (\n",
      "     Figure,\n",
      "@@ -93,7 +90,8 @@ def summary(self, results, all_passed: bool):\n",
      "         zScores = first_result.values[\"z-score\"]\n",
      "         dates = first_result.values[\"Date\"]\n",
      "         passFail = [\n",
      "-            \"Pass\" if z < self.params[\"zscore_threshold\"] else \"Fail\" for z in zScores\n",
      "+            \"Pass\" if abs(z) < self.params[\"zscore_threshold\"] else \"Fail\"\n",
      "+            for z in zScores\n",
      "         ]\n",
      " \n",
      "         return ResultSummary(\n",
      "@@ -116,25 +114,26 @@ def summary(self, results, all_passed: bool):\n",
      "         )\n",
      " \n",
      "     def run(self):\n",
      "+        # Initialize the test_results list\n",
      "+        test_results = []\n",
      "+\n",
      "         # Check if the index of dataframe is datetime\n",
      "         is_datetime = pd.api.types.is_datetime64_any_dtype(self.inputs.dataset.df.index)\n",
      "         if not is_datetime:\n",
      "             raise ValueError(\"Dataset must be provided with datetime index\")\n",
      " \n",
      "-        # Validate threshold paremeter\n",
      "+        # Validate threshold parameter\n",
      "         if \"zscore_threshold\" not in self.params:\n",
      "             raise ValueError(\"zscore_threshold must be provided in params\")\n",
      "         zscore_threshold = self.params[\"zscore_threshold\"]\n",
      " \n",
      "         temp_df = self.inputs.dataset.df.copy()\n",
      "         # temp_df = temp_df.dropna()\n",
      "-        typeset = ProfilingTypeSet(Settings())\n",
      "-        dataset_types = typeset.infer_type(temp_df)\n",
      "-        test_results = []\n",
      "-        test_figures = []\n",
      "-        num_features_columns = [\n",
      "-            k for k, v in dataset_types.items() if str(v) == \"Numeric\"\n",
      "-        ]\n",
      "+\n",
      "+        # Infer numeric columns\n",
      "+        num_features_columns = temp_df.select_dtypes(\n",
      "+            include=[\"number\"]\n",
      "+        ).columns.tolist()\n",
      " \n",
      "         outliers_table = self.identify_outliers(\n",
      "             temp_df[num_features_columns], zscore_threshold\n",
      "@@ -196,49 +195,39 @@ def _plot_outliers(self, df, outliers_table):\n",
      "             df (pandas.DataFrame): Input data with time series.\n",
      "             outliers_table (pandas.DataFrame): DataFrame with identified outliers.\n",
      "         Returns:\n",
      "-            matplotlib.figure.Figure: A matplotlib figure object with subplots for each variable.\n",
      "+            list: A list of Figure objects with subplots for each variable.\n",
      "         \"\"\"\n",
      "-        sns.set(style=\"darkgrid\")\n",
      "-        columns = list(self.inputs.dataset.df.columns)\n",
      "         figures = []\n",
      " \n",
      "-        for col in columns:\n",
      "-            plt.figure()\n",
      "-            fig, _ = plt.subplots()\n",
      "-            column_index_name = df.index.name\n",
      "-            ax = sns.lineplot(data=df.reset_index(), x=column_index_name, y=col)\n",
      "+        for col in df.columns:\n",
      "+            fig = go.Figure()\n",
      "+\n",
      "+            fig.add_trace(go.Scatter(x=df.index, y=df[col], mode=\"lines\", name=col))\n",
      " \n",
      "             if not outliers_table.empty:\n",
      "                 variable_outliers = outliers_table[outliers_table[\"Variable\"] == col]\n",
      "-                for idx, row in variable_outliers.iterrows():\n",
      "-                    date = row[\"Date\"]\n",
      "-                    outlier_value = df.loc[date, col]\n",
      "-                    ax.scatter(\n",
      "-                        date,\n",
      "-                        outlier_value,\n",
      "-                        marker=\"o\",\n",
      "-                        s=100,\n",
      "-                        c=\"red\",\n",
      "-                        label=\"Outlier\" if idx == 0 else \"\",\n",
      "+                fig.add_trace(\n",
      "+                    go.Scatter(\n",
      "+                        x=variable_outliers[\"Date\"],\n",
      "+                        y=df.loc[variable_outliers[\"Date\"], col],\n",
      "+                        mode=\"markers\",\n",
      "+                        marker=dict(color=\"red\", size=10),\n",
      "+                        name=\"Outlier\",\n",
      "                     )\n",
      "+                )\n",
      " \n",
      "-            plt.xticks(fontsize=18)\n",
      "-            plt.yticks(fontsize=18)\n",
      "-            ax.set_xlabel(\"\")\n",
      "-            ax.set_ylabel(\"\")\n",
      "-            ax.set_title(\n",
      "-                f\"Time Series with Outliers for {col}\", weight=\"bold\", fontsize=20\n",
      "+            fig.update_layout(\n",
      "+                title=f\"Time Series with Outliers for {col}\",\n",
      "+                xaxis_title=\"Date\",\n",
      "+                yaxis_title=col,\n",
      "             )\n",
      " \n",
      "-            ax.legend()\n",
      "             figures.append(\n",
      "                 Figure(\n",
      "                     for_object=self,\n",
      "-                    key=f\"{self.name}:{col}\",\n",
      "+                    key=f\"{self.name}:{col}_{self.inputs.dataset.input_id}\",\n",
      "                     figure=fig,\n",
      "                 )\n",
      "             )\n",
      " \n",
      "-        # Do this if you want to prevent the figure from being displayed\n",
      "-        plt.close(\"all\")\n",
      "         return figures\n",
      "diff --git a/validmind/tests/model_validation/ModelMetadataComparison.py b/validmind/tests/model_validation/ModelMetadataComparison.py\n",
      "new file mode 100644\n",
      "index 000000000..81b501291\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/ModelMetadataComparison.py\n",
      "@@ -0,0 +1,59 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+from validmind.utils import get_model_info\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"model_training\", \"metadata\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def ModelMetadataComparison(models):\n",
      "+    \"\"\"\n",
      "+    Compare metadata of different models and generate a summary table with the results.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to compare the metadata of different models, including information about their architecture, framework, framework version, and programming language.\n",
      "+\n",
      "+    **Test Mechanism**: The function retrieves the metadata for each model using `get_model_info`, renames columns according to a predefined set of labels, and compiles this information into a summary table.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - Inconsistent or missing metadata across models can indicate potential issues in model documentation or management.\n",
      "+    - Significant differences in framework versions or programming languages might pose challenges in model integration and deployment.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a clear comparison of essential model metadata.\n",
      "+    - Standardizes metadata labels for easier interpretation and comparison.\n",
      "+    - Helps identify potential compatibility or consistency issues across models.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the `get_model_info` function returns all necessary metadata fields.\n",
      "+    - Relies on the correctness and completeness of the metadata provided by each model.\n",
      "+    - Does not include detailed parameter information, focusing instead on high-level metadata.\n",
      "+    \"\"\"\n",
      "+    column_labels = {\n",
      "+        \"architecture\": \"Modeling Technique\",\n",
      "+        \"framework\": \"Modeling Framework\",\n",
      "+        \"framework_version\": \"Framework Version\",\n",
      "+        \"language\": \"Programming Language\",\n",
      "+    }\n",
      "+\n",
      "+    description = []\n",
      "+\n",
      "+    for model in models:\n",
      "+        model_info = get_model_info(model)\n",
      "+\n",
      "+        # Rename columns based on provided labels\n",
      "+        model_info_renamed = {\n",
      "+            column_labels.get(k, k): v for k, v in model_info.items() if k != \"params\"\n",
      "+        }\n",
      "+\n",
      "+        # Add model name or identifier if available\n",
      "+        model_info_renamed = {\"Model Name\": model.input_id, **model_info_renamed}\n",
      "+\n",
      "+        description.append(model_info_renamed)\n",
      "+\n",
      "+    description_df = pd.DataFrame(description)\n",
      "+\n",
      "+    return description_df\n",
      "diff --git a/validmind/tests/model_validation/ModelPredictionResiduals.py b/validmind/tests/model_validation/ModelPredictionResiduals.py\n",
      "new file mode 100644\n",
      "index 000000000..5761cf93a\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/ModelPredictionResiduals.py\n",
      "@@ -0,0 +1,103 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+import plotly.graph_objects as go\n",
      "+from scipy.stats import kstest\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"regression\")\n",
      "+@tasks(\"residual_analysis\", \"visualization\")\n",
      "+def ModelPredictionResiduals(\n",
      "+    datasets, models, nbins=100, p_value_threshold=0.05, start_date=None, end_date=None\n",
      "+):\n",
      "+    \"\"\"\n",
      "+    Plot the residuals and histograms for each model, and generate a summary table\n",
      "+    with the Kolmogorov-Smirnov normality test results.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to visualize the residuals of model predictions and\n",
      "+    assess the normality of residuals using the Kolmogorov-Smirnov test.\n",
      "+\n",
      "+    **Test Mechanism**: The function iterates through each dataset-model pair, calculates residuals, and generates\n",
      "+    two figures for each model: one for the time series of residuals and one for the histogram of residuals.\n",
      "+    It also calculates the KS test for normality and summarizes the results in a table.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - If the residuals are not normally distributed, it could indicate issues with model assumptions.\n",
      "+    - High skewness or kurtosis in the residuals may indicate model misspecification.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a clear visualization of residuals over time and their distribution.\n",
      "+    - Includes statistical tests to assess the normality of residuals.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with a .df attribute to access\n",
      "+      the pandas DataFrame.\n",
      "+    - Only generates plots for datasets with a datetime index, and will raise an error for other types of indices.\n",
      "+    \"\"\"\n",
      "+\n",
      "+    figures = []\n",
      "+    summary = []\n",
      "+\n",
      "+    for dataset, model in zip(datasets, models):\n",
      "+        df = dataset.df.copy()\n",
      "+\n",
      "+        # Filter DataFrame by date range if specified\n",
      "+        if start_date:\n",
      "+            df = df[df.index >= pd.to_datetime(start_date)]\n",
      "+        if end_date:\n",
      "+            df = df[df.index <= pd.to_datetime(end_date)]\n",
      "+\n",
      "+        y_true = dataset.y\n",
      "+        y_pred = dataset.y_pred(model)\n",
      "+        residuals = y_true - y_pred\n",
      "+\n",
      "+        # Plot residuals\n",
      "+        residuals_fig = go.Figure()\n",
      "+        residuals_fig.add_trace(\n",
      "+            go.Scatter(x=df.index, y=residuals, mode=\"lines\", name=\"Residuals\")\n",
      "+        )\n",
      "+        residuals_fig.update_layout(\n",
      "+            title=f\"Residuals for {model.input_id}\",\n",
      "+            xaxis_title=\"Date\",\n",
      "+            yaxis_title=\"Residuals\",\n",
      "+            font=dict(size=16),\n",
      "+            showlegend=False,\n",
      "+        )\n",
      "+        figures.append(residuals_fig)\n",
      "+\n",
      "+        # Plot histogram of residuals\n",
      "+        hist_fig = go.Figure()\n",
      "+        hist_fig.add_trace(go.Histogram(x=residuals, nbinsx=nbins, name=\"Residuals\"))\n",
      "+        hist_fig.update_layout(\n",
      "+            title=f\"Histogram of Residuals for {model.input_id}\",\n",
      "+            xaxis_title=\"Residuals\",\n",
      "+            yaxis_title=\"Frequency\",\n",
      "+            font=dict(size=16),\n",
      "+            showlegend=False,\n",
      "+        )\n",
      "+        figures.append(hist_fig)\n",
      "+\n",
      "+        # Perform KS normality test\n",
      "+        ks_stat, p_value = kstest(\n",
      "+            residuals, \"norm\", args=(residuals.mean(), residuals.std())\n",
      "+        )\n",
      "+        ks_normality = \"Normal\" if p_value > p_value_threshold else \"Not Normal\"\n",
      "+\n",
      "+        summary.append(\n",
      "+            {\n",
      "+                \"Model\": model.input_id,\n",
      "+                \"KS Statistic\": ks_stat,\n",
      "+                \"p-value\": p_value,\n",
      "+                \"KS Normality\": ks_normality,\n",
      "+                \"p-value Threshold\": p_value_threshold,\n",
      "+            }\n",
      "+        )\n",
      "+\n",
      "+    # Create a summary DataFrame for the KS normality test results\n",
      "+    summary_df = pd.DataFrame(summary)\n",
      "+\n",
      "+    return (summary_df, *figures)\n",
      "diff --git a/validmind/tests/model_validation/TimeSeriesPredictionWithCI.py b/validmind/tests/model_validation/TimeSeriesPredictionWithCI.py\n",
      "new file mode 100644\n",
      "index 000000000..910256d1c\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/TimeSeriesPredictionWithCI.py\n",
      "@@ -0,0 +1,131 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+import numpy as np\n",
      "+import plotly.graph_objects as go\n",
      "+from scipy.stats import norm\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"model_predictions\", \"visualization\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def TimeSeriesPredictionWithCI(dataset, model, confidence=0.95):\n",
      "+    \"\"\"\n",
      "+    Plot actual vs predicted values for a time series with confidence intervals and compute breaches.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to visualize the actual versus predicted values for time series data, including confidence intervals, and to compute and report the number of breaches beyond these intervals.\n",
      "+\n",
      "+    **Test Mechanism**: The function calculates the standard deviation of prediction errors, determines the confidence intervals, and counts the number of actual values that fall outside these intervals (breaches). It then generates a plot with the actual values, predicted values, and confidence intervals, and returns a DataFrame summarizing the breach information.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - A high number of breaches indicates that the model's predictions are not reliable within the specified confidence level.\n",
      "+    - Significant deviations between actual and predicted values may highlight model inadequacies or issues with data quality.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a visual representation of prediction accuracy and the uncertainty around predictions.\n",
      "+    - Includes a statistical measure of prediction reliability through confidence intervals.\n",
      "+    - Computes and reports breaches, offering a quantitative assessment of prediction performance.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with a datetime index.\n",
      "+    - Requires that `dataset.y_pred(model)` returns the predicted values for the model.\n",
      "+    - The calculation of confidence intervals assumes normally distributed errors, which may not hold for all datasets.\n",
      "+    \"\"\"\n",
      "+    dataset_name = dataset.input_id\n",
      "+    model_name = model.input_id\n",
      "+    time_index = dataset.df.index  # Assuming the index of the dataset is datetime\n",
      "+\n",
      "+    # Get actual and predicted values\n",
      "+    y_true = dataset.y\n",
      "+    y_pred = dataset.y_pred(model)\n",
      "+\n",
      "+    # Compute the standard deviation of the errors\n",
      "+    errors = y_true - y_pred\n",
      "+    std_error = np.std(errors)\n",
      "+\n",
      "+    # Compute z-score for the given confidence level\n",
      "+    z_score = norm.ppf(1 - (1 - confidence) / 2)\n",
      "+\n",
      "+    # Compute confidence intervals\n",
      "+    lower_conf = y_pred - z_score * std_error\n",
      "+    upper_conf = y_pred + z_score * std_error\n",
      "+\n",
      "+    # Calculate breaches\n",
      "+    upper_breaches = (y_true > upper_conf).sum()\n",
      "+    lower_breaches = (y_true < lower_conf).sum()\n",
      "+    total_breaches = upper_breaches + lower_breaches\n",
      "+\n",
      "+    # Create DataFrame\n",
      "+    breaches_df = pd.DataFrame(\n",
      "+        {\n",
      "+            \"Confidence Level\": [confidence],\n",
      "+            \"Total Breaches\": [total_breaches],\n",
      "+            \"Upper Breaches\": [upper_breaches],\n",
      "+            \"Lower Breaches\": [lower_breaches],\n",
      "+        }\n",
      "+    )\n",
      "+\n",
      "+    # Plotting\n",
      "+    fig = go.Figure()\n",
      "+\n",
      "+    # Plot actual values\n",
      "+    fig.add_trace(\n",
      "+        go.Scatter(\n",
      "+            x=time_index,\n",
      "+            y=y_true,\n",
      "+            mode=\"lines\",\n",
      "+            name=\"Actual Values\",\n",
      "+            line=dict(color=\"blue\"),\n",
      "+        )\n",
      "+    )\n",
      "+\n",
      "+    # Plot predicted values\n",
      "+    fig.add_trace(\n",
      "+        go.Scatter(\n",
      "+            x=time_index,\n",
      "+            y=y_pred,\n",
      "+            mode=\"lines\",\n",
      "+            name=f\"Predicted by {model_name}\",\n",
      "+            line=dict(color=\"red\"),\n",
      "+        )\n",
      "+    )\n",
      "+\n",
      "+    # Add confidence interval lower bound as an invisible line\n",
      "+    fig.add_trace(\n",
      "+        go.Scatter(\n",
      "+            x=time_index,\n",
      "+            y=lower_conf,\n",
      "+            mode=\"lines\",\n",
      "+            line=dict(width=0),\n",
      "+            showlegend=False,\n",
      "+            name=\"CI Lower\",\n",
      "+        )\n",
      "+    )\n",
      "+\n",
      "+    # Add confidence interval upper bound and fill area\n",
      "+    fig.add_trace(\n",
      "+        go.Scatter(\n",
      "+            x=time_index,\n",
      "+            y=upper_conf,\n",
      "+            mode=\"lines\",\n",
      "+            fill=\"tonexty\",\n",
      "+            fillcolor=\"rgba(200, 200, 200, 0.5)\",\n",
      "+            line=dict(width=0),\n",
      "+            showlegend=True,\n",
      "+            name=\"Confidence Interval\",\n",
      "+        )\n",
      "+    )\n",
      "+\n",
      "+    # Update layout\n",
      "+    fig.update_layout(\n",
      "+        title=f\"Time Series Actual vs Predicted Values for {dataset_name} and {model_name}\",\n",
      "+        xaxis_title=\"Time\",\n",
      "+        yaxis_title=\"Values\",\n",
      "+        legend_title=\"Legend\",\n",
      "+        template=\"plotly_white\",\n",
      "+    )\n",
      "+\n",
      "+    return fig, breaches_df\n",
      "diff --git a/validmind/tests/model_validation/TimeSeriesPredictionsPlot.py b/validmind/tests/model_validation/TimeSeriesPredictionsPlot.py\n",
      "new file mode 100644\n",
      "index 000000000..111a13022\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/TimeSeriesPredictionsPlot.py\n",
      "@@ -0,0 +1,76 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import plotly.graph_objects as go\n",
      "+import plotly.express as px\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"model_predictions\", \"visualization\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def TimeSeriesPredictionsPlot(datasets, models):\n",
      "+    \"\"\"\n",
      "+    Plot actual vs predicted values for time series data and generate a visual comparison for each model.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to visualize the actual versus predicted values for time series data across different models.\n",
      "+\n",
      "+    **Test Mechanism**: The function iterates through each dataset-model pair, plots the actual values from the dataset, and overlays the predicted values from each model using Plotly for interactive visualization.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - Large discrepancies between actual and predicted values indicate poor model performance.\n",
      "+    - Systematic deviations in predicted values can highlight model bias or issues with data patterns.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a clear visual comparison of model predictions against actual values.\n",
      "+    - Uses Plotly for interactive and visually appealing plots.\n",
      "+    - Can handle multiple models and datasets, displaying them with distinct colors.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with a datetime index.\n",
      "+    - Requires that `dataset.y_pred(model)` returns the predicted values for the model.\n",
      "+    - Visualization might become cluttered with a large number of models or datasets.\n",
      "+    \"\"\"\n",
      "+    fig = go.Figure()\n",
      "+\n",
      "+    # Use Plotly's color sequence for different model predictions\n",
      "+    colors = px.colors.qualitative.Plotly\n",
      "+\n",
      "+    # Plot actual values from the first dataset\n",
      "+    dataset = datasets[0]\n",
      "+    time_index = dataset.df.index  # Assuming the index of the dataset is datetime\n",
      "+    fig.add_trace(\n",
      "+        go.Scatter(\n",
      "+            x=time_index,\n",
      "+            y=dataset.y,\n",
      "+            mode=\"lines\",\n",
      "+            name=\"Actual Values\",\n",
      "+            line=dict(color=\"blue\"),\n",
      "+        )\n",
      "+    )\n",
      "+\n",
      "+    # Plot predicted values for each dataset-model pair\n",
      "+    for idx, (dataset, model) in enumerate(zip(datasets, models)):\n",
      "+        model_name = model.input_id\n",
      "+        y_pred = dataset.y_pred(model)\n",
      "+        fig.add_trace(\n",
      "+            go.Scatter(\n",
      "+                x=time_index,\n",
      "+                y=y_pred,\n",
      "+                mode=\"lines\",\n",
      "+                name=f\"Predicted by {model_name}\",\n",
      "+                line=dict(color=colors[idx % len(colors)]),\n",
      "+            )\n",
      "+        )\n",
      "+\n",
      "+    # Update layout\n",
      "+    fig.update_layout(\n",
      "+        title=\"Time Series Actual vs Predicted Values\",\n",
      "+        xaxis_title=\"Time\",\n",
      "+        yaxis_title=\"Values\",\n",
      "+        legend_title=\"Legend\",\n",
      "+        template=\"plotly_white\",\n",
      "+    )\n",
      "+\n",
      "+    return fig\n",
      "diff --git a/validmind/tests/model_validation/TimeSeriesR2SquareBySegments.py b/validmind/tests/model_validation/TimeSeriesR2SquareBySegments.py\n",
      "new file mode 100644\n",
      "index 000000000..83d0eb911\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/TimeSeriesR2SquareBySegments.py\n",
      "@@ -0,0 +1,102 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+\n",
      "+import pandas as pd\n",
      "+import plotly.express as px\n",
      "+from sklearn import metrics\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"model_performance\", \"sklearn\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def TimeSeriesR2SquareBySegments(datasets, models, segments=None):\n",
      "+    \"\"\"\n",
      "+    Plot R-Squared values for each model over specified time segments and generate a bar chart\n",
      "+    with the results.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to plot the R-Squared values for different models applied to various segments of the time series data.\n",
      "+\n",
      "+    **Parameters**:\n",
      "+    - datasets: List of datasets to evaluate.\n",
      "+    - models: List of models to evaluate.\n",
      "+    - segments: Dictionary with 'start_date' and 'end_date' keys containing lists of start and end dates for each segments. If None, the time series will be segmented into two halves.\n",
      "+\n",
      "+    **Test Mechanism**: The function iterates through each dataset-model pair, calculates the R-Squared values for specified time segments, and generates a bar chart with these results.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - If the R-Squared values are significantly low for certain segments, it could indicate that the model is not explaining much of the variability in the dataset for those segments.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a visual representation of model performance across different time segments.\n",
      "+    - Allows for identification of segments where models perform poorly.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with `y`, `y_pred`, and `feature_columns` attributes.\n",
      "+    - Requires that `dataset.y_pred(model)` returns the predicted values for the model.\n",
      "+    - Assumes that `y_true` and `y_pred` are pandas Series with datetime indices.\n",
      "+    \"\"\"\n",
      "+    results_list = []\n",
      "+\n",
      "+    for dataset, model in zip(datasets, models):\n",
      "+        dataset_name = dataset.input_id\n",
      "+        model_name = model.input_id\n",
      "+\n",
      "+        y_true = dataset.y\n",
      "+        y_pred = dataset.y_pred(model)\n",
      "+\n",
      "+        # Ensure y_true and y_pred are pandas Series with the same index\n",
      "+        if not isinstance(y_true, pd.Series):\n",
      "+            y_true = pd.Series(y_true, index=dataset.df.index)\n",
      "+        if not isinstance(y_pred, pd.Series):\n",
      "+            y_pred = pd.Series(y_pred, index=dataset.df.index)\n",
      "+\n",
      "+        index = dataset.df.index\n",
      "+\n",
      "+        if segments is None:\n",
      "+            mid_point = len(index) // 2\n",
      "+            segments = {\n",
      "+                \"start_date\": [index.min(), index[mid_point]],\n",
      "+                \"end_date\": [index[mid_point - 1], index.max()],\n",
      "+            }\n",
      "+\n",
      "+        for segment_index, (start_date, end_date) in enumerate(\n",
      "+            zip(segments[\"start_date\"], segments[\"end_date\"])\n",
      "+        ):\n",
      "+            mask = (index >= start_date) & (index <= end_date)\n",
      "+            y_true_segment = y_true.loc[mask]\n",
      "+            y_pred_segment = y_pred.loc[mask]\n",
      "+\n",
      "+            if len(y_true_segment) > 0 and len(y_pred_segment) > 0:\n",
      "+                r2s = metrics.r2_score(y_true_segment, y_pred_segment)\n",
      "+                results_list.append(\n",
      "+                    {\n",
      "+                        \"Model\": model_name,\n",
      "+                        \"Dataset\": dataset_name,\n",
      "+                        \"Segments\": f\"Segment {segment_index + 1}\",\n",
      "+                        \"Start Date\": start_date,\n",
      "+                        \"End Date\": end_date,\n",
      "+                        \"R-Squared\": r2s,\n",
      "+                    }\n",
      "+                )\n",
      "+\n",
      "+    # Convert results list to a DataFrame\n",
      "+    results_df = pd.DataFrame(results_list)\n",
      "+\n",
      "+    # Plotting\n",
      "+    fig = px.bar(\n",
      "+        results_df,\n",
      "+        x=\"Segments\",\n",
      "+        y=\"R-Squared\",\n",
      "+        color=\"Model\",\n",
      "+        barmode=\"group\",\n",
      "+        title=\"R-Squared Comparison by Segment and Model\",\n",
      "+        labels={\n",
      "+            \"R-Squared\": \"R-Squared Value\",\n",
      "+            \"Segment\": \"Time Segment\",\n",
      "+            \"Model\": \"Model\",\n",
      "+        },\n",
      "+    )\n",
      "+\n",
      "+    return fig, results_df\n",
      "diff --git a/validmind/tests/model_validation/sklearn/FeatureImportanceComparison.py b/validmind/tests/model_validation/sklearn/FeatureImportanceComparison.py\n",
      "new file mode 100644\n",
      "index 000000000..e00154e2b\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/sklearn/FeatureImportanceComparison.py\n",
      "@@ -0,0 +1,83 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+from sklearn.inspection import permutation_importance\n",
      "+\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"model_explainability\", \"sklearn\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def FeatureImportanceComparison(datasets, models, num_features=3):\n",
      "+    \"\"\"\n",
      "+    Compare feature importance scores for each model and generate a summary table\n",
      "+    with the top important features.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to compare the feature importance scores for different models applied to various datasets.\n",
      "+\n",
      "+    **Test Mechanism**: The function iterates through each dataset-model pair, calculates permutation feature importance (PFI) scores, and generates a summary table with the top `num_features` important features for each model.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - If key features expected to be important are ranked low, it could indicate potential issues with model training or data quality.\n",
      "+    - High variance in feature importance scores across different models may suggest instability in feature selection.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a clear comparison of the most important features for each model.\n",
      "+    - Uses permutation importance, which is a model-agnostic method and can be applied to any estimator.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with `x_df` and `y_df` methods to access feature and target data.\n",
      "+    - Requires that `model.model` is compatible with `sklearn.inspection.permutation_importance`.\n",
      "+    - The function's output is dependent on the number of features specified by `num_features`, which defaults to 3 but can be adjusted.\n",
      "+\n",
      "+\n",
      "+    \"\"\"\n",
      "+    results_list = []\n",
      "+\n",
      "+    for dataset, model in zip(datasets, models):\n",
      "+        x = dataset.x_df()\n",
      "+        y = dataset.y_df()\n",
      "+\n",
      "+        pfi_values = permutation_importance(\n",
      "+            model.model,\n",
      "+            x,\n",
      "+            y,\n",
      "+            random_state=0,\n",
      "+            n_jobs=-2,\n",
      "+        )\n",
      "+\n",
      "+        # Create a dictionary to store PFI scores\n",
      "+        pfi = {\n",
      "+            column: pfi_values[\"importances_mean\"][i]\n",
      "+            for i, column in enumerate(x.columns)\n",
      "+        }\n",
      "+\n",
      "+        # Sort features by their importance\n",
      "+        sorted_features = sorted(pfi.items(), key=lambda item: item[1], reverse=True)\n",
      "+\n",
      "+        # Extract the top `num_features` features\n",
      "+        top_features = sorted_features[:num_features]\n",
      "+\n",
      "+        # Prepare the result for the current model and dataset\n",
      "+        result = {\n",
      "+            \"Model\": model.input_id,\n",
      "+            \"Dataset\": dataset.input_id,\n",
      "+        }\n",
      "+\n",
      "+        # Dynamically add feature columns to the result\n",
      "+        for i in range(num_features):\n",
      "+            if i < len(top_features):\n",
      "+                result[f\"Feature {i + 1}\"] = (\n",
      "+                    f\"[{top_features[i][0]}; {top_features[i][1]:.4f}]\"\n",
      "+                )\n",
      "+            else:\n",
      "+                result[f\"Feature {i + 1}\"] = None\n",
      "+\n",
      "+        # Append the result to the list\n",
      "+        results_list.append(result)\n",
      "+\n",
      "+    # Convert the results list to a DataFrame\n",
      "+    results_df = pd.DataFrame(results_list)\n",
      "+    return results_df\n",
      "diff --git a/validmind/tests/model_validation/sklearn/PermutationFeatureImportance.py b/validmind/tests/model_validation/sklearn/PermutationFeatureImportance.py\n",
      "index 112a5d09d..83f62bea2 100644\n",
      "--- a/validmind/tests/model_validation/sklearn/PermutationFeatureImportance.py\n",
      "+++ b/validmind/tests/model_validation/sklearn/PermutationFeatureImportance.py\n",
      "@@ -121,7 +121,7 @@ def run(self):\n",
      "             figures=[\n",
      "                 Figure(\n",
      "                     for_object=self,\n",
      "-                    key=\"pfi\",\n",
      "+                    key=f\"pfi_{self.inputs.dataset.input_id}_{self.inputs.model.input_id}\",\n",
      "                     figure=fig,\n",
      "                 ),\n",
      "             ],\n",
      "diff --git a/validmind/tests/model_validation/sklearn/RegressionErrorsComparison.py b/validmind/tests/model_validation/sklearn/RegressionErrorsComparison.py\n",
      "new file mode 100644\n",
      "index 000000000..0b3de28e6\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/sklearn/RegressionErrorsComparison.py\n",
      "@@ -0,0 +1,76 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import numpy as np\n",
      "+import pandas as pd\n",
      "+from sklearn import metrics\n",
      "+\n",
      "+from validmind.logging import get_logger\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+logger = get_logger(__name__)\n",
      "+\n",
      "+\n",
      "+@tags(\"model_performance\", \"sklearn\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def RegressionErrorsComparison(datasets, models):\n",
      "+    \"\"\"\n",
      "+    Compare regression error metrics for each model and generate a summary table\n",
      "+    with the results.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to compare the regression errors for different models applied to various datasets.\n",
      "+\n",
      "+    **Test Mechanism**: The function iterates through each dataset-model pair, calculates various error metrics (MAE, MSE, MAPE, MBD), and generates a summary table with these results.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - High Mean Absolute Error (MAE) or Mean Squared Error (MSE) indicates poor model performance.\n",
      "+    - High Mean Absolute Percentage Error (MAPE) suggests large percentage errors, especially problematic if the true values are small.\n",
      "+    - Mean Bias Deviation (MBD) significantly different from zero indicates systematic overestimation or underestimation by the model.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides multiple error metrics to assess model performance from different perspectives.\n",
      "+    - Includes a check to avoid division by zero when calculating MAPE.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with `y`, `y_pred`, and `feature_columns` attributes.\n",
      "+    - The function relies on the `logger` from `validmind.logging` to warn about zero values in `y_true`, which should be correctly implemented and imported.\n",
      "+    - Requires that `dataset.y_pred(model)` returns the predicted values for the model.\n",
      "+    \"\"\"\n",
      "+    results_list = []\n",
      "+\n",
      "+    for dataset, model in zip(datasets, models):\n",
      "+        dataset_name = dataset.input_id\n",
      "+        model_name = model.input_id\n",
      "+\n",
      "+        y_true = dataset.y\n",
      "+        y_pred = dataset.y_pred(model)  # Assuming dataset has X for features\n",
      "+        y_true = y_true.astype(y_pred.dtype)\n",
      "+\n",
      "+        mae = metrics.mean_absolute_error(y_true, y_pred)\n",
      "+        mse = metrics.mean_squared_error(y_true, y_pred)\n",
      "+\n",
      "+        if np.any(y_true == 0):\n",
      "+            logger.warning(\n",
      "+                \"y_true contains zero values. Skipping MAPE calculation to avoid division by zero.\"\n",
      "+            )\n",
      "+            mape = None\n",
      "+        else:\n",
      "+            mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
      "+        mbd = np.mean(y_pred - y_true)\n",
      "+\n",
      "+        # Append results to the list\n",
      "+        results_list.append(\n",
      "+            {\n",
      "+                \"Model\": model_name,\n",
      "+                \"Dataset\": dataset_name,\n",
      "+                \"Mean Absolute Error (MAE)\": mae,\n",
      "+                \"Mean Squared Error (MSE)\": mse,\n",
      "+                \"Mean Absolute Percentage Error (MAPE)\": mape,\n",
      "+                \"Mean Bias Deviation (MBD)\": mbd,\n",
      "+            }\n",
      "+        )\n",
      "+\n",
      "+    # Convert results list to a DataFrame\n",
      "+    results_df = pd.DataFrame(results_list)\n",
      "+    return results_df\n",
      "diff --git a/validmind/tests/model_validation/sklearn/RegressionR2SquareComparison.py b/validmind/tests/model_validation/sklearn/RegressionR2SquareComparison.py\n",
      "new file mode 100644\n",
      "index 000000000..a67d504bc\n",
      "--- /dev/null\n",
      "+++ b/validmind/tests/model_validation/sklearn/RegressionR2SquareComparison.py\n",
      "@@ -0,0 +1,63 @@\n",
      "+# Copyright © 2023-2024 ValidMind Inc. All rights reserved.\n",
      "+# See the LICENSE file in the root of this repository for details.\n",
      "+# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial\n",
      "+\n",
      "+import pandas as pd\n",
      "+from sklearn import metrics\n",
      "+\n",
      "+from validmind.tests.model_validation.statsmodels.statsutils import adj_r2_score\n",
      "+from validmind import tags, tasks\n",
      "+\n",
      "+\n",
      "+@tags(\"model_performance\", \"sklearn\")\n",
      "+@tasks(\"regression\", \"time_series_forecasting\")\n",
      "+def RegressionR2SquareComparison(datasets, models):\n",
      "+    \"\"\"\n",
      "+    Compare R-Squared and Adjusted R-Squared values for each model and generate a summary table\n",
      "+    with the results.\n",
      "+\n",
      "+    **Purpose**: The purpose of this function is to compare the R-Squared and Adjusted R-Squared values for different models applied to various datasets.\n",
      "+\n",
      "+    **Test Mechanism**: The function iterates through each dataset-model pair, calculates the R-Squared and Adjusted R-Squared values, and generates a summary table with these results.\n",
      "+\n",
      "+    **Signs of High Risk**:\n",
      "+    - If the R-Squared values are significantly low, it could indicate that the model is not explaining much of the variability in the dataset.\n",
      "+    - A significant difference between R-Squared and Adjusted R-Squared values might indicate that the model includes irrelevant features.\n",
      "+\n",
      "+    **Strengths**:\n",
      "+    - Provides a quantitative measure of model performance in terms of variance explained.\n",
      "+    - Adjusted R-Squared accounts for the number of predictors, making it a more reliable measure when comparing models with different numbers of features.\n",
      "+\n",
      "+    **Limitations**:\n",
      "+    - Assumes that the dataset is provided as a DataFrameDataset object with `y`, `y_pred`, and `feature_columns` attributes.\n",
      "+    - The function relies on `adj_r2_score` from the `statsmodels.statsutils` module, which should be correctly implemented and imported.\n",
      "+    - Requires that `dataset.y_pred(model)` returns the predicted values for the model.\n",
      "+\n",
      "+    \"\"\"\n",
      "+    results_list = []\n",
      "+\n",
      "+    for dataset, model in zip(datasets, models):\n",
      "+        dataset_name = dataset.input_id\n",
      "+        model_name = model.input_id\n",
      "+\n",
      "+        y_true = dataset.y\n",
      "+        y_pred = dataset.y_pred(model)  # Assuming dataset has X for features\n",
      "+        y_true = y_true.astype(y_pred.dtype)\n",
      "+\n",
      "+        r2s = metrics.r2_score(y_true, y_pred)\n",
      "+        X_columns = dataset.feature_columns\n",
      "+        adj_r2 = adj_r2_score(y_true, y_pred, len(y_true), len(X_columns))\n",
      "+\n",
      "+        # Append results to the list\n",
      "+        results_list.append(\n",
      "+            {\n",
      "+                \"Model\": model_name,\n",
      "+                \"Dataset\": dataset_name,\n",
      "+                \"R-Squared\": r2s,\n",
      "+                \"Adjusted R-Squared\": adj_r2,\n",
      "+            }\n",
      "+        )\n",
      "+\n",
      "+    # Convert results list to a DataFrame\n",
      "+    results_df = pd.DataFrame(results_list)\n",
      "+    return results_df\n"
     ]
    }
   ],
   "source": [
    "print(github_urls[0].prs[7].git_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare outputs\n",
    "\n",
    "We can take a look at the outputs of each PR and choose which version we like better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json:\n",
    "            print(\"Git diff and code explain:\\n\")\n",
    "            print(pr.explained_diff) \n",
    "            print(\"\\n\")\n",
    "            print(\"Using PR body and OpenAI editing:\\n\")\n",
    "            print(pr.edited_text)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            if input(\"Enter 0 for Git diff, 1 for PR body\") == 0: # should get replaced by ChatGPT prompt that checks for the better one\n",
    "                pr.final_text = pr.explained_diff\n",
    "            else:\n",
    "                pr.final_text = pr.edited_text # default will be the PR body if input is empty as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc13_'></a>Edit each title [](#toc0_)\n",
    "This block does the same as above for the titles of each PR. The output below will show the original PR title, the title after some algorithmic changes, and the title after ChatGPT edits it. If you find that it's not good after editing with ChatGPT, feel free to edit the prompt below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_instructions_title = \"\"\"\n",
    "    Please edit the provided technical content according to the following guidelines:\n",
    "\n",
    "    - Use simple and neutral language in the active voice.\n",
    "    - Address users directly in the second person with \"you\".\n",
    "    - Use present tense by avoiding the use of \"will\".\n",
    "    - Apply sentence-style capitalization to text\n",
    "    - Always capitalize the first letter of text on each line.\n",
    "    - Rewrite sentences that are longer than 25 words as multiple sentences.\n",
    "    - Only split text across multiple lines if the text contains more than three sentences.\n",
    "    - Avoid handwaving references to \"it\" or \"this\" by including the text referred to. \n",
    "    - Treat short text of less than ten words without a period at the end as a heading. \n",
    "    - Enclose any words joined by underscores in backticks (`) if they aren't already.\n",
    "    - Remove exclamation marks from text.\n",
    "    - Remove quotes around non-code words.\n",
    "    - Remove the text \"feat:\" from the output\n",
    "    - Maintain existing punctuation at the end of sentences.\n",
    "    - Maintain all original hyperlinks for reference.\n",
    "    - Preserve all comments in the format <!--- COMMENT ---> as they appear in the text.\n",
    "    \"\"\"\n",
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json: \n",
    "            pr.title = pr.data_json['title']\n",
    "            pr.clean_title(editing_instructions_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc14_'></a>Set labels for each PR [](#toc0_)\n",
    "This block takes the label data from each PR and assigns it to the PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json: pr.labels = [label['name'] for label in pr.data_json['labels']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc15_'></a>Assign PR details to PR [](#toc0_)\n",
    "This block compiles all the data we found earlier for each PR into one place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json: pr.pr_details = {\n",
    "            'pr_number': pr.pr_number,\n",
    "            'title': pr.cleaned_title,\n",
    "            'full_title': pr.data_json['title'],\n",
    "            'url': pr.data_json['url'],\n",
    "            'labels': \", \".join(pr.labels),\n",
    "            'notes': pr.edited_text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc16_'></a>Combine all PR data into the same release notes components [](#toc0_)\n",
    "Now, we can take all the details we compiled above and append them to our final release notes components. Since we want to show features in order of importance, we sort by the priority of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for url in github_urls:\n",
    "    for pr in url.prs:\n",
    "        if pr.data_json:\n",
    "            assigned = False \n",
    "            for priority_label in label_hierarchy:\n",
    "                if priority_label in pr.labels:\n",
    "                    release_components[priority_label].append(pr.pr_details)\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                release_components.setdefault('other', []).append(pr.pr_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc17_'></a>Write release notes to file [](#toc0_)\n",
    "Now that `release_components` contains everything we need for the release notes, we can write it to our release notes file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write categorized PRs to the file\n",
    "with open(output_file, \"a\") as file:\n",
    "    write_prs_to_file(file, release_components, label_to_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc18_'></a>Update sidebar [](#toc0_)\n",
    "This block will go into our `_quarto.yml` file and add the new release notes so it shows up on the sidebar of the docsite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_quarto_yaml(release_date):\n",
    "    \"\"\"Updates the _quarto.yml file to include the release notes file so it can be accessed on the website.\n",
    "\n",
    "    Params:\n",
    "        release_date - release notes use the release date as the file name.\n",
    "    \n",
    "    Modifies:\n",
    "        _quarto.yml file\n",
    "    \"\"\"\n",
    "    yaml_filename = \"../site/_quarto.yml\"\n",
    "    temp_yaml_filename = \"../site/_quarto_temp.yml\"\n",
    "\n",
    "    # Copy the original YAML file to a temporary file\n",
    "    shutil.copyfile(yaml_filename, temp_yaml_filename)\n",
    "\n",
    "    with open(temp_yaml_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Format the release date for insertion into the YAML file\n",
    "    formatted_release_date = release_date.strftime(\"%Y-%b-%d\").lower()\n",
    "\n",
    "    with open(yaml_filename, 'w') as file:\n",
    "        add_release_content = False\n",
    "        insert_index = -1\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            file.write(line)\n",
    "            if line.strip() == \"# MAKE-RELEASE-NOTES-EMBED-MARKER\":\n",
    "                add_release_content = True\n",
    "                insert_index = i\n",
    "\n",
    "            if add_release_content and i == insert_index:\n",
    "                file.write(f'        - releases/{formatted_release_date}/release-notes.qmd\\n')\n",
    "                add_release_content = False\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_yaml_filename)\n",
    "    \n",
    "    print(f\"Added release notes to _quarto.yml, line {insert_index + 2}\")\n",
    "\n",
    "update_quarto_yaml(release_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc19_'></a>Show files to commit [](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After completing all tasks, print git status to show output files\n",
    "try:\n",
    "    result = subprocess.run([\"git\", \"status\", \"--short\"], check=True, text=True, capture_output=True)\n",
    "    lines = result.stdout.split('\\n')\n",
    "    print(\"Files to commit:\")\n",
    "    for line in lines:\n",
    "        if line.startswith((' M', '??', 'A ')):\n",
    "            print(line)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Failed to run git status:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc20_'></a>Preview and edit changes [](#toc0_)\n",
    "Run this cell to preview your changes, and make edits to the release notes file you just generated. See our [internal guide](https://www.notion.so/validmind/On-release-notes-20de4e7ea03f402587514f6c9eda3bb1) on editing release notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../site\n",
    "quarto preview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
